<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="y0-26jFM_8wn6Slpy1ahkB8ndR7w0OOGyAU6IaXjLUI" />














  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="cv," />










<meta name="description" content="神经网络和反向传播损失函数和优化在大多数机器学习模型中，都会有一个损失函数(lost function)。比如常见的MSE：  L(w,b) &#x3D; \frac{1}{N}\sum_{i&#x3D;1}^n(y_i-f(wx_i+b))^2损失函数用来衡量机器学习模型的精确度。一般来说，损失函数的值越小，模型的精确度就越高。如果要提高机器学习模型的精确度，就需要尽可能降低损失函数的值。而降低损失函数的值，我们一">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络和反向传播">
<meta property="og:url" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/index.html">
<meta property="og:site_name" content="Jason‘s Blog">
<meta property="og:description" content="神经网络和反向传播损失函数和优化在大多数机器学习模型中，都会有一个损失函数(lost function)。比如常见的MSE：  L(w,b) &#x3D; \frac{1}{N}\sum_{i&#x3D;1}^n(y_i-f(wx_i+b))^2损失函数用来衡量机器学习模型的精确度。一般来说，损失函数的值越小，模型的精确度就越高。如果要提高机器学习模型的精确度，就需要尽可能降低损失函数的值。而降低损失函数的值，我们一">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/1.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/2.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/3.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/4.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/5.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/6.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/7.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/9.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/10.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/11.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/12.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/13.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/14.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/15.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/16.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/17.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/18.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/19.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/20.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/21.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/22.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/23.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/24.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/25.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/26.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/27.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/28.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/29.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/30.png">
<meta property="og:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/31.png">
<meta property="article:published_time" content="2021-12-02T14:44:42.000Z">
<meta property="article:modified_time" content="2021-12-27T04:30:46.000Z">
<meta property="article:author" content="Jason">
<meta property="article:tag" content="cv">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://jasonxqh.github.io/2021/12/02/神经网络和反向传播/"/>





  <title>神经网络和反向传播 | Jason‘s Blog</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-170027658-1', 'auto');
  ga('send', 'pageview');
</script>





<meta name="generator" content="Hexo 4.2.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">
  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
<a href="https://github.com/JasonXQH/JasonXQH.github.io" target="_blank" rel="noopener" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jason‘s Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            Sitemap
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">神经网络和反向传播</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-12-02T22:44:42+08:00">
                2021-12-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  6.8k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  28
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="神经网络和反向传播"><a href="#神经网络和反向传播" class="headerlink" title="神经网络和反向传播"></a>神经网络和反向传播</h1><h2 id="损失函数和优化"><a href="#损失函数和优化" class="headerlink" title="损失函数和优化"></a>损失函数和优化</h2><p>在大多数机器学习模型中，都会有一个<strong>损失函数(lost function)</strong>。比如常见的MSE：</p>
<script type="math/tex; mode=display">
L(w,b) = \frac{1}{N}\sum_{i=1}^n(y_i-f(wx_i+b))^2</script><p>损失函数用来衡量机器学习模型的精确度。一般来说，损失函数的值越小，模型的精确度就越高。如果要提高机器学习模型的精确度，就需要尽可能降低损失函数的值。而降低损失函数的值，我们一般采用梯度下降这个方法。所以，<strong>梯度下降的目的，就是为了最小化损失函数。</strong></p>
<h3 id="梯度下降的原理"><a href="#梯度下降的原理" class="headerlink" title="梯度下降的原理"></a>梯度下降的原理</h3><p>寻找损失函数的最低点，就像我们在山谷里行走，希望找到山谷里最低的地方。那么如何寻找损失函数的最低点呢？在这里，我们使用了微积分里导数，通过求出函数导数的值，从而找到函数下降的方向或者是最低点（极值点）。</p>
<p>损失函数里一般有两种参数，一种是控制输入信号量的权重(Weight, 简称 $w$ ），另一种是调整函数与真实值距离的偏差（Bias，简称 $b$ ）。我们所要做的工作，就是通过梯度下降方法，不断地调整权重$w$和偏差$b$，使得损失函数的值变得越来越小。</p>
<p>假设某个损失函数里，模型损失值 $L$与权重 $w$ 下图这样的关系。实际模型里，可能会有多个权重$w$，这里为了简单起见，举只有一个权重$w$的例子。权重$w$目前的位置是在A点。此时如果求出A点的梯度 $\frac{dL}{dw}$ ，便可以知道如果我们向右移动，可以使损失函数的值变得更小。</p>
<p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/1.png" style="zoom:150%;"></p>
<p>用数学语言来解释一下：假设只有一个参数w</p>
<ul>
<li>那么导数$\frac{dL(w)}{dw}=\lim\limits_{h\rightarrow0}\frac{L(w+h)-L(w)}{h}$ 就代表L在w的切线斜率，即$L(w)$在该店的变化速率和方向。那么，只要导数不为0，我们往反方向微调就可以减小$L(w)$</li>
</ul>
<p>当有多维情况下，会有多个权重，此时$\boldsymbol W$ 为向量</p>
<ul>
<li>那么偏导数$[\frac{\partial L(\boldsymbol W)}{\partial w<em>1},\frac{\partial L(\boldsymbol W)}{\partial w_2},\cdots,\frac{\partial L(\boldsymbol W)}{\partial w_n}]$ 代表L在$\boldsymbol W$ 处沿每个维度的变化速率和方向，称为梯度和(gradient)，记为$\nabla</em>{\boldsymbol W}L$ 或$grad(L(\boldsymbol W))$ 。这个时候，我们需要把各个样本数据的权重梯度加起来，并求出它们的平均值，用这个平均值来作为样本整体的权重梯度。</li>
<li>$\nabla<em>{\boldsymbol W}L$ 和方向向量$\boldsymbol v$ 的点积即为该方向的斜率(方向导数)，公式为$\nabla</em>{\boldsymbol W}L\cdot v=|\nabla<em>{\boldsymbol W}L||v|\cos\theta$。当$cos(\theta)=1$ 的时候达到最大值。因此，负梯度 $-\nabla</em>{\boldsymbol W}L$ 的方向即为 L 在$\boldsymbol W$处下降最快的方向， 沿$-\nabla_{\boldsymbol W}L$方向微调就可以快速减小$L(\boldsymbol W)$(即梯度下降)</li>
<li>梯度下降的公式为： $W<em>{\text{new}} = W-\lambda\nabla</em>{\boldsymbol W}L$  ，其中$\lambda$是超参数，其含义就是<strong>学习率(Learning Rate)</strong>或者<strong>步长(Step size)</strong></li>
</ul>
<p>如果学习率$\lambda$设置得过大，有可能我们会错过损失函数的最小值；如果设置得过小，可能我们要迭代式子(2)非常多次才能找到最小值，会耗费较多的时间。因此，在实际应用中，我们需要为学习率设$\lambda$置一个合适的值。</p>
<h3 id="梯度下降的过程"><a href="#梯度下降的过程" class="headerlink" title="梯度下降的过程"></a>梯度下降的过程</h3><p>我们把上面的内容稍微整理一下，可以得到梯度下降的整体过程：</p>
<ol>
<li>for i = 0 to 训练数据的个数：</li>
</ol>
<p>(1) 计算第 i 个训练数据的权重 $w$ 和偏差 $b$ 相对于损失函数的梯度。于是我们最终会得到每一个训练数据的权重和偏差的梯度值。</p>
<p>(2) 计算所有训练数据权重  $w$ 的梯度的总和。</p>
<p>(3) 计算所有训练数据偏差 $b$ 的梯度的总和。</p>
<ol>
<li>做完上面的计算之后，我们开始执行下面的计算：</li>
</ol>
<p>(1) 使用上面第(2)、(3)步所得到的结果，计算所有样本的权重和偏差的梯度的平均值。</p>
<p>(2) 使用下面的式子，更新每个样本的权重值和偏差值。</p>
<script type="math/tex; mode=display">
w_{i+1} = w_i-\lambda\cdot\frac{dL}{d_{w_i}}\\
b_{i+1} = b_i-\lambda\cdot\frac{dL}{d_{w_i}}</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">def train(X, y, W, B, alpha, max_iters):</span><br><span class="line">    &#39;‘’</span><br><span class="line">    选取所有的数据作为训练样本来执行梯度下降</span><br><span class="line">    X : 训练数据集</span><br><span class="line">    y : 训练数据集所对应的目标值</span><br><span class="line">    W : 权重向量</span><br><span class="line">    B ： 偏差变量</span><br><span class="line">    alpha ： 学习速率</span><br><span class="line">    max_iters : 梯度下降过程最大的迭代次数</span><br><span class="line">   &#39;&#39;&#39;</span><br><span class="line">   dW &#x3D; 0 # 初始化权重向量的梯度累加器</span><br><span class="line">   dB &#x3D; 0 # 初始化偏差向量的梯度累加器</span><br><span class="line">   m &#x3D; X.shape[0] # 训练数据的数量</span><br><span class="line">   </span><br><span class="line">   # 开始梯度下降的迭代</span><br><span class="line">   for i in range(max_iters): </span><br><span class="line">       dW &#x3D; 0 # 重新设置权重向量的梯度累加器</span><br><span class="line">       dB &#x3D; 0 # 重新设置偏差向量的梯度累加器</span><br><span class="line">       </span><br><span class="line">       # 对所有的训练数据进行遍历</span><br><span class="line">       for j in range(m):</span><br><span class="line">           # 1. 遍历所有的训练数据</span><br><span class="line">           # 2. 计算每个训练数据的权重向量梯度w_grad和偏差向量梯度b_grad</span><br><span class="line">           # 3. 把w_grad和b_grad的值分别累加到dW和dB两个累加器里</span><br><span class="line">       </span><br><span class="line">       W &#x3D; W - alpha * (dW &#x2F; m) # 更新权重的值</span><br><span class="line">       B &#x3D; B - alpha * (dB &#x2F; m) # 更新偏差的值</span><br><span class="line"></span><br><span class="line">    return W, B # 返回更新后的权重和偏差。</span><br></pre></td></tr></table></figure>
<h3 id="其他常见的梯度下降算法"><a href="#其他常见的梯度下降算法" class="headerlink" title="其他常见的梯度下降算法"></a>其他常见的梯度下降算法</h3><p>上面介绍的梯度下降算法里，在迭代每一次梯度下降的过程中，都对所有样本数据的梯度进行计算。虽然最终得到的梯度下降的方向较为准确，但是运算会耗费过长的时间。于是人们在上面这个算法的基础上对样本梯度的运算过程进行了改进，得到了下面这两种也较为常见的算法：</p>
<h4 id="小批量样本梯度下降（Mini-Batch-GD"><a href="#小批量样本梯度下降（Mini-Batch-GD" class="headerlink" title="小批量样本梯度下降（Mini Batch GD)"></a>小批量样本梯度下降（Mini Batch GD)</h4><p>这个算法在每次梯度下降的过程中，只选取一部分的样本数据进行计算梯度，比如整体样本1/100的数据。在数据量较大的项目中，可以明显地减少梯度计算的时间。</p>
<h4 id="随机梯度下降（Stochastic-GD"><a href="#随机梯度下降（Stochastic-GD" class="headerlink" title="随机梯度下降（Stochastic GD)"></a>随机梯度下降（Stochastic GD)</h4><p>随机梯度下降算法只随机抽取<strong>一个</strong>样本进行梯度计算，由于每次梯度下降迭代只计算一个样本的梯度，因此运算时间比小批量样本梯度下降算法还要少很多，但由于训练的数据量太小（只有一个），因此下降路径很容易受到训练数据自身噪音的影响，看起来就像醉汉走路一样，变得歪歪斜斜的。</p>
<blockquote>
<p>GD的优势：每次迭代loss下降快</p>
<p>GD的劣势：一次迭代需要遍历所有数据，并且容易陷入local minima(局部极小值)</p>
<p>SGD的优势：迭代更新速度快，并且往往因为minibatch含有噪声而避开local minima</p>
<p>SGD的劣势：每次迭代，loss下降较慢</p>
</blockquote>
<p>由于数据量较大，训练深度神经网络基本使用SGD，以及其他性能更加的优化方法</p>
<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>我们在<a href="https://jasonxqh.github.io/2020/11/07/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">人工神经网络</a> 这篇博客里面感性的认识了一下神经网络的架构，现在我们回顾并从数学角度重新学习一遍。</p>
<p>我们学过很多线性分类器，但是有很多非线性的分类情况我们是很难用线性分类器来解决的：</p>
<p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/2.png" style="zoom: 67%;"></p>
<p>比如这种分类，我们只能够通过增加高阶多项事项来让分类器可行。那么当输入变量有n个的时候，最多会导致$O(n^n)$级别的参数，显然是不可行的。</p>
<h3 id="单层神经网络"><a href="#单层神经网络" class="headerlink" title="单层神经网络"></a>单层神经网络</h3><p>因此我们需要用到神经网络.我们从最简单的神经网络看起：</p>
<p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/3.png"></p>
<p>在上面这个神经网络中，每条边都有一个权重，即$w_i$, 然后让权重和输入值相乘，就得到了$f(W,x)$ .这里$\boldsymbol W =[-30,20,20]$，对应一个偏置项(神经网络都需要的一个常数)和两个输入变量。我们用一层神经网络搭配Sigmoid激活函数可以模拟出 $y=x_1 \&amp;~x_2$的情况。</p>
<p>到这一步，神经网络还是线性的，那么就需要一个非线性的激活函数来打破这个线性。这里我们选择的是Sigmoid激活函数</p>
<p>我们可以给输入值和输出值做一个表格：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>x1</th>
<th>x2</th>
<th>$\sigma(f(W,x))$</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>$\sigma(-30)\approx0$</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>$\sigma(-10)\approx 0$</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>$\sigma(-10)\approx 0$</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>$\sigma(10)\approx 1$</td>
</tr>
</tbody>
</table>
</div>
<p>同理，我们可以通过修改权重，使得这个神经网络可以模拟$y=x_1|x_2$ 的情况，如下图所示：</p>
<p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/4.png"></p>
<p>最早使用的激活函数就是 Sigmoid, 但是后来发现Sigmoid函数存在一些缺点，因此后来的神经网络大多使用tanh或者ReLU，我们在神经网络的训练那节会细讲。</p>
<p>进而我们可以设立多层的神经网络，将输出值当做下一层的输入值，再激活，再输入。以此来实现一个复杂的高维空间的模型。当神经网络的层数变多的时候，中间的几层就像是被“隐藏”起来了一样，因此我们称其为<strong>hidden layer(隐藏层)</strong></p>
<h3 id="hidden-layer"><a href="#hidden-layer" class="headerlink" title="hidden layer"></a>hidden layer</h3><p>在正常情况下，hidden layer肯定不止一个神经元，因为需要不同的神经元来捕捉不同的特征</p>
<p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/5.png"></p>
<p>比如说上面这个两层的神经网络：输入层有三个变量，一个偏置项；隐藏层有四个神经元和一个偏置项。</p>
<p>那么我们可以得到hidden layer的计算公式：</p>
<script type="math/tex; mode=display">
a_1 = \sigma(w_{11}^1x_1+w_{12}^1x_2+w_{13}^1x_3+b_1^1)\\
a_2 = \sigma(w_{21}^1x_1+w_{22}^1x_2+w_{23}^1x_3+b_2^1)\\
a_3 = \sigma(w_{31}^1x_1+w_{32}^1x_2+w_{33}^1x_3+b_3^1)\\
a_4 = \sigma(w_{41}^1x_1+w_{42}^1x_2+w_{43}^1x_3+b_4^1)\\</script><p>用矩阵的思路来说，就是 </p>
<script type="math/tex; mode=display">
\boldsymbol a = \sigma(\boldsymbol {W^1x}+\boldsymbol b)\\</script><p>现在，hidden layer的输出结果变成了output layer 的输入结果，此时output layer一共有3个神经元，计算公式如下：(注意，这里的上标不代表平方而代表该权重位于第二层)</p>
<script type="math/tex; mode=display">
h_1 = \sigma(w_{11}^2a_1+w_{12}^2a_2+w_{13}^2a_3+w_{14}^2a_4+b_1^2)\\
h_2 = \sigma(w_{21}^2a_1+w_{22}^2a_2+w_{23}^2a_3+w_{24}^2a_4+b_2^2)\\
h_3 = \sigma(w_{31}^2a_1+w_{32}^2a_2+w_{33}^2a_3+w_{34}^2a_4+b_3^2)\\</script><h4 id="矩阵思路"><a href="#矩阵思路" class="headerlink" title="矩阵思路"></a>矩阵思路</h4><p>如果我们先不考虑偏置项，但考虑输入和输出，我们从矩阵的思路来理解一下每个层数的计算。</p>
<p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/6.png"></p>
<p>那么，对于hidden layer来说，其权重矩阵的大小就是$H$行$D$列：</p>
<script type="math/tex; mode=display">
\boldsymbol{W^1} \in \mathbb R^{H\times D}</script><p>对于output layer来说，其权重矩阵的大小就是$C$行$H$列：</p>
<script type="math/tex; mode=display">
\boldsymbol W^2\in\mathbb R^{C\times H}</script><p>那么，如果是三层神经网络呢？</p>
<p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/7.png"></p>
<p>此时，对于第一层隐藏层，$\boldsymbol{W^1} \in \mathbb R^{H_1\times D}$</p>
<p>对于第二层隐藏层，$\boldsymbol{W^2} \in \mathbb R^{H_2\times H_1}$</p>
<p>对于第三层输出层，$\boldsymbol W^3\in \mathbb R^{C\times H^2}$</p>
<p>用代码来表示，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">f &#x3D; lambda x: 1.0&#x2F;(1.0+np.exp(-x)) # activation function(use sigmoid)</span><br><span class="line">x &#x3D; np.random.randn(3,1) # random input vector of three numbers(3x1)</span><br><span class="line">h1 &#x3D; f(np.dot(W1,x)+b1)	 # calculate first hidden layer activations(4x1)</span><br><span class="line">h2 &#x3D; f(np.dot(W2,h1)+b2) # calculate second hidden layer activations(4x1)</span><br><span class="line">out &#x3D; np.dot(W3,h2)+b3	 # output neuron(1x1)</span><br></pre></td></tr></table></figure>
<h4 id="随机连接的神经网络"><a href="#随机连接的神经网络" class="headerlink" title="随机连接的神经网络"></a>随机连接的神经网络</h4><p>事实上，神经网络之间是可以随机连接的，并不需要像我们上面介绍的那样全连接，如下图所示：</p>
<p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/9.png"></p>
<h3 id="两层神经网络"><a href="#两层神经网络" class="headerlink" title="两层神经网络"></a>两层神经网络</h3><p>上面引入了多层神经网络的概念，那么用简单的两层神经网络可以解决什么问题呢？表示 异或 关系，如下图所示：</p>
<p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/10.png"></p>
<p>同样用Sigmoid做激活函数，可以得到如下表格：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>x1</th>
<th>x2</th>
<th>a1</th>
<th>a2</th>
<th>h</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>$\sigma(-30)\approx0$</td>
<td>$\sigma(10)\approx1$</td>
<td>$\sigma(10)\approx1$</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>$\sigma(-10)\approx0$</td>
<td>$\sigma(-10)\approx0$</td>
<td>$\sigma(-10)\approx0$</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>$\sigma(-10)\approx0$</td>
<td>$\sigma(-10)\approx0$</td>
<td>$\sigma(-10)\approx0$</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>$\sigma(10)\approx1$</td>
<td>$\sigma(-30)\approx0$</td>
<td>$\sigma(10)\approx1$</td>
</tr>
</tbody>
</table>
</div>
<h2 id="神经网络的计算"><a href="#神经网络的计算" class="headerlink" title="神经网络的计算"></a>神经网络的计算</h2><p>现在我们来看看一个神经网络是如何被计算的</p>
<p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/11.png"></p>
<p>对于这个神经网络，我们训练集是：$(x<em>i,y_i)</em>{i=1}^N$ ,其中$x_i$是向量，$y_i$是标签。激活函数式ReLU</p>
<ol>
<li>第一步，我们计算 $f(x;\boldsymbol W^1)$  ,然后输入到ReLU中去激活得到 $\boldsymbol a$</li>
<li>接着我们把$\boldsymbol a$ 和 $\boldsymbol W^2$ 计算得到 $\boldsymbol s = f(\boldsymbol a,\boldsymbol W^2)$ </li>
<li>用Softmax 来计算 $h<em>k = \frac{e^{s_k}}{\sum</em>{j}e^{s_j}}$  得到输出值</li>
<li>计算损失函数，其组成为softmax的损失函数加上第一层、第二层权重的正则项 $L = \frac{1}{N} \sum_{i=1}^Nl(\boldsymbol h_i,y_i)+\lambda R(\boldsymbol W^1)+\lambda R(\boldsymbol W^2)$</li>
<li>优化损失函数：计算解析梯度 $\frac{\partial L}{\partial \boldsymbol W^1},\frac{\partial L}{\partial \boldsymbol W^2}$ ，可以通过数值梯度去计算，得到$\nabla<em>{\boldsymbol W}L = \nabla</em>{\boldsymbol W}[\frac{1}{N}\sum_{i=1}^N(l(\boldsymbol h_i,y_i)+\lambda R(\boldsymbol W^1)+\lambda R(\boldsymbol W^2))]$</li>
</ol>
<p>这边只有两层，我们就发现解析梯度是如此的难求，那么如果神经网络的梯度再往上加的话，对计算机来说计算梯度是很复杂的一件事，因此，我们就需要<strong>反向传播</strong>这个技巧了</p>
<h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>我们在高数里面，已经学会了链式法则。那么，在计算梯度的时候，我们也可以用这套法则来较为方便得计算出权重的梯度。</p>
<p>比如说，我们得到了损失函数 $L$, 根据公式: $\frac{\partial L}{\partial \boldsymbol W^2} = \frac{\partial L}{\partial\boldsymbol h}\frac{\partial \boldsymbol h}{\partial \boldsymbol W^2}$， 我们可以先求出L关于$\boldsymbol h$的导数，然后再去求$h$关于$\boldsymbol W^2$的导数。</p>
<p>如果我们想计算$\frac{\partial L}{\partial \boldsymbol W^1}$ ，那么我们就在再向前推一层，根据公式：$\frac{\partial L}{\partial \boldsymbol W^1} = \frac{\partial L}{\partial\boldsymbol h}\frac{\partial  \boldsymbol h}{\partial \boldsymbol a}\frac{\partial \boldsymbol a}{\partial\boldsymbol W^1}$ 。</p>
<p>可以看到，反向传播的原理就是每一层的输出对这一层的输入求一个梯度，也就是按照从右往左的顺序来求梯度。</p>
<h4 id="具体例子"><a href="#具体例子" class="headerlink" title="具体例子"></a>具体例子</h4><p>我们把某个神经网络的计算图画出来。计算图可以看做是一个可视化的神经网络，把每一步的计算都用一个结构来表示，有了计算图就可以以很清晰的思路来编写代码了</p>
<p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/12.png"></p>
<h5 id="前向传播1"><a href="#前向传播1" class="headerlink" title="前向传播1"></a>前向传播1</h5><p>上面是$x_1,w_1,x_2,w_2,b$ 的初始值，根据这些初始值，我们可以前向计算来得到每个节点的输出值：</p>
<p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/13.png"></p>
<h5 id="反向传播1"><a href="#反向传播1" class="headerlink" title="反向传播1"></a>反向传播1</h5><p>接下来，我们要反向传播，从最后一步向前计算，首先我们假设这已经到了最后一层，因此最后一层的梯度就定为$\frac{\partial h}{\partial \sigma}=1$。然后我们从最后一层开始计算倒数第二层的梯度</p>
<p>对于倒数第二层，我们要知道链式法则：$\frac{\partial h}{\partial x} = \frac{\partial h}{\partial \sigma}\frac{\partial \sigma}{\partial x}$ </p>
<p>现在已知上游梯度 $\frac{\partial h}{\partial \sigma}=1$, 只要计算这层<strong>局部梯度</strong>，再把这层的输入值带入即可求得。因为$f(x) = \frac{1}{x}$ 所以 $\frac{df}{dx} = -\frac{1}{x^2}$ ,再把这层的输入值1.37带入，得到：$\frac{\partial h}{\partial x} = 1\times(-1/1.37^2)=-0.53$</p>
<p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/14.png"></p>
<h5 id="反向传播2"><a href="#反向传播2" class="headerlink" title="反向传播2"></a>反向传播2</h5><p>现在，我们已经计算得到了两层梯度，继续向前推进。现在，$f(x) = x+1$,因此局部梯度 $\frac{df}{dx} = 1$, 输入值为0.37。但是这层得到的梯度和输入值无关，因此这层反向传播得到的值为$-0.53\times 1 = -0.53$</p>
<p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/15.png"></p>
<h5 id="反向传播3"><a href="#反向传播3" class="headerlink" title="反向传播3"></a>反向传播3</h5><p>再向前推， 现在$f(x) = e^x$, 上游梯度为-0.53, 局部梯度为 $\frac{df}{dx} =e^x$, 输入值为-1。这层反向传播得到的值为 $-0.53\times e^{-1}=-0.2$</p>
<p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/16.png"></p>
<h5 id="反向传播4"><a href="#反向传播4" class="headerlink" title="反向传播4"></a>反向传播4</h5><p>现在$f(x) = x-1$ ，局部梯度为 $-1$ ，和输入值无关，上游梯度为-0.2。 这层反向传播得到的值为：$-0.2\times -1=0.2$</p>
<p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/17.png"></p>
<h5 id="反向传播5"><a href="#反向传播5" class="headerlink" title="反向传播5"></a>反向传播5</h5><p>到这一部分，神经网络开始分叉了，在这里 $f(x)$ 为两个输入值的和。我们分别把每个输入值都当做主元，那么对于b，上游梯度为0.2, 局部梯度为$\frac{df}{dx} = 1$ ，与输入值无关，因此反向传播得到的值为0.2</p>
<p>对于另外一个神经元，同理，与输入值无关，反向传播得到的值为0.2</p>
<p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/18.png"></p>
<h5 id="反向传播6"><a href="#反向传播6" class="headerlink" title="反向传播6"></a>反向传播6</h5><p>在这一层神经网络又分叉了，但这层还是两个输入值相加, 和上一层是一样的，因此都是0.2</p>
<p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/19.png"></p>
<h5 id="反向传播7"><a href="#反向传播7" class="headerlink" title="反向传播7"></a>反向传播7</h5><p>在这一层，我们先来看上面的分叉。$f(x)$为两个输入值的乘积，当我们把$x_1$当做主元的时候，$f(x) = -x$, 因此 $\frac{df}{dx_1}=-1$, 与输入值无关，上游梯度为0.2,因此反向传播得到的值为$0.2\times -1 =-0.2$</p>
<p>当把 $w_1$看做主元的时候，$f(x)= 2x,\frac{df}{dw_1} = 2$,与输入值无关，上游梯度的值为0.2. 因此反向传播得到的值为 $0.2\times 2 = 0.4$</p>
<p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/20.png"></p>
<h5 id="反向传播8"><a href="#反向传播8" class="headerlink" title="反向传播8"></a>反向传播8</h5><p>现在来看下层的分叉。同样是输入值的乘积。</p>
<p>当我们把$x_2$当做主元的时候，$f(x) = -2x$, 因此 $\frac{df}{dx_2}=-2$, 与输入值无关，上游梯度为0.2,因此反向传播得到的值为$0.2\times -2=-0.4$</p>
<p>当把 $w_2$看做主元的时候，$f(x)= -3x,\frac{df}{dw_2} = 3$,与输入值无关，上游梯度的值为0.2. 因此反向传播得到的值为 $0.2\times -3 = -0.6$</p>
<p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/21.png"></p>
<h4 id="分块计算图"><a href="#分块计算图" class="headerlink" title="分块计算图"></a>分块计算图</h4><p>细心地同学可能已经发现了，在正向传播的时候， 导数第1、2、3、4步合起来刚好是一个 $\frac{1}{1+e^{-x}}$ 的函数，也就是我们选择的Sigmoid激活函数。事实上，Sigmoid有自己的梯度形式，因此可以把计算图简单化。</p>
<p>这样，只要求 Sigmoid的本地梯度就可以了，相当于多步并一步：</p>
<script type="math/tex; mode=display">
\frac{d\sigma(x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2} = (\frac{1+e^{-x}-1}{1+e^{-x}})(\frac{1}{1+e^{-x}}) = (1-\sigma(x))\sigma(x)</script><p>这样一来，上游梯度就是1，输入值$x=1$, $\sigma(x)=0.73$,  局部梯度如上所示，反向传播得到的结果是：</p>
<script type="math/tex; mode=display">
1\times[(1-0.73)\times0.73]=0.2</script><p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/22.png"></p>
<p>同样的，我们可以把前面的神经元也合并成一块，用线性函数$ y=w_1x_1+w_2x_2+b$ 来表示</p>
<p>这样，对于 $w_i,x_i$的导数，我们可以用链式法则来求得：</p>
<script type="math/tex; mode=display">
\frac{\partial \sigma}{\partial w_i} = \frac{\partial \sigma}{\partial y}\frac{\partial y}{\partial w_i}\\
\frac{\partial \sigma}{\partial x_i} = \frac{\partial \sigma}{\partial y}\frac{\partial y}{\partial x_i}\\</script><p>比如，对于$y=-x_1-2x_2-3$ ,上游梯度为0.2</p>
<script type="math/tex; mode=display">
\begin{align}
&\frac{\partial \sigma}{\partial w_1} = \frac{\partial \sigma}{\partial y}\frac{\partial y}{\partial w_1}=0.2\times2=0.4\\
&\frac{\partial \sigma}{\partial x_1} = \frac{\partial \sigma}{\partial y}\frac{\partial y}{\partial x_1}=0.2\times-1=-0.2\\
&\frac{\partial \sigma}{\partial w_2} = \frac{\partial \sigma}{\partial y}\frac{\partial y}{\partial w_2}=0.2\times-3=-0.6\\
&\frac{\partial \sigma}{\partial x_2} = \frac{\partial \sigma}{\partial y}\frac{\partial y}{\partial x_2}=0.2\times2=-0.4\\
&\frac{\partial \sigma}{\partial b} = \frac{\partial \sigma}{\partial y}\frac{\partial y}{\partial b}=0.2\times1=0.2\\
\end{align}</script><p>如下图所示：</p>
<p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/23.png"></p>
<p>根据这个计算图，我们可以很有条理地写出代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(w1,w2,w3,w4)</span>:</span></span><br><span class="line">	s1 = w1*x1</span><br><span class="line">	s2 = w2*x2</span><br><span class="line">	s3 = s1+s2</span><br><span class="line">	s4 = s3+b</span><br><span class="line">	L = sigmoid(s4)</span><br><span class="line">	</span><br><span class="line">	grad_L = <span class="number">1.0</span></span><br><span class="line">	grad_s4 = grad_L*(<span class="number">1</span>-L)*(L)</span><br><span class="line">	grad_b = grad_s4</span><br><span class="line">	grad_s3 = grad_s4</span><br><span class="line">	grad_s1 = grad_s3</span><br><span class="line">	grad_s2 = grad_s3</span><br><span class="line">	grad_w1 = grad_s1*x1</span><br><span class="line">	grad_x1 = grad_s1*w1</span><br><span class="line">	grad_w2 = grad_s2*x2</span><br><span class="line">	grad_x2 = grad_s2*w2</span><br></pre></td></tr></table></figure>
<h4 id="梯度流的常见模式"><a href="#梯度流的常见模式" class="headerlink" title="梯度流的常见模式"></a>梯度流的常见模式</h4><p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/24.png"></p>
<h3 id="反向传播的矩阵运算"><a href="#反向传播的矩阵运算" class="headerlink" title="反向传播的矩阵运算"></a>反向传播的矩阵运算</h3><p>前面我们都是计算标量，但事实上每一层都有很多神经元，因此，在真正运算的时候，都是矩阵运算。</p>
<h4 id="前向运算"><a href="#前向运算" class="headerlink" title="前向运算"></a>前向运算</h4><p>我们假设输入的<strong>x是D维的横向向量</strong>，<strong>隐藏层有H个</strong>，因此权重矩阵 $W\in\mathbb R^{D\times H}$ </p>
<p>输入乘以权重矩阵后，矩阵的形状为 $(1,H)$ , 如下图所示</p>
<p>然后对每个元素进行Sigmoid激活，得到$\boldsymbol\sigma$</p>
<p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/25.png"></p>
<h4 id="反向运算-输入向量"><a href="#反向运算-输入向量" class="headerlink" title="反向运算-输入向量"></a>反向运算-输入向量</h4><p>每一层计算得到的梯度矩阵的形状和这一层的输入的矩阵形状是一样的。我们首先假设在sigmoid这层反向计算得到的梯度为：$\frac{\partial h}{\partial \sigma} = [4,1,2,-1]$</p>
<h5 id="反向传播1-1"><a href="#反向传播1-1" class="headerlink" title="反向传播1"></a>反向传播1</h5><p>现在，我们来计算y这层的局部梯度： $\frac{\partial\sigma}{\partial y}$</p>
<p>我们知道，矩阵对矩阵求导，需要$\sigma$矩阵中的每一个元素对$y$矩阵中的每一个元素求导，也就是Jacobian Matrix：</p>
<script type="math/tex; mode=display">
\boldsymbol J = [\frac{\partial \boldsymbol f}{\partial x_1},\cdots,\frac{\partial \boldsymbol f}{\partial x_n}] = \begin{bmatrix}\frac{\partial f_1}{\partial x_1}&\cdots&\frac{\partial f_1}{\partial x_n}\\\vdots&\ddots&\vdots\\\frac{\partial f_m}{\partial x_1}&\cdots&\frac{\partial f_m}{\partial x_n} \end{bmatrix}</script><p>也就是说，当隐藏层矩阵的形状为$(1,H)$的时候，$\frac{\partial \sigma}{\partial y}\in \mathbb R^{H\times H}$,  显然这是很复杂的计算，当$H=4096$的时候，我们甚至需要16M的内存来储存这个Jacobian矩阵</p>
<p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/26.png"></p>
<p>事实上，我们根本不需要实现 Jacobian Matirx。 因为我们发现</p>
<script type="math/tex; mode=display">
(\frac{\partial \sigma}{\partial y})_{1,1}= (1-0.007)\times0.007=0.007\\
(\frac{\partial \sigma}{\partial y})_{1,2}=(\frac{\partial \sigma}{\partial y})_{1,3}=(\frac{\partial \sigma}{\partial y})_{1,4}=0</script><p>说明$y_i$只影响$\sigma_i$, 因此只需要保留对角线上的元素即可，如下图所示：</p>
<p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/27.png"></p>
<p>因此，我们可以求出这一层的梯度，也就是两个梯度对应元素相乘后累加：</p>
<script type="math/tex; mode=display">
\frac{\partial h}{\partial y} = \frac{\partial h}{\partial \sigma}\frac{\partial \sigma}{\partial y} = \begin{bmatrix}4&1&2&-1\end{bmatrix}
\begin{bmatrix}0.007&0&0&0\\0&0.01&0&0\\0&0&0.11&0\\0&0&0&0\end{bmatrix}
=\begin{bmatrix}0.028&0.01&0.22&0\end{bmatrix}</script><p>因此，我们只要固定i，然后求一个pairwise的梯度计算，得到局部梯度；再乘以上游梯度，就可以得到这一层的梯度</p>
<h5 id="反向传播2-1"><a href="#反向传播2-1" class="headerlink" title="反向传播2"></a>反向传播2</h5><p>现在，我们要把y这层的梯度往前传，求出$(\frac{\partial y}{\partial x})_{n,m}=(\frac{\partial y_n}{\partial x_m})\Rightarrow\frac{\partial y}{\partial x}\in\mathbb R^{H\times D}$</p>
<p>我们发现，$R^{H\times D}$其实和 $\boldsymbol W^T$ 的形状是一样的. 为什么呢？我们看一下 $y^T$是如何被计算得到的就豁然明了了</p>
<script type="math/tex; mode=display">
y^T = \begin{bmatrix}x_1w_{11}+x_2w_{21}+x_3w_{31}\\x_1w_{12}+x_2w_{22}+x_3w_{32}\\x_1w_{13}+x_2w_{23}+x_3w_{33}\\x_1w_{14}+x_2w_{24}+x_3w_{34} \end{bmatrix}</script><p>那么，如果y对x求偏导的话，得到的雅克比矩阵就是：</p>
<script type="math/tex; mode=display">
\frac{\partial y}{\partial x} =\begin{bmatrix}w_{11} &w_{21}&w_{31}\\w_{12}&w_{22}&w_{32}\\w_{13}&w_{23}&w_{33}\\w_{14}&w_{24}&w_{34} \end{bmatrix} = W^T</script><p>也就是说：</p>
<script type="math/tex; mode=display">
\frac{\partial h}{\partial x} = \frac{\partial h}{\partial y}\frac{\partial y}{\partial x} = \frac{\partial h}{\partial y}W^T\\
shape=(1,H)(H\times D) = (1,D)</script><p>$\frac{\partial h}{\partial x}$ 的形状和$\boldsymbol x$ 的形状是一样的</p>
<h5 id="反向传播3-1"><a href="#反向传播3-1" class="headerlink" title="反向传播3"></a>反向传播3</h5><p>现在来看$\boldsymbol W$这层的梯度，我们首先列出公式</p>
<script type="math/tex; mode=display">
\frac{\partial h}{\partial W} =\frac{\partial h}{\partial W_{ij}}</script><p>虽然$\boldsymbol W$是矩阵，按照雅克比行列式来计算的话，需要每个$y<em>j$对这行中每一个元素$w</em>{ij}$求偏导，这太麻烦了。我们还是一对一对求就好，因为$w_{ij}$只影响 $y_j$ ,因此，上式可以简化为：</p>
<script type="math/tex; mode=display">
\frac{\partial h}{\partial W}=\frac{\partial h}{\partial y_j}\frac{\partial y_j}{\partial w_{ij}} = \frac{\partial h}{\partial y_j}x_i=x_i\frac{\partial h}{\partial y_j}\\</script><p>那么，因为 $\boldsymbol W$ 有三行，对每一行进行如上计算，就会得到一个  $3\times 4$  的矩阵，即</p>
<script type="math/tex; mode=display">
\frac{\partial h}{\partial W}=x^T\frac{\partial h}{\partial y}
=\begin{bmatrix}2\\1\\3\end{bmatrix}\begin{bmatrix}0.028&0.01&0.22&0\end{bmatrix}</script><p>发现这和$\boldsymbol W$的形状也是一样的 </p>
<p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/28.png"></p>
<h5 id="反向传播4-1"><a href="#反向传播4-1" class="headerlink" title="反向传播4"></a>反向传播4</h5><p>上面因为简化，我们没有加入偏置项b，现在我们加上试试：</p>
<script type="math/tex; mode=display">
y^T = \begin{bmatrix}x_1w_{11}+x_2w_{21}+x_3w_{31}+b\\x_1w_{12}+x_2w_{22}+x_3w_{32}+b\\x_1w_{13}+x_2w_{23}+x_3w_{33}+b\\x_1w_{14}+x_2w_{24}+x_3w_{34}+b \end{bmatrix}</script><p>此时，要求偏置项的梯度，我们可以列出公示：</p>
<script type="math/tex; mode=display">
\frac{\partial h}{\partial b} = \frac{\partial h}{\partial y}\frac{\partial y}{\partial b} = \frac{\partial h}{\partial y}\begin{bmatrix}1\\\vdots\\1\end{bmatrix}_H = \sum_{j}\frac{\partial h}{\partial y_j}</script><p>因为这里y关于b的导数始终是1，最后得到的关于b的梯度也是一个数</p>
<h4 id="反向运算-minibatch"><a href="#反向运算-minibatch" class="headerlink" title="反向运算-minibatch"></a>反向运算-minibatch</h4><p>上面，我们输入的只是个向量，即输入一个样本，因此计算起来还是效率不够高。因此我们可以采用minibatch的方法，输入一个矩阵来计算</p>
<h5 id="反向传播1-2"><a href="#反向传播1-2" class="headerlink" title="反向传播1"></a>反向传播1</h5><p>当输入变成一个矩阵的时候，发现$y<em>{ij}$仍然只会影响$\sigma</em>{ij}$,因此我们可以一对一计算，如下</p>
<p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/29.png"></p>
<h5 id="反向传播2-2"><a href="#反向传播2-2" class="headerlink" title="反向传播2"></a>反向传播2</h5><p>和只输入向量一样，$x_{ij}$只会影响y的<strong>第i行</strong>，因此</p>
<script type="math/tex; mode=display">
\frac{\partial h}{\partial x_{ij}} = \sum_{k}\frac{\partial h}{\partial y_{ik}}\frac{\partial y_{ik}}{\partial x_{ij}} = \sum_{k}\frac{\partial h}{\partial y_{ik}}w_{jk}\Rightarrow\frac{\partial h}{\partial x} = \frac{\partial h}{\partial y}W^T</script><p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/30.png"></p>
<h5 id="反向传播3-2"><a href="#反向传播3-2" class="headerlink" title="反向传播3"></a>反向传播3</h5><p>$w_{ij}$只会影响y的<strong>第j列</strong>，因此：</p>
<script type="math/tex; mode=display">
\frac{\partial h}{\partial W}=\sum_{k}\frac{\partial h}{\partial y_{kj}}\frac{\partial y_{kj}}{\partial w_{ij}} =\sum_k \frac{\partial h}{\partial y_{kj}}x_{ki}=\sum_kx_{ki}\frac{\partial h}{\partial y_{kj}}\Longrightarrow\frac{\partial h}{\partial W} = X^T\frac{\partial h}{\partial y}</script><p><img src="/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/31.png"></p>
<h5 id="反向传播4-2"><a href="#反向传播4-2" class="headerlink" title="反向传播4"></a>反向传播4</h5><p>在输入为向量的情况下，b只是个数，但当输入为一个矩阵的时候，偏置项b就变成了N维的向量。$b\in\mathbb R^N$</p>
<p>此时，要求偏置项的梯度，就是</p>
<script type="math/tex; mode=display">
\frac{\partial h}{\partial b} = \sum_j \frac{\partial h}{\partial y_{ij}}</script><p>令b原式的形状是 $(M,)$  上游梯度的形状为$(N\times M)$ ，那么相当于是对上游梯度每一列求和，最后得到形状为$(M,)$的梯度</p>
<h4 id="多层神经网络反向传播"><a href="#多层神经网络反向传播" class="headerlink" title="多层神经网络反向传播"></a>多层神经网络反向传播</h4><p>从上面的神经网络，我们可以推广到当神经网络有很多层的时候，可以这样来求各层的权重：</p>
<script type="math/tex; mode=display">
\begin{align}
&\frac{\partial L}{\partial \boldsymbol W^l} = \frac{\partial L}{\partial \boldsymbol h}\frac{\partial\boldsymbol h}{\boldsymbol W^l}\\
&\frac{\partial L}{\partial \boldsymbol W^{l-1}} = \frac{\partial L}{\partial \boldsymbol h}\frac{\partial\boldsymbol h}{\partial\boldsymbol  a^l}\frac{\partial\boldsymbol a^l}{\boldsymbol W^{l-1}}\\
&\frac{\partial L}{\partial \boldsymbol W^{l-2}} = \frac{\partial L}{\partial \boldsymbol h}\frac{\partial\boldsymbol h}{\partial\boldsymbol  a^l}\frac{\partial\boldsymbol a^l}{\partial\boldsymbol a^{l-1}}\frac{\partial\boldsymbol a^{l-1}}{\boldsymbol W^{l-2}}\\
\vdots\\\vdots\\
&\frac{\partial L}{\partial \boldsymbol W^{l-2}} = \frac{\partial L}{\partial \boldsymbol h}\frac{\partial\boldsymbol h}{\partial\boldsymbol  a^l}\frac{\partial\boldsymbol a^l}{\partial\boldsymbol a^{l-1}}\frac{\partial\boldsymbol a^{l-1}}{\partial\boldsymbol a^{l-2}}\cdots\frac{\partial \boldsymbol a^2}{\boldsymbol W^{l-2}}
\end{align}</script><p>注意，不要忘记每一层的偏执权重b，以及正则项</p>

      
    </div>
    
    
    
    <div>
      
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束，感谢您的阅读-------------</div>
    
</div>

      
    </div>
    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:</strong>
    Jason
  </li>
  <li class="post-copyright-link">
    <strong>Post link:</strong>
    <a href="https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/" title="神经网络和反向传播">https://jasonxqh.github.io/2021/12/02/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice: </strong>
    All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally.
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/cv/" rel="tag"><i class="fa fa-tag"></i> cv</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2021/12/02/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%AE%AD%E7%BB%83/" rel="next" title="计算机视觉-神经网络的训练">
                <i class="fa fa-chevron-left"></i> 计算机视觉-神经网络的训练
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2021/12/06/Django%E5%AD%A6%E4%B9%A02/" rel="prev" title="Django学习2">
                Django学习2 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="lv-container" data-id="city" data-uid="MTAyMC80OTgyMC8yNjMxMQ=="></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Jason</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives%7C%7C%20archive">
              
                  <span class="site-state-item-count">437</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">61</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/JasonXQH" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:10195501423@stu.ecnu.edu.cn" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://yanghong.tech/" title="友链:杨弘的博客" target="_blank">友链:杨弘的博客</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://ankate.github.io/" title="友链:赵奕轲的博客" target="_blank">友链:赵奕轲的博客</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://github.com/JasonXQH/JasonXQH.github.io" title="Like it, STAR ME" target="_blank">Like it, STAR ME</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#神经网络和反向传播"><span class="nav-number">1.</span> <span class="nav-text">神经网络和反向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#损失函数和优化"><span class="nav-number">1.1.</span> <span class="nav-text">损失函数和优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度下降的原理"><span class="nav-number">1.1.1.</span> <span class="nav-text">梯度下降的原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度下降的过程"><span class="nav-number">1.1.2.</span> <span class="nav-text">梯度下降的过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#其他常见的梯度下降算法"><span class="nav-number">1.1.3.</span> <span class="nav-text">其他常见的梯度下降算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#小批量样本梯度下降（Mini-Batch-GD"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">小批量样本梯度下降（Mini Batch GD)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#随机梯度下降（Stochastic-GD"><span class="nav-number">1.1.3.2.</span> <span class="nav-text">随机梯度下降（Stochastic GD)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#神经网络"><span class="nav-number">1.2.</span> <span class="nav-text">神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#单层神经网络"><span class="nav-number">1.2.1.</span> <span class="nav-text">单层神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hidden-layer"><span class="nav-number">1.2.2.</span> <span class="nav-text">hidden layer</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#矩阵思路"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">矩阵思路</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#随机连接的神经网络"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">随机连接的神经网络</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#两层神经网络"><span class="nav-number">1.2.3.</span> <span class="nav-text">两层神经网络</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#神经网络的计算"><span class="nav-number">1.3.</span> <span class="nav-text">神经网络的计算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#反向传播"><span class="nav-number">1.3.1.</span> <span class="nav-text">反向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#具体例子"><span class="nav-number">1.3.1.1.</span> <span class="nav-text">具体例子</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#前向传播1"><span class="nav-number">1.3.1.1.1.</span> <span class="nav-text">前向传播1</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#反向传播1"><span class="nav-number">1.3.1.1.2.</span> <span class="nav-text">反向传播1</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#反向传播2"><span class="nav-number">1.3.1.1.3.</span> <span class="nav-text">反向传播2</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#反向传播3"><span class="nav-number">1.3.1.1.4.</span> <span class="nav-text">反向传播3</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#反向传播4"><span class="nav-number">1.3.1.1.5.</span> <span class="nav-text">反向传播4</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#反向传播5"><span class="nav-number">1.3.1.1.6.</span> <span class="nav-text">反向传播5</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#反向传播6"><span class="nav-number">1.3.1.1.7.</span> <span class="nav-text">反向传播6</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#反向传播7"><span class="nav-number">1.3.1.1.8.</span> <span class="nav-text">反向传播7</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#反向传播8"><span class="nav-number">1.3.1.1.9.</span> <span class="nav-text">反向传播8</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#分块计算图"><span class="nav-number">1.3.1.2.</span> <span class="nav-text">分块计算图</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度流的常见模式"><span class="nav-number">1.3.1.3.</span> <span class="nav-text">梯度流的常见模式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#反向传播的矩阵运算"><span class="nav-number">1.3.2.</span> <span class="nav-text">反向传播的矩阵运算</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#前向运算"><span class="nav-number">1.3.2.1.</span> <span class="nav-text">前向运算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#反向运算-输入向量"><span class="nav-number">1.3.2.2.</span> <span class="nav-text">反向运算-输入向量</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#反向传播1-1"><span class="nav-number">1.3.2.2.1.</span> <span class="nav-text">反向传播1</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#反向传播2-1"><span class="nav-number">1.3.2.2.2.</span> <span class="nav-text">反向传播2</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#反向传播3-1"><span class="nav-number">1.3.2.2.3.</span> <span class="nav-text">反向传播3</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#反向传播4-1"><span class="nav-number">1.3.2.2.4.</span> <span class="nav-text">反向传播4</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#反向运算-minibatch"><span class="nav-number">1.3.2.3.</span> <span class="nav-text">反向运算-minibatch</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#反向传播1-2"><span class="nav-number">1.3.2.3.1.</span> <span class="nav-text">反向传播1</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#反向传播2-2"><span class="nav-number">1.3.2.3.2.</span> <span class="nav-text">反向传播2</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#反向传播3-2"><span class="nav-number">1.3.2.3.3.</span> <span class="nav-text">反向传播3</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#反向传播4-2"><span class="nav-number">1.3.2.3.4.</span> <span class="nav-text">反向传播4</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#多层神经网络反向传播"><span class="nav-number">1.3.2.4.</span> <span class="nav-text">多层神经网络反向传播</span></a></li></ol></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart" aria-hidden="true"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jason</span>
 <!--
  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">1297.9k</span>
  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>



-->
        







        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('-1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
