<!DOCTYPE html>
<html lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jasonxqh.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Jason‘s Blog">
<meta property="og:url" content="https://jasonxqh.github.io/page/6/index.html">
<meta property="og:site_name" content="Jason‘s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Jason">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://jasonxqh.github.io/page/6/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Jason‘s Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Jason‘s Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/" class="post-title-link" itemprop="url">高级数据库-存储</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-02 19:25:30" itemprop="dateCreated datePublished" datetime="2022-03-02T19:25:30+08:00">2022-03-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-06-25 01:41:14" itemprop="dateModified" datetime="2022-06-25T01:41:14+08:00">2022-06-25</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="高级数据库-存储"><a href="#高级数据库-存储" class="headerlink" title="高级数据库-存储"></a>高级数据库-存储</h1><p>参考博客：<a href="https://blog.csdn.net/helloworld_ptt/article/details/105801262" target="_blank" rel="noopener">https://blog.csdn.net/helloworld_ptt/article/details/105801262</a></p>
<p>衡量或者评价一个数据库的性能，通常我们会从四个方面来进行：计算、事务、存储、高可用。现在，我们主要来学习数据库架构中存储这一块的知识。</p>
<h2 id="传统数据库的存储"><a href="#传统数据库的存储" class="headerlink" title="传统数据库的存储"></a>传统数据库的存储</h2><h3 id="KVS接口与设计需求"><a href="#KVS接口与设计需求" class="headerlink" title="KVS接口与设计需求"></a>KVS接口与设计需求</h3><p>KVS(Key Value System) 可以被认为属关系型数据库的基础。如果可以实现KVS，那么其中的Key就相当于关系型数据表中的主键，而Value则相当于数据表中的非主键列。最后，在KVS基础上封装一层Schema，就可以实现关系数据库的基本功能了。</p>
<p>那么在设计KVS的时候，我们需要 考虑哪几方面的内容？</p>
<ul>
<li><strong>需求</strong></li>
<li><strong>功能接口</strong></li>
</ul>
<p>这两点是最重要的，决定了整个项目的成败</p>
<ul>
<li>架构</li>
<li>模块</li>
<li>模块内数据结构、算法</li>
<li>模块间接口</li>
<li>测试</li>
</ul>
<p>上面几方面都和数据库性能有关</p>
<h4 id="功能需求"><a href="#功能需求" class="headerlink" title="功能需求"></a>功能需求</h4><p>那么，在KVS中，我们需要实现什么需求？</p>
<ul>
<li>点操作，也就是 Put和Get操作，即通过put把数据存储到该系统中，再通过get从系统中取出数据</li>
<li>范围操作，即一次需要操作多个数据，比如说用get读取一片连续的数据。因此key的需要存储在连续的空间当中。</li>
<li>容错，因为可能会出现程序错误、宕机，因此在这种情况下，我们需要对已经写入的数据，能确保其不会丢失</li>
<li>一致性，即指事务执行的结果必须是使数据库从一个一致性状态变到另一个一致性状态，在单机状态下实现难度比较小。</li>
</ul>
<h4 id="基本接口"><a href="#基本接口" class="headerlink" title="基本接口"></a>基本接口</h4><p>最终来说，我们需要KVS支持：</p>
<p>单点读和写</p>
<ul>
<li><code>db-&gt;Get(leveldb::ReadOptions(),key1,&amp;value);</code></li>
<li><code>if(s.ok())s=db-&gt;Put(leveldb::WriteOptions(),key2.value)</code></li>
<li><code>if(s.ok())s=db-&gt;Delete(leveldb::WriteOptions(),key1s)</code></li>
</ul>
<p>对范围的支持，我们可以用迭代器来实现：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">leveldb::Iterator* it = db-&gt;Newlterator(leveldb::ReadOptions));</span><br><span class="line"><span class="keyword">for</span> (it-&gt; Seek(start);</span><br><span class="line">  it-&gt; Valid( &amp;&amp; it-&gt;key0.ToString0 &lt; limit;</span><br><span class="line">  it-&gt;Next) &#123;...&#125;</span><br></pre></td></tr></table></figure>
<h3 id="B-Tree"><a href="#B-Tree" class="headerlink" title="B+ Tree"></a>B+ Tree</h3><p>我们学过很多树形的数据结构，但是可能只有B+树可以满足KVS的需求。在 <a href="https://jasonxqh.github.io/2020/06/17/B树/#B-树">这篇博客</a> 中，我们学习了B+树的增删改查。</p>
<p>那么为什么B+树可以满足KVS的设计呢？这就要联系到计算机体系结构的相关知识了。</p>
<p>我们知道计算机的存储都是以块为单位的，最底层的块设备(ssd或者磁盘)的块大小是4KB(不同存储介质不同)。因此，在这种情况下使用二叉树，是不可能使得一个节点充满整一个块的。</p>
<p>我们现在来看B+树的结构：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/1.png"></p>
<p>现在可以发现B+树的优点了 </p>
<ul>
<li>B+树的一个节点可以容纳多个数据，而且可以和底层的块一一对应</li>
<li>B+树和红黑树不一样，它是平衡的。这意味访问任何一个页所需要的深度是相等的，这样使得整体访问代价比较均衡。而且通常来说，一个三层的B+树就可以存储很多数据了，因此一颗B+树的并不会很高</li>
<li>B+树支持范围查询，因为倒数第二层节点之间存在指针(严格意义上B+树并不是树)</li>
</ul>
<h3 id="append-only-btree"><a href="#append-only-btree" class="headerlink" title="append-only btree"></a>append-only btree</h3><p>但是如果我们要做内存中的存储，那么B+树就会显得性能不够，因此我们来学习一种改进后的数据结构： COW(copy on write) B+ Tree</p>
<p>考虑这个 3-level 的b+ 树。有两级分支页面（根为分支页面）和5个叶子节点。数据存储在叶子节点中。</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/2.png" style="zoom:50%;"></p>
<p>这个数据结构不支持叶子与叶子之间的指针(B+树支持) 以便于顺序访问，因为它需要在每次更新时重写整个树。</p>
<p>我们看到，实际上，这些节点是按照顺序存储在数据库文件中的，meta10节点包括指向根页面的指针，SHA1和统计计数器，当文件打开时，它被逐页向后扫描以找到有效的元页面，从而找到树的根。如下所示：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/3.png"></p>
<p> 在COW B+ Tree中，叶子(数据块)是存放在连续空间上的，当我对某一个块进行修改的时候，并不是就地修改页面，而是新建一个页面。并更改索引。如下所示：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/4.png"></p>
<p>当我更新了 leaf 8 中的值，不是就地更改页面，而是生成一个 leaf12。</p>
<p>因为页面的位置发生了变化，所以每个父页面都需要更新以指向新的位置：</p>
<ul>
<li>branch6是 leaf7和leaf8的父页面，因此需要生成一个新的branch11指向leaf7(保持不变)和新生成的页面leaf12</li>
<li>root9是树根，自然需要发生变化，因此需要生成一个root13，指针指向branch1.branch4和branch11。需要注意，root9并不是被删除了，图表中的虚线和指针仍然存在与文件中，只是并不是当前版本罢了。</li>
</ul>
<p>因此，使用这种数据结构，通过将新页面附加到原有文件之后来写入页面。使得已经写入的页面永远都不会被修改。</p>
<p>将树写成顺序形状，如下所示：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/5.png"></p>
<p>那么有人就要问了：每一次都要生成一个新版本的树，不是很浪费空间吗？但事实上，这是一种以空间换时间的方法，将<strong>连续</strong>页面写入磁盘比<strong>随机</strong>写入位置更加有效，而且不需要记录事务日志，因为数据库文件就是事务日志。</p>
<p>但是我们要知道，无论使用B+树还是COW B+树，都会导致<strong>写放大</strong>。因为在写入的时候，除了更新页数据之外，还要更新元数据。因此实际写入的物理数据量是写入数据量的多倍。</p>
<h3 id="Database-Buffer"><a href="#Database-Buffer" class="headerlink" title="Database Buffer"></a>Database Buffer</h3><p>我们知道，如果所有数据都存放在磁盘，每次数据库访问的时候都需要到磁盘里读取，那么，数据访问就讲受限于磁盘的IO，严重影响访问性能。因此，就引入了缓冲区架构：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/7.png" style="zoom:67%;"></p>
<p>缓冲区相当于在数据库和磁盘中间的一层，由缓存管理器管理，它将决定那些Page需要放到缓冲区。这样一来，任何数据库的请求都需要经过缓冲区才能实现。</p>
<p>缓冲区内部结构可以具象为下图：我们把缓冲区中的每一格称为一个Frame，每个Frame可以放一个Page</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/6.png"></p>
<p>问题是，我们要设计一个高效的缓冲区的话，从管理上必须考虑这几个问题：</p>
<ul>
<li>怎么知道哪些Frame是空的？<ul>
<li>扫描</li>
<li>维护一个空的Frame列表</li>
<li>使用一个Bitmap</li>
</ul>
</li>
<li><p>知道Dist page存放在哪个Frame当中</p>
<ul>
<li>使用哈希表，将 page id 映射到frame id</li>
</ul>
</li>
<li><p>如果缓冲区已经满了，该怎么办？</p>
<ul>
<li>在CSAPP中，我们学习过LRU策略，就是淘汰最近没有使用的页，其实还有很多替换策略</li>
</ul>
</li>
</ul>
<h3 id="Implement-LRU-Cache"><a href="#Implement-LRU-Cache" class="headerlink" title="Implement LRU-Cache"></a>Implement LRU-Cache</h3><p>上面我们说LRU策略是比较常用的淘汰机制。那么LRU有哪些缺点呢？</p>
<ol>
<li>LRU对循环读取的场景不太友好，比如我们循环读取第1、2、3、4页，那么我们刚把1淘汰掉，就需要去读取第1页了。</li>
<li>LRU会引入很冷的数据。比如某一页只出现了一次，也会出现在缓冲区当中，并在相当一段时间里面占有一个位置。</li>
</ol>
<p>因此，我们可以对LRU做一个优化——LRU-K</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/8.png" style="zoom: 50%;"></p>
<p>LRU-K需要多维护一个队列或者更多，用于记录所有缓存数据被访问的历史。只有当数据的访问次数达到K次的时候，才将数据放入缓存。当需要淘汰数据时，LRU-K会淘汰第K次访问时间距当前时间最大的数据。</p>
<ul>
<li>第一步添加数据照样放入第一个队列的头部<ul>
<li>如果数据在该队列里访问没有达到K次（该数值根据具体系统qps来定）则会继续到达链表底部直至淘汰；</li>
<li>如果该数据在队列中时访问次数达到了K次，那么它会被加入到接下来的2级(具体需要几级结构也同样结合系统分析)链表中，按照时间顺序在2级链表中排列</li>
<li>接下来2级链表中的操作与上面算法相同，链表中的数据如果再次被访问则移到头部，链表满时，底部数据淘汰</li>
</ul>
</li>
</ul>
<p>相比LRU，LRU-K需要多维护一个队列，用于记录所有缓存数据被访问的历史，所以需要更多的内存空间来用来构建缓存，但优点也很明显，较好的降低了数据的污染率提高了缓存的命中率，对于系统来说可以用一定的硬件成本来换取系统性能也不失为一种办法。</p>
<h4 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h4><p>我们要实现一个LRU Cache的话，需要满足什么要求，又要采用什么数据结构？</p>
<p>首先我们要考虑，怎么能让页号在内存里的位置，然后需要考虑LRU淘汰机制</p>
<ul>
<li>如何判断数据是否在缓存里，是否命中？<ul>
<li>使用哈希表</li>
</ul>
</li>
<li>如何筛选LRU的数据？<ul>
<li>我们可以使用链表，把新插入的数据放在表头，更新的数据也放在表头，这样不访问的数据会慢慢聚集到表尾</li>
</ul>
</li>
</ul>
<p>因此，最后的数据结构应该是一个哈希表和链表的结合——用哈希链地址法，另外做一个链表，把哈希表中所有的节点链起来</p>
<p>如下所示：</p>
<ul>
<li>Lru ： 链表的头指针</li>
<li>Prev/next: 链表的前驱后继指针</li>
<li>Hash_next:哈希表的后继指针，用来指向value</li>
</ul>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/9.png" style="zoom: 50%;"></p>
<h2 id="bitcask结构"><a href="#bitcask结构" class="headerlink" title="bitcask结构"></a>bitcask结构</h2><h3 id="Log-structured-store"><a href="#Log-structured-store" class="headerlink" title="Log-structured store"></a>Log-structured store</h3><p>事实上，对于数据该如何存储在磁盘中，也非常有门道——之前我们在学习COW B+树时知道顺序写入的性能是要优于随机写入的性能的。</p>
<p>这里我们再介绍一种存储方法——Log-structured 存储，也就是说，将数据像记录日志一样存储在磁盘当中。近些年来，这种存储方式被越来越多的用在数据库存储中。</p>
<p>Log-structured 存储系统的基本单位就是一个只能附加在原有数据之后的日志。每当我想写入新的数据的时候，只需要将其附加到日志的末尾，而不是在磁盘中随变找一个位置。同样的，对meta data也采用这种方式，它的更新也被附加到日志中。这可能看起来效率低下，实际上，每次写入时我们需要更新的索引节点的数量通常非常少。</p>
<p>我们来看一个例子，下面是一个仅包含单个数据项的日志 Foo以及它的索引节点A:</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/10.png"></p>
<p>现在，我们要添加第二个元素Bar，将其添加到日志的末尾，然后更新索引条目A’,并将更新后的版本也附加到日志中：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/11.png"></p>
<p>我们看到，旧索引A仍然在日志文件当中，但是不再被使用了。因为他已经被新的索引A’替代。新索引指向Foo和新文件Bar。当上层想要读取我们的文件系统的时候，首先会找到索引的根节点(即A’),然后从根节点出发去寻找其它节点。</p>
<p>接下来我们看看当修改文件的时候会怎么样, 比如说我们修改的是Foo:</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/12.png"></p>
<p>那么首先，修改过后的Foo’ 会被写入日志末尾。然后，我们再次更新索引节点A’ ,变成一个新的A’’,并将其也写在日志末尾。Foo的旧副本会保留在日志当中，但是不会再被新的索引引用.</p>
<p>可以意识到，这个系统并不是可以一直运行下去的，因为无用的文件始终在累计，总有一个时刻我们将耗尽存储空间。因此，在文件系统中，我们将整个磁盘作为一个可以循环的缓冲区，当放不下新的数据的时候，会将旧的日志数据覆盖掉。但在上层看来，新数据就好像是新写入的，而不是覆盖原本数据保存下来的。在常规的文件系统中，随着磁盘越来越满，文件系统需要花费越来越多的时间进行垃圾收集，并将数据写回到日志头部。实际上，当磁盘消耗的存储空间到达80%的时候，文件系统就会停止运行。</p>
<p>但是，我们可以不用这种数据结构来管理文件，我们可以将Log-structured存储用于数据库引擎。也就是说，我们不再底层应用这种模式，而在文件系统的上层来实现它。比如说，我们可以将数据库拆分为多个固定长度的块，那么当我们需要回收一点空间的时候，将有用的数据写在一个块里，删除其他块。比如说对于上面那张图，占用了6页，假设块的大小为4页，我们就可以将其缩短为一个块的大小：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/13.png"></p>
<p>对于上图，第一个日志段完全是空的，因此可以删除。</p>
<h4 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h4><p>和底层的文件系统比起来，使用Log-structured存储有以下几个优势：</p>
<ol>
<li>首先，使用这种存储方式在删除时不需要首先删除最旧的数据——如果中间段几乎是空的话，我们可以对其进行垃圾收集。这对一些需要长时间存放数据的数据库非常有用。<ul>
<li>对于何时进行垃圾收集，我们通常可以等到一个段的大部分已经”过期”以后再进行垃圾收集，进一步减少了额外工作量</li>
</ul>
</li>
<li>可以避免重写相同的未修改的数据，因此在这方面可以节省时间，提高性能。</li>
<li>使用这种方法备份也更容易：在传统的B tree上，如果我更新了数据，那么旧的页同时也丢失了；但是使用这种数据结构我们可以通过在完成时将每个新日志段复制到备份介质来连续、增量地备份我们的数据库。如果要恢复，我们根据备份来恢复即可。</li>
<li>使用这个系统的最后一个优势与更好的并发性。我们知道，为了提供事务的一致性(consistency), 大多数数据库会使用复杂的锁系统。根据所需的一致性级别，使用不同的锁。使用锁的缺点就是：当并发量很高时，写入数据相对较低，性能显著下降。<ul>
<li>因此，可以使用MVCC(Multiversion Concurrency Control) 来解决这个问题。当一个节点想从数据库读取数据的时候，它会去查找当前的根索引，并将其用于这个节点的事务的剩余部分。因为在一个log-based 系统中，存在的数据永远不会被修改，所以当我们读取了根节点的时候，相当于获取了整个数据库的快照(当前根节点有指向所有可用的数据页的指针)。比如说，我们读取到了A’’’,我们就能读取Foo’和Bar,而其他并发的事务如果对数据进行修改的话，是追加在A’’’之后的，对当前事务并无影响。因此，从这个角度看，我们实现了无锁读取。</li>
<li>在写回数据的时候，我们可以使用Optimistic concurrency(OCC,在<a href="https://jasonxqh.github.io/2022/05/11/高级数据库-事务/">高级数据库-事务</a>中有详细介绍)，顾名思义，就是乐观的并发，这和使用锁并等待的悲观机制有所不同。在写入对数据的更改的时候，我们获取写入锁。然后我们要验证当前数据和我们在读取阶段的数据是一致的——我们可以读取索引，判断索引指向的文件是否与之前的相同，地址是否相同。如果相同，说明在这段时间里面并没有发生写入，因此我们可以对其进行追加写；如果不同的话，就发生了事务冲突，我们只需要回滚并从读取阶段重新开始。</li>
</ul>
</li>
</ol>
<h3 id="bitcask"><a href="#bitcask" class="headerlink" title="bitcask"></a>bitcask</h3><p>学习了上面基于日志结构的存储系统，我们现在来学Bitcask架构，也就是<strong>日志型的基于哈希表结构的键值对存储系统</strong>。比如说Berkeley DB，BeansDB，Ranmcloud都采取了这种架构。 它有以下几个特点：</p>
<ol>
<li>通过内存中哈希表的key来索引磁盘中数据的位置</li>
<li>数据文件采用追加写的方式</li>
<li>比较低的读写时延。</li>
<li>比较高的随机写吞吐率。</li>
<li>能够控制更大的数据库。</li>
<li>容易备份和恢复。</li>
<li>相对简单，容易理解。</li>
<li>可预计的高访问压力情况。</li>
</ol>
<p>bitcask只支持追加操作(Append-only)，即所有的写操作只追加而不修改老的数据，每个文件都有一定的大小限制，当文件增加到相应的大小，就会产生一个新的文件，老的文件只读不写。在任意时刻，只有一个文件是可写的，用于追加数据，被称为<strong>活跃数据文件</strong>(active data file)，其他已经达到文件大小限制的，被称为老数据文件(older data file)。</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/15.png"></p>
<p>活跃数据文件仅支持追加写入，因此所有的写入操作都是串形化的而不用磁盘随机定位。写入的键值对格式如下：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/14.png"></p>
<ol>
<li>crc用于验证</li>
<li>timestamp 时间戳</li>
<li>key size : 键的长度</li>
<li>value size:值的长度</li>
<li>key 数据</li>
<li>Value 数据</li>
</ol>
<h4 id="哈希索引"><a href="#哈希索引" class="headerlink" title="哈希索引"></a>哈希索引</h4><p>在bitcask模型中，使用了Hash表的索引结构。在内存中有一张哈希表，通过哈希表中的key值可以快速地定位到磁盘中的数据，大致结构如下所示：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/16.png"></p>
<p>hash表对应的这个结构中包括了三个用于定位数据value的信息，分别是文件id号(<code>file_id</code>)，value值在文件中的位置（<code>value_pos</code>）,value值的大小（<code>value_sz</code>），于是我们通过读取<code>file_id</code>对应文件的<code>value_pos</code>开始的<code>value_sz</code>个字节，就得到了我们需要的value值。整个过程如下图所示：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/14.jpeg"></p>
<p>在一个具体的哈希索引例子中，我们可以更清楚的看到数据是如何被找到的：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/19.png"></p>
<h4 id="文件的merge"><a href="#文件的merge" class="headerlink" title="文件的merge"></a>文件的merge</h4><p>合并操作是定时对所有的旧数据文件进行扫描并生成新的数据文件（其本质是将同一个Key的多个操作进行合并。）在这个过程中，并没有包含active date file，因为它还还在不停写入。</p>
<p>键值对的删除也是追加写的方式写入活动数据文件中，真正的删除会在下一次的数据合并中进行。</p>
<p>这里的merge其实就是将对同一个key的多个操作以<strong>只保留最新</strong>原则进行删除。每次merge之后，新生成的数据文件就不再有冗余了。如下图所示：对于yawn,保留最新的511；对于scratch，保留最新的252；对于mew，保留最新的1087；对purr，保留最新的2114</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/18.png"></p>
<h4 id="容量限制"><a href="#容量限制" class="headerlink" title="容量限制"></a>容量限制</h4><p>我们假设内存是有限的，磁盘空间是无限的。可以给出一个元数据的格式：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//BeansDB文件元数据格式</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">t_item</span> &#123;</span></span><br><span class="line">	<span class="keyword">unit32_t</span> pos;	<span class="comment">//数据记录在</span></span><br><span class="line">	<span class="keyword">int32_t</span> ver;	<span class="comment">//数据记录版本号</span></span><br><span class="line">	<span class="keyword">unit16_t</span> hash;<span class="comment">//数据记录hash</span></span><br><span class="line">	<span class="keyword">unit8_t</span> length;<span class="comment">//key的长度</span></span><br><span class="line">	<span class="keyword">char</span> key[<span class="number">1</span>];	<span class="comment">//key的实际值</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>beansDB将pos的低8位用来存储<strong>文件的id</strong>，这样一便可以最多存储256个文件。然后用剩下的24位用来存储文件内偏移。每个文件的大小一般被定义为2GB,因此该系统最大支持的存储空间为512GB。每个文件元数据占据内存的总存储空间为：4B+4B+2B+1B+size(key), 按照size(key) = 20B来计算。 100M(20*100*1024)个文件也只会占用2GB的内存。</p>
<h4 id="容错"><a href="#容错" class="headerlink" title="容错"></a>容错</h4><p>从上面我们可以知道，索引的Hash表存放在内存中，如果发生系统重启，则须要扫描磁盘中的数据重建Hash表，如果数据量非常大，这个过程是非常耗时的。也就是说，<strong>bitcask启动速度是很慢的</strong>。 因此，bitcask模型中还要对older data file生成一个hint file，在这处文件中，数据结构与磁盘中的数据文件非常相似，不同的是 , 它不存储具体的value值，而是存储value的位置信息。相当于对旧数据哈希表的物化。其结构如下图：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/17.png"></p>
<h2 id="LSM-Tree-架构存储"><a href="#LSM-Tree-架构存储" class="headerlink" title="LSM Tree 架构存储"></a>LSM Tree 架构存储</h2><h3 id="Skiplist"><a href="#Skiplist" class="headerlink" title="Skiplist"></a>Skiplist</h3><p>在简单的链表中，如果我们要在里面找到一个元素的话，需要将整个链表都遍历一次。因此我们可以设计一种新型的链表——skiplist(跳表), 来提高查询的效率。跳表的平均期望的查找、插入、删除时间复杂度都是$O(\log n)$ ，n代表跳表的长度。</p>
<p>许多知名的开源软件（库）中的数据结构均采用了跳表这种数据结构。</p>
<ul>
<li>Redis中的有序集合zset</li>
<li>LevelDB、RocksDB、HBase中Memtable</li>
<li>ApacheLucene中的TermDictionary、Posting List</li>
</ul>
<h4 id="skiplist的构建"><a href="#skiplist的构建" class="headerlink" title="skiplist的构建"></a>skiplist的构建</h4><p>首先我们不满足只有一层的链表，那么我们是不是可以在原有链表的基础上增加一层？加入我们每相邻两个节点来增加一个指针，让指针去指向下下个节点，如下图：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/21.png"></p>
<p>这样一来，第二层包含的节点数只有原来的一半。现在，当我们想要查找数据的时候，可以先沿着第二层的链表进行查找。当碰到比待查数据大的节点时，再回到原来的链表中进行查找。比如我想查找23，就是这样一个查找路径：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/22.png"></p>
<ul>
<li>首先，23和7比较，再和19比较，比它们都大；继续向后比较，发现比26小，因此要掉到下面的链表(L0)</li>
<li>在L0中，从19开始，23比22要大；那么继续向后比较</li>
<li>发现23比26小，说明待查数据23在原链表中不存在</li>
</ul>
<p>我们发现，在这个查找过程中，由于新增加的指针，我们不需要与链表中的每个节点去逐个进行比较了。需要比较的节点数大概只有原来的一半。</p>
<p>因此，我们可以采用同样的方式再往上加一层，产生第三层链表L2</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/23.png"></p>
<p>在这个新的三层链表结构上，如果我们还是查找23，那么：</p>
<ul>
<li>沿着最上层链表首先要比较的是19，发现23比19大，但19已经是L2的最后一层了，因此跳到L1</li>
<li>查找方式如二层跳表。</li>
</ul>
<p>可以想象，当链表足够长的时候，这种多层链表的查找方式能让我们跳过很多下层节点，大大加快查找的速度</p>
<p>Skiplist就是根据这种思想而设计出来的。但是，如果我们按照上面的这种方法来构建跳表，<strong>在新插入一个节点之后，就会打乱上下相邻两层链表上节点个数严格2:1的对应关系。如果要维持这种对应关系，就必须把新插入的节点后面的所有节点（也包括新插入的节点）重新进行调整，这会让时间复杂度重新退化成O(n)</strong>。</p>
<p>因此跳表引入了一个很好的机制。为了避免这种情况的发生，它不要求上下两层链表之间的节点个数严格满足1:2, 而是在生成上层链表的时候，会给每一个节点”抛硬币”——假设抛到正面就会上升一层，抛到反面就按兵不动。</p>
<p>一般来说，跳表的高度是$\log(n)$ ，n为链表中元素个数。最终结果如下：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/20.png"></p>
<h4 id="skiplist的查找"><a href="#skiplist的查找" class="headerlink" title="skiplist的查找"></a>skiplist的查找</h4><p>Skiplist的查找和上面所说的方法基本是一样的，即从高层向底层查找，不断缩小范围，最后锁定目标。</p>
<h4 id="skiplist的插入"><a href="#skiplist的插入" class="headerlink" title="skiplist的插入"></a>skiplist的插入</h4><p>skiplist的插入是需要一定的技巧的</p>
<p>首先是进行查找，比如对上面这张跳表，我想插入15。那么我们首先要查找到15该插入的位置，同时记录每一层的小于插入值的最大节点，比如在L2，15处于6和25之间，我们就将6保存下来，之后有用。</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/24.png"></p>
<p>在这里我们保留每个红色箭头的起始节点:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>层数</th>
<th>保留节点</th>
</tr>
</thead>
<tbody>
<tr>
<td>L3</td>
<td>6</td>
</tr>
<tr>
<td>L2</td>
<td>6</td>
</tr>
<tr>
<td>L1</td>
<td>9</td>
</tr>
<tr>
<td>L0</td>
<td>12</td>
</tr>
</tbody>
</table>
</div>
<p>经过了查找，我们知道15应该插入到12到17 中间，那么问题来了——15节点的层数应该是多少？</p>
<p>这里我们还是采用随机的方法，对15节点抛硬币，逻辑如下</p>
<ul>
<li>如果是正面，那么就加一层，并继续抛硬币；<ul>
<li>如果还是正面，就继续抛，一直到最高层</li>
<li>如果是反面，就立刻停止</li>
</ul>
</li>
<li>如果是反面，那么就不再增加层数了。</li>
</ul>
<p>最后，假设15节点的层高为3，那么就需要先断开三层链表，并进行重连该节点的前后指针。</p>
<p>伪代码如下：</p>
<ul>
<li>首先，每个节点肯定都有第1层指针（每个节点都在第1层链表里）</li>
<li>如果一个节点有第i层(i&gt;=1)指针（即节点已经在第1层到第i层链表中），那么它有第(i+1)层指针的<strong>概率为p</strong></li>
<li>节点最大的层数不允许超过一个最大值，记为<strong>MaxLevel</strong><br>这个计算随机层数的伪码如下所示：</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">randomLevel()</span><br><span class="line">    level := <span class="number">1</span></span><br><span class="line">    <span class="comment">// random()返回一个[0...1)的随机数</span></span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">random</span>() &lt; p <span class="keyword">and</span> level &lt; MaxLevel <span class="keyword">do</span></span><br><span class="line">        level := level + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> level</span><br></pre></td></tr></table></figure>
<h4 id="skiplist的删除"><a href="#skiplist的删除" class="headerlink" title="skiplist的删除"></a>skiplist的删除</h4><p>删除和插入一样，首先要进行查找，同时记录每个前驱结点，因为在删除L0节点之后需要断开上面几层的指针</p>
<h4 id="skiplist每个点的结构"><a href="#skiplist每个点的结构" class="headerlink" title="skiplist每个点的结构"></a>skiplist每个点的结构</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">nodeStructure</span> *<span class="title">node</span>;</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">nodeStructure</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">  keyType key; 			<span class="comment">// key值</span></span><br><span class="line">  valueType value;  <span class="comment">// value值</span></span><br><span class="line">  <span class="comment">// 向前指针数组，根据该节点层数的不同指向不同大小的数组</span></span><br><span class="line">  node forward[<span class="number">1</span>]; 	<span class="comment">// 这是一个指针数组首地址，实际分配内存大小时多于一个</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// newNodeOfLevel生成一个nodeStructure结构体，同时生成l个node* 数组指针</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> newNodeOfLevel(l) (node)malloc(sizeof(struct nodeStructure)+(l)*sizeof(node *));</span></span><br></pre></td></tr></table></figure>
<p>注意，节点其实只有一层，而不是多层链表</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/25.png"></p>
<h4 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h4><p>在这一部分，我们来简单分析一下skiplist的时间复杂度和空间复杂度，以便对于skiplist的性能有一个直观的了解。</p>
<p>我们先来计算一下<strong>每个节点所包含的平均指针数目（概率期望）</strong>。节点包含的指针数目，相当于这个算法在空间上的额外开销(overhead)，可以用来度量空间复杂度</p>
<h5 id="平均指针数目"><a href="#平均指针数目" class="headerlink" title="平均指针数目"></a>平均指针数目</h5><p>根据前面randomLevel()的伪码，我们很容易看出，<strong>产生越高的节点层数，概率越低</strong>。定量的分析如下：</p>
<ul>
<li>节点层数至少为1。而大于1的节点层数，满足一个概率分布</li>
<li>节点层数恰好等于1的概率为$1-p$</li>
<li>节点层数大于等于2的概率为$p$，而节点层数恰好等于2的概率为$p(1-p)$</li>
<li>节点层数大于等于3的概率为$p^2$，而节点层数恰好等于3的概率为$p^2(1-p)$</li>
<li>节点层数大于等于4的概率为$p^3$，而节点层数恰好等于4的概率为$p^3(1-p)$ </li>
</ul>
<p>一般来说我们取$p=0.5$</p>
<p>因此，一个节点的平均层数（也即包含的平均指针数目），计算如下：</p>
<script type="math/tex; mode=display">
1\times(1-p)+2p(1-p)+3p^2(1-p) +...=(1-p)\sum_{k=1}^{+\infty}kp^{k-1}\\
\sum_{k=1}^{+\infty}\int kp^{k-1}d_k = \sum_{k=1}^{\infty}p^k =p(\frac{1-p^k}{1-p}) = \frac{1}{1-p}-1\\</script><p>再对积分求导数，可得:</p>
<script type="math/tex; mode=display">
1\times(1-p)+2p(1-p)+3p^2(1-p) +...=(1-p)\sum_{k=1}^\infty kp^{k-1} = (1-p)\frac{1}{(1-p)^2} = \frac{1}{1-p}</script><p>现在很容易计算出：</p>
<ul>
<li>当p=1/2时，每个节点所包含的平均指针数目为2；</li>
<li>当p=1/4时，每个节点所包含的平均指针数目为1.33。这也是Redis里的skiplist实现在空间上的开销。</li>
</ul>
<h5 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h5><p>接下来，为了分析时间复杂度，我们计算一下skiplist的<strong>平均查找长度</strong>。查找长度指的是查找路径上跨越的跳数，而查找过程中的比较次数就等于查找长度加1。</p>
<p>为了计算查找长度，这里我们需要利用一点小技巧。我们注意到，每个节点插入的时候，它的层数是由随机函数randomLevel()计算出来的，而且随机的计算不依赖于其它节点，每次插入过程都是完全独立的。所以，<strong>从统计上来说，一个skiplist结构的形成与节点的插入顺序无关</strong></p>
<p>这样的话，为了计算查找长度，我们可以将查找过程倒过来看，从右下方第1层上最后到达的那个节点开始，沿着查找路径向左向上回溯，类似于爬楼梯的过程。我们假设当回溯到某个节点的时候，它才被插入，这虽然相当于改变了节点的插入顺序，但从统计上不影响整个skiplist的形成结构</p>
<p>现在假设我们从一个层数为i的节点x出发，需要向左向上攀爬k层。这时我们有两种可能：</p>
<ul>
<li>如果节点x有第(i+1)层指针，那么我们需要向上走。这种情况概率为p</li>
<li>如果节点x没有第(i+1)层指针，那么我们需要向左走。这种情况概率为(1-p)</li>
</ul>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/27.png"></p>
<p>用C(k)表示向上攀爬k个层级所需要走过的平均查找路径长度（概率期望），那么：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">C(0)&#x3D;0C(k)&#x3D;(1-p)×(上图中情况b的查找长度) + p×(上图中情况c的查找长度)</span><br><span class="line">1</span><br></pre></td></tr></table></figure>
<p>代入，得到一个差分方程并化简：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">C(k)&#x3D;(1-p)(C(k)+1) + p(C(k-1)+1) </span><br><span class="line">C(k)&#x3D;1&#x2F;p+C(k-1) </span><br><span class="line">C(k)&#x3D;k&#x2F;p</span><br></pre></td></tr></table></figure>
<p>这个结果的意思是，我们每爬升1个层级，需要在查找路径上走 1/p 步。而我们总共需要攀爬的层级数等于整个skiplist的总层数-1。</p>
<p>那么接下来我们需要分析一下当skiplist中有n个节点的时候，它的总层数的概率均值是多少。这个问题直观上比较好理解。根据节点的层数随机算法，容易得出：</p>
<ul>
<li>第1层链表固定有$n$个节点；</li>
<li>第2层链表平均有$n\times p$个节点；</li>
<li>第3层链表平均有$n\times p^2$个节点；</li>
</ul>
<p>所以，从第1层到最高层，各层链表的平均节点数是一个指数递减的等比数列。容易推算出，总层数的均值为$\log\frac{1}{pn}$，而最高层的平均节点数为$1/p$。</p>
<p>综上，粗略来计算的话，平均查找长度约等于 $C(\log\frac{1}{pn-1}) = \frac{\log 1/(pn-1)}{p}$ ，也就是时间复杂度为 $O(log(n))$</p>
<h4 id="Skiplist和B树以及哈希表的对比"><a href="#Skiplist和B树以及哈希表的对比" class="headerlink" title="Skiplist和B树以及哈希表的对比"></a>Skiplist和B树以及哈希表的对比</h4><ul>
<li>skiplist和各种平衡树（如AVL、红黑树等）的元素是有序排列的，而哈希表不是有序的。因此，<strong>在哈希表上只能做单个key的查找，不适宜做范围查找。</strong> 所谓范围查找，指的是查找那些大小在指定的两个值之间的所有节点</li>
<li>在做范围查找的时候，平衡树比skiplist操作要复杂。<ul>
<li>在平衡树上，我们找到指定范围的小值之后，还需要以中序遍历的顺序继续寻找其它不超过大值的节点。如果不对平衡树进行一定的改造，这里的中序遍历并不容易实现。</li>
<li>而在skiplist上进行范围查找就非常简单，只需要在找到小值之后，对第1层链表进行若干步的遍历就可以实现</li>
</ul>
</li>
<li>平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而<strong>skiplist的插入和删除只需要修改相邻节点的指针</strong>，操作简单又快速</li>
<li>从内存占用上来说，skiplist比平衡树更灵活一些。一般来说，平衡树每个节点包含2个指针（分别指向左右子树），而skiplist每个节点包含的指针数目平均为1/(1-p)，具体取决于参数p的大小。如果像Redis里的实现一样，取p=1/4，那么平均每个节点包含1.33个指针，比平衡树更有优势</li>
<li>查找单个key，skiplist和平衡树的时间复杂度都为$O(\log n)$，大体相当；而哈希表在保持较低的哈希值冲突概率的前提下，查找时间复杂度接近O(1)，性能更高一些。所以我们平常使用的各种Map或dictionary结构，大都是基于哈希表实现的</li>
<li>从算法实现难度上来比较，skiplist比平衡树要简单得多</li>
</ul>
<h3 id="Log-structured-Merge-Tree-Model"><a href="#Log-structured-Merge-Tree-Model" class="headerlink" title="Log-structured Merge Tree Model"></a>Log-structured Merge Tree Model</h3><p> LSM的基本思想是将修改的数据保存在内存，达到一定数量后在将修改的数据批量写入磁盘，在写入的过程中与之前已经存在的数据做合并。同B树存储模型一样，<strong>LSM存储模型也支持增、删、读、改以及顺序扫描操作。LSM模型利用批量写入解决了随机写入的问题，虽然牺牲了部分读的性能，但是大大提高了写的性能。</strong></p>
<p>下图是LSM 树存储模型的基本架构：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/28.png" style="zoom:67%;"></p>
<p>我们需要了解三个最重要的组件：MemTable，Immutable MemTable和SSTable</p>
<h4 id="Memtable"><a href="#Memtable" class="headerlink" title="Memtable"></a>Memtable</h4><p> LSM本身由MemTable,Immutable MemTable,SSTable等多个部分组成，其中<strong>MemTable位于内存</strong>，用于记录最近修改的数据，<strong>一般用跳跃表来组织(也可以用平衡树)</strong>。当MemTable达到一定大小后，将其冻结起来变成Immutable MemTable，然后开辟一个新的MemTable用来记录新的记录。而Immutable MemTable则等待转存到磁盘。</p>
<h4 id="Immutable-MemTable"><a href="#Immutable-MemTable" class="headerlink" title="Immutable MemTable"></a>Immutable MemTable</h4><p>顾名思义，就是只能读不能写的内存表。内存部分已经有了MemTable，为什么还要使用Immutable MemTable？个人认为其原因是<strong>为了不阻塞写操作</strong>。因为转存的过程中必然要保证内存表的记录不变，否则如果新插入的记录夹在两条已经转存到磁盘的记录中间，处理上会很麻烦，转存期间势必要锁住全表，这样一来就会阻塞写操作。所以不如将原有的MemTable变成只读Immutable MemTable,再开辟一个新的MemTable用于写入，即简单，又不影响写操作。</p>
<h4 id="SSTable"><a href="#SSTable" class="headerlink" title="SSTable"></a>SSTable</h4><p>SSTable是本意是指<strong>有序的键值对集合</strong>( a set of sorted key-value pairs )。是一个简单有用的集合，正如它的名字一样，它存储的就是一系列的键值对。当文件较大的时候，还可以为其建立一个键-值的位置的索引，指明每个键在SSTable文件中的偏移距离。这样可以加速在SSTable中的查询。 </p>
<p>使用MemTable和SSTable这两个组件，可以构建一个最简单的LSM存储模型。这个模型与Bitcask模型相比，<strong>不存在启动时间长的问题</strong>，<strong>但是这个模型的读性能非常差</strong>。在之后会详细展开</p>
<h4 id="读写操作"><a href="#读写操作" class="headerlink" title="读写操作"></a>读写操作</h4><ul>
<li>读取操作</li>
</ul>
<p>之前我们说了在LSMT中，读取数据的性能会很差。因为一但在MemTable找不到相应的键，则需要在根据SSTable文件生成的时间，从最近到较早在SSTable中寻找，如果都不存在的话，则会遍历完所有的SSTable文件。因此，如果SSTable文件个数很多或者没有建立SSTable的文件内索引的话，读性能则会大大下降。</p>
<p>除了在对SSTable内部建立索引外，还可以使用Bloom Filter,  提高Key不在SSTable的判定速度。同样，定期合并旧的SSTable文件，在减少存储的空间的同时，也能提高读取的速度。 正如上图所示，SSTable分为好多层，Level1就是由Level0合并而来的，而Level2也是由Level1合并而来的。</p>
<ul>
<li>写入操作</li>
</ul>
<ol>
<li>首先，将这条KV记录以顺序写的方式追加到log文件末尾</li>
<li>如果写入log文件成功，那么这条KV记录插入内存中的Memtable中</li>
<li>SklipList先查找合适的插入位置，然后修改相应的链接指针将新纪录插入即可</li>
</ol>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/29.png" style="zoom:67%;"></p>
<h4 id="Compation操作"><a href="#Compation操作" class="headerlink" title="Compation操作"></a>Compation操作</h4><ul>
<li>Minor compaction：当内存中的memtable大小到了一定值的时候，将内容保存到磁盘文件中</li>
<li>Major compaction:  对多个文件中的所有记录重新进行排序。也就是Level0向Level1，Level1向Level2的转变过程</li>
</ul>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/30.png" style="zoom:67%;"></p>
<p>在最本章的最后一节我们会详细学习两种操作的具体内容。</p>
<h3 id="LevelDB-的-SSTable结构"><a href="#LevelDB-的-SSTable结构" class="headerlink" title="LevelDB 的 SSTable结构"></a>LevelDB 的 SSTable结构</h3><p>上面整体整体了一下LSMT中的SSTable，现在我们来看一下其内部结构：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/31.png" style="zoom:67%;"></p>
<p>从逻辑布局角度看：可以将<code>.sst</code>文件划分为数据存储区和数据管理区，数据存储区存放实际的kv数据，数据管理区则提供一些索引指针等管理数据，目的是更快速便捷地查找相应记录。</p>
<ul>
<li>DataBlock: 存储 Key-Value记录，分为data,type, CRC 三部分</li>
<li>MetaBlock：索引DataBlock的元数据块</li>
<li>MetaBlock_index： 记录 Filter的相关信息</li>
<li>IndexBlock：描述一个DataBlock，存储着对应DataBlock的最大Key值，DataBlock在<code>.sst</code>文件中的偏移量和大小</li>
<li>Footer：索引的索引，记录IndexBlock和MetaIndexBlock在SSTable中的<strong>偏移量</strong>和<strong>大小</strong></li>
</ul>
<h4 id="Footer"><a href="#Footer" class="headerlink" title="Footer"></a>Footer</h4><p>我们先从索引的索引Footer说起，它记录IndexBlock和MetaIndexBlock在SSTable中的偏移量和大小。其结构如下：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/32.png" style="zoom:67%;"></p>
<p>Footer位于SSTable的尾部，占用48个字节。 其中，最主要的就是<code>metaindex_handle</code>和<code>index_handle</code>两个信息。</p>
<p><code>metaindex_handle</code>与<code>index_handle</code>物理上占用了40个字节。每一个handle的结构如右图，逻辑上就是offset+size(大小和偏移量)，在内存中占用16个字节。因此，两个handle的实际存储可能不到32个字节，剩下是一些padding填充。</p>
<h4 id="DataBlock"><a href="#DataBlock" class="headerlink" title="DataBlock"></a>DataBlock</h4><p>数据块主要分为<strong>数据、重启点、尾部</strong>三个部分</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/33.png" style="zoom:67%;"></p>
<ul>
<li>每个Record的格式如下所示：</li>
</ul>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/34.png" style="zoom:67%;"></p>
<p>我们发现，里面有key共享长度和key非共享长度，这是为什么？</p>
<p>事实上LevelDB做了一些存储上的优化：在DataBlock中，key是有序存储的，相邻的key之间是可能有重复的，因此存储时采用前缀压缩，后一个key只存与前一个key不同的部分。因此出现了共享长度和非共享长度。比如说，Record1是 abcd，Record2是abce，那么共享长度就是abc, 非共享长度就是d和e。</p>
<ul>
<li><p>重启点我们看到是一个数组的形式，其指向的位置就表示该key不按前缀压缩，而是完整存储的key值。除了减少压缩空间之外，重启点的第二个作用就是加速读取，<strong>更快的读取</strong>！一般来说一个Block大小为4K，里面可能包含了很多Record。重启点可以通过二分的方法来定位具体的重启点位置，进一步减少了需要读取的数据。如果不设重启点，那么将Block读进内存之后，要找一个特定的record只能通过顺序搜索实现，影响性能</p>
</li>
<li><p>尾部主要分为type和crc。这里的CRC即循环冗余校验码 ( Cyclic Redundancy Check ）：是数据通信领域中最常用的一种查错校验码，其特征是信息字段和校验宇段的长度可以任意选定。循环元余检查（CRC）是一种数<br>据传输检错功能，对数据进行多项式计算，并将得到的结果附在帧的后面，接收设备也执行类似的算法 ，<strong>以保证数据传输的正确性和完整性</strong>。在Sstable中， 写数据时生成CRC， 读时进行校验</p>
</li>
</ul>
<h4 id="Data-Index-Block"><a href="#Data-Index-Block" class="headerlink" title="Data Index Block"></a>Data Index Block</h4><p>因为一个DataBlock的大小只有4KB，但是一个SSTable的大小可能有16MB，因此里面包含了许多DataBlock。将其全部加载到内存里并顺序找过来肯定是不现实的。 因此SSTable中设计了Data Index Block,然后用来实现对DataBlock的二分查找。在查找时，先将Data Index Block其加载到内存中，然后再去磁盘中读取想要的DataBlock</p>
<p>因为我们在DataBlock中，Key都是按照顺序存储的。理论上，只要保存了每个块中最大的Key，就可以实现二分查找，但是LevelDB在这里做了个优化：</p>
<p>它并非保存最大的Key，而是保存了一个能分隔两个DataBlock的<strong>最短Key</strong>。比如，假定DataBlock1的最后一个Key为 <code>abcdefg</code> ,  DataBlock2的第一个Key为 <code>abzxcv</code> , 那么，index就可以记录DataBlock1的索引Key为：<code>abd</code>。这样的分割串可能有很多，只要保证DataBlock1中的所有key都小于等于此索引，DataBlock2中所有Key都大于此索引即可。<strong>这种优化缩减了索引长度，查询时可以有效减少比较次数</strong></p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/35.png" style="zoom:67%;"></p>
<p>我们再举一个例子，如上图，DataBlock1 的最后一个索引是world, DataBlock2的第一个索引是www，我们可以令DataBlock1的索引为 word ，因为 <code>world&lt;word&lt;www</code></p>
<p> 最后，我们将上面讲的综合起来，写成一个细致的SSTable结构：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/36.png"></p>
<h3 id="SSTable文件管理"><a href="#SSTable文件管理" class="headerlink" title="SSTable文件管理"></a>SSTable文件管理</h3><p>在这一块我们来介绍文件合并的相关流程。</p>
<h4 id="文件管理模块"><a href="#文件管理模块" class="headerlink" title="文件管理模块"></a>文件管理模块</h4><p>首先，我们要知道<strong>每个SSTable</strong>都是一个文件。因此有一个文件信息的结构 <code>FileMetaData</code></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">FileMetaData</span> &#123;</span></span><br><span class="line">    <span class="keyword">int</span> refs:								</span><br><span class="line">    <span class="keyword">int</span> allowed seeks:			<span class="comment">// Seeks allowed until compaction</span></span><br><span class="line">    uint64 + number;</span><br><span class="line">    uint64t file <span class="built_in">size</span>;			<span class="comment">// File size in bytes</span></span><br><span class="line">    Internalkey smallest;		<span class="comment">// Smallest internal key served by table</span></span><br><span class="line">    Internalkey largest;		<span class="comment">// Largest internal key served by table</span></span><br><span class="line"></span><br><span class="line">	FileMetaData() : refs(0), allowed seeks (1 &lt;&lt; 30), file size(0) &#123; &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>文件在写入的时候会存在许许多多个版本，因此一个可以进行文件管理的模块是非常重要的</p>
<p>我们引入Version，Version Edit 和Version Set 三个概念：</p>
<h5 id="Version"><a href="#Version" class="headerlink" title="Version"></a>Version</h5><p>每次的文件融合，都会使得SSTable发生变化，每个SSTable的状态，我们称之为一个Version.</p>
<p>Version包含了什么东西？其实，只要记录当前版本的文件信息即可：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span> &lt;FileMetaData*&gt; <span class="comment">//有几层数组就有多长</span></span><br><span class="line">files_[<span class="built_in">config</span>::kNumLevels];</span><br></pre></td></tr></table></figure>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/38.png"></p>
<p>我们看到，其实version就是存储了文件信息的二维数组。为什么是数组？因为SSTable有好几层，每一层的状态都需要被记录下来。</p>
<h5 id="Version-Edit"><a href="#Version-Edit" class="headerlink" title="Version Edit"></a>Version Edit</h5><p>既然Version是保留文件信息的，那么当SSTable发生变化的时候，就需要这样一个文件来记录变化的内容。这个文件就是Version Edit.</p>
<p>其主要元素 就是：删除的文件，以及新产生的文件</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DeletedFileSet deleted_files_;<span class="comment">//删除的文件</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="built_in">std</span>::pair&lt;<span class="keyword">int</span>, FileMetaData&gt;&gt;new_files_;<span class="comment">//新产生的文件</span></span><br></pre></td></tr></table></figure>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/39.png"></p>
<p>Version之间是通过增量的方式演进的，而两个Version之间的差异就称之为一个Version Edit ： <code>Version0+VersionEdit = Version1</code></p>
<h5 id="Version-Set"><a href="#Version-Set" class="headerlink" title="Version Set"></a>Version Set</h5><p>从旧的到新的所有Version都存储在Version Set当中，所以，Version Set可以理解为一个Version的链表。生成新Version并加入到Version Set的流程示意图如下：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/37.jpeg"></p>
<p>此外，VersionSet需要做一个<strong>持久化</strong>，因为，如果数据库重新启动或者恢复的时候，没有VersionSet持久化会导致需要将所有的SSTable文件全部遍历一遍，严重降低启动速度。</p>
<h4 id="Minor-Compaction-流程"><a href="#Minor-Compaction-流程" class="headerlink" title="Minor Compaction 流程"></a>Minor Compaction 流程</h4><p>之前我们说过，Minor Conpaction就是当内存中的immutable memtable大小到了一定值的时候，将其保存到SSTable的Level 0中</p>
<p>每次插入操作，都会检查是否memtable超过限制，超过了就会触发函数，进行compaction</p>
<p>合并流程：</p>
<ul>
<li>构建SSTable</li>
<li>寻找合适的Level</li>
<li>生成Version Edit</li>
<li>应用到当前Version</li>
</ul>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/40.jpeg" style="zoom:67%;"></p>
<h4 id="Major-Compaction-流程"><a href="#Major-Compaction-流程" class="headerlink" title="Major Compaction 流程"></a>Major Compaction 流程</h4><p>大合并的过程就是将Level n层合并成Level n+1层的过程。</p>
<p>首先我们要判定什么时候会触发Major Compaction</p>
<ul>
<li>某个Level的 SSTable太多</li>
<li>某个Levl的某个SSTable不成功seek次数太多。</li>
</ul>
<p>现在开始进行合并，需要确定参与compaction的文件。怎么理解呢？比如说下面这个SSTable，Level1中间的.sst 触发了大合并，和这个文件相关的，是level2中的两个.sst文件，这两个文件和Level1有交集。</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/41.png"></p>
<p>合并文件结束的时候，需要更新Version</p>
<h2 id="并发索引"><a href="#并发索引" class="headerlink" title="并发索引"></a>并发索引</h2><p>在数据库中有两处需要并发：1. 上层的事务的并发2. 数据层面的索引并发。这两者是没有关联的。</p>
<h3 id="Concurrent-Linklist"><a href="#Concurrent-Linklist" class="headerlink" title="Concurrent Linklist"></a>Concurrent Linklist</h3><p>我们用的数据结构就是有序列表， 假定有序链表不允许重复元素。</p>
<p>我们要实现3种方法的并发：</p>
<ul>
<li><code>add(x)</code> 增加一个元素</li>
<li><code>remove(x)</code> 删除一个元素 </li>
<li><code>contains(x)</code> 查找一个元素</li>
</ul>
<p>在链表中，每个节点包括：</p>
<ul>
<li>Key</li>
<li>一个指向后面节点的指针 next</li>
</ul>
<h4 id="并发控制的异常"><a href="#并发控制的异常" class="headerlink" title="并发控制的异常"></a>并发控制的异常</h4><p>要实现并发控制常常从可能出现的异常出发。常见的异常就是：</p>
<ul>
<li>丢失更新</li>
</ul>
<p>丢失更新常常发生在两个插入操作之间的，比如下图，6号先插入，但是后来插入的7号把六号给覆盖了，那么此时就发生了丢失更新</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/43.png"></p>
<ul>
<li>假删除</li>
</ul>
<p>假删除发生在两次remove的时候,如下图，同时删除3和5的情况。</p>
<p>此时对于节点5来说，Prev指针指向3，cur指针指向5；对于节点3来说，Prev指针指向1，cur指针指向5。</p>
<p>如果先删掉5，那么就让 <code>prev-&gt;next = cur-&gt;next</code></p>
<p>在并发时删掉3的话，由于此时对于3号节点的cur先于删掉节点5前获得，因此在执行<code>prev-&gt;next = cur-&gt;next</code> 的时候还是会指向节点5。也就是说5号节点被恢复了。</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/44.png"></p>
<ul>
<li><p>插入节点被删除。</p>
<p>比如我想删除b然后插入c. 但此时，由于b已经删掉了，而c在插入时是连接在b上面的，因此相当于c也被删掉了。</p>
</li>
</ul>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/45.png"></p>
<h4 id="并发控制的方法"><a href="#并发控制的方法" class="headerlink" title="并发控制的方法"></a>并发控制的方法</h4><p>寻求并发控制方法就是防止上述异常的发生。通常来说有这么几种方法：</p>
<h5 id="粗粒度锁"><a href="#粗粒度锁" class="headerlink" title="粗粒度锁"></a>粗粒度锁</h5><p>粗粒度锁就是对add(), remove(), contains()三个操作 加锁，锁住整个数据结构</p>
<ul>
<li>访问链表时加锁，操作完成后释放锁 </li>
<li>最安全  </li>
<li>效率最差</li>
</ul>
<h5 id="细粒度锁-hand-over-hand-locking-或-lock-coupling"><a href="#细粒度锁-hand-over-hand-locking-或-lock-coupling" class="headerlink" title="细粒度锁(hand over hand locking 或 lock coupling)"></a>细粒度锁(hand over hand locking 或 lock coupling)</h5><p>细粒度锁其实这是用一对锁。链表向前推进的过程中，先获取 curr锁，然后再释放prev锁 。数据操作时，<strong>同时锁住前驱和后继</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">head.lock();</span><br><span class="line">pred = head:</span><br><span class="line"><span class="keyword">try</span>&#123;</span><br><span class="line">		curr = pred.next;</span><br><span class="line">		curr.lock();</span><br><span class="line">		<span class="keyword">try</span>&#123;</span><br><span class="line">      <span class="comment">// 推进过程</span></span><br><span class="line">      <span class="keyword">while</span>(curr.key &lt; key)&#123;</span><br><span class="line">          pred.unlock();</span><br><span class="line">          pred = curr;</span><br><span class="line">          curr = curr.next;</span><br><span class="line">          curr.lock();</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="keyword">if</span>(curr.key == key) &#123;</span><br><span class="line">		    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//插入操作</span></span><br><span class="line">    NodeWithLock&lt;T&gt; node = <span class="keyword">new</span> NodeWithLock&lt;T&gt;(item);</span><br><span class="line">    node.next = curr:</span><br><span class="line">    pred.next = node;</span><br><span class="line">    <span class="comment">//插入操作停止</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;<span class="keyword">finally</span>&#123;</span><br><span class="line">		    curr.unlock();</span><br><span class="line">		&#125;</span><br><span class="line">&#125;<span class="keyword">finally</span>&#123;</span><br><span class="line">	pred.unlock();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>比如说我想在节点8和节点10之间加入节点9。那么首先curr指针会指向节点10，pred节点会指向8。在停止推进时，pred和curr都没有解锁。只有当插入操作停止值之后，才依次放开两个锁</p>
<p>使用了这种细粒度锁，可以避免上述异常的发生，我们以并发删除两个节点为例：</p>
<ol>
<li>首先，<code>remove(c)</code> 先操作，锁住哨兵节点</li>
</ol>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/54.png"></p>
<ol>
<li><code>remove(c)</code> 向前推进，同时释放前面的锁。这里红色方锁住了b和a。与此同时，remove(b) 代表的绿色方也开始遍历链表。</li>
</ol>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/47.png"></p>
<ol>
<li>绿色方继续遍历链表，要删除b必须获取b节点上的锁，但是该锁现在在红色方手中。因此绿色方只能等待</li>
</ol>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/48.png"></p>
<ol>
<li>红色方继续遍历链表，获取了b节点和c节点上的锁</li>
</ol>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/49.png"></p>
<p>从这里也可以看出，如果只锁住了一个节点是行不通的。加入红方只锁了c，前面b没有锁，那么绿色方就可能获取到b的锁。那么此时红方和绿方的删除操作就可以同时进行了</p>
<p>如果我们只锁了b，那么如果存在另外一个删除操作transact2想要删除d节点，transct2就会锁上C。如果tran2先执行，那么它会让 <code>c-&gt;next = d-&gt;next</code> 也就是e节点；然后再执行我们的删除c操作：<code>b-&gt;next = c-&gt;next</code> ,也就是d节点。事实上，最终的结果并不是 b节点指向e节点，而是b指向d，c指向e。</p>
<ol>
<li>红色方执行删除操作</li>
</ol>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/50.png"></p>
<ol>
<li>红色方完成删除操作，释放锁，因此绿色方可以继续遍历</li>
</ol>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/51.png"></p>
<ol>
<li>绿色方获得了a和b节点的锁，可执行删除操作</li>
</ol>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/52.png"></p>
<ol>
<li><p>删除完成</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/53.png"></p>
</li>
</ol>
<h5 id="乐观锁"><a href="#乐观锁" class="headerlink" title="乐观锁"></a>乐观锁</h5><p>之前的Lock Coupling需要不停的去获得和释放锁 </p>
<p>那么我们可不可以这样？<strong>只在需要加锁的时候再加锁</strong></p>
<ol>
<li>只有在寻找到要加锁位置的时候才加锁，之前不加锁 </li>
<li>需要加锁时，先加锁，再进行验证是否现场已经被修改 </li>
<li>如果验证失败就需要从头开始重试</li>
</ol>
<h5 id="Lazy-List"><a href="#Lazy-List" class="headerlink" title="Lazy List"></a>Lazy List</h5><p>对于乐观锁，虽然很乐观但还是需要加锁，有可能在验证时失败了需要重试，但是对于Lazy List，它使用<strong>标记删除 </strong>法</p>
<p>标记删除法的意思就是先做逻辑标记，然后在标记后进行删除。</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/56.png"></p>
<h5 id="无锁编程"><a href="#无锁编程" class="headerlink" title="无锁编程"></a>无锁编程</h5><p>这几种方法并发能力越来越强，但是实现起来复杂度也越来越高</p>
<h3 id="Concurrent-Index"><a href="#Concurrent-Index" class="headerlink" title="Concurrent Index"></a>Concurrent Index</h3><p>现在我们来看看B+树上并发控制的一些技术。</p>
<ul>
<li><p>正确的读操作：</p>
<ul>
<li><p>Read Case1:  <strong>不会读到一个处于中间状态的键值对</strong>：读操作访问中的键值对正在被另一个写操作修改 </p>
</li>
<li><p>Read Case2: <strong>不会找不到一个存在的键值对</strong>：读操作正在访问某个树节点，这个树节点上的键值对同时被另一个写操作（分裂/ 合并操作）移动到另一个树节点，导致读操作没有找到目标 键值对 </p>
</li>
</ul>
</li>
<li><p>正确的写操作： </p>
<ul>
<li>Write Case1 两个写操作不会同时修改同一个键值对</li>
</ul>
</li>
</ul>
<p>读和写的第一种情况是常见的读写冲突和写写冲突，如果是单版本存储，那么几乎任何时候都会存在。但是对于读的第二种情况，是B+树特殊的并发控制需求。</p>
<p>场景： 考虑一个读操作刚拿到一个叶子节点的指针，打算访问这个节点后半部分的数据，此时一个写操作发生使 得节点分裂，那么后半部分数据将分裂到另一个节点上， 因此读操作的指针无法获得目标数据。<br>这时候，简单的对数据加锁无法保证R2的需求。</p>
<p>如下图，P1是一个查找操作，需要找的内容是15；P2是一个插入操作，需要插入9。这两者毫不相关，但是在某些情况下却会造成冲突。</p>
<p>在P1已经拿到了指针，需要读取15的时候，却被P2操作“捷足先登”了，但是因为插入了9号，导致下一层节点发生裂变。这时候原指针指向的区域就没有15了，因为15到可新的孩子节点中去了。</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/57.png"></p>
<p>思路和之前的有向链表是类似的。</p>
<h5 id="粗粒度锁："><a href="#粗粒度锁：" class="headerlink" title="粗粒度锁："></a>粗粒度锁：</h5><p>对于整个索引上锁(对整个B+树数据结构上锁)</p>
<h5 id="细粒度锁"><a href="#细粒度锁" class="headerlink" title="细粒度锁"></a>细粒度锁</h5><p>不再对整个索引加锁，使用节点粒度的S/X锁(S锁：共享锁，用于读操作；X锁: 排他锁，用于写操作)</p>
<ul>
<li>写操作</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">current &lt;= root</span><br><span class="line">XL(current)</span><br><span class="line">While current is not leaf <span class="keyword">do</span> &#123;</span><br><span class="line">    XL(current-&gt;son)</span><br><span class="line">    current &lt;= current-&gt;son</span><br><span class="line">    If current is safe <span class="keyword">do</span> &#123; <span class="comment">//如何判断？</span></span><br><span class="line">        <span class="comment">/* Unlocked ancestors on stack. */</span></span><br><span class="line">        XU(locked ancestors)</span><br><span class="line">    &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="comment">/* Already lock the modified branch. */</span></span><br><span class="line"> Modify the leaf and upper nodes</span><br><span class="line"> XU(current) <span class="function">and <span class="title">XU</span><span class="params">(locked ancestors)</span></span></span><br></pre></td></tr></table></figure>
<p>其实这个操作和链表中的细粒度锁就很像了，在链表中是前后节点上锁，在B+树中就是对父子节点上锁</p>
<ul>
<li>读操作</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Algorithm4. 读操作 */</span></span><br><span class="line">current &lt;= root</span><br><span class="line">SL(current)</span><br><span class="line">While current is not leaf <span class="keyword">do</span> &#123;</span><br><span class="line">    SL(current-&gt;son)</span><br><span class="line">    SU(current)</span><br><span class="line">    current &lt;= current-&gt;son</span><br><span class="line">&#125;</span><br><span class="line">Read the leaf node</span><br><span class="line">SU(current)</span><br></pre></td></tr></table></figure>
<h5 id="乐观读"><a href="#乐观读" class="headerlink" title="乐观读"></a>乐观读</h5><p>使用一个节点的版本标记判断节点是否更新 – 减少R1带来的读操作带来的锁或latch开销.</p>
<p>B+树乐观读和链表中的乐观读</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">procedure <span class="title">traverse</span><span class="params">(root, key)</span></span>&#123;</span><br><span class="line">		node = root;</span><br><span class="line">    <span class="keyword">while</span> (node is not a leaf)&#123;</span><br><span class="line">        t = turn-off-LSB (node.ccinfo);</span><br><span class="line">        next = <span class="function">find <span class="title">next</span> <span class="params">(node, key)</span></span></span><br><span class="line"><span class="function">        <span class="title">if</span> <span class="params">(node.ccinfo = t)</span> node </span>= next</span><br><span class="line">  &#125;</span><br><span class="line">		<span class="keyword">return</span> node ; <span class="comment">//判断节点是否被修改过</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="在多核系统下加锁方式效率低"><a href="#在多核系统下加锁方式效率低" class="headerlink" title="在多核系统下加锁方式效率低"></a>在多核系统下加锁方式效率低</h4><p>在多核系统下，CPU的性能是主导的，IO性能反而没那么重要. 因为多核情况下每个核有缓存，每次加锁解锁都需要读入内存中去。而且每个核的缓存中的变量需要要求保持一致，因此，会出现 Cache Coherence的问题, 由此导致CPU效率低下。</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/58.png"></p>
<p>繁加锁操作在多核处理器上会产生Coherence Cache Miss过高的问题。以上图为例， 假设有4个处理器（p1/p2/p3/p4），每个处理器分别有自己的private cache （c1/c2/c3/c4）。假设有4个线程（p1/p2/p3/p4），与处理器一一绑定。下文中的 n1/n2/n3/n4/n5/n6/n7可以指的是树节点的锁，也可以指代树节点。为什么频繁加 锁会引入较高的Coherence Cache Miss开销？ </p>
<ul>
<li>p1访问树节点n1/n2/n4，然后将它们放在缓存c1； </li>
<li>p2访问树节点n1/n2/n5，然后将它们放在缓存c2； </li>
<li>p2修改的S锁会导致缓存c1中的n1/n2失效； </li>
<li>注意即使缓存c1中有足够大的空间，这些缓存缺失操作依然会发生；</li>
<li>p3访问树节点n1/n3/n6，然后导致缓存c2中的n1失效；</li>
<li>p4访问树节点n1/n3/n7，然后导致缓存c3中的n1/n3失效；</li>
</ul>
<h4 id="消除R2所带来的加锁-Blink-Tree"><a href="#消除R2所带来的加锁-Blink-Tree" class="headerlink" title="消除R2所带来的加锁-Blink Tree"></a>消除R2所带来的加锁-Blink Tree</h4><ul>
<li>这个Blink Tree和普通B+树不同，在每个节点添加一个 右指针，指向右兄弟节点</li>
<li>一旦search的key比节点最高值大，则使用右指针访问右兄弟而不是孩子 ，可以在这一层中查找。</li>
<li>假定节点写操作是原子的，那么search操作无需加锁</li>
</ul>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/59.png"></p>
<h4 id="SkipList上的并发控制问题"><a href="#SkipList上的并发控制问题" class="headerlink" title="SkipList上的并发控制问题"></a>SkipList上的并发控制问题</h4><p>我们仅关注插入和查找这两个问题。</p>
<p>首先我们讨论并发操作对数据结构本身的正确 性是否会产生破坏</p>
<ul>
<li><p>插入/插入操作 </p>
<ul>
<li>丢失更新的问题 </li>
</ul>
</li>
<li><p>查找/插入操作 </p>
<ul>
<li>插入一个数据，有没有可能让另一个已经存在数 据读取失败</li>
</ul>
</li>
</ul>
<p>其次，考虑查找/插入操作的一致性</p>
<p>比如说，在下面这个跳表中，我们插入17，有无可能让19或者12的查找失败？</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/60.png"></p>
<p>那就要看，新插入的节点的指针，是从下层往上层更新还是从上层往下层更新了。</p>
<p>如果是从上往下更新。那么查找12的时候，还是能找到的。</p>
<p>但是如果需要查找19，此时17的第二层指针已经更新，因此判断出19应该在17-25之间，因此下沉继续查找。但是下层指针还没有更新，那么就找不到19了。</p>
<p>如果从下往上更新，那么12、19都是可以找到的。</p>
<p>我们在来看看插入、查找操作的一致性问题。如果考虑插入17和查找17两者的并发，那么理论上，线性一致性可以原子化，称为时间轴上的一个点，以timestamp的形式存储。</p>
<p>假设插入17的timestamp是A, 查找是B </p>
<ul>
<li>如果A&gt;B , 那么查找17将返回找不到 </li>
<li>如果A&lt;B, 那么查找17能找到</li>
</ul>
<p>假定我们定插入/查找的timestamp分配时 间是他们操作开始的时候，是否能做到线性一致？</p>
<ul>
<li><p>如果A&gt;B, 一定能做到，因为可通过版本号直接判断 </p>
</li>
<li><p>如果A&lt;B(插入在前)，有可能无法保证，因为实际插入操作可能迟迟没有发生</p>
</li>
</ul>
<p>因此，上述定timestamp的方式无法做到线性一致</p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="SnapShot"><a href="#SnapShot" class="headerlink" title="SnapShot"></a>SnapShot</h3><p>我们需要了解SnapShot的概念：即系统在某个时间点的一个<strong>全量状态</strong>。比如我需要出2011的资产负债表，这需要数据保持在2011年12月31日零点时的状态。</p>
<p>快照,其本质类似于数据库的照片,也就是在某个特定时间点(创建快照的时间点)给数据库拍个照放在那儿.但是这个照片是一个新的数据库,可以应用SQL语句. </p>
<p>快照数据库里的数据是不变的.创建快照后,系统会对原数据库的所有数据页做个标识,如果数据页在创建快照后被修改,会复制一个数据页出来,没有修改的数据页则不会有快照(原数据库和快照数据库共用该数据页).</p>
<p>从这样来看,快照存在的时间越长,对系统的压力会越大(要维护的变化数据页太多). </p>
<h3 id="Memory-Allocation"><a href="#Memory-Allocation" class="headerlink" title="Memory Allocation"></a>Memory Allocation</h3><p>在系统实现的时候还需要考虑<strong>分配内存</strong>这个重要因素。</p>
<p>数据库里的内存分配和算法题中的内存分配完全不一样，通常会有一个完整的内存管理模块来统一管理内存。这个模块会一次申请大块的内存，多次分给客户，这样可以避免new/delete多次的申请和释放引起的内存碎片。</p>
<p>在一个内存管理模块中，通常需要两个函数：Allocate函数和AllocateAligned函数</p>
<ul>
<li>Allocate函数</li>
</ul>
<p>如下所示，管理器会先申请一大块内存，默认每次申请的大小为4KB，同时记录下剩余指针和剩余内存字节数量。这样可以让申请到的内存在一个连续的空间里面。</p>
<p>然后，每当有函数向管理器发送申请，如果当前剩余的字节能满足需要，则直接返回给用户；如果不能，对于超过1KB的请求，直接new返回(左边第二个红块)，小于1KB的请求，管理器则会向内存申请一个新的4KB块并从中分配一部分给用户。</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/42.png"></p>
<ul>
<li>AllocateAligned函数</li>
</ul>
<p>这个函数的作用就是做到地址对齐，让内存首地址是4或者8的倍数</p>
<h3 id="Bloomfilter"><a href="#Bloomfilter" class="headerlink" title="Bloomfilter"></a>Bloomfilter</h3><p>可参考这篇文章： <a href="https://jasonxqh.github.io/2021/09/24/数据科学与工程算法-哈希/#布隆过滤器">数据科学与工程算法-哈希：布隆过滤器</a></p>
<h3 id="Second-Index"><a href="#Second-Index" class="headerlink" title="Second Index"></a>Second Index</h3><p>什么是二级索引？二级索引就是定义在非主键上的索引，这在数据库中有很广泛的应用。</p>
<p>二级索引的作用是什么？为非主键提供数据访问的有效路径</p>
<p>二级索引的几种选择：可以是B+树，Block Index等</p>
<p>其实有一种更简单的方式：把二级索引看做是一张特殊的表——索引表</p>
<ul>
<li>比如说有数据表&lt;主键，非主键列1，非主键列2，非主键列3&gt;, 我们要在非主键列1上构建二级索引。我们就可以建一张索引表：&lt;非主键列1(作为主键)，主键列(作为 非主键)&gt;。这样，非主键列1自动就会有索引效果了。</li>
<li>当我们要查找非主键列的时候，先查询索引表，然后依靠索引表上的主键去找到主键列对应的数据，再根据主键列的值去原本的数据表中查找。</li>
</ul>
<p><strong>回表查询VS非回表查询</strong></p>
<ul>
<li>非回表查询：<strong>只查询索引表</strong>就能得到想要的结果。比如说只查找非主键列1</li>
<li>回表查询：<strong>需要通过索引表</strong>再返回数据表查询才能得到查询结果。比如说根据非主键列1去找到非主键列2</li>
</ul>
<h4 id="二级索引的更新"><a href="#二级索引的更新" class="headerlink" title="二级索引的更新"></a>二级索引的更新</h4><ul>
<li>同步更新<ul>
<li>在主表更新时一起更新</li>
</ul>
</li>
<li>异步更新<ul>
<li>在主表更新之后，查询之前更新索引表</li>
</ul>
</li>
</ul>
<p>这就引入了一致性问题，通常大多数数据库都是采用同步更新的方式。因此索引越多，数据库写入的速度也就会越慢</p>
<h3 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h3><p>压缩在数据库中是很常见的技术。可以参考博客：<a href="https://devopedia.org/database-compression#:~:text=While most compression algorithms are,techniques used in database applications." target="_blank" rel="noopener">Compression</a></p>
<p>压缩可以通过两种主要方式实现：</p>
<ul>
<li><strong>无损</strong>：可以从压缩数据中完全重建原始数据。</li>
<li><strong>有损</strong>：由于故意降低质量而减小数据</li>
</ul>
<p>我们用几个问题来学习一下数据库中的压缩技术</p>
<h4 id="数据库压缩有什么好处"><a href="#数据库压缩有什么好处" class="headerlink" title="数据库压缩有什么好处?"></a>数据库压缩有什么好处?</h4><p>大多数压缩算法都会构建一个<strong>内部编码字典</strong>来管理压缩关键字。<strong>当数据库大小较小</strong>时，由于创建了额外的字典，压缩文件可能比未压缩文件大。</p>
<p>对于任何数据库，<strong>压缩和解压缩都是</strong>高于其常规 DML/DDL 操作的任务开销，它消耗额外的 CPU/内存。所以必须是只有当优化页面读取所带来的CPU/内存收益远远大于压缩开销时，才可以尝试压缩。</p>
<p>虽然压缩可能作为后台任务并行发生，但<strong>解压缩会引入客户端延迟</strong>，因为它在客户端查询后会在前端进行解压缩。但是，像 Oracle 这样的商业数据库支持高级压缩技术，无需解压缩即可读取数据库。</p>
<p><strong>数据基数较差时</strong>不推荐压缩。对于数字数据和非重复字符串，不要压缩数据。对于数据<code>BLOB</code>（图像、音频）等数据类型，根据压缩算法，存储大小可以减少或增加。</p>
<p>总之，在实时数据库上实施之前，估计任何算法可能的存储和性能优势非常重要。</p>
<h4 id="压缩对常见数据类型有什么影响"><a href="#压缩对常见数据类型有什么影响" class="headerlink" title="压缩对常见数据类型有什么影响?"></a>压缩对常见数据类型有什么影响?</h4><p>压缩是一种 DDL 功能，可以使用<code>CREATE</code>,<code>ALTER</code>和<code>BACKUP</code>命令选择性地应用于表、索引或分区。</p>
<p>数据压缩适用于这些数据库对象——堆、聚集索引、非聚集索引、分区、索引视图。</p>
<p>行级压缩将固定长度数据类型转换为可变长度类型。因此，这适用于固定长度的文本和数字字段 ( <code>char</code>, <code>int</code>, <code>float</code>)。</p>
<ul>
<li>例如，将 23 存储在一<code>int</code>列中，压缩时只需要 1 个字节，而不是分配的全部 4 个字节。</li>
<li>char(100) 也不会占用100个字符空间</li>
<li>NULL 或 0 值不占用空间。</li>
</ul>
<h4 id="有哪些流行的数据库压缩技术"><a href="#有哪些流行的数据库压缩技术" class="headerlink" title="有哪些流行的数据库压缩技术?"></a>有哪些流行的数据库压缩技术?</h4><p>数据库通常避免在后端进行有损压缩。在无损技术中，所有压缩数据都以二进制形式存储。</p>
<ul>
<li>Run-Length Encoding。在这种技术中，会扫描顺序数据以查找重复符号，例如图像中的像素，并用称为“运行”的短代码替换。比如说，对于灰度图像， run-length code 可以表示为 ${S_i,C_i}$,其中<code>Si</code>是像素的符号或强度，<code>Ci</code>是 的计数或重复次数<code>Si</code>。</li>
<li>Prefix Encoding。之前我们在SSTable中介绍过这种方式，是另一种<strong>仅适用于同类数据的基本技术</strong>。数据字段的第一部分匹配通用性，而不是整个字段。因此我们将相同的前缀存储起来，做为编码。</li>
<li>Dictionary compression。字典压缩是另一种应用于页面的压缩技术。该算法计算对列的单个属性进行编码所需的位数 (X)，然后会计算这X位中有多少可以放入1、2、3、4个字节中，进行压缩。</li>
</ul>
<h4 id="数据库压缩与通用数据压缩技术有何不同"><a href="#数据库压缩与通用数据压缩技术有何不同" class="headerlink" title="数据库压缩与通用数据压缩技术有何不同?"></a>数据库压缩与通用数据压缩技术有何不同?</h4><p>通用数据压缩可以使用于字段，比如说，将单个数据项占用的空间最小化；但是，数据库中的压缩方法是一种<strong>aggregate technique(聚合技术)</strong>  ，它是将跨行、跨列、跨页面的数据进行压缩。</p>
<p>除了在数据库级别应用压缩外，在字段级别应用数据压缩也很常见：</p>
<ul>
<li>对于图像压缩，可以使用BMP到JPG的有损压缩。</li>
<li>又如可以用无损方法在DB级别压缩BLOB数据字段。</li>
<li>虽然NULL值无法单独进一步压缩，但是当DB中出现一系列NULL值时，Sparse Column等压缩技术可以共同优化NULL的存储</li>
</ul>
<p>数据库压缩技术不仅压缩实际数据，它们还作用于派生实体，如索引、视图、集群和堆。但是，对于索引等派生实体，在索引查找或扫描期间重建键列值的 CPU 开销很小。这可以通过在块中本地保留前缀来最小化。</p>
<h4 id="关系数据库和分层数据库的压缩技术"><a href="#关系数据库和分层数据库的压缩技术" class="headerlink" title="关系数据库和分层数据库的压缩技术"></a>关系数据库和分层数据库的压缩技术</h4><p>应用于关系数据库和Level DB的压缩算法没有很大的区别，只是命名方式有所不同。</p>
<p>Level DB 具有类似于 RDBMS 中的表的段。段中的字段与列进行比较。</p>
<p>但是，在分层数据库中，段之间是隐式连接的。因此，I/O 分页过程会有所不同，因此算法必须考虑页面大小。</p>
<h4 id="大型软件应用程序如何处理数据库压缩"><a href="#大型软件应用程序如何处理数据库压缩" class="headerlink" title="大型软件应用程序如何处理数据库压缩?"></a>大型软件应用程序如何处理数据库压缩?</h4><ul>
<li>Facebook 使用具有超快速解码器的实时压缩算法<strong>Zstandard</strong>以及用于小数据的字典压缩。</li>
<li>Google 在其 BigTable 和 MapReduce 系统中使用<strong>Snappy</strong>，这是一种高速压缩算法。</li>
<li>像SQL Server这样的商业 DBMS使用行、列和页面级别的压缩。SQL Server 使用专有的<strong>XPRESS</strong>算法进行备份压缩。</li>
<li><strong>Oracle 使用称为Oracle Advanced Compression</strong>的内部压缩技术。</li>
<li>开源 MySQL 使用<strong>LZ77</strong>算法进行 InnoDB 表压缩。</li>
</ul>
<h3 id="Design-Consideration"><a href="#Design-Consideration" class="headerlink" title="Design Consideration"></a>Design Consideration</h3><ul>
<li>Should be aware of the storage hierarchy </li>
</ul>
<p>对存储的层次结构(CPU缓存，内存，磁盘)要有很深刻的概念。因为数据库是数据密集型的应用，而不是计算密集型的应用。如何提高IO性能是我们要研究的方向。</p>
<ol>
<li>Database is usually data-intensive</li>
<li>cache miss is important 缓存命中率是很重要的</li>
</ol>
<ul>
<li>Pay attention to the hardware properties </li>
</ul>
<ol>
<li>Sequential access and random access is usually different</li>
<li>bandwith and IOPS</li>
</ol>
<ul>
<li>Design Issues 设计时的问题</li>
</ul>
<ol>
<li>choose a proper index</li>
<li>support variable length data</li>
<li>for most secondary storage, try avoid random writes</li>
<li>reduce the write amplication</li>
<li>group your writes</li>
</ol>
<ul>
<li>Optimization On LSM-tree 优化LSMT的性能</li>
</ul>
<ol>
<li>optimize write amplication</li>
<li>optimize read performance</li>
<li>other optimizations: cache, special workload</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/61.png"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2022/03/02/%E5%BD%93%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BDLab1-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/02/%E5%BD%93%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BDLab1-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" class="post-title-link" itemprop="url">当代人工智能Lab1-文本分类</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-02 13:48:16" itemprop="dateCreated datePublished" datetime="2022-03-02T13:48:16+08:00">2022-03-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-03-08 22:34:06" itemprop="dateModified" datetime="2022-03-08T22:34:06+08:00">2022-03-08</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="当代人工智能Lab1-文本分类"><a href="#当代人工智能Lab1-文本分类" class="headerlink" title="当代人工智能Lab1-文本分类"></a>当代人工智能Lab1-文本分类</h1><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><ul>
<li>python3.8</li>
<li>Tensorflow-text 2.8.*</li>
<li>Tf-models-offiial 2.7.0</li>
</ul>
<h2 id="模型选择与构建"><a href="#模型选择与构建" class="headerlink" title="模型选择与构建"></a>模型选择与构建</h2><h3 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h3><p>这里我选择的是BERT模型，它是最近几年比较流行的用来做自然语言处理的模型。BERT主要采用了双向编码器架构，因此相较于RNN，LSTM模型来说，有着更强的并发能力、能在多个不同层次提取关系特征，进而更全面地反映出句子的语义。</p>
<p>此外，BERT对TensorFlow的支持也很全面(都是谷歌开发的)，甚至可以直接通过API来下载预训练模型。因此只要选择合适的模型与环境，就能迅速地实现数据训练，事实上我们所做的就是对预训练模型中的权重进行微调。</p>
<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>首先我们拿到的训练数据是 json格式的文档。为了能让Pandas、TensorFlow导入方便,我们需要将其转换为csv文件。因为python比较缺乏成熟的自动化转换库，因此我们采用了比较偷懒的方法——把json导入mongodb，再以csv的格式导出。</p>
<p>此外，测试集由于给的是txt格式的文档，这里我用了python程序将其转换为一个csv文件。</p>
<p>这两个文件分别为<code>lab1_test.csv,lab1_train.csv</code> ，在根目录下可以找到。</p>
<p><img src="/2022/03/02/%E5%BD%93%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BDLab1-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/2.png"></p>
<p>接下来，我们需要对label进行独热编码，因为我们不能用连续的数字来表示标签，否则会导致语义上的一些差别。可以用<code>tf.keras.utils.to_categorical()</code>这个函数来操作</p>
<p>最后，我们使用<code>train_test_split</code> 对数据集和标签进行 分割，变成训练集和测试集(实际上是验证集)。</p>
<h3 id="模型的构建"><a href="#模型的构建" class="headerlink" title="模型的构建"></a>模型的构建</h3><ul>
<li><p>首先，我们需要用到Bert的文本预处理层: <a href="https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3" target="_blank" rel="noopener">下载地址</a>。这个预处理层从维基百科和BooksCorpus中提取英文单词并可以将每一行输入文字转换为一个<code>shape=(1,128)</code>的向量。这个向量将被作为下一层Transformer Encoder的输入。</p>
</li>
<li><p>然后，我们需要用到Bert的预训练模型：<a href="https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1" target="_blank" rel="noopener">下载地址</a> 。由于我们要处理的数据集的数据量并不是很大(8000条数据)，因此我们选择的是一个小型的Bert模型——具有4个隐藏层，每个隐藏层大小为512；8个注意力头。</p>
<p>这一层有两个输出：<code>pooled_output</code>和<code>sequence_output</code>,前者是句子的向量表示，大小为[batch size,512]; 后者代表句子中每个词语的向量表示，大小为[batch size,seq_length,512]. 在这里，我们选择前者作为下一层的输入。</p>
</li>
<li><p>接下来的Drop out主要是用来正则化的，来让模型简化，减小冗余，我们不希望很多神经元去学习同一个特征，我们希望神经网络中的神经元有自己的“特长”，各自负责各自的特征。这样就可以减少过拟合发生的概率。我们将随机丢失神经元的概率设置为20%</p>
</li>
<li><p>最后，我们添加一个全连接层。由于这个任务要求单标签多分类，一共有10类文本，因此我们要将输出向量设为(1,10)。同时设置激活函数为softmax</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_classifier_model</span><span class="params">()</span>:</span></span><br><span class="line">    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name=<span class="string">'text'</span>)</span><br><span class="line">    <span class="comment"># 文本预处理</span></span><br><span class="line">    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name=<span class="string">'preprocessing'</span>)</span><br><span class="line">    <span class="comment"># Bert模型编码器</span></span><br><span class="line">    encoder_inputs = preprocessing_layer(text_input)</span><br><span class="line">    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=<span class="literal">True</span>, name=<span class="string">'BERT_encoder'</span>)</span><br><span class="line">    outputs = encoder(encoder_inputs)</span><br><span class="line">    net = outputs[<span class="string">'pooled_output'</span>]</span><br><span class="line">    <span class="comment">#Dropout层</span></span><br><span class="line">    net = tf.keras.layers.Dropout(<span class="number">0.2</span>)(net)</span><br><span class="line">    <span class="comment">#全连接层</span></span><br><span class="line">    net = tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>, name=<span class="string">'output'</span>)(net)</span><br><span class="line">    <span class="keyword">return</span> tf.keras.Model(text_input, net)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/03/02/%E5%BD%93%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BDLab1-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/1.png" style="zoom:67%;"></p>
<p>在这里我们就可以利用预处理层和编码器层来查看不同类别的文本之间的相似程度：我选择了来自0、1、2三类各两条数据绘制成热力图。我们发现，相同类之间的颜色确实会更深一些——(1,1)-(2,2)这一个正方形；(3,3)-(4,4)这一个正方形，都比周围的色块要深。因此可以定性得说明经过bert模型编码之后，可以有效地反映出文本之间的相似程度。</p>
<p><img src="/2022/03/02/%E5%BD%93%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BDLab1-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/heatmap.png" alt="heatmap" style="zoom:67%;"></p>
<h3 id="模型的训练"><a href="#模型的训练" class="headerlink" title="模型的训练"></a>模型的训练</h3><h4 id="Optimizer选择与策略"><a href="#Optimizer选择与策略" class="headerlink" title="Optimizer选择与策略"></a>Optimizer选择与策略</h4><p>现在表现较好、较为流行的梯度下降策略就是adam算法了，在这里我没有使用<code>Tensorflow.train.AdamOptimizer</code>   。而使用的是<code>official.nlp</code>中的<code>optimization</code>模块。相比前者，这个模块创建的优化器可以提供更多的功能，而且对Bert模型有良好的支持。</p>
<p>在这个模块中，我们除了需要设置最基本的初始学习率之外，还要输入总的训练步长<code>num_train_steps</code>，以及预热步数<code>num_warmup_steps</code>。学习率预热是指在训练初期选择一个较小的学习率，然后在训练一定步数之后使用预先设置好的学习率。因为刚开始的时候模型是随机初始化权重，如果使用较大的学习率，会让模型不稳定，所以可以在一定训练步数内，使用较小的学习率，模型可以慢慢稳定，然后使用之前设定的学习率。 </p>
<p>把学习率预热和adam算法结合起来，学习率走势如下所示：</p>
<p><img src="/2022/03/02/%E5%BD%93%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BDLab1-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/3.jpeg" style="zoom:67%;"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">epochs = <span class="number">10</span></span><br><span class="line">steps_per_epoch = <span class="number">120</span></span><br><span class="line">num_train_steps = steps_per_epoch * epochs</span><br><span class="line">num_warmup_steps = int(<span class="number">0.1</span>*num_train_steps)</span><br><span class="line">init_lr = <span class="number">3e-5</span></span><br><span class="line">optimizer = optimization.create_optimizer(init_lr=init_lr,</span><br><span class="line">                                          num_train_steps=num_train_steps,</span><br><span class="line">                                          num_warmup_steps=num_warmup_steps,</span><br><span class="line">                                          optimizer_type=<span class="string">'adamw'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="引入早停机制"><a href="#引入早停机制" class="headerlink" title="引入早停机制"></a>引入早停机制</h4><p>为了避免模型过拟合，这里我设置了早停。对val_loss进行观测，如果连续三轮没有降低0.01、或者连续三轮没有降低到0.25以下的话，训练就会自动停止，并将val_loss最低(表现最好)的那轮的结果保存下来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置早停</span></span><br><span class="line">earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor = <span class="string">'val_loss'</span>,</span><br><span class="line">                                                      min_delta=<span class="number">0.01</span>, </span><br><span class="line">                                                      baseline=<span class="number">0.25</span>,</span><br><span class="line">                                                      patience=<span class="number">3</span>,</span><br><span class="line">                                                      restore_best_weights=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h4 id="评价函数的设置"><a href="#评价函数的设置" class="headerlink" title="评价函数的设置"></a>评价函数的设置</h4><p>此外，在训练过程中我们还需要记录一些实时观测的值。也就是设置metrics。首先，我们要看的一个指标就是预测的准确率，即<code>tf.keras.metrics.CategoricalAccuracy</code></p>
<p>此外，我们还要保留召回率(recall)、精确率(precision)、以及 f1值</p>
<p>召回率计算公式：$R = \frac{TP}{TP+FN}$,  TP+FN: 也就是全体完全满足文本标注的文本的条数 ; TP：被预测正确的文本个数</p>
<p>精确率计算公式：$P = \frac{TP}{TP+FP}$ ,TP+FP: 也就是全体被预测为正确的文本个数，TP: 被预测正确的文本个数</p>
<p>F1 值是precision和recall调和平均值：$F_1 = \frac{2TP}{2TP+FP+FN}$</p>
<h4 id="模型的编译"><a href="#模型的编译" class="headerlink" title="模型的编译"></a>模型的编译</h4><p>这里我们要用<code>model.compile()</code>方法, 其作用就是在训练之前告知模型训练时用的优化器、损失函数、准确率评测标准(metrics)。在这里，我们使用多分类的交叉熵作为损失函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">METRICS = [</span><br><span class="line">      tf.keras.metrics.CategoricalAccuracy(name=<span class="string">"accuracy"</span>),</span><br><span class="line">      balanced_recall,</span><br><span class="line">      balanced_precision,</span><br><span class="line">      balanced_f1_score</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">classifier_model.compile(optimizer=optimizer,</span><br><span class="line">                         loss=<span class="string">"categorical_crossentropy"</span>,</span><br><span class="line">                         metrics=METRICS)</span><br></pre></td></tr></table></figure>
<h4 id="模型的训练-1"><a href="#模型的训练-1" class="headerlink" title="模型的训练"></a>模型的训练</h4><p>在训练阶段，我们需要用到<code>model.fit</code> ，除了传入训练集、测试集之外，还需要设置训练集和测试集batch的大小、训练的代数、每一代训练的步数、以及是否开启多核处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">history = classifier_model.fit(x_train,  </span><br><span class="line">                               y_train, </span><br><span class="line">                               validation_data = (x_test, y_test),</span><br><span class="line">                               steps_per_epoch=steps_per_epoch,</span><br><span class="line">                               batch_size=<span class="number">10</span>,</span><br><span class="line">                               validation_batch_size = <span class="number">10</span>,</span><br><span class="line">                               callbacks = [earlystop_callback],</span><br><span class="line">                               use_multiprocessing=<span class="literal">True</span>, <span class="comment"># 使用多核，能让CPU占用率达到90%以上</span></span><br><span class="line">                               epochs=epochs)</span><br></pre></td></tr></table></figure>
<h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><p>模型训练完之后，我们可以对测试集进行预测，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预测函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_class</span><span class="params">(model,reviews)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [np.argmax(pred) <span class="keyword">for</span> pred <span class="keyword">in</span> model.predict(reviews)]</span><br><span class="line"><span class="comment"># 对text进行预测</span></span><br><span class="line">y_pred = predict_class(classifier_model,test_data[<span class="string">"text"</span>])</span><br></pre></td></tr></table></figure>
<p>最后，为了模型能继续使用，我们可以把模型保存下来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">classifier_model.save(<span class="string">"./models/text_classifier_v1"</span>)</span><br></pre></td></tr></table></figure>
<p>如果我们想继续预测别的测试集，可以这样来导入模型，并做训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">new_model = tf.keras.models.load_model(<span class="string">"./models/text_classifier_v1"</span>,</span><br><span class="line">                                       custom_objects=&#123;<span class="string">'balanced_recall'</span>:balanced_recall,</span><br><span class="line">                                                      <span class="string">'balanced_precision'</span>:balanced_precision,</span><br><span class="line">                                                      <span class="string">'balanced_f1_score'</span>:balanced_f1_score,</span><br><span class="line">                                                       <span class="string">'KerasLayer'</span>:hub.KerasLayer,</span><br><span class="line">                                                       <span class="string">'AdamWeightDecay'</span>: optimizer</span><br><span class="line">                                                      &#125;)</span><br><span class="line">[np.argmax(pred) <span class="keyword">for</span> pred <span class="keyword">in</span> new_model.predict(reviews)]</span><br></pre></td></tr></table></figure>
<h2 id="问题与困境"><a href="#问题与困境" class="headerlink" title="问题与困境"></a>问题与困境</h2><ol>
<li>环境的安装</li>
</ol>
<p>首先是TensorFlow那数不清的版本，因为现在TensorFlow1.x已经没啥人用了，但是我原来的python3.7里面装的是TensorFlow1.x的版本。重装以后又发现TensorFlow2.8所依赖的库python3.7并不支持。因此干脆直接新建了一个python3.8的环境专门用来跑项目。</p>
<p>其实性价比更高的方法是创建虚拟环境，管理卸载起来也并不麻烦。但由于我选择在jupyter下跑，因此在anaconda中新建环境更加方便。</p>
<ol>
<li>预处理</li>
</ol>
<p>在一开始我选择的是用<code>tf.data.experimental.make_csv_dataset</code>将csv文件直接转换为OrderedDict，但是我却怎么也不能将其放到模型中进行训练，会报奇奇怪怪的错误。</p>
<p>在调试了两天无果之后，作罢，采用最原始的方式——导入为dataframe之后进行切割，并放到fit函数当中。虽然代码更长了些，但少了很多调试的煎熬。</p>
<h2 id="超参数选择"><a href="#超参数选择" class="headerlink" title="超参数选择"></a>超参数选择</h2><p>由于我一开始就采用了官方文档中的初始学习率3e-5,发现效果就非常好，训练几代以后再验证集上的正确率都能达到90%以上，此外，囿于电脑算力的限制，训练一个模型的时间普遍长达2小时，因此我只是对初始学习率和batch大小进行了微调。</p>
<p>由于调试对模型最终的预测准确率影响不大，我把选择合适超参的重心放在正确率增长速度、正确率达到平稳值所需的时间上。我发现，当调大batch的值以后，每一轮所需要的步数变少了，虽然每步所需的时间有所增长，但总体而言，在内存能负担的情况下，调高batch的值可以有效地加快模型的训练时间。</p>
<p>epoch=10 batch=32 lr = 1e-5 每轮训练时间15-16分钟</p>
<p><img src="/2022/03/02/%E5%BD%93%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BDLab1-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/model1.png" style="zoom:150%;"></p>
<p>Epoch=6 batch=32 lr = 3e-5 每轮训练时间 10-12分钟</p>
<p><img src="/2022/03/02/%E5%BD%93%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BDLab1-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/model2.png" style="zoom:150%;"></p>
<p>Epoch=6 batch = 50   lr = 3e-5  每轮训练9-10分钟</p>
<p><img src="/2022/03/02/%E5%BD%93%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BDLab1-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/model3.png" style="zoom:150%;"></p>
<p>epoch=8 batch = 100 lr = 3e-5每轮训练时间 8-9 分钟</p>
<p><img src="/2022/03/02/%E5%BD%93%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BDLab1-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/model4.png" style="zoom:150%;"></p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>根据图片和一些保存的数据，最后我们选择的是将batch设置为100，初始学习率设置为 3e-5, 训练10轮。模型在训练8轮后由于早停机制停止训练，结果如下图所示。</p>
<p><img src="/2022/03/02/%E5%BD%93%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BDLab1-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/final.png" style="zoom:150%;"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/" class="post-title-link" itemprop="url">了解区块链</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-24 13:37:21" itemprop="dateCreated datePublished" datetime="2022-02-24T13:37:21+08:00">2022-02-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-01-04 10:07:34" itemprop="dateModified" datetime="2023-01-04T10:07:34+08:00">2023-01-04</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="了解区块链"><a href="#了解区块链" class="headerlink" title="了解区块链"></a>了解区块链</h1><p>部分内容引用自： <a href="https://www.zhihu.com/question/37290469" target="_blank" rel="noopener">https://www.zhihu.com/question/37290469</a></p>
<h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>首先我们要问自己三个问题：</p>
<h3 id="一个去中心化的数字货币系统是什么样的"><a href="#一个去中心化的数字货币系统是什么样的" class="headerlink" title="一个去中心化的数字货币系统是什么样的"></a>一个去中心化的数字货币系统是什么样的</h3><p>说到数字货币，我们首先想到的就是三个最明显的特征：防篡改、可追溯以及去中心化。那么什么是去中心化？简单来说就是P2P(peer to peer)模式，在这个模式中，没有中心式服务器，用户和用户之间是对等的。用户之间采用Gossip协议来传播消息。</p>
<p>这里岔开来简单介绍一下<a href="https://zhuanlan.zhihu.com/p/41228196" target="_blank" rel="noopener">Gossip协议</a>的执行过程：Gossip 过程是由种子节点发起，当一个种子节点有状态需要更新到网络中的其他节点时，它会随机的选择周围几个节点散播消息，收到消息的节点也会重复该过程，<strong>直至最终网络中所有的节点都收到了消息</strong>。这个过程可能需要一定的时间，由于不能保证某个时刻所有节点都收到消息，但是理论上最终所有节点都会收到消息，因此它是一个<strong>最终一致性协议</strong>。</p>
<p>那么，在这样一个去中心化的数字货币系统，在没有国家信用背书的情况下，应该设计什么？</p>
<ul>
<li>第一个问题是开户，在这样一个数字货币系统中开户，需要什么？</li>
</ul>
<p>这就要我们了解公钥和私钥的概念：我们可以把公钥理解为我们的身份证，把私钥理解为要支取货币、进行交易时候需要输入的密码。</p>
<ul>
<li>第二个问题是当没有了一个中心化的服务器，各个结点应该如何存储数据？</li>
</ul>
<p>答案是，每一份节点都需要存储全量的数据。因为当没有中心服务器的时候，如果每个人只存放了部分数据，一些人可能会篡改交易记录，导致违背了防篡改的原则。</p>
<h3 id="一个永不停机的世界计算机系统是什么样的"><a href="#一个永不停机的世界计算机系统是什么样的" class="headerlink" title="一个永不停机的世界计算机系统是什么样的"></a>一个永不停机的世界计算机系统是什么样的</h3><p>既然说，这个系统要永不停机，而且还要是P2P的。就要求这个系统有高可用、高可靠的特性。因此，这个问题的答案和上一个问题有一定的重合——要求每一个节点要有全量的数据。</p>
<p>为了解释清楚为什么要这样做，我们可以说一个故事：</p>
<p><strong>【1：一个公共账本】</strong></p>
<p>假如你现在在上大学，你们寝室是标准的四人寝，除了你之外还有<strong>小王，小黄和小白</strong>三个室友。平时你们亲是内部的活动很多，于是经常会有人垫付饭钱，车费，还有水电费。你们大家发现，如果每次消费后，都要一一计算交结非常麻烦，于是你们决定采用记账的方案。</p>
<p>于是乎，你们买了一个公共的账本，本次产生消费后，就由付钱的人在账本上记清楚，谁应付给自己相应的金额。如此一来，只要每月月末统一结算即可，大大节省了时间精力。</p>
<p>时间一长，你们发现在纸上记账还是麻烦。于是你们决定”升级“这个账本，改成在电脑种建立一个excel表格，比如这个样子： </p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/1.jpeg"></p>
<p>但问题是，如果你们寝室里有个人不厚道，偷偷修改账本怎么办呢？</p>
<p>例如小王把自己要付钱记在了小白头上。如果这个问题不能得以解决，那这个账本的信用就将会大大折扣。</p>
<p><strong>【2：每个人都有账本】</strong></p>
<p>他们采用的解决方案就是，给四个人每人都配备一个账本。需要记录时，就由对应的操作人高喊交易内容，广播给寝室里的所有人。</p>
<p>例如，小王高喊，”<strong>小王需要支付给小白30元  “。</strong>然后寝室里其他人听到了，就在各自的小本本上记下，”小王需要支付给小白30元 “。</p>
<p>如此一来，就算小王故意使坏，把自己要付的钱记在别人身上，那也只能是篡改自己的账本。这样到月底时，小王的账本和其余三个人对应不上，便能知道小王的账本有问题。</p>
<p>从上面这个故事可以很清楚的了解，为什么每一个节点都需要保存全量的数据。但是我们还有一个很重要的问题没有解决：我们知道，在分布式数据库中，为了解决宕机、故障时，需要使用Paxos、raft协议来保证各个分布式数据库的数据不发生错误。但是，在这种区块链系统中，Paxos是否还能达到这种效果呢？</p>
<p>答案是不行的，因为在这个系统里面存在背叛者。也就是经典的拜占庭将军问题——如果小王恶作剧，不负责任的乱喊“<strong>小白需要支付给小王100元</strong>”，如此一来，很可能会有不明真相的舍友记录下来；或者说，付了钱而不喊出来。那么这个账本就乱套了。</p>
<p>因此，可以解决拜占庭容错的方案需要被提出，也就是<strong>数字签名</strong></p>
<p> <strong>【3：在交易记录后签名】</strong></p>
<p>这个问题在纸质帐本里很好解决，那就是在每一条记录后，由需付款的一方加上自己的手写签名，以示自己认可这笔记录。这个思路换到计算机中就是<strong>数字签名，</strong>所以我们要求每一笔记录后面，都要由需付款的一方加上自己的数字签名。</p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/1.jpeg"></p>
<h3 id="元宇宙中支撑虚拟资产交易的基础设施是什么"><a href="#元宇宙中支撑虚拟资产交易的基础设施是什么" class="headerlink" title="元宇宙中支撑虚拟资产交易的基础设施是什么"></a>元宇宙中支撑虚拟资产交易的基础设施是什么</h3><p>在中心化系统中，DBA具有至高无上的权利，但是在去中心化的场景下，是一个无政府主义情况，没有一个绝对的权威。那么，它的基础设施就是我们今天要学习的——区块链。</p>
<h2 id="比特币"><a href="#比特币" class="headerlink" title="比特币"></a>比特币</h2><p>上面这个账本还存在一些问题，使得只适合小范围使用，如果扩大到更大的范围，比如整个学校使用，这个时候交易量和用户数剧增，记录就会变得非常麻烦。</p>
<p>比特币的正是为了解决这个问题而对前面介绍的账本系统的改进。</p>
<h3 id="改进一：交易单位为比特币BTC"><a href="#改进一：交易单位为比特币BTC" class="headerlink" title="改进一：交易单位为比特币BTC"></a>改进一：交易单位为比特币BTC</h3><p>我们之前用的账本里面的交易单位是人民币，但在比特币系统中，我们的交易单位变为比特币。</p>
<h3 id="改进二：将记账改为事实交易支付"><a href="#改进二：将记账改为事实交易支付" class="headerlink" title="改进二：将记账改为事实交易支付"></a>改进二：将记账改为事实交易支付</h3><p>之前的账本是在月底结账，而现在我们把交易单位改成了虚拟的比特币，交易也由月底统一交割改为事实交割。而比特币就像是账本上的数字，随时都可以视大家的意见进行”套现“，即在现实中交割。</p>
<p>那么要完成比特币交易，就必须要求用户的账本里的比特币余额是足够多的。在传统的银行系统中(中心化系统)，银行会记录储户的账余额，判断能否进行交易。但是在比特币系统中我们并没有这样一个中心，所以不能像银行这样来进行操作。</p>
<p>因此比特币提出的解决方案是：<strong>每笔交易不余额为基础，而是以以前的交易为基础</strong>。在这里我们就要介绍比特币的记账模型：<strong>UTXO(Unspent Transaction Out)</strong>，字面意思就是尚未被花出去的交易额</p>
<h4 id="UTXO"><a href="#UTXO" class="headerlink" title="UTXO"></a>UTXO</h4><h5 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h5><p>这个记账模型的理念就是：一切的交易，都是<strong>可追溯的</strong>， 交易与交易之间组成了网状关系，一个交易的输出，成为了下一个交易的输入；下一个交易的输出，又成了下下一个交易的输入。</p>
<p>如下图：</p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/7.png" style="zoom:67%;"></p>
<p>我们可以举一个例子：现在user1和user2达成了一笔交易，user1需要支付给user2 一共37个BTC，那么首先user1的账户余额里面的BTC个数是要大于37个的。</p>
<ul>
<li>问题1：怎么计算user1账户中的余额？<ul>
<li>因为在比特币系统中，并不像银行一样直接存储账号里有多少钱。而存储的是一笔笔的交易，也就是一笔笔的UTXO。然后，某个人的钱包的余额就等于他的UTXO的总和。比如说在这个user1的账户中，一共有3笔UTXO，金额分别是5,15和20，加起来一共是40BTC。余额是大于37BTC的，因此这笔交易可以进行</li>
</ul>
</li>
<li>问题2：为什么不直接支付37元，而是要用更复杂的先支付40BTC，再让user2找user1 钱呢？<ul>
<li>因为在UTXO中，每笔交易的金额是不能够被拆分的。对user1来说，他将T2、T3转给user2之后，是不能将T1交易额中的5BTC拆出2BTC转给user2的，要给就一起给。</li>
<li>打个比方，就好比你拿了一堆纸币去买东西，你必须从这堆纸币中选出几张，是的纸币的面值之和大于这笔交易的金额。你只能要求商家将多余的钱找给你，而不能说把纸币撕成两半花</li>
</ul>
</li>
<li>问题3：在真实的比特币交易中，这种方法难道不会徒增麻烦吗？<ul>
<li>这只是比特币交易的底层实现逻辑。事实上在上层有比特币钱包这种软件可以帮你计算UTXO、选择哪几个UTXO进行支付等。对用户来说这是完全透明的</li>
</ul>
</li>
</ul>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/8.png"></p>
<p>总结：</p>
<ul>
<li>任何一笔Transaction ，汇划费多个UTXO(Input), 同时也会产生多个新的 UTXO(Output), 属于多个不同的收款人</li>
<li>1个UTXO  = 1个Transaction ID + Output Index</li>
<li>旧的UTXO不断消亡新的UTXO不断产生，所有的UTXO组成了UTXO Set的数据库，存在于每个节点。</li>
<li>任何一笔UTXO，有且尽可能被一个交易花费1次，也就是说具有不可拆分性。(以纸币类比)</li>
</ul>
<h5 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h5><p>UTXO（Unspent TX Output）译名为未花费的交易输出，UTXO模型的设计基于一种思路：除了铸币（Coinbase）交易生成的比特币，任意一笔钱不会凭空产生，也不会凭空消失。UTXO由四部分组成；</p>
<ul>
<li><p>address: 拥有此UTXO的地址；</p>
</li>
<li><p>amount: 此UTXO的金额；</p>
</li>
<li>Signature Script: UTXO解锁脚本，使用交易发送私钥加密交易内容得到的签名；</li>
<li>Pubkey Script: UTXO锁定脚本，包含交易获得方的公钥哈希。</li>
</ul>
<p>上面提及了比特币中的解锁脚本（Signature Script）与锁定脚本（PubKey Script）。比特币脚本是一种基于栈结构的无状态脚本语言，只能进行有限的操作，图灵不完备。虽然简单，但在数字货币领域，也意味着更少的金融风险。</p>
<p>在UTXO 模型中，交易只是代表了 UTXO 集合的变更。而账户和余额的概念是在 UTXO 集合上更高的抽象，账号和余额的概念只存在于钱包中。以比特币钱包为例，钱包管理的是一组私钥，对应的是一组公钥和地址。要查看钱包余额，必须从创世区块开始扫描每一笔交易，如果：</p>
<ol>
<li><p>遇到某笔交易的某个Output是钱包管理的地址之一，则钱包余额增加；</p>
</li>
<li><p>遇到某笔交易的某个Input是钱包管理的地址之一，则钱包余额减少。</p>
</li>
</ol>
<p>所以从狭义上来说，比特币钱包中并没有比特币，只有钱包地址关联的所有UTXO之和，代表着钱包地址拥有这些比特币的所有权。</p>
<p>现在我们举一个实际情况中转账交易的例子：假设Alice有amount个比特币</p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/16.png"></p>
<ol>
<li>Alice使用公钥解锁自己的UTXO，构造新的UTXO和一笔转账交易。</li>
<li>Alice使用私钥对UTXO的交易内容进行签名：将UTXO的输出地址为Bob的钱包地址，并把Bob的公钥Hash作为解锁此UTXO的凭据</li>
<li>经过6个区块确认(证明该笔交易极大概率合法)之后，这amount个比特币就属于Bob了。实际上，Bob拥有的是这笔转账交易的UTXO</li>
</ol>
<p>从比特币的记账模式来看，所有的交易记录都是可以被追踪的。因此，在一些灰色产业(如洗钱)中，用户为了抹去自己的交易记录，会采用<strong>洋葱路由</strong>的方式，将交易洗牌以后重新组合来规避法律风险。</p>
<h3 id="改进三：不留证明，改留比特币账号"><a href="#改进三：不留证明，改留比特币账号" class="headerlink" title="改进三：不留证明，改留比特币账号"></a>改进三：不留证明，改留比特币账号</h3><p>如上图，用户A并不用留下自己的名字，而是用一个字符串来代表自己进行交易：</p>
<ul>
<li><strong>1H6ZZpRmMnrw8ytepV3BYwMjYYnEkWDqVP</strong></li>
</ul>
<p>同理，用户B和C也是如此，只留下了一个字符串。所以你们室友在帐本里都不写名字，而是写下跟各自对应的字符串，即比特币账户。虽然对于你们四个人的寝室这纯属脱裤子放屁，但如果对于规模更大的系统，比如前文提到的整个学校而言，这种操作可以极大提升隐私性。</p>
<p>我们只能知道每个账户，而无法知道谁拥有这个账户，这就保证了隐私性。</p>
<h2 id="区块链"><a href="#区块链" class="headerlink" title="区块链"></a>区块链</h2><h3 id="区块链简介"><a href="#区块链简介" class="headerlink" title="区块链简介"></a>区块链简介</h3><p>前面我们提到了这个账本是分布式存储的，每个人都有一个自己独立管理的账本。</p>
<p>当这个账本系统变得很大时，一致性问题就必须要考虑。比如，如果你有室友在交易记录发布时不在寝室，那么他就错了这次消息，使得这次消息不会出现在他的帐本里。</p>
<p>换回比特币系统，也就是部分电脑可能处于关机或者未联网状态，会错过部分交易。此外，还可能会有黑客入侵部分电脑，篡改交易记录。</p>
<p>此外，还有一个更严重的问题就是，实际网络拓扑非常复杂，链路质量的随机性很大。因此，如果用户A（假设账户里有10BTC）连续广播两条相互矛盾的消息，比如：</p>
<ul>
<li><strong>交易信息1：用户A支付10BTC给B；</strong></li>
<li><strong>交易信息2：用户A支付10BTC给C；</strong></li>
</ul>
<p>有的读者应该会觉得，那我们就采信先收到的交易信息1，忽略与之矛盾的脚印信息2不就行了。</p>
<p>但问题是，因为网络链路的复杂性，所以很可能存在部分用户先收到交易信息1，又有部分用户先收到交易信息2。如果依靠先后顺序辨别有效性，那么就会存在不同用户记录的交易信息不一致。</p>
<p><strong>为了解决这个问题，中本聪提出了区块链的概念。</strong></p>
<p>每个用户如果愿意，都可以整理自己从网络中接收到的交易信息，然后检查其是否合理（每笔交易是否由足够余额？数字签名是否正确？）后，再将交易记录打包成一个区块。每个区块有多笔交易(两三千笔)。</p>
<p>因此每个交易记录都是以区块的形式存储，然后再广播到系统中的其他用户中。而区块之间相互连接，形成一条由系统内全体用户共同维护的区块链。 </p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/4.jpeg"></p>
<p>因此其他用户收到广播的区块时，就会把这个区块加到自己维护的账本，也就是区块链的尾部。</p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/5.jpeg"></p>
<p>但如果只是这样，并没有解决任何问题。互联网节点遍布全球，广播过程也需要时间，因此肯定会存在不同节点收到不同区块存储的问题。</p>
<p>以此图为例，设备A和设备B几乎同时发布了自己的区块，两者的区块并不相同。很好理解：</p>
<ul>
<li><strong>红色区域中的用户离设备A近，会先收到设备A发的区块甲；</strong></li>
<li><strong>蓝色区域中的用户离设备B近，会先收到设备B发的区块乙；</strong></li>
</ul>
<p><strong>然后，不同的用户会将不同的区块加入到自己维护的区块链尾部，生产不同的区块链。</strong></p>
<p>如此一来这个网络就乱套了。为了降低传播时间的影响，一个简单粗暴的方式就是从系统设计中就限制区块生成的速度。比特币系统的核心思想采就是<strong>用算力限制区块的生成速度。</strong></p>
<p>比特币系统要求，每个用户在发布新区块前，必须先完成一个任务。这个任务就是：</p>
<ul>
<li><strong>根据前一区块的一些信息加上新区块的一些信息，生成一个字符串S；</strong></li>
<li><strong>选择一个字符串B，与S合并成“BC”，且要求“BC”的哈希映射满足某个条件，比如映射结果的前72位为0（概率：2^(-72)）；</strong></li>
</ul>
<p>上面的第2步，除了一个个试以外是没有捷径的，而且结果是否满足要求可以快速试出。举个例子就相当于给你一个银行卡，让你挨个试密码。那你没有任何办法，只能000000-999999挨个试验，最后虽然能试出来，但也会<strong>花费大量时间</strong>。而且你把试出来的结果告诉别人后，别人可以很快验证你的结果是否正确</p>
<p>时间恰好是我们宇宙中最稀缺的资源。因此我们可以调节这个难度，比如使得系统中所有平均<strong>每20分钟</strong>才会有一台设备完成要求的任务，猜出符合要求的字符串B。如此一来，撞车的概率就会大大降低。补充一句，这个难度是不断调节的，以适应硬件算力的提升。</p>
<h3 id="区块链分类"><a href="#区块链分类" class="headerlink" title="区块链分类"></a>区块链分类</h3><p>我们可以大致把联盟链分为三类：公有链、私有链、联盟链</p>
<ul>
<li>公有链：Public blockchains<ul>
<li>公有链没有官方组织及管理机构，没有中心服务器，参与的节点按照系统规则自由接入网络、不受控制，节点间基于共识机制开展工作</li>
</ul>
</li>
</ul>
<p>比特币就是一个典型的公有链</p>
<ul>
<li>私有链：Private blockchains<ul>
<li>建立在某个企业内部，系统的运作规则根据企业要求进行设定，仅有少数节点拥有修改、读取的权限，同时仍保留着区块链的真实性和部分去中心化的特性。</li>
</ul>
</li>
<li>联盟链<ul>
<li>联盟链由若干机构联合发起，介于公有链和私有链之间，兼具部分去中心化的特性。联盟管理员可以根据业务要求限制某些用户的读取权限。并限制某些节点参与共识计算</li>
<li>可以理解为一个特殊的分布式数据库</li>
</ul>
</li>
</ul>
<p>私有链和联盟链可以被统称为<strong>许可链</strong>，公有链又可以被称为<strong>非许可链</strong>。要进入许可链，需要一个<strong>身份证</strong>。</p>
<h3 id="挖矿-POW"><a href="#挖矿-POW" class="headerlink" title="挖矿(POW)"></a>挖矿(POW)</h3><h4 id="比特币机制"><a href="#比特币机制" class="headerlink" title="比特币机制"></a>比特币机制</h4><p>系统为了鼓励大家生成新的区块，于是在开始时就定下规则：</p>
<ul>
<li><strong>每当一个新区块加入主链，这个区块的发行者就会被赠与50个BTC；</strong></li>
<li><strong>每21万个区块后，奖励额度缩水一半；</strong></li>
</ul>
<p>这也就解释了，为什么BTC的发行上限是：</p>
<script type="math/tex; mode=display">
21000\times50\times(1+\frac{1}{2}+\frac{1}{4}+\cdots) = 21000000</script><p>也就是2100万个。</p>
<p>在 这个网站 中 <a href="https://blockexplorer.one/bitcoin/mainnet，我们可以看到最新被挖出的区块的编号，可以看到最新被挖出的编号是725025,每个区块的奖励是6.25个BTC，已发行的BTC的数量是18562811个" target="_blank" rel="noopener">https://blockexplorer.one/bitcoin/mainnet，我们可以看到最新被挖出的区块的编号，可以看到最新被挖出的编号是725025,每个区块的奖励是6.25个BTC，已发行的BTC的数量是18562811个</a></p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/3.png"></p>
<h3 id="区块链的结构"><a href="#区块链的结构" class="headerlink" title="区块链的结构"></a>区块链的结构</h3><p>区块链的内部结构如下：</p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/1.png" style="zoom:150%;"></p>
<p>Header 中包含的信息主要是下面几项：</p>
<ul>
<li>Version 。区块的版本号。</li>
<li>之前一个区块的 header 哈希。</li>
<li>Merkle根哈希。这个哈希是由区块中包含的所有交易的哈希值运算得出的，如果有人篡改了任何交易，这个哈希值就会变。header 中保留了这个值意味着只要 header 安全了，整个区块就安全了，这解释了为何计算 nouce 的时候，只需要计算 header 的哈希，而忽略区块主体数据。</li>
<li>时间。矿工开始运算 header 哈希的那个时间点。</li>
<li>nBits 。压缩格式的 target 值。这一项是跟 nonce 紧密相关的，后面我们还会详细介绍。</li>
<li>Nonce 。简单来说 nonce 就是每个区块 header 之中都会保存的一个数,下面详细介绍。</li>
</ul>
<p>计算 nonce 值的过程就是对区块 header 不断的运算哈希，直至找到能使区块哈希小于 target 的 nonce。</p>
<p>Target 就是一个 256-bit 的数。因为计算 header 哈希使用的<a href="https://link.zhihu.com/?target=https%3A//happypeter.github.io/binfo/hash">哈希算法</a>是 Sha256 ，运算得到的哈希值是 256-bit 的一个数，所以恰好可以用来和 target 对比。所有比特币客户端在计算 nonce 的时候<strong>都使用相同的 target</strong> ，并且这个 target 值会被记录到 header 中，保存到了 nbits 这一项。nBits 是一个 32-bit 的数，所以保存的是 target 的压缩形式，具体换算关系可以参考<a href="https://link.zhihu.com/?target=https%3A//bitcoin.org/en/developer-reference%23target-nbits">这篇文档</a>。Target 要小于这个数 0x00000000FFFF0000000000000000000000000000000000000000000000000000 。</p>
<p>每次计算 header 哈希就让 nonce 值加1。具体 nonce 的运算过程就是不断的修改 nonce 值，然后对这个 header 重新运算哈希的过程。每得到一个哈希值就去跟 target 对比，如果哈希小于或者等于 target ，那么运算过程就结束了，当前 nonce 值会被最终记录到 header 中。否则，就把 nonce 值加1，再次计算 header 哈希。</p>
<p>显然，各个比特币客户端开始了一场寻找 nonce 的比赛，谁的硬件速度快，就有更大的概率率先找到 nonce 值，也就是宏观意义上抢到的记账权，当然，哈希的运算过程非常随机，所以这场比赛也跟抓彩票一样有很强的运气成分。</p>
<p>以上就是 nonce 的生成步骤了</p>
<h4 id="Merkle树"><a href="#Merkle树" class="headerlink" title="Merkle树"></a>Merkle树</h4><p>从上面的结构可知，比特币的区块体就是由Merkle树组成的。那么，为什么我们需要Merkle树呢？</p>
<p>首先，我们要了解Merkle树的生成过程。由区块链的结构可知，每个区块由前一个区块的一些信息和新区块的一些信息组成的。其中我们要计算的这个Merkle根就属于新区块的一些信息</p>
<p>如何计算Merkle根？</p>
<ul>
<li>首先，对于每一个区块，节点收到了一批交易以后，它们会组成Merkle树的叶子节点。</li>
<li>然后，我们要给每一项交易都生成一个哈希值。</li>
<li>接着，将树根中的哈希值两两哈希，生成倒数第二层节点。</li>
<li>重复对每一层节点进行两两哈希，生成它们的父亲节点。</li>
<li>直到生成Merkle树的树根，这个最后的哈希值就是默克尔树的根哈希。</li>
</ul>
<h5 id="SPV验证"><a href="#SPV验证" class="headerlink" title="SPV验证"></a>SPV验证</h5><p>SPV的目标是验证某个支付是否真实存在，并得到了多少个确认。</p>
<p>首先我们要来了解三种节点：</p>
<ul>
<li>轻节点指的是：节点本地只保存与其自身相关的交易数据（尤其是可支配交易数据），但<strong>并不保存完整区块链信息</strong>的技术。轻节点的目标不仅是支付验证，而且是用于管理节点自身的资产收入、支付等信息。因此，<strong>轻节点仍需下载每个新区块的全部数据</strong>并进行解析，获取并在本地存储与自身相关的交易数据，只是无须在本地保存全部数据而已。</li>
<li>全节点是指：<strong>维持包含全部交易信息</strong>的完整区块链的节点。更加准确地说，这样的节点应当被称为完整区块链节点。</li>
<li>SPV节点是指：和轻节点不同，<strong>SPV节点不需要下载新区块的全部数据</strong>，只需保存区块的头部信息即可。虽然轻节点部分借鉴了SPV的理念，但和SPV是完全不同的。</li>
</ul>
<p>Merkle树结构，可以用来验证或确保一个数字货币交易已经在对应区块链的一个区块中。这就是SPV验证。</p>
<p>在验证某一个交易的真实性的时候，SPV节点只需要把该交易哈希值向网络中连接的全节点发起询问。网络里面的全节点只需要回复最小量必要数据给SPV节点，即可验证交易的真实性。如果SPV节点不信任提供交易验证数据的全节点，还可以同时发起多个全节点的询问，来确保交易验证的最大可靠性。</p>
<p>以下是SPV节点的验证过程</p>
<ul>
<li>从网络上获取并保存最长链的所有block header到本地</li>
<li>计算 该交易的hash值 ：tx_hash</li>
<li><p>定位到包含该tx_hash所在的区块，验证block header是否包含在已知的最长链中</p>
<ul>
<li>怎么定位？最简单的方法是顺序查找。</li>
</ul>
</li>
<li><p>从区块中获取构建merkle tree所需的hash值</p>
</li>
<li>根据这些hash值计算merkle_root_hash</li>
<li>若计算结果与block header中的merkle_root_hash相等，则交易真实存在</li>
<li>根据该block header所处的位置，确定该交易已经得到多少个确认。</li>
</ul>
<h4 id="Target"><a href="#Target" class="headerlink" title="Target"></a>Target</h4><p> <strong>通过调整 target 值，可以来调整挖矿难度</strong>，挖矿难度每两周会调整一次。</p>
<p>Target 的值越小，挖矿难度就越大。POW 算法下的挖矿难度就体现在找到 nonce 值的难度，挖矿难度越大找到 nonce 值所需要的时间就越长。寻找 nonce 意味着找到一个满足特定范围的区块哈希，这个范围越大找到满足条件的 nonce 就越容易，反之则越难。而这个范围就是大于零小于 target ，所以 target 越小，找到合适的 nonce 就越难。</p>
<p>挖矿难度每两周会调整一次。首先说，为什么要调整呢？由于寻找 nonce 的过程带有一定的随机性，每次找到 nonce 的时间都不一定是网络所期待的十分钟，所以如果时间大于十分钟，说明挖矿难度有可能设置的太高了，那么下一次我们就需要把挖矿难度降低，从而尽量保持平均找到 nonce 的时间是十分钟。而实际中，并不是每次出块后都会调整挖矿难度，比特币网络规定挖矿难度每2016个区块调整一次，2016个十分钟恰好是两周，所以每次难度调整的时间间隔是大约两周。</p>
<p>总之，伴随 target 的调整，挖矿难度也随着调整，可以保证平均的每次找到 nonce 的时间尽量趋近十分钟。</p>
<h3 id="安全性"><a href="#安全性" class="headerlink" title="安全性"></a>安全性</h3><p>采用了挖矿机制之后，虽然概率大大降低，但仍然无法排除存在撞车的概率。因此，区块链验证中最核心的思想就是——<strong>相信最长的区块链</strong></p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/6.jpeg"></p>
<p>我们拿上图举例，在原有的到区块链N的情况后，短时间内出现了两个不同的区块，分别用蓝色和红色表示。</p>
<p>如此一来，就会有：</p>
<ul>
<li><strong>部分用户的区块链：原区块链+蓝色区块</strong></li>
<li><strong>部分用户的区块链：原区块链+红色区块</strong></li>
</ul>
<p>这个时候，我们需要做的只有一件事情，那就是『等待』。因为同时产生区块的小概率事件，总不可能连续发生。这样我们只需要等下一个区块产生，看这个新区块是连在蓝色后还是红色后。</p>
<p>图例里是有个新区块连在了蓝色区块后面，这个时候：</p>
<ul>
<li><strong>链长度（原区块链+蓝色区块+新区块）&gt;链长度（原区块链+红色区块）</strong></li>
</ul>
<p><strong>因此，我们选择长度更长的“原区块链+蓝色区块+新区块”作为大家共同维护的主链。</strong></p>
<h4 id="双花攻击"><a href="#双花攻击" class="headerlink" title="双花攻击"></a>双花攻击</h4><p>双花攻击(Double Spend attach)，就是说攻击者几乎同时将同一笔钱(UTXO)用作不同交易。那么如何避免双花攻击？</p>
<ul>
<li>虽然确实可以几乎同时发起两笔交易，这些交易单可以任意顺序进行广播，但是它们被加入区块时必定呈现一定的顺序。我们只要认定第一笔交易合法，剩余的交易非法即可。</li>
</ul>
<p>那么，如果这个黑客向两个不同的网络，用一个UTXO发起两笔交易，分别加入两个区块。会怎么样呢?</p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/10.png"></p>
<ul>
<li>这也是不影响的。因为在比特币中我们最终认的是最长链。即使通过双花生成了两个分叉，但这些交易也是暂时的，最后只有在最长链的那笔交易会被保留下来。</li>
</ul>
<h4 id="女巫攻击"><a href="#女巫攻击" class="headerlink" title="女巫攻击"></a>女巫攻击</h4><p>女巫攻击(Sybil attach)也是很常见的攻击手段，如果投票机制存在，那么就可能出现一个人分饰多角给某一个人投票，出现作弊的现象。</p>
<p>但是在区块链中，几乎不可能出现女巫攻击的情况 </p>
<p>如果有人想对区块链造假，那么他就需要一直抢先生成假的新区块，并广播出去。</p>
<p>这就要求造假者生产新区块的速度，要快于系统中的其他所有用户的生成能力之和。换句话说，造假者需要用户有和其他用户算力之和匹敌的算力。</p>
<p>假设这个造假者拥有系统总算力的80%，而剩下用户拥有20%（注：这个假设显然不可能）。如此一来，在造假者广播了一个假的区块后，就有80%的概率先于其他用户生成新区块，然后连在自己之前生成的假区块后。此时造假才有可能成功。</p>
<p>但实际是，每个用户用的算力相比系统总算力都是微不足道的。而且随着比特币网络的逐渐壮大，系统就会越来越鲁棒。这就使得造假变得不可能，也使得造假的成本变得无法接受。</p>
<h4 id="可锻性攻击"><a href="#可锻性攻击" class="headerlink" title="可锻性攻击"></a>可锻性攻击</h4><p>首先，我们需要知道密码学中的一个常识：在椭圆曲线数字签名算法中，会生成两个大整数$r$和$s$并组合起来作为签名。同时 ,$r$ 和 $BN-s$ 也同样可以作为签名来验证交易，(BN是一个很大的数)。</p>
<p>因此出现了这样一个状况:同样的输入和输出，可以修改签名，使得交易ID出现不同。同时可以通过验签。</p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/18.png"></p>
<p>那么这时候，攻击者就可以侦听比特币网络中的交易，利用交易签名算法的特征修改原交易中的input签名，生成拥有一样input和output的新交易，然后广播导网络中形成双花</p>
<p>攻击过程如下：</p>
<ol>
<li>首先需要有足够多的比特币矿机接入网络，以增加伪造的请求被优先处理的可能性</li>
<li>攻击者在第三方交易平台提交一个提款请求，并获得一个交易ID。第三方交易平台只根据TransactionID来判断交易是否成功。根据交易信息伪造一个签名，同时生成一个完全不同的交易ID，并将伪造的请求发出</li>
<li>若伪造的交易被优先处理，则<strong>原始交易失败</strong></li>
<li>攻击者可以再次提交现请求，第三方交易平台确认之前的交易失败后会再次发送提现交易，至此，攻击成功</li>
</ol>
<p>也就是说，我可以给自己转一笔钱，成功了，但是我可以伪装成自己失败了并从交易所获得退款。所以真正受到攻击的是交易所。</p>
<ul>
<li>如何防范？</li>
</ul>
<ol>
<li>当使用TXID查找不到对应的交易时，需要用TXIN_OUTPOINT再查一下；</li>
<li>如果确实需要重发，引用的UTXO一定要与原来保持一致，把这种麻烦事儿交给比特币网络来处，系统规则确保了最终只会确认其中的一个（不会出现双花）。</li>
</ol>
<h4 id="挖矿的意义"><a href="#挖矿的意义" class="headerlink" title="挖矿的意义"></a>挖矿的意义</h4><p>挖矿的意义是什么？有些人会对挖矿机制产生质疑——这不是在浪费电力资源吗？</p>
<p>其实，在比特币这样的公有链中，挖矿是一个绝顶聪明的发明。通过挖矿其实是实现了一个<strong>共识机制</strong>，也就是一个<strong>选主的过程</strong>。因为在区块链中，每一个节点都会收到一批交易，那么到底认谁收到的那批交易是”正统“的呢？</p>
<p>答案很简单，谁的算力强，先算出来nonce的值，谁就被认可。而且通过挖矿和认可最长链的机制，使得比特币系统可以抵御很多种类的攻击</p>
<h3 id="区块链的应用场景"><a href="#区块链的应用场景" class="headerlink" title="区块链的应用场景"></a>区块链的应用场景</h3><p>当某一个场景符合以下几点特征的时候，运用区块链技术可以很好的解决这些困境。</p>
<ul>
<li>多方参与</li>
<li>互不信任</li>
</ul>
<p>上面两点我们很熟悉，正式因为互不信任我们才会使用P2P网络。否则相信中心化就好了</p>
<h4 id="链下数据可信的保证——Oracle-预言机"><a href="#链下数据可信的保证——Oracle-预言机" class="headerlink" title="链下数据可信的保证——Oracle 预言机"></a>链下数据可信的保证——Oracle 预言机</h4><p>这两点就比较难以理解了，链上数据的安全性能够保证，但是涉及到链下数据的时候，我们怎么能保证其上链之前没有被篡改过呢？有一种好方法就是 Oracle预言机。</p>
<p>比如说我和同学A打赌，明天是否下雨，并每人给100元作为赌注。如果下雨我赢，如果不下雨则他赢。那么怎么判断明天到底是否下雨呢？——存在一个公证人C会将明天的天气情况输入到系统中。但是这样存在一个问题，即我可以收买公证人C，使其输入到系统中的天气始终为下雨。这就存在链下数据遭篡改的可能。这时，就需要Oracle预言机了。我们可以在以太坊官网学习Oracle的相关知识 <a href="https://ethereum.org/zh/developers/docs/oracles/" target="_blank" rel="noopener">https://ethereum.org/zh/developers/docs/oracles/</a></p>
<h4 id="自治"><a href="#自治" class="headerlink" title="自治"></a>自治</h4><p>自治就相当于在一个无政府主义的社会，每个人都回去遵循大家一起定下来的规则去行事。</p>
<p>我们可以来看一个例子：航空公司要做一个延误险，保险双方分别为保险公司和乘客。如果航班延误，那么保险公司就会赔付相应的钱给顾客，如果航班准点到达，那么乘客投保的钱就给了保险公司。</p>
<p>问题就是，我们是怎么知道航班延误了？很自然的想法是，到航空公司的网站上去查找飞机到达时间，但这显示是从外部获取的信息，属于链下数据。既然是链下数据就存在一定的不安全性，因此需要Oracle预言机来保证输入数据的正确性，但是引入Oracle预言机会使得镇各系统变得复杂。因此有没有内部的方法，不引用外部的信息源，仅通过共识来保证数据的安全性、正确性呢？</p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/11.jpeg"></p>
<p>我们要了解区块链中的一个原则：<strong>一个不信任，永远在验证</strong> 。在这种情况下，仅用”1个“外部见证人肯定是不行的。因此我们可以这样——我让下一个航班买了延误险的顾客来对这个航班的晚点情况进行投票(人数够多，对这些人来说航班是否晚点是客观事实)。</p>
<ul>
<li>如果一个顾客投了延误，实际上真的延误了，那么他就会获得一定的以太币奖励；</li>
<li>如果顾客投了没有延误，实际上确实没有延误，那么也会获得一定的以太币奖励；</li>
<li>如果一个顾客投了延误，但实际上并没有延误，就说明他说谎，因此我们可以取消他延误险的有效性， 即使延误了他也无法获得赔付 ，以此作为惩罚。</li>
</ul>
<p>在这个系统中，就使用了<strong>自治</strong>的方法，对于个人来说，他说假话并不会得到任何的好处，而说实话则会得到奖励。因此大家都会倾向于说真话。基于此，就相当于通过<strong>人肉筛选</strong>来避免了直接从外部源获得信息，从而使整个系统运行起来。</p>
<h2 id="区块链2-0"><a href="#区块链2-0" class="headerlink" title="区块链2.0"></a>区块链2.0</h2><p>区块链2.0最重大的特征：支持智能合约(smart contract)</p>
<p>那么智能合约是什么？在比特币系统中，它支持的脚本是非常有限的，主要以转账为主。但是到了2.0，我们就可以在这个平台上编程了，我可以在上面开发应用。我们把这些应用统称为智能合约</p>
<p>智能合约可以对接收到的信息进行回应，可以接收和存储价值，还可以对外发送信息和价值。</p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/12.png"></p>
<p>注意：智能合约程序不只是一个可以自动执行的计算机程序，其自己就是一个<strong>系统参与者</strong>。这意味着智能合约可以调用另一个智能合约</p>
<h3 id="软件架构"><a href="#软件架构" class="headerlink" title="软件架构"></a>软件架构</h3><p>下面是区块链2.0的软件架构</p>
<ul>
<li>在共识层，需要用到挖矿算法以及拜占庭容错的方法</li>
<li>在智能合约引擎中，包括智能合约的代码以及其存储的数据。</li>
</ul>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/13.png"></p>
<h3 id="业务流程"><a href="#业务流程" class="headerlink" title="业务流程"></a>业务流程</h3><p>开发人员可以写一个自己的智能合约，然后我们调用系统提供的SDK，并将数据存储在数据库当中。如下图所示：</p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/14.png"></p>
<h2 id="区块链小结"><a href="#区块链小结" class="headerlink" title="区块链小结"></a>区块链小结</h2><ol>
<li>区块链是一个分布式的账本，一种通过<strong>去中心化、去信任</strong>的方式由参与各方维护一个<strong>可靠的、不可篡改交易记录的</strong>技术方案。</li>
<li>从数据库角度，可以将区块链比作一种<strong>分布式数据库</strong>技术，通过维护数据块的链式结构，可以维持持续增长的、不可篡改的数据库记录。</li>
</ol>
<h3 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h3><ul>
<li>匿名：交易参与方与网络中的相关节点都是匿名的</li>
<li>不可信：开放的环境需要通过<strong>数字签名</strong>技术进行验证，按照系统既定的规则运行。</li>
<li><p>去中心化：基于P2P，不存在中心化的设备和管理机构</p>
<ul>
<li>任何能都可以参与到区块链网络，每一台设备都能作为一个节点，每个节点都允许获得一份完整的数据库拷贝</li>
<li>节点间基于一套共识机制，通过竞争计算共同维护整个区块链</li>
<li>任一节点失效，其余节点仍能正常工作</li>
</ul>
</li>
<li><p>不可篡改性</p>
<ul>
<li>批量交易一旦达成共识，就会作为一个区块添加到区块链中，会被永久地存储起来</li>
<li>单个甚至多个节点对交易数据的修改是无效的，除非能控制整个网络中超过51%的节点同时修改，但这几乎是不可能发生的。</li>
</ul>
</li>
<li><p>可追溯</p>
<ul>
<li>区块链中每一笔交易都通过密码学方法与相邻两个区块串联(UTXO)，因此可以追溯到任何一笔交易的前世今生.</li>
</ul>
</li>
<li><p>共识</p>
<ul>
<li>采用协商一致的规范和协议(POW) 来保证数据质量，越多人在这个系统中，这个区块链系统就会越鲁棒。</li>
</ul>
</li>
</ul>
<h3 id="应用情况"><a href="#应用情况" class="headerlink" title="应用情况"></a>应用情况</h3><h4 id="金融服务"><a href="#金融服务" class="headerlink" title="金融服务"></a>金融服务</h4><p>金融服务是区块链最早的应用领域之一，也是区块链应用数量最多、普及程度最高的领域之-</p>
<ul>
<li>防金融欺诈、资产托管交易、金融审计、跨境支付、对账与清结算、供应链金融以及保险理赔等；</li>
<li>典型案例：基于区块链的机构间对账平台、差异账检查系统，以及通过区块链技术改造的跨境直联清算业务系统等。</li>
</ul>
<h4 id="供应链管理"><a href="#供应链管理" class="headerlink" title="供应链管理"></a>供应链管理</h4><p>供应链核心企业、商业银行、电商平台等相关力量不断加强区块链在供应链管理领域的应用探索。</p>
<ul>
<li>防伪溯源：京东、蚂蚁金服、众安科技等科技企业；</li>
<li>供应链金融：央行数宇货币研究所、央行深圳市中心支行推动“粤港澳大湾区贸易金融区块链平合”，万向区块链、乎安壹账通、京东、腾讯等众多企业；</li>
<li>防伪溯源和物流等领域：更注重与物联网、人工智能等技术的融合发展。</li>
</ul>
<h4 id="智能城市"><a href="#智能城市" class="headerlink" title="智能城市"></a>智能城市</h4><p>区块链在建设智慧城市中的应用涵盖智薏园区、智薏物联网、智慧资产、智慧交通、能源电力、电子政务、法律应用等广阔领域。</p>
<ul>
<li>采用分布式点对点的网络结构，可以使设备之问保持共识，实现,点对点传输数据，減少甚至无需与中心服务器的数据库进行验证，避免对中心化设施的依赖；</li>
<li>利用区块链数据防篡改、可追溯的特性可以帮助打通政府各部门数据孤岛，为公众提供更加可信和有价值的服务。</li>
</ul>
<h4 id="公共服务"><a href="#公共服务" class="headerlink" title="公共服务"></a>公共服务</h4><p>部分地方政府大力推进“区块链＋政务”服务，以满足公共服务在信息共享、权限控制和隐私保护等方面的高要求。</p>
<ul>
<li>鉴证确权：将公民财产、数宇版权相关的所有权证明存储在区块链账本中，大大优化权益登记和转让流程，减少产权交易过程中的欺诈行为；</li>
<li>身份验证：将身份证、护照、驾照、出生证明等存储在区块链账本中，实现无需任何物理签名即可在线处理繁琐的流程，并能实时控制文件的使用权限：</li>
<li>信息共享：区块链技术用于机构内部以及机枸之问的信息共享和实时同步，能有效解决各行政部门间协同工作中流程繁琐、信息孤立等问题。</li>
</ul>
<h3 id="技术发展"><a href="#技术发展" class="headerlink" title="技术发展"></a>技术发展</h3><h4 id="区块链系统架构"><a href="#区块链系统架构" class="headerlink" title="区块链系统架构"></a>区块链系统架构</h4><p> <img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/15.png"></p>
<h4 id="安全技术"><a href="#安全技术" class="headerlink" title="安全技术"></a>安全技术</h4><p>主要包括：数宇摘要算法、数字签名和加密算法。<br>经典算法：</p>
<ul>
<li>SHA256、SM3等主流的数宇摘要算法</li>
<li>RSA、 ECDSA、SM2等常用的数宇签名算法</li>
<li>AES、SM4等对称加密箅法</li>
<li>RSA、 SM2等非对称加密算法。</li>
</ul>
<h4 id="隐私保护技术"><a href="#隐私保护技术" class="headerlink" title="隐私保护技术"></a>隐私保护技术</h4><p><strong>挑战</strong></p>
<ul>
<li>量子计算的技术演进对现有的密码学安全机制的巨大影响；</li>
<li>根据Shor算法，经典非对称算法（基于大数分解、离散对数等算法，如RSA、 ECDSA和SM2等）可以被稳定、可用的量子计算机攻破；</li>
<li>密码学家正积极探索能够抵抗量子计算机攻击的密码机制，如基于格的密码机制、基于纠错码的密码机制、多变量密码机制等。</li>
</ul>
<p><strong>目标</strong>：</p>
<ul>
<li>“身份的隐私性”和“数据的机密性”</li>
<li>身份的隐私性”主要是对区块链参与者身份的保护；</li>
<li>“数据的机密性”主要是对记录内容、合约逻辑等数据的保护。</li>
</ul>
<p><strong>相关技术：环签名、同态加密、零知识证明和安全多方计算等。</strong></p>
<ul>
<li>环签名允许一个成员代表一个群组进行签名而不泄漏签名者信息，可以实现签名者完全匿名。</li>
<li>同态加密除了具有一般的加密操作之外，还能实现直接对密文的计算操作，通常分为加法同态、乘法同态、全同态等类型。</li>
<li>零知识证明是指一方（证明者）向另一方（验证者）证明某个事实的论断，同时不透露该事实的其他信息的方法。</li>
<li>安全多方计算能够在保证输入数据隐私的前提下，为缺之信任的参与方提供协同计算功能。</li>
</ul>
<h4 id="跨链技术"><a href="#跨链技术" class="headerlink" title="跨链技术"></a>跨链技术</h4><h4 id="分片技术"><a href="#分片技术" class="headerlink" title="分片技术"></a>分片技术</h4><p>分片技术本身是一种传统数据库技术，此前主要用于将大型数据库分成更小、更快、更容易管理的数据碎片。在区块链中，可将区块链网络分成很多更小的部分，即进行“分片”处理。</p>
<p>分片技术主要有网络分片、交易分片和状态分片三类</p>
<ul>
<li>网络分片是利用随机函数随机抽取节点形成分片，支持更海量的共识节点。</li>
<li>交易分片分为同账本分片和跨账本分片，主要思想是确保双花交易在相同的分片中或在跨分片通信后得到验证。</li>
<li>状态分片关键是将整个存储区分开，让不同的碎片存储不同的部分，每个节点只负责托管自身的分片数据，而不是存储完整的区块链状态。</li>
</ul>
<h4 id="数据存储"><a href="#数据存储" class="headerlink" title="数据存储"></a>数据存储</h4><p>区块的存储由链式结构发展为有向无环图 (DAG)，DAG 区块链在并行性、可扩展性上有较大改善。</p>
<p>链外数据的存储，除了传统集中的数据中心存储、云存储以外，产生了新的互联网点对点文件系统</p>
<ul>
<li>如融合Git、自证明文件系统(SFS)、BitTorrent 和DHT 等技术的星际 文件系统（IPFS)</li>
</ul>
<h4 id="共识机制"><a href="#共识机制" class="headerlink" title="共识机制"></a>共识机制</h4><p>主要集中在：提高系统吞吐量及降低网络带宽</p>
<ul>
<li><p>CFT (崩溃容错)</p>
<ul>
<li>Paxos, Raft</li>
</ul>
</li>
<li><p>BFT (拜占庭容错)</p>
<ul>
<li>POW系列：POW、POS、DPOS</li>
<li>PBFT, DBFT</li>
<li>DAG</li>
</ul>
</li>
</ul>
<h4 id="智能合约"><a href="#智能合约" class="headerlink" title="智能合约"></a>智能合约</h4><p>主要方面：形式化验证框架与通用型合约编程语言安全性是智能合约的关键性问题</p>
<ul>
<li>智能合约编程语言逐渐从脚本型语言向通用型语言演变</li>
<li>智能合约的执行逐渐从显式调用执行向由链上触发器（如预言机机制）自动触发执行的方向发展</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2022/02/22/Hadoop%E5%AE%9E%E8%B7%B5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/22/Hadoop%E5%AE%9E%E8%B7%B5/" class="post-title-link" itemprop="url">Hadoop实践</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-22 22:17:28" itemprop="dateCreated datePublished" datetime="2022-02-22T22:17:28+08:00">2022-02-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-08-22 11:17:18" itemprop="dateModified" datetime="2022-08-22T11:17:18+08:00">2022-08-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Hadoop实践"><a href="#Hadoop实践" class="headerlink" title="Hadoop实践"></a>Hadoop实践</h1><h3 id="Installing-the-MovieLens-Dataset"><a href="#Installing-the-MovieLens-Dataset" class="headerlink" title="Installing the MovieLens Dataset"></a>Installing the MovieLens Dataset</h3><p>首先，我们可以进入HDFS的管理界面，我们看到在这这有一个NameNode，一个SNameNode (Secondary NameNode,即NameNode的备份，当NameNode发生故障的时候，可以使用SNameNode进行修复) ,还有一个DataNode，负责存储文件。 </p>
<p><img src="/2022/02/22/Hadoop%E5%AE%9E%E8%B7%B5/1.png">然后，我们可以点击右上角的图标进入File View</p>
<p><img src="/2022/02/22/Hadoop%E5%AE%9E%E8%B7%B5/3.png"></p>
<p>进入文件管理系统后，我们可以点击右上角的图标新建文件夹，或者上传数据文件</p>
<p><img src="/2022/02/22/Hadoop%E5%AE%9E%E8%B7%B5/2.png"></p>
<p>我们选中了 .data文件进行上传</p>
<p><img src="/2022/02/22/Hadoop%E5%AE%9E%E8%B7%B5/4.png"></p>
<p>使用好以后可以对其进行删除。</p>
<h3 id="Install-the-MovieLens-dataset-into-HDFS-using-the-command-line"><a href="#Install-the-MovieLens-dataset-into-HDFS-using-the-command-line" class="headerlink" title="Install the MovieLens dataset into HDFS using the command line"></a>Install the MovieLens dataset into HDFS using the command line</h3><h3 id="Installing-Python-MRJob-and-nano"><a href="#Installing-Python-MRJob-and-nano" class="headerlink" title="Installing Python, MRJob, and nano"></a>Installing Python, MRJob, and nano</h3><h3 id="Code-up-the-ratings-histogram-MapReduce-job-and-run-it"><a href="#Code-up-the-ratings-histogram-MapReduce-job-and-run-it" class="headerlink" title="Code up the ratings histogram MapReduce job and run it"></a>Code up the ratings histogram MapReduce job and run it</h3><h3 id="Rank-movies-by-their-popularity"><a href="#Rank-movies-by-their-popularity" class="headerlink" title="Rank movies by their popularity"></a>Rank movies by their popularity</h3><h3 id="Note-Sorting-will-only-work-by-partition"><a href="#Note-Sorting-will-only-work-by-partition" class="headerlink" title="Note Sorting will only work by partition"></a>Note Sorting will only work by partition</h3><h3 id="Check-your-results-against-mine"><a href="#Check-your-results-against-mine" class="headerlink" title="Check your results against mine"></a>Check your results against mine</h3>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2022/02/22/AI-agents-and-environments/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/22/AI-agents-and-environments/" class="post-title-link" itemprop="url">AI-agents_and_environments</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-02-22 20:26:53 / Modified: 21:57:58" itemprop="dateCreated datePublished" datetime="2022-02-22T20:26:53+08:00">2022-02-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Agents-and-Environments"><a href="#Agents-and-Environments" class="headerlink" title="Agents and Environments"></a>Agents and Environments</h1><p>我们所学的课程主要是 CS188,那么要入门AI，首先要搞明白，什么是智能体，什么是环境。</p>
<p>举一个很简单的例子，有一个小孩子，当它伸左手的时候，给他一颗糖；伸右手的时候，打他一下，那么久而久之，他再也不会伸右手了。显然，这里小孩子是个智能体，而我们的行为就是环境。</p>
<h2 id="Agent-and-environments"><a href="#Agent-and-environments" class="headerlink" title="Agent and environments"></a>Agent and environments</h2><p><img src="/2022/02/22/AI-agents-and-environments/1.png" style="zoom:67%;"></p>
<p>对于每个智能体来说，它是如何与环境打交道的呢？——感知；当智能体感知到环境以后，会做出一定的行为。</p>
<p>因此，一个智能体中一定需要包含两个非常重要的元素——sensor(感知器) 和 actuator(致动器)</p>
<p>人类是智能体吗？——当然是，那么人类的感知器和致动器是什么？</p>
<ul>
<li>Sensors：视觉，听觉，触觉，味觉，嗅觉，本体感受</li>
<li>Actuators: 肌肉，分泌物，改变大脑的状态</li>
</ul>
<p>比如说，当人看到美丽的风景，竖起了大拇指，这时候眼睛是sensor而大脑和手上的肌肉是actuator</p>
<p>计算器是智能体吗？——也是</p>
<ul>
<li>Sensors: 计算器上的按键</li>
<li>Actuators: 电子屏</li>
</ul>
<p>AI致力于两点：</p>
<ol>
<li>Agent 可以模拟人类的行为</li>
<li>在复杂的环境中，通过AI计算可以得到一些重要的决策</li>
</ol>
<h3 id="Agent-functions"><a href="#Agent-functions" class="headerlink" title="Agent functions"></a>Agent functions</h3><p>智能体从环境中得到了一系列的感知，从这个历史感知序列做出相应的反馈，序列和行为是对应的，这就叫做<strong>Agent function(智能体函数)</strong></p>
<p>举一个例子，很好理解：我和张三一起玩剪刀石头布，他一直出剪刀，那么刚开始我可能会输几次，当我知道他在整个序列中都出了剪刀，我就会出石头。</p>
<p>再来举一个例子，我们可以感知到已经落下的方块、正在落下的方块以及将要落下的方块。基于此，我们会对方块的落下位置做一个变化。</p>
<p><img src="/2022/02/22/AI-agents-and-environments/2.png" style="zoom:67%;"></p>
<p>在现实中，要对一台及其进行训练，让其能够像人一样玩俄罗斯方块的话，就需要我们编写一个<strong>agent program</strong> ， 让机器不断去训练出一个 agent function。 </p>
<p>那么是不是所有的agent function都可以由agent program去逼近、去实现呢？显然不是的。当问题本身是一个np-hard问题，那么我们只能近似求解，无法逼近。</p>
<p>最后我们用一个真空吸尘器的例子来加深对agent function和agent program的理解。</p>
<p><img src="/2022/02/22/AI-agents-and-environments/3.png" style="zoom:67%;"></p>
<p>对于正空吸尘器感知到的序列有两个参数： 位置和该房间的状态。</p>
<p>同时有四个动作： 左移、右移、吸、不操作</p>
<p>我们可以列出其Agent function以及可以实现(逼近)该function的Agent Program。这段Agent Program的代码，很好地覆盖(实现)了左边Agent function的各种状态。</p>
<p><img src="/2022/02/22/AI-agents-and-environments/4.png" style="zoom:67%;"></p>
<p>现在我们只考虑了左移右移，那么现在如果我们给左移右移添加一些成本的话，右边这段agent program就并不能完美地实现agent function了，这就要引出我们下一节需要了解的内容——Rationality(理性)</p>
<h2 id="Rationality"><a href="#Rationality" class="headerlink" title="Rationality"></a>Rationality</h2><p>首先我们要了解 performance measure 的概念，其意思就是说，当我每次做出了一个行为，环境应该给我一个反馈(奖励或者触发)。比如说，小孩伸左手，扣10分；伸右手，加1分</p>
<p>一个理性的智能体(rational agent) ，会和环境“打交道”，从开始的一张白纸，到后面希望得到最大化的performance measure的值。也就是说，它是一个“精致利己主义者”</p>
<ul>
<li>rational agent 并不是无所不能的<ul>
<li>因为他们需要感知到环境才能做出反应。</li>
</ul>
</li>
<li>rational agent 是否能明察秋毫环境中的变化<ul>
<li>不一定，比如自动驾驶中，环境的一些变化可能不能被感知到</li>
</ul>
</li>
<li>rational agent是否应该一直在探索、学习<ul>
<li>是的</li>
</ul>
</li>
<li>rational agent是否会发生错误<ul>
<li>并不，这是比较迷惑的，对于rational agent来说，只有失败或不失败，但没有错误或不错误。</li>
</ul>
</li>
<li>所有的rational agent是否都可以自治？<ul>
<li>是的，它们在探索的过程中在不断地成长</li>
</ul>
</li>
</ul>
<h2 id="PEAS"><a href="#PEAS" class="headerlink" title="PEAS"></a>PEAS</h2><h2 id="Environment-types"><a href="#Environment-types" class="headerlink" title="Environment types"></a>Environment types</h2><h2 id="Agent-types"><a href="#Agent-types" class="headerlink" title="Agent types"></a>Agent types</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2022/02/21/Hadoop%E6%A0%B8%E5%BF%83-HDFS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/21/Hadoop%E6%A0%B8%E5%BF%83-HDFS/" class="post-title-link" itemprop="url">Hadoop核心-HDFS</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-21 22:31:03" itemprop="dateCreated datePublished" datetime="2022-02-21T22:31:03+08:00">2022-02-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-03-14 15:01:58" itemprop="dateModified" datetime="2022-03-14T15:01:58+08:00">2022-03-14</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Hadoop核心-HDFS"><a href="#Hadoop核心-HDFS" class="headerlink" title="Hadoop核心-HDFS"></a>Hadoop核心-HDFS</h1><p>现在我们来详细学习一下HDFS和MR的原理</p>
<h3 id="HDFS的设计思想"><a href="#HDFS的设计思想" class="headerlink" title="HDFS的设计思想"></a>HDFS的设计思想</h3><p>HDFS的全称是 The Hadoop Distributed File System, 即Hadoop分布式文件系统。那么作为分布式数据管理系统，HDFS最主要要解决这几个问题：</p>
<ul>
<li><strong>如何存储上百GB/TB级别大文件？</strong></li>
<li><strong>如何保证文件系统的容错？</strong></li>
<li><strong>如何进行大文件的并发读写控制？</strong></li>
</ul>
<p>首先，它最大的特点就是，可以处理大数据文件。</p>
<p>那么具体是怎么处理的呢？HDFS会将大文件分割成一个一个block，每个block的大小为128MB(或者是64MB)，然后将其存放到不同的机器(节点)上面，如下图所示：</p>
<p><img src="/2022/02/21/Hadoop%E6%A0%B8%E5%BF%83-HDFS/1.png" style="zoom:67%;"></p>
<p>其次，我们可以通过<strong>分块冗余存储</strong>来保证文件系统的容错性。也就是说，将大文件分割成的小块也可以做冗余备份。这些备份会被放在不同的节点上，因此处分存放某一文件块的所有节点都发生故障，否则HDFS在部分节点故障的情况下依然可以访问到该文件块。如下图：绿色块存放在DataNode1、2、5，黑色块存放在DataNode1、3、5</p>
<p><img src="/2022/02/21/Hadoop%E6%A0%B8%E5%BF%83-HDFS/5.png"></p>
<p>最后，为了让HDFS可以进行大文件的并发读写，我们对文件的读写做了简化——采用<strong>一次写入，多次读取</strong>的方式，即可避免读写冲突。在HDFS中，只支持顺序写入，不支持随机写入。而且不允许修改上传后的文件，如果需要修改，需要先删除，再上传修改完后的文件。</p>
<p>一个很自然的问题是，HDFS把大文件切成块之后，这些块和操作系统中的块有什么区别？</p>
<ul>
<li>显然，在HDFS将大文件切成128MB的块之后，会让一些机器去存储，对于这些机器来说，128MB的块就相当于一个文件。机器会将块继续拆分成32KB的块存放在磁盘里面。因此我们可以说HDFS切成的块在上层，操作系统分块在底层。</li>
</ul>
<h3 id="HDFS的架构"><a href="#HDFS的架构" class="headerlink" title="HDFS的架构"></a>HDFS的架构</h3><p>HDFS采用master/slave架构。一个HDFS集群是由一个NameNode、一个Secondary NameNode和一定数目的DataNode组成。NameNode是一个中心服务器，负责<strong>管理文件系统的名字空间</strong>(namespace)以及<strong>客户端对文件的访问</strong>；Sencondary NameNode 所在节点是主节点的备份节点；集群中的DataNode一般是一个节点一个，负责<strong>管理它所在节点上的存储</strong>。</p>
<p><img src="/2022/02/21/Hadoop%E6%A0%B8%E5%BF%83-HDFS/2.png" style="zoom:67%;"></p>
<p>接下来，我们来详细介绍每个节点的作用。</p>
<h4 id="NameNode内部结构"><a href="#NameNode内部结构" class="headerlink" title="NameNode内部结构"></a>NameNode内部结构</h4><p>NameNode执行文件系统的名字空间操作，比如打开、关闭、重命名文件或目录。它也负责确定数据块到具体DataNode节点的映射。</p>
<p><img src="/2022/02/21/Hadoop%E6%A0%B8%E5%BF%83-HDFS/6.png" style="zoom:67%;"></p>
<p>如上图，NameNode在内存中维护了HDFS的树形结构的目录。这和Linux文件系统的目录结构时分类似。但这个目录只指示了文件块存储的位置，而不实际存储文件。HDFS将目录结构存放在磁盘上的FsImage文件中，可以看做是HDFS目录结构的一个本地快照。</p>
<p>此外，Editlog文件主要记录了对目录以及HDFS文件块的创建、删除、重命名等操作。</p>
<h4 id="Sencondary-NameNode与NameNode的交互"><a href="#Sencondary-NameNode与NameNode的交互" class="headerlink" title="Sencondary NameNode与NameNode的交互"></a>Sencondary NameNode与NameNode的交互</h4><p>Sencondary NameNode ：充当NameNode的备份，一般在另外一台物理机上运行。一旦NameNode出现故障的时候，就利用Secondary NameNode进行恢复</p>
<p><img src="/2022/02/21/Hadoop%E6%A0%B8%E5%BF%83-HDFS/7.png"></p>
<p>其中，Secondary NameNode和NameNode之间同步的过程如上图所示：</p>
<ol>
<li>当Sec NameNode 开始请求备份数据的时候，NameNode暂时将新到达的修改操作追加到新的EditLog.new 当中</li>
<li>Sec NameNode拉取NameNode中的FsImage和EditLog两个文件</li>
<li>Sec NameNode将这两个文件进行合并形成最新的文件目录结构，形成检查点文件FsImage.ckpt</li>
<li>Sec NameNode将检查点文件返回给NameNode</li>
<li>NameNode使用FsImage.ckpt 替换旧的FsImage文件，并使用EditLog.new替换旧的EditLog文件。</li>
</ol>
<ul>
<li>DataNode处理文件系统客户端的读写请求。在NameNode的统一调度下进行数据块的创建、删除和复制。</li>
</ul>
<p>NameNode和DataNode 被设计成可以在普通的商用机器上运行。这些机器一般运行着GNU/Linux操作系统(OS)。HDFS采用Java语言开发，因此任何支持Java的机器都可以部署NameNode或DataNode。由于采用了可移植性极强的Java语言，使得HDFS可以部署到多种类型的机器上。一个典型的部署场景是一台机器上只运行一个NameNode实例，而集群中的其它机器分别运行一个DataNode实例。这种架构并不排斥在一台机器上运行多个DataNode，只不过这样的情况比较少见。</p>
<h3 id="节点之间的读与写"><a href="#节点之间的读与写" class="headerlink" title="节点之间的读与写"></a>节点之间的读与写</h3><h4 id="文件的分块与备份"><a href="#文件的分块与备份" class="headerlink" title="文件的分块与备份"></a>文件的分块与备份</h4><p>一般来说，HDFS中每个文件块<strong>都有三个副本</strong>，在写入文件块时，NameNode使用以下<strong>启发式策略</strong>来决定副本放置，并不是严格最优的</p>
<ul>
<li><p>第一个副本:  如果客户端和某一DataNode位于同一个物理节点，那么HDFS将第一个副本放置在该DataNode。如果客户端不与任何DataNode放在同一物理节点，那么HDFS随机挑选一台磁盘不太满、CPU不太忙的节点。这种副本放置策略的好处就是<strong>支持快速写入</strong></p>
</li>
<li><p>第二个副本：NameNode将第二个副本放置在与第一个副本不同的机架的某一节点上。例如下图的结构：某一文件块的第一个副本在机架1，那么第二个副本就需要放在机架2上。这样一来，如果机架2上某一个节点要读取这个文件块，就可以直接读取第二个副本了。</p>
<p>因此可以有效地减少跨机架的网络流量。</p>
</li>
<li><p>第三个副本：NameNode将第三个副本放在第一个副本所在机架的不同节点上。这样，如果第一个副本所在的节点宕机了，那么该节点可以读取第三个副本。而且即便机架与交换机之间存在故障也不影响。</p>
<p>因此这种副本放置策略有利于应对故障发生时的文件块读取。</p>
</li>
<li><p>如果还有更多的副本，NameNode将随机选择节点来放置</p>
</li>
</ul>
<p><img src="/2022/02/21/Hadoop%E6%A0%B8%E5%BF%83-HDFS/8.jpeg" style="zoom: 67%;"></p>
<h4 id="文件写入"><a href="#文件写入" class="headerlink" title="文件写入"></a>文件写入</h4><p><img src="/2022/02/21/Hadoop%E6%A0%B8%E5%BF%83-HDFS/4.png" style="zoom:150%;"></p>
<p>首先我要将一个200M文件存到HDFS集群中。</p>
<ol>
<li>客户端通过RPC（远程服务）访问NameNode，请求写入一个文件。</li>
<li>NameNode检查客户端是否有权限写入，如果有权限返回一个响应。如果没有客户端就会抛出一个异常。</li>
<li>客户端会将文件按BlckSize大小（默认128M）将文件切分成一个一个Block块，然后请求写入第一个Block块。</li>
<li>NameNode会根据它的负载均衡机制，给客户端返回满足其副本数量（默认是3）的列表（BlockId：主机，端口号，存放的目录）。</li>
<li>客户端根据返回的列表，开始建立管道（pipeline）。客户端-&gt;第一个节点-&gt;第二个节点-&gt;第三个节点。</li>
<li>开始传输数据，Block按照Packet一次传输，当一个Packet成功传输到第一个DataNode上以后，第一个DodaNode就把这个Packet开始进行复制，并将这个Packet通过管道传输到下一个DataNode上，下一个DataNode接收到Packet后，继续进行复制，再传输到下一个DataNode上。这就是<strong>单个文件块传输的流水线方式</strong></li>
<li>当一个Block块成功传输完以后，从最后一个DataNode开始，依次从管道返回ACK队列，到客户端。最后，DataNode1会向客户端发送确认信息，表示该文件已成功写入。</li>
<li>客户端会在自己内部维护着一个ACK队列，跟返回来的ACK队列进行匹配，只要有一台DataNode写成功，就认为这次写操作是完成的。</li>
<li>开始进行下一个Block块的写入。重复3-8。 </li>
</ol>
<p>这样一来，集群中单一NameNode的结构大大简化了系统的架构。NameNode是所有HDFS元数据的仲裁者和管理者。用户数据永远不会流过NameNode。</p>
<p>如果在传输的时候，有的DataNode宕机了，这个DataNode就会从这个管道中退出。剩下的DataNode继续传输。然后，等传输完成以后，NameNode会再分发出一个节点，去写成功的DataNode上复制出一份Block块，写到新的DataNode上。</p>
<h5 id="流水线复制"><a href="#流水线复制" class="headerlink" title="流水线复制"></a>流水线复制</h5><p>当客户端向HDFS文件写入数据的时候，一开始是写到本地临时文件中。假设该文件的副本系数设置为3，当本地临时文件累积到一个数据块的大小时，客户端会从NameNode获取一个DataNode列表用于存放副本。然后客户端开始向第一个DataNode传输数据，第一个DataNode一小部分一小部分(4 KB)地接收数据，将每一部分写入本地仓库，并同时传输该部分到列表中第二个DataNode节点。第二个DataNode也是这样，一小部分一小部分地接收数据，写入本地仓库，并同时传给第三个DataNode。最后，第三个DataNode接收数据并存储在本地。因此，DataNode能流水线式地从前一个节点接收数据，并在同时转发给下一个节点，数据以流水线的方式从前一个DataNode复制到下一个。</p>
<h4 id="文件读取"><a href="#文件读取" class="headerlink" title="文件读取"></a>文件读取</h4><p>现在我们来学习，在这种架构下，当客户端需要读取数据的时候，系统是如何操作的。</p>
<ol>
<li>客户端回合NameNode通信，请求读取一个文件</li>
<li>NameNode会根据文件的路径等信息，判断读取请求是否合法。如果合法则向客户端返回文件所有数据块的存放地址</li>
<li>对于第一个数据块，客户端从最近的存放该数据块的DataNode读取数据</li>
<li>当第一个数据块读取完毕后，客户端从最近的存放第二个数据块的DataNode读取数据</li>
<li>以此类推，客户端读取下一个数据块，直到读取完所有数据块。</li>
</ol>
<p><img src="/2022/02/21/Hadoop%E6%A0%B8%E5%BF%83-HDFS/8.png"></p>
<p>读取和写入的不同在于，当我写入文件的时候，客户端在每次写入一个文件块的时候，都需要询问NameNode该文件块存放的位置，并不是NameNode一次性告诉客户端所有的文件块应该存放的位置；而读取的时候，客户端则可以一下子知道所有文件块应该存放的位置。这很好理解，在写入的时候节点状态时刻在发生变化。</p>
<h4 id="文件读写与一致性"><a href="#文件读写与一致性" class="headerlink" title="文件读写与一致性"></a>文件读写与一致性</h4><p>如果在HDFS上对同一个文件进行并发读写访问，那么就需要加锁来保证互斥访问。这会增加编程的复杂度。因此我们可以约定：只会有一个写文件的请求发生，文件写入之后就不会更改了，文件写入后的读取操作可以并发。</p>
<p>基于这种思想，HDFS采用了<strong>一次写入、多次读取</strong>和简化一致性模型：</p>
<ul>
<li>一个文件经过创建、写入和关闭后，就不能改变文件中已有的内容</li>
<li>已经写入到HDFS的文件，仅允许在文件末尾追加数据，即append</li>
<li>当对一个文件进行写入操作或者追加操作的时候，NameNode将拒绝其他针对该文件的读、写请求</li>
<li>当对一个文件进行读取操作时，NameNode允许其他针对该文件的读请求。</li>
</ul>
<p><strong>提问</strong>： 读取的时候可以并行读取吗？</p>
<p>如果以还原文件为目的的话，是不可以并行的，因为文件的块与块之间需要做一个拼接，如果并行读取的话，会导致文件数据混乱无法还原。</p>
<h3 id="Namespace"><a href="#Namespace" class="headerlink" title="Namespace"></a>Namespace</h3><p>Namespace的中文名是：文件系统的名字空间 </p>
<p>HDFS支持传统的层次型文件组织结构。用户或者应用程序可以创建目录，然后将文件保存在这些目录里。文件系统名字空间的层次结构和大多数现有的文件系统类似：用户可以创建、删除、移动或重命名文件。当前，HDFS不支持用户磁盘配额和访问权限控制，也不支持硬链接和软链接。但是HDFS架构并不妨碍实现这些特性。</p>
<p>NameNode负责<strong>维护文件系统的名字空间</strong>，任何对文件系统名字空间或属性的修改都将被NameNode记录下来。应用程序可以设置HDFS保存的文件的副本数目。文件副本的数目称为文件的副本系数，这个信息也是由NameNode保存的。</p>
<h4 id="Back-up-Metadata"><a href="#Back-up-Metadata" class="headerlink" title="Back up Metadata"></a>Back up Metadata</h4><p>HDFS被设计成能够在一个大集群中跨机器可靠地存储超大文件。它将每个文件存储成一系列的数据块，除了最后一个，所有的数据块都是同样大小的。<strong>为了容错，文件的所有数据块都会有副本。</strong>每个文件的数据块大小和副本系数都是可配置的。应用程序可以指定某个文件的副本数目。副本系数可以在文件创建的时候指定，也可以在之后改变。HDFS中的文件都是一次性写入的，并且严格要求在任何时候只能有一个写入者。</p>
<p>NameNode全权管理数据块的复制，它周期性地从集群中的每个DataNode接收心跳信号和块状态报告(Blockreport)。接收到心跳信号意味着该DataNode节点工作正常。块状态报告包含了一个该DataNode上所有数据块的列表。</p>
<p>就比如说</p>
<h4 id="副本存放-最最开始的一步"><a href="#副本存放-最最开始的一步" class="headerlink" title="副本存放: 最最开始的一步"></a>副本存放: 最最开始的一步</h4><p>副本的存放是HDFS可靠性和性能的关键。优化的副本存放策略是HDFS区分于其他大部分分布式文件系统的重要特性。这种特性需要做大量的调优，并需要经验的积累。HDFS采用一种称为机架感知(rack-aware)的策略来改进数据的可靠性、可用性和网络带宽的利用率。目前实现的副本存放策略只是在这个方向上的第一步。实现这个策略的短期目标是验证它在生产环境下的有效性，观察它的行为，为实现更先进的策略打下测试和研究的基础。</p>
<p>大型HDFS实例一般运行在跨越多个机架的计算机组成的集群上，不同机架上的两台机器之间的通讯需要经过交换机。在大多数情况下，同一个机架内的两台机器间的带宽会比不同机架的两台机器间的带宽大。</p>
<p>通过一个<a href="https://hadoop.apache.org/docs/r1.0.4/cn/cluster_setup.html#Hadoop的机架感知" target="_blank" rel="noopener">机架感知</a>的过程，NameNode可以确定每个DataNode所属的机架id。一个简单但没有优化的策略就是将副本存放在不同的机架上。这样可以有效防止当整个机架失效时数据的丢失，并且允许读数据的时候充分利用多个机架的带宽。这种策略设置可以将副本均匀分布在集群中，有利于当组件失效情况下的负载均衡。但是，因为这种策略的一个写操作需要传输数据块到多个机架，这增加了写的代价。</p>
<p>在大多数情况下，副本系数是3，HDFS的存放策略是将一个副本存放在本地机架的节点上，一个副本放在同一机架的另一个节点上，最后一个副本放在不同机架的节点上。这种策略减少了机架间的数据传输，这就提高了写操作的效率。机架的错误远远比节点的错误少，所以这个策略不会影响到数据的可靠性和可用性。于此同时，因为数据块只放在两个（不是三个）不同的机架上，所以此策略减少了读取数据时需要的网络传输总带宽。在这种策略下，副本并不是均匀分布在不同的机架上。三分之一的副本在一个节点上，三分之二的副本在一个机架上，其他副本均匀分布在剩下的机架中，<strong>这一策略在不损害数据可靠性和读取性能的情况下改进了写的性能</strong>。 </p>
<h4 id="副本选择"><a href="#副本选择" class="headerlink" title="副本选择"></a>副本选择</h4><p>为了降低整体的带宽消耗和读取延时，<strong>HDFS会尽量让读取程序读取离它最近的副本</strong>。如果在读取程序的同一个机架上有一个副本，那么就读取该副本。如果一个HDFS集群跨越多个数据中心，那么客户端也将首先读本地数据中心的副本。</p>
<h4 id="安全模式"><a href="#安全模式" class="headerlink" title="安全模式"></a>安全模式</h4><p>NameNode启动后会进入一个称为安全模式的特殊状态。处于安全模式的NameNode是不会进行数据块的复制的。NameNode从所有的 DataNode接收心跳信号和块状态报告。块状态报告包括了某个DataNode所有的数据块列表。每个数据块都有一个指定的最小副本数。当NameNode检测确认某个数据块的副本数目达到这个最小值，那么该数据块就会被认为是副本安全(safely replicated)的；在一定百分比（这个参数可配置）的数据块被NameNode检测确认是安全之后（加上一个额外的30秒等待时间），NameNode将退出安全模式状态。接下来它会确定还有哪些数据块的副本没有达到指定数目，并将这些数据块复制到其他DataNode上。</p>
<h3 id="文件系统元数据的持久化"><a href="#文件系统元数据的持久化" class="headerlink" title="文件系统元数据的持久化"></a>文件系统元数据的持久化</h3><p>NameNode上保存着HDFS的名字空间。对于任何对文件系统元数据产生修改的操作，NameNode都会使用一种称为EditLog的事务日志记录下来。例如，在HDFS中创建一个文件，NameNode就会在Editlog中插入一条记录来表示；同样地，修改文件的副本系数也将往Editlog插入一条记录。NameNode在本地操作系统的文件系统中存储这个Editlog。整个文件系统的名字空间，包括数据块到文件的映射、文件的属性等，都存储在一个称为FsImage的文件中，这个文件也是放在NameNode所在的本地文件系统上。</p>
<p>NameNode在内存中保存着整个文件系统的名字空间和文件数据块映射(Blockmap)的映像。这个关键的元数据结构设计得很紧凑，因而一个有4G内存的NameNode足够支撑大量的文件和目录。当NameNode启动时，它从硬盘中读取Editlog和FsImage，将所有Editlog中的事务作用在内存中的FsImage上，并将这个新版本的FsImage从内存中保存到本地磁盘上，然后删除旧的Editlog，因为这个旧的Editlog的事务都已经作用在FsImage上了。这个过程称为一个检查点(checkpoint)。在当前实现中，检查点只发生在NameNode启动时，在不久的将来将实现支持周期性的检查点。</p>
<p>DataNode将HDFS数据以文件的形式存储在本地的文件系统中，它并不知道有关HDFS文件的信息。它把每个HDFS数据块存储在本地文件系统的一个单独的文件中。DataNode并不在同一个目录创建所有的文件，实际上，它用试探的方法来确定每个目录的最佳文件数目，并且在适当的时候创建子目录。在同一个目录中创建所有的本地文件并不是最优的选择，这是因为本地文件系统可能无法高效地在单个目录中支持大量的文件。当一个DataNode启动时，它会扫描本地文件系统，产生一个这些本地文件对应的所有HDFS数据块的列表，然后作为报告发送到NameNode，这个报告就是块状态报告。</p>
<h3 id="通讯协议"><a href="#通讯协议" class="headerlink" title="通讯协议"></a>通讯协议</h3><p>所有的HDFS通讯协议都是建立在TCP/IP协议之上。客户端通过一个可配置的TCP端口连接到NameNode，通过ClientProtocol协议与NameNode交互。而DataNode使用DataNodeProtocol协议与NameNode交互。一个远程过程调用(RPC)模型被抽象出来封装ClientProtocol和DataNodeprotocol协议。在设计上，NameNode不会主动发起RPC，而是响应来自客户端或 DataNode 的RPC请求。</p>
<h3 id="容错机制"><a href="#容错机制" class="headerlink" title="容错机制"></a>容错机制</h3><p>HDFS的主要目标就是即使在出错的情况下也要保证数据存储的可靠性。常见的三种出错情况是：NameNode出错, DataNode出错和网络割裂(network partitions)。</p>
<h4 id="磁盘数据错误，心跳检测和重新复制"><a href="#磁盘数据错误，心跳检测和重新复制" class="headerlink" title="磁盘数据错误，心跳检测和重新复制"></a>磁盘数据错误，心跳检测和重新复制</h4><p>每个DataNode节点周期性地向NameNode发送心跳信号。网络割裂可能导致一部分DataNode跟NameNode失去联系。NameNode通过心跳信号的缺失来检测这一情况，并将这些近期不再发送心跳信号DataNode标记为宕机，不会再将新的IO请求发给它们。任何存储在宕机DataNode上的数据将不再有效。DataNode的宕机可能会引起一些数据块的副本系数低于指定值，NameNode不断地检测这些需要复制的数据块，一旦发现就启动复制操作。在下列情况下，可能需要重新复制：某个DataNode节点失效，某个副本遭到损坏，DataNode上的硬盘错误，或者文件的副本系数增大。</p>
<h4 id="集群均衡"><a href="#集群均衡" class="headerlink" title="集群均衡"></a>集群均衡</h4><p>HDFS的架构支持数据均衡策略。如果某个DataNode节点上的空闲空间低于特定的临界点，按照均衡策略系统就会自动地将数据从这个DataNode移动到其他空闲的DataNode。当对某个文件的请求突然增加，那么也可能启动一个计划创建该文件新的副本，并且同时重新平衡集群中的其他数据。这些均衡策略目前还没有实现。</p>
<h4 id="数据完整性"><a href="#数据完整性" class="headerlink" title="数据完整性"></a>数据完整性</h4><p>从某个DataNode获取的数据块有可能是损坏的，损坏可能是由DataNode的存储设备错误、网络错误或者软件bug造成的。HDFS客户端软件实现了对HDFS文件内容的校验和(checksum)检查。当客户端创建一个新的HDFS文件，会计算这个文件每个数据块的校验和，并将校验和作为一个单独的隐藏文件保存在同一个HDFS名字空间下。当客户端获取文件内容后，它会检验从DataNode获取的数据跟相应的校验和文件中的校验和是否匹配，如果不匹配，客户端可以选择从其他DataNode获取该数据块的副本。</p>
<h4 id="元数据磁盘错误"><a href="#元数据磁盘错误" class="headerlink" title="元数据磁盘错误"></a>元数据磁盘错误</h4><p>FsImage和Editlog是HDFS的核心数据结构。如果这些文件损坏了，整个HDFS实例都将失效。因而，NameNode可以配置成支持维护多个FsImage和Editlog的副本。任何对FsImage或者Editlog的修改，都将同步到它们的副本上。这种多副本的同步操作可能会降低NameNode每秒处理的名字空间事务数量。然而这个代价是可以接受的，因为即使HDFS的应用是数据密集的，它们也非元数据密集的。当NameNode重启的时候，它会选取最近的完整的FsImage和Editlog来使用。</p>
<p>NameNode是HDFS集群中的单点故障(single point of failure)所在。如果NameNode机器故障，是需要手工干预的。目前，自动重启或在另一台机器上做NameNode故障转移的功能还没实现。</p>
<h4 id="快照"><a href="#快照" class="headerlink" title="快照"></a>快照</h4><p>快照支持某一特定时刻的数据的复制备份。利用快照，可以让HDFS在数据损坏时恢复到过去一个已知正确的时间点。HDFS目前还不支持快照功能，但计划在将来的版本进行支持。</p>
<h3 id="数据组织"><a href="#数据组织" class="headerlink" title="数据组织"></a>数据组织</h3><h4 id="数据块"><a href="#数据块" class="headerlink" title="数据块"></a>数据块</h4><p>HDFS被设计成支持大文件，适用HDFS的是那些需要处理大规模的数据集的应用。这些应用都是只写入数据一次，但却读取一次或多次，并且读取速度应能满足流式读取的需要。HDFS支持文件的“一次写入多次读取”语义。一个典型的数据块大小是64MB(或128MB)。因而，HDFS中的文件总是按照64M被切分成不同的块，每个块尽可能地存储于不同的DataNode中。</p>
<h4 id="Staging"><a href="#Staging" class="headerlink" title="Staging"></a>Staging</h4><p>客户端创建文件的请求其实并没有立即发送给NameNode，事实上，在刚开始阶段HDFS客户端会先将文件数据缓存到本地的一个临时文件。应用程序的写操作被透明地重定向到这个临时文件。当这个临时文件累积的数据量超过一个数据块的大小，客户端才会联系NameNode。NameNode将文件名插入文件系统的层次结构中，并且分配一个数据块给它。然后返回DataNode的标识符和目标数据块给客户端。接着客户端将这块数据从本地临时文件上传到指定的DataNode上。当文件关闭时，在临时文件中剩余的没有上传的数据也会传输到指定的DataNode上。然后客户端告诉NameNode文件已经关闭。此时NameNode才将文件创建操作提交到日志里进行存储。如果NameNode在文件关闭前宕机了，则该文件将丢失。</p>
<p>上述方法是对在HDFS上运行的目标应用进行认真考虑后得到的结果。这些应用需要进行文件的流式写入。如果不采用客户端缓存，由于网络速度和网络堵塞会对吞估量造成比较大的影响。这种方法并不是没有先例的，早期的文件系统，比如AFS，就用客户端缓存来提高性能。为了达到更高的数据上传效率，已经放松了POSIX标准的要求。</p>
<h3 id="可访问性"><a href="#可访问性" class="headerlink" title="可访问性"></a>可访问性</h3><p>HDFS给应用提供了多种访问方式。用户可以通过<a href="https://hadoop.apache.org/docs/current/api/index.html" target="_blank" rel="noopener">Java API</a>接口访问，通过命令行访问，通过Ambari可视化界面访问，也可以通过<a href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/LibHdfs.html" target="_blank" rel="noopener">C语言的封装API</a>访问，还可以通过浏览器的方式访问HDFS中的文件。 </p>
<h4 id="DFS-Shell"><a href="#DFS-Shell" class="headerlink" title="DFS Shell"></a>DFS Shell</h4><p>HDFS以文件和目录的形式组织用户数据。它提供了一个命令行的接口(DFSShell)让用户与HDFS中的数据进行交互。命令的语法和用户熟悉的其他shell(例如 bash, csh)工具类似。下面是一些动作/命令的示例：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">动作</th>
<th style="text-align:center">命令</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建一个名为 <code>/foodir</code>的目录</td>
<td style="text-align:center"><code>bin/hadoop dfs -mkdir /foodir</code></td>
</tr>
<tr>
<td style="text-align:center">创建一个名为 <code>/foodir</code> 的目录</td>
<td style="text-align:center"><code>bin/hadoop dfs -mkdir /foodir</code></td>
</tr>
<tr>
<td style="text-align:center">查看名为<code>/foodir/myfile.txt</code>的文件内容</td>
<td style="text-align:center"><code>bin/hadoop dfs -cat /foodir/myfile.txt</code></td>
</tr>
</tbody>
</table>
</div>
<p>DFSShell 可以用在那些通过脚本语言和文件系统进行交互的应用程序上。</p>
<h4 id="DFS-Admin"><a href="#DFS-Admin" class="headerlink" title="DFS Admin"></a>DFS Admin</h4><p>DFSAdmin 命令用来管理HDFS集群。这些命令只有HDSF的管理员才能使用。下面是一些动作/命令的示例：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">动作</th>
<th style="text-align:center">命令</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">将集群置于安全模式</td>
<td style="text-align:center">bin/hadoop dfsadmin -safemode enter</td>
</tr>
<tr>
<td style="text-align:center">显示DataNode列表</td>
<td style="text-align:center">bin/hadoop dfsadmin -report</td>
</tr>
<tr>
<td style="text-align:center">使DataNode节点 DataNodename退役</td>
<td style="text-align:center">bin/hadoop dfsadmin -decommission DataNodename</td>
</tr>
</tbody>
</table>
</div>
<h4 id="浏览器接口"><a href="#浏览器接口" class="headerlink" title="浏览器接口"></a>浏览器接口</h4><p>一个典型的HDFS安装会在一个可配置的TCP端口开启一个Web服务器用于暴露HDFS的名字空间。用户可以用浏览器来浏览HDFS的名字空间和查看文件的内容。</p>
<h3 id="存储空间回收"><a href="#存储空间回收" class="headerlink" title="存储空间回收"></a>存储空间回收</h3><h4 id="文件的删除和恢复"><a href="#文件的删除和恢复" class="headerlink" title="文件的删除和恢复"></a>文件的删除和恢复</h4><p>当用户或应用程序删除某个文件时，这个文件并没有立刻从HDFS中删除。实际上，HDFS会将这个文件重命名转移到<code>/trash</code>目录。只要文件还在<code>/trash</code>目录中，该文件就可以被迅速地恢复。文件在<code>/trash</code>中保存的时间是可配置的，当超过这个时间时，NameNode就会将该文件从名字空间中删除。删除文件会使得该文件相关的数据块被释放。注意，从用户删除文件到HDFS空闲空间的增加之间会有一定时间的延迟。</p>
<p>只要被删除的文件还在<code>/trash</code>目录中，用户就可以恢复这个文件。如果用户想恢复被删除的文件，可以浏览<code>/trash</code>目录找回该文件。<code>/trash</code>目录仅仅保存被删除文件的最后副本。<code>/trash</code>目录与其他的目录没有什么区别，除了一点：在该目录上HDFS会应用一个特殊策略来自动删除文件。目前的默认策略是删除<code>/trash</code>中保留时间超过6小时的文件。将来，这个策略可以通过一个被良好定义的接口配置。</p>
<h4 id="减少副本系数"><a href="#减少副本系数" class="headerlink" title="减少副本系数"></a>减少副本系数</h4><p>当一个文件的副本系数被减小后，NameNode会选择过剩的副本删除。下次心跳检测时会将该信息传递给DataNode。DataNode遂即移除相应的数据块，集群中的空闲空间加大。同样，在调用setReplication API结束和集群中空闲空间增加间会有一定的延迟。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2022/02/20/%E4%BA%86%E8%A7%A3Hadoop/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/20/%E4%BA%86%E8%A7%A3Hadoop/" class="post-title-link" itemprop="url">了解Hadoop</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-20 22:48:25" itemprop="dateCreated datePublished" datetime="2022-02-20T22:48:25+08:00">2022-02-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-02-21 21:18:10" itemprop="dateModified" datetime="2022-02-21T21:18:10+08:00">2022-02-21</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="了解Hadoop"><a href="#了解Hadoop" class="headerlink" title="了解Hadoop"></a>了解Hadoop</h1><p>Hadoop 可以理解为一个开源的<strong>软件平台</strong>，它的作用是在计算机集群上分布式存储和分布式处理非常大的数据集。</p>
<p>Hadoop的框架最核心的设计就是：HDFS和 MapReduce。HDFS为海量的数据提供了存储，而MapReduce则为海量的数据提供了计算 </p>
<p>那么我们为什么要用Hadoop来处理大量数据呢？当数据以 TB为单位产生的时候，使用单个数据库显然是不现实的，因此我们需要使用分布式数据库。</p>
<h2 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h2><ul>
<li><p>Hadoop最早起源于Yahoo! 的”Nutch”。Nutch的设计目标是构建一个大型的全网搜索引擎，包括网页抓取、索引、查询等功能，但随着抓取网页数量的增加，遇到了严重的可扩展性问题——如何解决数十亿网页的存储和索引问题。</p>
</li>
<li><p>2003年、2004年谷歌发表的两篇论文为该问题提供了可行的解决方案。<br>——分布式文件系统（GFS），可用于处理海量网页的存储<br>——分布式计算框架MapReduce，可用于处理海量网页的索引计算问题。</p>
</li>
<li>Nutch的开发人员完成了相应的开源实现HDFS和MapReduce，并从Nutch中剥离成为独立项目Hadoop，到2008年1月，Hadoop成为Apache顶级项目(同年，cloudera公司成立)，迎来了它的快速发展期。</li>
<li>狭义上来说，hadoop就是单独指代hadoop这个软件;</li>
<li>广义上来说，hadoop指代大数据的一个生态圈，包括很多其他的软件 </li>
</ul>
<h2 id="Hadoop的核心架构"><a href="#Hadoop的核心架构" class="headerlink" title="Hadoop的核心架构"></a>Hadoop的核心架构</h2><p><img src="/2022/02/20/%E4%BA%86%E8%A7%A3Hadoop/1.png" style="zoom:67%;"></p>
<h3 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h3><p>HDFS是Hadoop项目的一部分，项目发起者是Doug Cutting，最初Hadoop只是Hadoop Lucene的子项目Nutch（文本搜索库）的一部分。2003年和2004年，Google先后发表了GFS和MapReduce两篇论文，Doug Cutting认为GFS和MapReduce不仅可以解决超大规模的网页存储和分析处理问题，而且是一个通用处理技术。因此Doug Cutting根据GFS和MapReduce的思想创建了Hadoop项目，并从Lucene项目中独立出来。</p>
<h3 id="YARN"><a href="#YARN" class="headerlink" title="YARN"></a>YARN</h3><p>Yarn最初是为MapReduce设计的一种资源管理器，后成为通用的资源管理系统，为上层应用提供统一的资源管理和调度。Yarn的引入为集群在利用率、资源统一管理和数据共享方面带来了好处。2012年8月，Yarn成为了Apache Hadoop的一个子项目。</p>
<p>我们认为：引入Yarn之前的MapReduce为第一代MapReduce，引入Yarn之后的MapReduce为第二代MapReduce，第一代MapReduce存在局限性，其中最为显著的是资源管理和作业紧密耦合。Yarn的出现使得资源管理模块从第一代MapReduce中独立出来，成为一个通用资源管理平台，而MapReduce和Spark等则作为运行于该平台之上的框架。</p>
<p><img src="/2022/02/20/%E4%BA%86%E8%A7%A3Hadoop/2.png" style="zoom:67%;"></p>
<h3 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h3><p>通常所说的MapReduce一般是指Hadoop项目中的MapReduce，它是Google发表学术论文的一种开源实现，而Google公司内部使用的MapReduce系统并不是开源Hadoop项目中的MapReduce。</p>
<p>MapReduce用于处理大批量静态数据，MapReduce被纳入批处理系统范畴。</p>
<p><img src="/2022/02/20/%E4%BA%86%E8%A7%A3Hadoop/3.png" style="zoom:67%;"></p>
<h3 id="Pig"><a href="#Pig" class="headerlink" title="Pig"></a>Pig</h3><p>Pig是Hadoop数据操作的客户端是一个数据分析引擎，采用了一定的语法操作HDFS中的数据（Pig应该说是一种语言，有人说Pig是类SQL的语言我这里只能说它的功能类似Sql语言和数据库的关系，而且这里的Sql更像是PLSQL而不是标准SQL，Hadoop中更像标准Sql的应该是Hive或者叫HiveQL），它的语言比较像Shell脚本，可以嵌入Hadoop的JAVA程序中，从而达到简化代码的功能，Pig的脚本叫Pig Latin，之所以说Pig是一个数据分析引擎，是因为Pig相当于一个翻译器，将Pig Latin语句翻译成MapReduce程序，而Pig Latin语句是一种用于处理大规模数据的脚本语言。Pig Latin可完成排序（Order By）、过滤(Where)、求和(Sum)、分组(Group By)、关联(Join)等操作，支持自定义函数；Pig Latin是把类似Sql的语句转换成MapReduce过程进行处理，减少Java 代码的书写，Pig的运行方式有Grunt Shell方式，脚本方式和嵌入式方式。 </p>
<p>因此，运用pig 脚本我们可以避免用Java或者Python编写MapReduce程序</p>
<h3 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h3><p><a href="https://www.cnblogs.com/jinb/p/6627521.html" target="_blank" rel="noopener">https://www.cnblogs.com/jinb/p/6627521.html</a></p>
<ol>
<li><p>hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供完整的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。</p>
</li>
<li><p>Hive是建立在 Hadoop 上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取转化加载（ETL），这是一种可以存储、查询和分析存储在 Hadoop 中的大规模数据的机制。Hive 定义了简单的类 SQL 查询语言，称为 HQL，它允许熟悉 SQL 的用户查询数据。同时，这个语言也允许熟悉 MapReduce 开发者的开发自定义的 mapper 和 reducer 来处理内建的 mapper 和 reducer 无法完成的复杂的分析工作。</p>
</li>
</ol>
<h3 id="Ambari"><a href="#Ambari" class="headerlink" title="Ambari"></a>Ambari</h3><p>Ambari是Apache推出的一个集中管理Hadoop的集群的一个平台，可以快速帮助搭建Hadoop及相关组件的平台，管理集群方便。在 HDP中，就是用Ambari来对各类组件进行管理，大部分Hadoop组件都可以通过Ambari安装部署管理，包括HDFS、MapReduce、Hive、Pig、HBase、Zookeeper、Sqoop等。它提供一个可视的仪表盘来查看集群的状态，诊断其性能特征。</p>
<p><img src="/2022/02/20/%E4%BA%86%E8%A7%A3Hadoop/4.png" style="zoom:67%;"></p>
<p>对于Ambari的功能介绍，可以看这篇博客： <a href="https://blog.csdn.net/zhangxiongcolin/article/details/83585666" target="_blank" rel="noopener">https://blog.csdn.net/zhangxiongcolin/article/details/83585666</a></p>
<h3 id="MESOS"><a href="#MESOS" class="headerlink" title="MESOS"></a>MESOS</h3><p>Mesos是一个集群管理平台，和YARN是类似的。 可以理解为是一种分布式系统的kernel， 负责集群资源的分配， 这里的资源指的是CPU资源， 内存资源， 存储资源， 网络资源等。 在Mesos可以运行Spark， Storm， Hadoop， Marathon等多种Framework（框架）。 </p>
<p>Mesos的架构主要有Masters（主节点）， Slaves（从节点）， 和 及在Mesos上运行的Framework（框架）组成。 各个部分的分工如下：</p>
<p>Master： 负责处理Slave节点和Framework间的资源通讯， 根据指定的策略来决定分配多少资源给framework。 </p>
<p>Slave:  启动本地进程， 同时向Master报告有哪些资源可用。 </p>
<p>Framework： 接收来自Master提供的Slave节点的资源（如CPU和内存）， Framework由调度器（负责监控和管理Slave的状态）和执行器（负责在服务器执行应用程序代码）组成。 </p>
<p>Mesos本身只提供资源的分配， 并不涉及存储， 任务调度等功能， 所以它要和其它软件或者系统搭配使用才能构成完整的分布式系统。 Mesos， Docker， Marathon/Chronos,  RabbitMQ,  HDFS/Ceph构成了一个完整的分布式系统，  分别负责资源分配， 进程管理，任务调度，  进程间通信和文件系统的功能。 这里可以和Linux做一个横向的比较。 如图1所示: </p>
<p><img src="/2022/02/20/%E4%BA%86%E8%A7%A3Hadoop/5.png" style="zoom:67%;"></p>
<h3 id="TEZ"><a href="#TEZ" class="headerlink" title="TEZ"></a>TEZ</h3><p>MapReduce模型虽然很厉害，但是它不够的灵活，一个简单的join都需要很多骚操作才能完成，又是加标签又是笛卡尔积。那有人就说我就是不想这么干那怎么办呢？Tez是一种新的解决方法</p>
<p>Tez采用了DAG（有向无环图）来组织MR任务（DAG中一个节点就是一个RDD，边表示对RDD的操作）。它的核心思想是把将Map任务和Reduce任务进一步拆分，Map任务拆分为Input-Processor-Sort-Merge-Output，Reduce任务拆分为Input-Shuffer-Sort-Merge-Process-output，Tez将若干小任务灵活重组，形成一个大的DAG作业。</p>
<p><img src="/2022/02/20/%E4%BA%86%E8%A7%A3Hadoop/6.jpeg" style="zoom:67%;"></p>
<p>上图中蓝色框表示Map任务，绿色框表示Reduce任务，云图表示写动作，可以看出，Tez去除了MR中不必要的写过程和Map，形成一张大的DAG图，在数据处理过程中没有往hdfs写数据，直接向后继节点输出，从而提升了效率。</p>
<p><img src="/2022/02/20/%E4%BA%86%E8%A7%A3Hadoop/7.jpeg"></p>
<p>上图是Hadoop1到Hadoop2所做的改变，Hadoop1主要使用MapReduce引擎，到了Hadoop2，基于yarn，可以部署spark，tez等计算引擎，这里MapReduce作为一种引擎实现用的越来越少了，但是作为框架思路，tez本身也是MapReduce的改进。</p>
<h3 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h3><p>Spark是加州大学伯克利分校AMP实验室（Algorithms, Machines, and People Lab）开发的通用内存并行计算框架</p>
<p>Spark是在借鉴了MapReduce之上发展而来的，继承了其分布式并行计算的优点并改进了MapReduce明显的缺陷，具体如下：</p>
<p>首先，Spark把中间数据放到内存中，迭代运算效率高。MapReduce中计算结果需要落地，保存到磁盘上，这样势必会影响整体速度，而Spark支持DAG图的分布式并行计算的编程框架，减少了迭代过程中数据的落地，提高了处理效率。</p>
<p>其次，Spark容错性高。Spark引进了弹性分布式数据集RDD (Resilient Distributed Dataset) 的抽象，它是分布在一组节点中的只读对象集合，这些集合是弹性的，如果数据集一部分丢失，则可以根据“血统”（即充许基于数据衍生过程）对它们进行重建。另外在RDD计算时可以通过CheckPoint来实现容错，而CheckPoint有两种方式：CheckPoint Data，和Logging The Updates，用户可以控制采用哪种方式来实现容错。</p>
<p>最后，Spark更加通用。不像Hadoop只提供了Map和Reduce两种操作，Spark提供的数据集操作类型有很多种，大致分为：Transformations和Actions两大类。Transformations包括Map、Filter、FlatMap、Sample、GroupByKey、ReduceByKey、Union、Join、Cogroup、MapValues、Sort和PartionBy等多种操作类型，同时还提供Count, Actions包括Collect、Reduce、Lookup和Save等操作。另外各个处理节点之间的通信模型不再像Hadoop只有Shuffle一种模式，用户可以命名、物化，控制中间结果的存储、分区等。 </p>
<p><img src="/2022/02/20/%E4%BA%86%E8%A7%A3Hadoop/8.png"></p>
<h3 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h3><p>Apache HBase 是 Hadoop 数据库，一个分布式、可伸缩的<strong>大数据存储</strong>。</p>
<p>HBase是依赖Hadoop的。为什么HBase能存储海量的数据？<strong>因为HBase是在HDFS的基础之上构建的，HDFS是分布式文件系统</strong>。</p>
<p>HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。另一个不同的是HBase基于列的而不是基于行的模式。</p>
<p>此外，Pig和Hive还为HBase提供了高层语言支持，使得在HBase上进行数据统计处理变的非常简单。 Sqoop则为HBase提供了方便的RDBMS数据导入功能，使得传统数据库数据向HBase中迁移变的非常方便。</p>
<h3 id="ZooKeeper"><a href="#ZooKeeper" class="headerlink" title="ZooKeeper"></a>ZooKeeper</h3><p> ZooKeeper 顾名思义 动物园管理员，他是拿来管<a href="http://hadoop.apache.org/" target="_blank" rel="noopener">大象(Hadoop)</a> 、 <a href="http://hive.apache.org/" target="_blank" rel="noopener">蜜蜂(Hive)</a> 、<a href="http://pig.apache.org/" target="_blank" rel="noopener"> 小猪(Pig)</a> 的管理员， Apache Hbase和 Apache Solr 以及<a href="http://sna-projects.com/sensei" target="_blank" rel="noopener">LinkedIn sensei </a> 等项目中都采用到了 Zookeeper。ZooKeeper是一个分布式的，开放源码的<strong>分布式应用程序协调服务</strong>，ZooKeeper是以<a href="http://rdc.taobao.com/blog/cs/?p=261" target="_blank" rel="noopener">Fast Paxos</a>算法为基础，实现同步服务，配置维护和命名服务等分布式应用。</p>
<p>ZooKeeper可以用来跟踪各个节点的状态</p>
<h3 id="Oozie"><a href="#Oozie" class="headerlink" title="Oozie"></a>Oozie</h3><p>Oozie是一个工作流调度系统。 最初是由Cloudear公司开发，后来贡献给Apache。 它能够提供对Hadoop MapReduce和Pig Jobs的任务调度与协调。Oozie需要部署到Java Servlet容器中运行。’</p>
<p>Oozie定义了控制流节点（Control Flow Nodes）和动作节点（Action Nodes），其中控制流节点定义了流程的开始和结束，以及控制流程的执行路径（Execution Path），如decision、fork、join等；而动作节点包括Hadoop map-reduce、Hadoop文件系统、Pig、SSH、HTTP、eMail和Oozie子流程。 </p>
<h3 id="Apache-Storm"><a href="#Apache-Storm" class="headerlink" title="Apache Storm"></a>Apache Storm</h3><p>Apache  Storm是一个免费开源、分布式、高容错的<strong>实时计算系统</strong>。Storm令持续不断的<strong>流计算</strong>变得容易，弥补了Hadoop批处理所不能满足的实时要求。Storm经常用于在实时分析、在线机器学习、持续计算、分布式远程调用和ETL等领域。</p>
<p>Storm主要分为两种组件Nimbus和Supervisor。这两种组件都是快速失败的，没有状态。任务状态和心跳信息等都保存在<strong>Zookeeper</strong>上的，提交的代码资源都在本地机器的硬盘上。</p>
<ul>
<li>Nimbus 负责在集群里面发送代码，分配工作给机器，并且监控状态。全局只有一个。</li>
<li>Supervisor 会监听分配给它那台机器的工作，根据需要启动/关闭工作进程Worker。每一个要运行Storm的机器上都要部署一个，并且，按照机器的配置设定上面分配的槽位数。</li>
<li>Zookeeper是Storm重点依赖的外部资源。Nimbus和Supervisor甚至实际运行的Worker都是把心跳保存在Zookeeper上的。Nimbus也是根据Zookeerper上的心跳和任务运行状况，进行调度和任务分配的。</li>
<li>Topology 处理的最小的消息单位是一个Tuple，也就是一个任意对象的数组。Storm提交运行的程序称为Topology。</li>
<li>Topology 由Spout和Bolt构成。Spout是发出Tuple的结点。Bolt可以随意订阅某个Spout或者Bolt发出的Tuple。Spout和Bolt都统称为component。 </li>
</ul>
<p><img src="/2022/02/20/%E4%BA%86%E8%A7%A3Hadoop/9.png"></p>
<h3 id="Data-Ingestion"><a href="#Data-Ingestion" class="headerlink" title="Data Ingestion"></a>Data Ingestion</h3><h4 id="Sqoop"><a href="#Sqoop" class="headerlink" title="Sqoop"></a>Sqoop</h4><p><strong>Sqoop</strong>是Apache旗下的一款开源工具，该项目开始于2009年，最早是作为Hadoop的一个第三方模块存在，后来为了让使用者能够快速部署，也为了让开发人员能够更快速的迭代开发，并在2013年，独立成为Apache的一个顶级开源项目。</p>
<p>Sqoop主要用于在Hadoop和关系数据库或大型机之间传输数据，可以使用Sqoop工具将数据从关系数据库管理系统导入(import)到Hadoop分布式文件系统中，或者将Hadoop中的数据转换导出(export)到关系数据库管理系统，其功能如下图所示。</p>
<p><img src="/2022/02/20/%E4%BA%86%E8%A7%A3Hadoop/10.png"></p>
<h4 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h4><p>Flume是一种分布式，可靠且可用的服务，用于有效地<strong>收集，聚合和移动</strong>大量日志数据。Flume构建在日志流之上一个简单灵活的架构。它具有可靠的可靠性机制和许多故障转移和恢复机制，具有强大的容错性。使用Flume这套架构实现对日志流数据的实时在线分析。Flume支持在日志系统中定制各类数据发送方，用于收集数据</p>
<h4 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h4><p>  Apache Kafka是一个分布式发布 - 订阅消息系统和一个强大的队列，可以处理大量的数据，并使你能够将消息从一个端点传递到另一个端点。 Kafka适合离线和在线消息消费。 Kafka消息保留在磁盘上，并在群集内复制以防止数据丢失。 Kafka构建在ZooKeeper同步服务之上。 它与Apache Storm和Spark非常好地集成，用于实时流式数据分析。</p>
<p>  Kafka 是一个分布式消息队列，具有高性能、持久化、多副本备份、横向扩展能力。生产者往队列里写消息，消费者从队列里取消息进行业务逻辑。一般在架构设计中起到解耦、削峰、异步处理的作用。</p>
<h2 id="外部数据存储工具"><a href="#外部数据存储工具" class="headerlink" title="外部数据存储工具"></a>外部数据存储工具</h2><ul>
<li><p>MySQL</p>
</li>
<li><p>mongoDB</p>
</li>
<li><p>cassandra</p>
<ul>
<li><p>cassandra是一套开源分布式NoSQL数据库系统</p>
</li>
<li><p>cassandra的特点：</p>
<p>1、弹性可扩展。</p>
<p>2、始终基于架构，没有单点故障。</p>
<p>3、快速线性性能。</p>
<p>4、灵活的数据存储，支持结构化，半结构化和非结构化。</p>
<p>5、便捷数据分发，多数据中心间复制数据。</p>
<p>6、支持事务。</p>
<p>7、快速写入。</p>
</li>
</ul>
</li>
</ul>
<h2 id="搜索引擎"><a href="#搜索引擎" class="headerlink" title="搜索引擎"></a>搜索引擎</h2><h3 id="Apache-Drill"><a href="#Apache-Drill" class="headerlink" title="Apache Drill"></a>Apache Drill</h3><p>在大数据时代，对于Hadoop中的信息，越来越多的用户需要能够获得快速且互动的分析方法。大数据面临的一个很大的问题是大多数分析查询都很缓慢且非交互式。目前来看，MapReduce通常用于执行Hadoop数据上的批处理分析，但并不适合于你想快速得到结果或者重新定义查询参数。Google的Dremel能以极快的速度处理网络规模的海量数据。据谷歌的研究报告显示，Dremel能以拍字节（petabyte，PB，1PB等于1024TB）的数量级来进行查询，而且只需几秒钟时间就能完成。而其对应的开源版本就是Drill。（ps：drill其实就是一个分布式实时数据分析查询的引擎。Drill，一个专为互动分析大型数据集的分布式系统。） </p>
<h3 id="Hue"><a href="#Hue" class="headerlink" title="Hue"></a>Hue</h3><p> HUE=Hadoop User Experience</p>
<p> Hue是一个开源的Apache Hadoop UI系统，由Cloudera Desktop演化而来，最后Cloudera公司将其贡献给Apache基金会的Hadoop社区，它是基于Python Web框架Django实现的。</p>
<p>通过使用Hue，可以在浏览器端的Web控制台上与Hadoop集群进行交互，来分析处理数据，例如操作HDFS上的数据，运行MapReduce Job，执行Hive的SQL语句，浏览HBase<a href="https://cloud.tencent.com/solution/database?from=10680" target="_blank" rel="noopener">数据库</a>等等。</p>
<ul>
<li>Hue能做什么<ul>
<li>访问HDFS和文件浏览</li>
<li>通过web调试和开发hive以及数据结果展示</li>
<li>查询solr和结果展示，报表生成</li>
<li>通过web调试和开发impala交互式SQL Query</li>
<li>spark调试和开发</li>
<li>Pig开发和调试</li>
<li>oozie任务的开发，监控，和工作流协调调度</li>
<li>Hbase数据查询和修改，数据展示</li>
<li>Hive的元数据（metastore）查询</li>
<li>MapReduce任务进度查看，日志追踪</li>
<li>创建和提交MapReduce，Streaming，Java job任务</li>
<li>Sqoop2的开发和调试</li>
<li>Zookeeper的浏览和编辑</li>
</ul>
</li>
</ul>
<h3 id="Presto"><a href="#Presto" class="headerlink" title="Presto"></a>Presto</h3><p>presto是Facebook开源的，完全基于内存的并⾏计算的，分布式SQL交互式查询引擎</p>
<p>是一种Massively parallel processing (MPP)架构，多个节点管道式执⾏⽀持任意数据源（通过扩展式Connector组件），数据规模GB~PB级</p>
<p>使用的技术，如向量计算，动态编译执⾏计划，优化的ORC和Parquet Reader等</p>
<p>presto不太支持存储过程，支持部分标准sql</p>
<p>presto的查询速度比hive快5-10倍</p>
<p>和hive的对比：</p>
<p>hive是一个数据仓库，是一个交互式比较弱一点的查询引擎，交互式没有presto那么强，而且只能访问hdfs的数据</p>
<p>presto是一个交互式查询引擎，可以在很短的时间内返回查询结果，秒级，分钟级，能访问很多数据源</p>
<p>hive在查询100Gb级别的数据时，消耗时间已经是分钟级了</p>
<p>但是presto是取代不了hive的，因为p全部的数据都是在内存中，限制了在内存中的数据集大小，比如多个大表的join，这些大表是不能完全放进内存的，实际应用中，对于在presto的查询是有一定规定条件的，比比如说一个查询在presto查询超过30分钟，那就kill掉吧，说明不适合在presto上使用，主要原因是，查询过大的话，会占用整个集群的资源，这会导致你后续的查询是没有资源进行查询的，这跟presto的设计理念是冲突的，就像是你进行一个查询，但是要等个5分钟才有资源继续查询，这是很不合理的，交互式就变得弱了很多</p>
<h3 id="Phoenix"><a href="#Phoenix" class="headerlink" title="Phoenix"></a>Phoenix</h3><p>phoenix:构建在hbase上的一个SQL层，让我们可以用标准的JDBC APIs来创建表，插入数据和对HBase数据进程查询。</p>
<h3 id="Apache-Zeppelin"><a href="#Apache-Zeppelin" class="headerlink" title="Apache Zeppelin"></a>Apache Zeppelin</h3><p> <a href="http://zeppelin.incubator.apache.org/" target="_blank" rel="noopener">Apache Zeppelin</a>提供了web版的类似ipython的notebook，用于做数据分析和可视化。背后可以接入不同的数据处理引擎，包括spark, hive, tajo等，原生支持scala, java, shell, markdown等。它的整体展现和使用形式和Databricks Cloud是一样的，就是来自于当时的demo。</p>
<p>Zeppelin 提供了内置的 <a href="http://lib.csdn.net/base/spark" target="_blank" rel="noopener">Apache Spark</a> 集成。你不需要单独构建一个模块、插件或者库。<br>Zeppelin的Spark集成提供了：<br>- <strong>自动引入SparkContext 和 SQLContext</strong><br>- <strong>从本地文件系统或maven库载入运行时依赖的jar包。</strong><br>- <strong>可取消job 和 展示job进度</strong></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2022/01/16/Django%E5%AD%A6%E4%B9%A03/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/16/Django%E5%AD%A6%E4%B9%A03/" class="post-title-link" itemprop="url">Django学习3</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-01-16 22:27:19" itemprop="dateCreated datePublished" datetime="2022-01-16T22:27:19+08:00">2022-01-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-08-25 19:34:10" itemprop="dateModified" datetime="2022-08-25T19:34:10+08:00">2022-08-25</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Django学习3"><a href="#Django学习3" class="headerlink" title="Django学习3"></a>Django学习3</h1><h2 id="Uploading-Files"><a href="#Uploading-Files" class="headerlink" title="Uploading Files"></a>Uploading Files</h2><p>在这一章，我们将学习如何搭建一个RESTFul API，能实现图片上传的操作</p>
<h3 id="Managing-Media-Files"><a href="#Managing-Media-Files" class="headerlink" title="Managing Media Files"></a>Managing Media Files</h3><p>首先我们要把用户上传的图片给存储到特定地方，然后能够被 url检索到。在这里，我们可以创建一个 media 文件夹。</p>
<p>然后，我们需要修改<code>settings.py</code>, 在<code>settings.py</code> 中有 <code>STATIC_URL = &#39;/static/&#39;</code> 它是用来检索app的静态文件的(如CSS,JAVASCRIPT,HTML) ，因为这属于前端的工作，在这个app中我们就没有创建static文件夹。</p>
<p>那么，当用户上传图片后，我们希望通过<code>/media/{图片路径}</code>来获取图片，因此我们需要创建  MEIDA_URL 和 MEDIA_ROOT 这两个值. 其中<code>MEDIA_URL = &#39;/media/&#39;</code> 代表需要去 media文件夹检索图片; 而 <code>MEDIA_ROOT = os.path.join(BASE_DIR, &#39;media&#39;)</code> 代表将 <code>localhost:9000/</code>和<code>media/</code> 连接起来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MEDIA_URL = <span class="string">'/media/'</span></span><br><span class="line"></span><br><span class="line">MEDIA_ROOT = os.path.join(BASE_DIR, <span class="string">'media'</span>)</span><br></pre></td></tr></table></figure>
<p> 现在，比如说我在media文件夹下存放了一张 <code>dog.jpg</code> 的照片，就可以通过<code>http://127.0.0.1:9000/media/dog.jpg</code> 在网页显示这张照片了</p>
<h3 id="Adding-Images-to-Products"><a href="#Adding-Images-to-Products" class="headerlink" title="Adding Images to Products"></a>Adding Images to Products</h3><p>现在我们要修改models.py, 增加一个 ProductImage的类，因为一件商品可以有多张图片，因此这里需要建立一个一对多的关系：</p>
<p>这里，用 <code>ImageField</code> 来存储一个图片对象，存放在<code>store/images</code> 文件夹当中。</p>
<p>注意，这里一定要确定<code>related_name</code> 要不然Product无法反向找到ProductImage对象</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProductImage</span><span class="params">(models.Model)</span>:</span></span><br><span class="line">    product = models.ForeignKey(Product,</span><br><span class="line">                                on_delete = models.CASCADE,</span><br><span class="line">                                related_name=<span class="string">'images'</span>)</span><br><span class="line">    image = models.ImageField(upload_to=<span class="string">'store/images'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Building-an-API-to-Upload-Images"><a href="#Building-an-API-to-Upload-Images" class="headerlink" title="Building an API to Upload Images"></a>Building an API to Upload Images</h3><p>现在我们要创建一个API来上传图片。创建一个API的流程我们已经很熟悉了：先Serializer—View—Router构建出大致框架，再细化Serializer和View实现具体功能</p>
<h4 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h4><p>首先我们来确定Serializer中的Meta信息：我们想要呈现的是一个物品的所有图片，以及每张图片的编号和路径。因此我们可以这样来写：</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProductImageSerializer</span><span class="params">(serializers.ModelSerializer)</span>:</span></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Meta</span>:</span></span><br><span class="line">        model = ProductImage</span><br><span class="line">        fields = [<span class="string">'id'</span>, <span class="string">'image'</span>]</span><br></pre></td></tr></table></figure>
<p>然后，我们创建 <code>ProductImageViewSet</code> ，在这里一开始要确定<code>serializer_class</code>和<code>queryset</code> ，由于url的模板是这样的<code>store/product/{product_pk}/images/{pk}</code> ，在检索图片的时候，我们就需要用到url中的参数 <code>product_pk</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProductImageViewSet</span><span class="params">(ModelViewSet)</span>:</span></span><br><span class="line">    serializer_class = ProductImageSerializer</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_queryset</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> ProductImage.objects.filter(product_id=self.kwargs[<span class="string">'product_pk'</span>])</span><br></pre></td></tr></table></figure>
<p>最后我们注册路由，由于product和images是一对多的关系，这里需要使用嵌套路由，即：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#...</span></span><br><span class="line">products_router = routers.NestedDefaultRouter(</span><br><span class="line">    router, <span class="string">'products'</span>, lookup=<span class="string">'product'</span>)</span><br><span class="line">products_router.register(<span class="string">'images'</span>, views.ProductImageViewSet,</span><br><span class="line">                         basename=<span class="string">'product-images'</span>)</span><br><span class="line"><span class="comment">#...</span></span><br></pre></td></tr></table></figure>
<h4 id="具体功能"><a href="#具体功能" class="headerlink" title="具体功能"></a>具体功能</h4><p>这个API的具体功能就是希望将用户上传的图片保存在特定的文件夹当中. 因此，实现的关键就是如何保存图片。</p>
<p>我们知道serializer是无法直接获取到url中的参数的，只能获取请求体中的数据(validated_data).因此需要在views中利用<code>get_serializer_context</code> 方法将该商品的id传入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProductImageViewSet</span><span class="params">(ModelViewSet)</span>:</span></span><br><span class="line">    serializer_class = ProductImageSerializer</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_serializer_context</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'product_id'</span>: self.kwargs[<span class="string">'product_pk'</span>]&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_queryset</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> ProductImage.objects.filter(product_id=self.kwargs[<span class="string">'product_pk'</span>])</span><br></pre></td></tr></table></figure>
<p>然后，我们在serializer中重写create函数，在里面通过<code>self.context[&#39;product_id&#39;]</code>获取到product_pk这个参数，并以此创建一个新的<code>ProductImage</code>对象</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProductImageSerializer</span><span class="params">(serializers.ModelSerializer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create</span><span class="params">(self, validated_data)</span>:</span></span><br><span class="line">        product_id = self.context[<span class="string">'product_id'</span>]</span><br><span class="line">        <span class="keyword">return</span> ProductImage.objects.create(product_id=product_id, **validated_data)</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Meta</span>:</span></span><br><span class="line">        model = ProductImage</span><br><span class="line">        fields = [<span class="string">'id'</span>, <span class="string">'image'</span>]</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<p><img src="/2022/01/16/Django%E5%AD%A6%E4%B9%A03/1.gif" style="zoom:67%;"></p>
<h3 id="Returning-Images-from-the-API"><a href="#Returning-Images-from-the-API" class="headerlink" title="Returning Images from the API"></a>Returning Images from the API</h3><p>现在，我们希望，在对<code>http://127.0.0.1:9000/store/products/</code> 进行 GET请求的时候，可以将每个product的图片信息显示出来：</p>
<p>怎么修改？首先我们要修改Serializer，让他把image加进来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProductSerializer</span><span class="params">(serializers.ModelSerializer)</span>:</span></span><br><span class="line">    images = ProductImageSerializer(many=<span class="literal">True</span>, read_only=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Meta</span>:</span></span><br><span class="line">        model = Product</span><br><span class="line">        fields = [<span class="string">'id'</span>,  <span class="string">'title'</span>, <span class="string">'description'</span>, <span class="string">'slug'</span>,</span><br><span class="line">                  <span class="string">'inventory'</span>, <span class="string">'unit_price'</span>, <span class="string">'price_with_tax'</span>, <span class="string">'collection'</span>, <span class="string">'images'</span>]</span><br><span class="line"></span><br><span class="line">    price_with_tax = serializers.SerializerMethodField(</span><br><span class="line">        method_name=<span class="string">'calculate_tax'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate_tax</span><span class="params">(self, product: Product)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> product.unit_price * Decimal(<span class="number">1.1</span>)</span><br></pre></td></tr></table></figure>
<p>为了优化查询，我们需要对ProductViewSet进行修改，即用<code>prefetch_related</code> 将image在初次查询的时候就囊括进来，不用筛选出products以后再单独去进行反向查询</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProductViewSet</span><span class="params">(ModelViewSet)</span>:</span></span><br><span class="line">    queryset = Product.objects.prefetch_related(<span class="string">'images'</span>).all()</span><br><span class="line">    <span class="comment">#...</span></span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/16/Django%E5%AD%A6%E4%B9%A03/2.png" style="zoom:67%;"></p>
<h3 id="Validating-Uploaded-Files"><a href="#Validating-Uploaded-Files" class="headerlink" title="Validating Uploaded Files"></a>Validating Uploaded Files</h3><p>之前我们使用ImageField来确定上传文件的类型，在这背后其实是 Pillow 包做的底层逻辑，对文件是否为图片进行了一个判断。</p>
<p>那么，如果我们希望对图片的大小进行一个限制，比如说不能超过50kb，该怎么写？</p>
<p>我们可以单独创建一个validators.py来存放验证器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> django.core.exceptions <span class="keyword">import</span> ValidationError</span><br><span class="line"><span class="comment"># 在这里我们引入 ValidationError 模块。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">validate_file_size</span><span class="params">(file)</span>:</span></span><br><span class="line">    max_size_kb = <span class="number">50</span></span><br><span class="line"><span class="comment"># 如果文件大小(以B为单位)，大于规定大小，我们就会返回一个错误。    </span></span><br><span class="line">    <span class="keyword">if</span> file.size &gt; max_size_kb*<span class="number">1024</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValidationError(<span class="string">f'Files cannot be larger than <span class="subst">&#123;max_size_kb&#125;</span> KB!'</span>)</span><br></pre></td></tr></table></figure>
<p>然后，在models.py中引入这个验证器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProductImage</span><span class="params">(models.Model)</span>:</span></span><br><span class="line">    product = models.ForeignKey(Product,</span><br><span class="line">                                on_delete=models.CASCADE,</span><br><span class="line">                                related_name=<span class="string">'images'</span>)</span><br><span class="line">    image = models.ImageField(upload_to=<span class="string">'store/images'</span>,</span><br><span class="line">                              validators=[validate_file_size])</span><br></pre></td></tr></table></figure>
<p>此外，如果该属性是 <code>FileField</code> ,我们只希望接收特定格式的文件，比如jpg ，怎么办？ 我们可以使用<code>FileExtensionValidator</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> django.core.validators <span class="keyword">import</span> MinValueValidator,FileExtensionValidator</span><br><span class="line"></span><br><span class="line"><span class="comment">#...</span></span><br><span class="line">image = models.FileField(upload_to=<span class="string">'store/images'</span>,</span><br><span class="line">                         validators=[FileExitensionValidator(allowed_extensions=[<span class="string">'jpg'</span>])])</span><br></pre></td></tr></table></figure>
<h3 id="Setting-Up-the-Client-App"><a href="#Setting-Up-the-Client-App" class="headerlink" title="Setting Up the Client App"></a>Setting Up the Client App</h3><p>我们打开part3 中的storefront3_client文件夹，把它复制到电脑硬盘中(直接在移动硬盘中操作会出现奇奇怪怪的问题)，然后运行<code>npm install</code> 安装依赖，接着运行<code>npm start</code> 启动前端项目</p>
<p>结果如下：</p>
<p><img src="/2022/01/16/Django%E5%AD%A6%E4%B9%A03/3.png" style="zoom:67%;"></p>
<h3 id="Enabling-CORS"><a href="#Enabling-CORS" class="headerlink" title="Enabling CORS"></a>Enabling CORS</h3><p>但是，上面这个前端是没有办法向后端去传递消息的，因为没有进行跨域.什么是跨域呢？ 因为前端是运行在8001端口的，而后端是运行在9001 端口的，这两者之间如果不进行跨域的话，是没有办法进行信息传输的。</p>
<p>CORS 即 Cross-origin Resource Sharing 中文名是跨源资源共享，即跨域。它的作用就是将在一个域名接收到的请求转发到另外一个域名，然后将另外一个域名的返回的结果在转发到这个域名上来。类似一个中转服务器</p>
<p>为了在Django中实现CORS，我们需要用到<code>django-cors-headers</code> 这个Github项目，地址：<a href="https://github.com/adamchainz/django-cors-headers" target="_blank" rel="noopener">https://github.com/adamchainz/django-cors-headers</a> 。 里面有细致的安装步骤。</p>
<p>首先，我们运行 <code>pipenv install django-cors-headers</code> 安装 corsheaders APP</p>
<p>然后，我们在 <code>settings.py</code> 中注册 corsheaders</p>
<p>接着，将<code>&#39;corsheaders.middleware.CorsMiddleware&#39;</code> 复制到中间件列表的最前面</p>
<p>最后，注册允许跨域的前端域名，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CORS_ALLOWED_ORIGINS = [</span><br><span class="line">    <span class="string">'http://localhost:8001'</span>,</span><br><span class="line">    <span class="string">'http://127.0.0.1:8001'</span>,</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>然后，因为我们的后端试监听在9000端口的，因此需要修改前端中的<code>uploadFile</code>函数(默认是8000)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">const uploadFile &#x3D; (file) &#x3D;&gt; &#123;</span><br><span class="line">  const apiUrl &#x3D; &#39;http:&#x2F;&#x2F;127.0.0.1:9000&#x2F;store&#x2F;products&#x2F;1&#x2F;images&#x2F;&#39;;</span><br><span class="line">#...</span><br></pre></td></tr></table></figure>
<p>我们可以尝试上传一张图片：</p>
<p><img src="/2022/01/16/Django%E5%AD%A6%E4%B9%A03/4.png" style="zoom:67%;"></p>
<p>然后我们查看请求的url信息：</p>
<p><img src="/2022/01/16/Django%E5%AD%A6%E4%B9%A03/5.png" style="zoom:67%;"></p>
<h3 id="Managing-Images-in-the-Admin"><a href="#Managing-Images-in-the-Admin" class="headerlink" title="Managing Images in the Admin"></a>Managing Images in the Admin</h3><p>现在我们想给以Admin管理图片的功能，能让他在后台可以直接查看到图片，添加\删除图片，怎么办？</p>
<p>由于 图片和商品是一个多对一的关系，想要在特定的商品界面显示与其关联的图片，可以使用inline的方式嵌入：</p>
<p>那么首先我们要到<code>store&gt;admin.py</code> 中创建 <code>ProductImageInline</code> 类：</p>
<p>在这个类中，我们首先规定 models 属性，让其等于 ProductImage</p>
<p>此外，为了显示图片，我们需要额外创建一个字段，暂且称其为 <code>thumnail</code>，然后，我们要使用<code>format_html</code>方法将图片转换成html语言显示出来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProductImageInline</span><span class="params">(admin.TabularInline)</span>:</span></span><br><span class="line">    model = models.ProductImage</span><br><span class="line">    readonly_fields = [<span class="string">'thumnail'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">thumnail</span><span class="params">(self, instance)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> instance.image.name != <span class="string">''</span>:</span><br><span class="line">            <span class="keyword">return</span> format_html(<span class="string">f'&lt;img src="<span class="subst">&#123;instance.image.url&#125;</span>" class = "thumnail"/&gt;'</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">''</span></span><br></pre></td></tr></table></figure>
<p>然后，我们要在product中将<code>ProductImageInline</code>类导入进去，之前我们在<code>core&gt;admin.py</code> 中为了实现Generic Model Tag的内联实现，我们重新定义了inline字段，因此要在<code>core&gt;admin.py</code>中导入<code>ProductImageInline</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomProductAdmin</span><span class="params">(ProductAdmin)</span>:</span></span><br><span class="line">    inlines = [TagInline, ProductImageInline]</span><br><span class="line"></span><br><span class="line">admin.site.unregister(Product)</span><br><span class="line">admin.site.register(Product, CustomProductAdmin)</span><br></pre></td></tr></table></figure>
<p>但这还是不够的，虽然能在admin界面显示图片，但是是以原大小显示的，很不美观，因此我们需要用css来调节一下。</p>
<p>为了项目的可维护性，我们需要在store文件夹下创建 static 文件夹，专门存放html、css之类的静态文件。</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.thumnail</span> &#123;</span><br><span class="line">  <span class="attribute">width</span>: <span class="number">100px</span>;</span><br><span class="line">  <span class="attribute">height</span>: <span class="number">100px</span>;</span><br><span class="line">  <span class="attribute">object-fit</span>: cover;<span class="comment">/*等比例缩放*/</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最后，我们要在 ProductAdmin中引入这个css文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@admin.register(models.Product)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProductAdmin</span><span class="params">(admin.ModelAdmin)</span>:</span></span><br><span class="line"> 		<span class="comment">#..</span></span><br><span class="line">    <span class="comment"># Media是内置的类，和Meta类似，会自动到static文件夹中找静态文件</span></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Media</span>:</span></span><br><span class="line">        css = &#123;</span><br><span class="line">            <span class="string">'all'</span>: [<span class="string">'store/style.css'</span>]</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<p><img src="/2022/01/16/Django%E5%AD%A6%E4%B9%A03/6.png" style="zoom:67%;"></p>
<h2 id="Sending-Emails"><a href="#Sending-Emails" class="headerlink" title="Sending Emails"></a>Sending Emails</h2><h3 id="Setting-up-a-Fake-SMTP-Server"><a href="#Setting-up-a-Fake-SMTP-Server" class="headerlink" title="Setting up a Fake SMTP Server"></a>Setting up a Fake SMTP Server</h3><p>在计算机网络中，我们学过<a href="https://jasonxqh.github.io/2021/05/03/计算机网络-应用层总结/#SMTP-电子邮件">SMTP协议</a> ，也就是一个简单邮件传输协议。现在我们如果想要我们的app来发送邮件，就需要创建一个简单的SMTP服务器  </p>
<p>项目网址如下： <a href="https://github.com/rnwood/smtp4dev" target="_blank" rel="noopener">https://github.com/rnwood/smtp4dev</a></p>
<p>我们需要在Docker中安装这个项目，因此需要先安装Docker。然后运行<code>docker run --rm -it -p 3000:80 -p 2525:25 rnwood/smtp4dev</code> 安装</p>
<p>但是对于m1 mac，可能会出现找不到合适版本的问题，我们可以指定一个版本让其安装：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -it -p 3000:80 -p 2525:25 rnwood&#x2F;smtp4dev:3.1.3-ci20211206101</span><br></pre></td></tr></table></figure>
<p>然后，访问<code>localhost:3000</code> ，就可以打开 smtp4dev</p>
<p><img src="/2022/01/16/Django%E5%AD%A6%E4%B9%A03/7.png" style="zoom:67%;"></p>
<p>这是一个假的smtp服务器，他不会真的发送邮件，但它的作用是检测通过这个端口(2525)利用SMTP协议发送的邮件</p>
<h3 id="Configuring-the-Email-Backend"><a href="#Configuring-the-Email-Backend" class="headerlink" title="Configuring the Email Backend"></a>Configuring the Email Backend</h3><p>首先我们要在Django中对电子邮件后端进行一些配置。</p>
<p>我们</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">EMAIL_BACKEND = <span class="string">'django.core.mail.backends.smtp.EmailBackend'</span></span><br><span class="line">EMAIL_HOST = <span class="string">'localhost'</span></span><br><span class="line">EMAIL_HOST_USER = <span class="string">''</span></span><br><span class="line">EMAIL_HOST_PASSWORD = <span class="string">''</span></span><br><span class="line">EMAIL_PORT = <span class="number">2525</span></span><br><span class="line">DEFAULT_FROM_EMAIL = <span class="string">''</span></span><br></pre></td></tr></table></figure>
<p><code>EMAIL_BACKEND</code> 设置邮件后台，默认为smtp，还有console(邮件显示在终端),file(将邮件显示在文件里)</p>
<p><code>EMAIL_HOST</code> 由于smtp4dev是开在本地的，因此这里写 ‘localhost’</p>
<p><code>EMAIL_HOST_USER</code> 和 <code>EMAIL_HOST_PASSWORD</code> 由于对这个邮件服务器我们没有设置用户和密码，因此直接写空串</p>
<p><code>EMAIL_PORT</code> 是邮件走的端口号，由于 smtp4dev 监听在2525，这里写2525</p>
<p><code>DEFAULT_FROM_EMAIL</code> 是默认邮箱，即从这个邮箱往其他邮箱发送邮件</p>
<h3 id="Sending-Emails-1"><a href="#Sending-Emails-1" class="headerlink" title="Sending  Emails"></a>Sending  Emails</h3><p>我们可以用 <code>django.core.mail</code> 中的 <code>send_mail</code> 函数来发送信息。由于存在发送错误的情况，我们需要用 try-except 进行错误捕捉。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> django.shortcuts <span class="keyword">import</span> render</span><br><span class="line"><span class="keyword">from</span> django.core.mail <span class="keyword">import</span> send_mail, mail_admins, BadHeaderError</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个函数第一个参数是邮件主题；第二个参数是正文；第三个参数是发送者邮箱；第四个是接收者邮箱(列表)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">say_hello</span><span class="params">(request)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        send_mail(<span class="string">'subject'</span>, <span class="string">'message'</span>,</span><br><span class="line">                  <span class="string">'jason@from.com'</span>, [<span class="string">'jason@to.com'</span>])</span><br><span class="line">    <span class="keyword">except</span> BadHeaderError:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">return</span> render(request, <span class="string">'hello.html'</span>, &#123;<span class="string">'name'</span>: <span class="string">'Jason'</span>&#125;)</span><br></pre></td></tr></table></figure>
<p>在访问 <code>http://127.0.0.1:9000/playground/hello/</code> 之后我们看到 smtp4dev服务器已经捕捉到这封邮件了</p>
<p><img src="/2022/01/16/Django%E5%AD%A6%E4%B9%A03/8.png" style="zoom:67%;"></p>
<h3 id="Attaching-Files"><a href="#Attaching-Files" class="headerlink" title="Attaching Files"></a>Attaching Files</h3><p>如果我们想发送一封带有附件的文件，该怎么办？</p>
<p>我们可以先确定邮件的内容，然后再利用<code>attach_file</code> 来加载附件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">say_hello</span><span class="params">(request)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        message = EmailMessage(<span class="string">'subject'</span>, <span class="string">'message'</span>,</span><br><span class="line">                               <span class="string">'jason@from.com'</span>, [<span class="string">'jason@to.com'</span>])</span><br><span class="line">        message.attach_file(<span class="string">'playground/static/1.png'</span>)</span><br><span class="line">        message.send()</span><br><span class="line">    <span class="keyword">except</span> BadHeaderError:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">return</span> render(request, <span class="string">'hello.html'</span>, &#123;<span class="string">'name'</span>: <span class="string">'Jason'</span>&#125;)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/01/16/Django%E5%AD%A6%E4%B9%A03/9.png" style="zoom:67%;"></p>
<h3 id="Sending-Templated-Emails"><a href="#Sending-Templated-Emails" class="headerlink" title="Sending Templated Emails"></a>Sending Templated Emails</h3><p>我们可以下载<code>django-templated-mail</code> 这个app，来实现发送模板邮件的功能。</p>
<p>首先我在 <code>playground</code> 下的 <code>templates/emails</code> 文件夹下创建一个<code>hello.html</code>文件，用来存放模板</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;% block subject%&#125; This is a long subject &#123;% endblock %&#125;</span><br><span class="line"></span><br><span class="line">&#123;%block html_body%&#125;</span><br><span class="line"><span class="tag">&lt;<span class="name">h1</span>&gt;</span>Hello<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line">My name is &#123;&#123;name&#125;&#125;</span><br><span class="line">&#123;%endblock%&#125;</span><br></pre></td></tr></table></figure>
<p><code>views.py</code>中，我们要用 BaseEmailMessage 去替换原来的 EmailMessage</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> django.shortcuts <span class="keyword">import</span> render</span><br><span class="line"><span class="keyword">from</span> django.core.mail <span class="keyword">import</span> EmailMessage, send_mail, mail_admins, BadHeaderError</span><br><span class="line"><span class="keyword">from</span> templated_mail.mail <span class="keyword">import</span> BaseEmailMessage</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">say_hello</span><span class="params">(request)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        message = BaseEmailMessage(</span><br><span class="line">            template_name=<span class="string">'emails/hello.html'</span>,</span><br><span class="line">            context=&#123;<span class="string">'name'</span>: <span class="string">'Jason'</span>&#125;</span><br><span class="line">        )</span><br><span class="line">        message.send([<span class="string">'jason@to.com'</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">except</span> BadHeaderError:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">return</span> render(request, <span class="string">'hello.html'</span>, &#123;<span class="string">'name'</span>: <span class="string">'Jason'</span>&#125;)</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<p><img src="/2022/01/16/Django%E5%AD%A6%E4%B9%A03/10.png"></p>
<h2 id="Running-Background-Tasks"><a href="#Running-Background-Tasks" class="headerlink" title="Running Background Tasks"></a>Running Background Tasks</h2><h3 id="Introduction-to-Celery"><a href="#Introduction-to-Celery" class="headerlink" title="Introduction to Celery"></a>Introduction to Celery</h3><p>在遇到一些上传文件、发送邮件的任务时，我们希望它能够在后台运行。这时候我们就可以用到Celery：</p>
<p>Celery是一个简单，灵活，可靠的分布式系统，用于处理大量消息，同时为操作提供维护此类系统所需的工具。</p>
<p>它是一个任务队列，专注于实时处理，同时还支持任务调度。</p>
<h4 id="celery-的优点"><a href="#celery-的优点" class="headerlink" title="celery 的优点"></a>celery 的优点</h4><ul>
<li><p>简单：celery的 配置和使用还是比较简单的, 非常容易使用和维护和不需要配置文件</p>
</li>
<li><p>高可用：当任务执行失败或执行过程中发生连接中断，celery 会自动尝试重新执行任务</p>
</li>
<li><p>如果连接丢失或发生故障，worker和client 将自动重试，并且一些代理通过主/主或主/副本复制方式支持HA。</p>
</li>
<li><p>快速：一个单进程的celery每分钟可处理上百万个任务</p>
</li>
<li><p>灵活： 几乎celery的各个组件都可以被扩展及自定制 </p>
</li>
</ul>
<h4 id="celery-的5个角色"><a href="#celery-的5个角色" class="headerlink" title="celery 的5个角色"></a>celery 的5个角色</h4><ul>
<li>Task</li>
</ul>
<p>就是任务，有异步任务和定时任务</p>
<ul>
<li>Broker</li>
</ul>
<p>中间人，接收生产者发来的消息即Task，将任务存入队列。任务的消费者是Worker。</p>
<p>Celery本身不提供队列服务，推荐用Redis或RabbitMQ实现队列服务。Redis只是一个数据库，但是可以用来实现队列服务，而RabbitMQ是一个企业级的 Broker</p>
<ul>
<li>Worker</li>
</ul>
<p>执行任务的单元，它实时监控消息队列，如果有任务就获取任务并执行它。</p>
<ul>
<li>Beat</li>
</ul>
<p>定时任务调度器，根据配置定时将任务发送给Broker。</p>
<ul>
<li>Backend</li>
</ul>
<p>用于存储任务的执行结果。 </p>
<p>其结构如下所示：</p>
<p><img src="/2022/01/16/Django%E5%AD%A6%E4%B9%A03/11.png"></p>
<h3 id="Installing-Redis"><a href="#Installing-Redis" class="headerlink" title="Installing Redis"></a>Installing Redis</h3><p><code>pipenv install redis</code></p>
<p><code>docker run -d -p 6379:6379 redis</code></p>
<p><img src="/2022/01/16/Django%E5%AD%A6%E4%B9%A03/12.png"></p>
<h3 id="Setting-Up-Celery"><a href="#Setting-Up-Celery" class="headerlink" title="Setting Up Celery"></a>Setting Up Celery</h3><p>Celery与Windows系统不兼容，所以如果是windows系统请使用 wsl linux环境运行这个Django项目。</p>
<p>首先，我们要在storefront中新创建一个celery模块：先运行<code>pipenv install celery</code> 下载Celery</p>
<p>进行一个简单的配置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> celery <span class="keyword">import</span> Celery</span><br><span class="line"><span class="comment"># 设置环境变量，将DJANGO_SETTINGS_MODULE设置为这个项目的settings.py</span></span><br><span class="line">os.environ.setdefault(<span class="string">'DJANGO_SETTINGS_MODULE'</span>, <span class="string">'storefront.settings'</span>)</span><br><span class="line"><span class="comment"># 创建一个Celery实例</span></span><br><span class="line">celery = Celery(<span class="string">'storefront'</span>)</span><br><span class="line"><span class="comment"># 确定Celery可以在那里找到配置变量，我们指向了django.conf模块中的settings对象</span></span><br><span class="line"><span class="comment"># 第二个参数代表，所有在setting对象中的变量都需要以CELERY开头</span></span><br><span class="line">celery.config_from_object(<span class="string">'django.conf:settings'</span>, namespace=<span class="string">'CELERY'</span>)</span><br><span class="line">celery.autodiscover_tasks()</span><br></pre></td></tr></table></figure>
<p>然后在<code>settings.py</code>  中 设置<code>CELERY_BROKER_URL</code>变量，6379代表redis运行的端口号，<code>/1</code>代表我们这个项目所属数据库的名字</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CELERY_BROKER_URL = <span class="string">'redis://localhost:6379/1'</span></span><br></pre></td></tr></table></figure>
<p>在 storefront中的<code>__init__.py</code> 中导入我们设置好的celery模块：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> .celery <span class="keyword">import</span> celery</span><br></pre></td></tr></table></figure>
<p>最后，我们用<code>celery -A storefront worker --loglevel=info</code> 来启动celery</p>
<p><img src="/2022/01/16/Django%E5%AD%A6%E4%B9%A03/13.png"></p>
<p>我们看到，celery已经启动了，一共有8个cpu核可供我们进行并行任务处理</p>
<p>然而，现在我们的项目已经有点复杂了，一共需要3条命令，3个终端窗口才能实现</p>
<ul>
<li>一个 运行项目 ： <code>python manage.py run server</code></li>
<li>一个运行redis ： <code>docker run -p 6379:6379 redis</code></li>
<li>一个运行celery:  <code>celery -A storefront worker</code></li>
</ul>
<h3 id="Creating-and-Executing-Tasks"><a href="#Creating-and-Executing-Tasks" class="headerlink" title="Creating and Executing Tasks"></a>Creating and Executing Tasks</h3><p>我们在<code>playground</code> 中创建<code>tasks.py</code> 用来模拟需要运行的任务。这里我们模拟发送一封信件，需要消耗10s。注意，如果要让celery去执行，需要使用<code>shared_task</code>来修饰</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep</span><br><span class="line"><span class="keyword">from</span> celery <span class="keyword">import</span> shared_task</span><br><span class="line"></span><br><span class="line"><span class="meta">@shared_task</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">notify_customers</span><span class="params">(message)</span>:</span></span><br><span class="line">    print(<span class="string">'Sending 10k emails...'</span>)</span><br><span class="line">    print(message)</span><br><span class="line">    sleep(<span class="number">10</span>)</span><br><span class="line">    print(<span class="string">'Emails were successfully sent!'</span>)</span><br></pre></td></tr></table></figure>
<p>然后，我们可以再<code>views.py</code>中运行这个任务：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> .tasks <span class="keyword">import</span> notify_customers</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">say_hello</span><span class="params">(request)</span>:</span></span><br><span class="line">  	<span class="comment">#需要用到 .delay()方法 </span></span><br><span class="line">    notify_customers.delay(<span class="string">'hello'</span>)</span><br><span class="line">    <span class="keyword">return</span> render(request, <span class="string">'hello.html'</span>, &#123;<span class="string">'name'</span>: <span class="string">'Jason'</span>&#125;)</span><br></pre></td></tr></table></figure>
<p>结果如下，我访问<code>http://localhost:9000/playground/hello/</code>会直接返回html文件：</p>
<p><img src="/2022/01/16/Django%E5%AD%A6%E4%B9%A03/15.gif"></p>
<p>此时，该任务在后台运行，我们发现，过了10秒以后，才显示 Email were successfully sent</p>
<p><img src="/2022/01/16/Django%E5%AD%A6%E4%B9%A03/14.png"></p>
<p>需要注意的是，这里我即使使用 ctrl+c 关闭了 Celery ，刷新界面之后也会直接返回html文件，这是因为，redis还开在后台，项目会把这个任务存放在redis(broker)当中，当celery重新启动的时候，redis会尝试重新发送这个任务给worker。</p>
<p>但是如果我关掉redis，那么程序就会报错，因为任务无法发送到6379端口</p>
<h3 id="Scheduling-Periodic-Tasks"><a href="#Scheduling-Periodic-Tasks" class="headerlink" title="Scheduling Periodic Tasks"></a>Scheduling Periodic Tasks</h3><p>如果我希望每隔特定的时间重复运行一个任务，我们可以使用 celery beat</p>
<p>要使用celery beat，需要在 <code>settings.py</code>中设置 <code>CELERY_BEAT_SCHEDULE</code> 变量，如下所示：</p>
<p>首先，我们要确定重复的任务：notify_customers，设置这个任务的位置, 即task属性，以及什么时候重复。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">CELERY_BEAT_SCHEDULE = &#123;</span><br><span class="line">    <span class="string">'notify_customers'</span>: &#123;</span><br><span class="line">        <span class="string">'task'</span>: <span class="string">'playground.tasks.notify_customers'</span>,</span><br><span class="line">        <span class="string">'schedule'</span>: <span class="number">2</span>,</span><br><span class="line">        <span class="string">'args'</span>: [<span class="string">'Hello world'</span>]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在这里，我直接使用了数字2，代表每间隔2秒进行一次重复。当然，我也可以使用<code>celery.schedules</code>中的 <code>crontab</code>模块：<code>&#39;schedule&#39;: crontab(day_of_week=1, hour=7, minute=5)</code></p>
<p>这个就代表，每一周的第一天的七点零五分运行这个任务。</p>
<p>或者<code>&#39;schedule&#39;: crontab(minute=&#39;*/15&#39;)</code> 代表每间隔15分钟运行一次，当然<code>&#39;schedule&#39;: 15*60</code> 也可以达到相同的结果。</p>
<p>最后，我们打开一个新的终端，运行<code>celery -A storefront beat</code></p>
<p><img src="/2022/01/16/Django%E5%AD%A6%E4%B9%A03/16.gif"></p>
<h3 id="Monitoring-Celery-Tasks"><a href="#Monitoring-Celery-Tasks" class="headerlink" title="Monitoring Celery Tasks"></a>Monitoring Celery Tasks</h3><p>最后，我们来讲讲怎么监控 Celery Task , 这里我们需要用到一个小工具： Flower。 我们可以用<code>pipenv install flower</code> 来下载</p>
<p>然后用<code>celery -A storefront flower</code>来启动flower：</p>
<p>然后访问<code>http://localhost:5555</code> 来监控后台任务</p>
<p><img src="/2022/01/16/Django%E5%AD%A6%E4%B9%A03/17.png"></p>
<h2 id="Automated-Testing"><a href="#Automated-Testing" class="headerlink" title="Automated Testing"></a>Automated Testing</h2><h3 id="What-is-Automated-Testing"><a href="#What-is-Automated-Testing" class="headerlink" title="What is Automated Testing"></a>What is Automated Testing</h3><p>现在我们要对项目做单元测试。单元测试是代码正确性验证的最重要的工具，也是系统测试当中最重要的环节。也是唯一需要编写代码才能进行测试的一种测试方法。在标准的开发过程中，单元测试的代码与实际程序的代码具有同等的重要性。每一个单元测试，都是用来定向测试其所对应的一个单元的数据是否正确。</p>
<p>比如说对于我们store app中的products，对它们可以进行创建、更新、删除等操作。但有些操作只有特定权限的用户才能够进行。因此我们需要手动测试这些api，是否只有特定的人才能实现特定的功能。</p>
<p>然而，单元测试的代码会随着项目的增长称指数型爆发，在后期会难以为继，因此，我们就需要更有效地工具——自动测试(Automated Testing) , 我们可以将测试集成在一起，然后统一运行，这样节省了时间，又能使测试覆盖面更广。</p>
<h3 id="Test-Behaviours-Not-Implementations"><a href="#Test-Behaviours-Not-Implementations" class="headerlink" title="Test Behaviours,Not Implementations"></a>Test Behaviours,Not Implementations</h3><p>写单元测试的时候，我们要有一个原则： 测试api的行为，不测试api的实现逻辑。就好比我们要测试电视遥控板的功能是否正常，而不是把电视遥控板拆开去测一个电阻是否正常工作</p>
<p>比如说，我们对 /collections 的POST方法进行测试，我们需要囊括以下结果：</p>
<ul>
<li>Anonymous -&gt; 401</li>
<li>Non-admin -&gt; 403</li>
<li>Admin &amp;&amp; invalid data -&gt; 401</li>
<li>Admin &amp;&amp; valid data -&gt; 401</li>
</ul>
<p>这是这个collection的behavior，而不是内部的实现逻辑</p>
<h3 id="Tooling"><a href="#Tooling" class="headerlink" title="Tooling"></a>Tooling</h3><p>在Python中，我们有不同的测试工具，主要是两个测试框架：</p>
<ul>
<li>unittest</li>
<li>pytest</li>
</ul>
<p>在这里我们使用pytest, 因为它有三个优点：</p>
<ul>
<li>More features</li>
<li>Tons of  plugins</li>
<li>Less boilerplate(代码量少)</li>
</ul>
<p>我们可以用 <code>pipenv install --dev pytest</code> 来下在pytest</p>
<p>此外，还需要下载一个插件： <code>pipenv install --dev pytest-django</code></p>
<h3 id="Your-First-Test"><a href="#Your-First-Test" class="headerlink" title="Your First Test"></a>Your First Test</h3><p>现在我们来写第一个test，首先我们在store文件夹下创建一个test文件夹，然后创建<code>test_collection.py</code> ，注意，命名规则需要一个<code>test_</code> 开始。</p>
<p>然后，为了方面组织测试函数，我们把测试一个功能的所有函数都放在一个类里，这个类叫做<code>TestCreateCollection</code> 注意，这个类也要以<code>Test</code> 开始，否则pytest不会把这个测试囊括在内。</p>
<p>在这个类中，我们可以写一个个测试函数。注意，函数命名要以<code>test_</code>开始。</p>
<p>接下来我们正式开始写测试，首先，我们需要用到APIClient，它可以模拟一个客户端向后端发送请求。</p>
<p>因为这里我们想要测试创建collection，因此需要用到<code>client.post</code></p>
<p>然后，我们判断 response的状态码是否等于预想的401，如果等于，测试就会通过，否则就会报错。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> rest_framework.test <span class="keyword">import</span> APIClient</span><br><span class="line"><span class="keyword">from</span> rest_framework <span class="keyword">import</span> status</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestCreateCollection</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_if_user_is_anonymous_returns_401</span><span class="params">(self)</span>:</span></span><br><span class="line">        client = APIClient()</span><br><span class="line">        response = client.post(<span class="string">'/store/collections'</span>, &#123;<span class="string">'title'</span>: <span class="string">'a'</span>&#125;)</span><br><span class="line">        <span class="keyword">assert</span> response.status_code == status.HTTP_401_UNAUTHORIZED</span><br></pre></td></tr></table></figure>
<h3 id="Running-Tests"><a href="#Running-Tests" class="headerlink" title="Running Tests"></a>Running Tests</h3><p>接下来我们运行上面这个测试：</p>
<h3 id="Skipping-Tests"><a href="#Skipping-Tests" class="headerlink" title="Skipping Tests"></a>Skipping Tests</h3><h3 id="Continuous-Testing"><a href="#Continuous-Testing" class="headerlink" title="Continuous Testing"></a>Continuous Testing</h3><h3 id="Running-and-Debugging-Tests-in-VSCode"><a href="#Running-and-Debugging-Tests-in-VSCode" class="headerlink" title="Running and Debugging Tests in VSCode"></a>Running and Debugging Tests in VSCode</h3><h3 id="Authenticating-the-User"><a href="#Authenticating-the-User" class="headerlink" title="Authenticating the User"></a>Authenticating the User</h3><h3 id="Single-or-Multiple-Assertions"><a href="#Single-or-Multiple-Assertions" class="headerlink" title="Single or Multiple Assertions"></a>Single or Multiple Assertions</h3><h3 id="Fixtures"><a href="#Fixtures" class="headerlink" title="Fixtures"></a>Fixtures</h3><h3 id="Creating-Model-Instances"><a href="#Creating-Model-Instances" class="headerlink" title="Creating Model Instances"></a>Creating Model Instances</h3><h2 id="Performance-Testing"><a href="#Performance-Testing" class="headerlink" title="Performance Testing"></a>Performance Testing</h2><h3 id="Why-Performance-Testing"><a href="#Why-Performance-Testing" class="headerlink" title="Why Performance Testing"></a>Why Performance Testing</h3><h3 id="Installing-Locust"><a href="#Installing-Locust" class="headerlink" title="Installing Locust"></a>Installing Locust</h3><h3 id="Creating-a-Test-Script"><a href="#Creating-a-Test-Script" class="headerlink" title="Creating a Test Script"></a>Creating a Test Script</h3><h3 id="Running-a-Test-Script"><a href="#Running-a-Test-Script" class="headerlink" title="Running a Test Script"></a>Running a Test Script</h3><h3 id="Running-a-Performance-Test"><a href="#Running-a-Performance-Test" class="headerlink" title="Running a Performance Test"></a>Running a Performance Test</h3><h3 id="Performance-Optimization-Techniques"><a href="#Performance-Optimization-Techniques" class="headerlink" title="Performance Optimization Techniques"></a>Performance Optimization Techniques</h3><h3 id="Profiling-with-Silk"><a href="#Profiling-with-Silk" class="headerlink" title="Profiling with Silk"></a>Profiling with Silk</h3><h3 id="Verifying-Optimizations"><a href="#Verifying-Optimizations" class="headerlink" title="Verifying Optimizations"></a>Verifying Optimizations</h3><h3 id="Stress-Testing"><a href="#Stress-Testing" class="headerlink" title="Stress Testing"></a>Stress Testing</h3><h2 id="Caching"><a href="#Caching" class="headerlink" title="Caching"></a>Caching</h2><h3 id="What-is-Caching"><a href="#What-is-Caching" class="headerlink" title="What is Caching"></a>What is Caching</h3><h3 id="Cache-Backends"><a href="#Cache-Backends" class="headerlink" title="Cache Backends"></a>Cache Backends</h3><h3 id="Simulating-a-Slow-API"><a href="#Simulating-a-Slow-API" class="headerlink" title="Simulating a Slow API"></a>Simulating a Slow API</h3><h3 id="Getting-a-Baseline-Performance-Benchmark"><a href="#Getting-a-Baseline-Performance-Benchmark" class="headerlink" title="Getting a Baseline Performance Benchmark"></a>Getting a Baseline Performance Benchmark</h3><h3 id="Installing-Redis-1"><a href="#Installing-Redis-1" class="headerlink" title="Installing Redis"></a>Installing Redis</h3><h3 id="Configuring-Caching"><a href="#Configuring-Caching" class="headerlink" title="Configuring Caching"></a>Configuring Caching</h3><h3 id="Using-the-Low-level-Cache-API"><a href="#Using-the-Low-level-Cache-API" class="headerlink" title="Using the Low-level Cache API"></a>Using the Low-level Cache API</h3><h3 id="Caching-Views-mp4"><a href="#Caching-Views-mp4" class="headerlink" title="Caching Views.mp4"></a>Caching Views.mp4</h3><h3 id="Verifying-Optimizations-1"><a href="#Verifying-Optimizations-1" class="headerlink" title="Verifying Optimizations"></a>Verifying Optimizations</h3><h3 id="Managing-Redis-Cache-Content"><a href="#Managing-Redis-Cache-Content" class="headerlink" title="Managing Redis Cache Content"></a>Managing Redis Cache Content</h3><h2 id="Preparing-for-Production"><a href="#Preparing-for-Production" class="headerlink" title="Preparing for Production"></a>Preparing for Production</h2><h3 id="Adding-the-Home-Page"><a href="#Adding-the-Home-Page" class="headerlink" title="Adding the Home Page"></a>Adding the Home Page</h3><h3 id="Adding-Static-Assets"><a href="#Adding-Static-Assets" class="headerlink" title="Adding Static Assets"></a>Adding Static Assets</h3><h3 id="Collecting-Static-Assets"><a href="#Collecting-Static-Assets" class="headerlink" title="Collecting Static Assets"></a>Collecting Static Assets</h3><h3 id="Serving-Static-Assets"><a href="#Serving-Static-Assets" class="headerlink" title="Serving Static Assets"></a>Serving Static Assets</h3><h3 id="Configuring-Logging"><a href="#Configuring-Logging" class="headerlink" title="Configuring Logging"></a>Configuring Logging</h3><h3 id="Logging"><a href="#Logging" class="headerlink" title="Logging"></a>Logging</h3><h3 id="Managing-Development-and-Production-Settings"><a href="#Managing-Development-and-Production-Settings" class="headerlink" title="Managing Development and Production Settings"></a>Managing Development and Production Settings</h3><h3 id="Serving-the-Application-with-Gunicorn"><a href="#Serving-the-Application-with-Gunicorn" class="headerlink" title="Serving the Application with Gunicorn"></a>Serving the Application with Gunicorn</h3><h2 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h2><h3 id="Hosting-Options"><a href="#Hosting-Options" class="headerlink" title="Hosting Options"></a>Hosting Options</h3><h3 id="Adding-Project-to-Git"><a href="#Adding-Project-to-Git" class="headerlink" title="Adding Project to Git"></a>Adding Project to Git</h3><h3 id="Getting-Started-with-Heroku"><a href="#Getting-Started-with-Heroku" class="headerlink" title="Getting Started with Heroku"></a>Getting Started with Heroku</h3><h3 id="Creating-a-Heroku-App"><a href="#Creating-a-Heroku-App" class="headerlink" title="Creating a Heroku App"></a>Creating a Heroku App</h3><h3 id="Setting-Environment-Variables"><a href="#Setting-Environment-Variables" class="headerlink" title="Setting Environment Variables"></a>Setting Environment Variables</h3><h3 id="Creating-a-Procfile"><a href="#Creating-a-Procfile" class="headerlink" title="Creating a Procfile"></a>Creating a Procfile</h3><h3 id="Provisioning-a-MySQL-Database"><a href="#Provisioning-a-MySQL-Database" class="headerlink" title="Provisioning a MySQL Database"></a>Provisioning a MySQL Database</h3><h3 id="Provisioning-a-Redis-Instance"><a href="#Provisioning-a-Redis-Instance" class="headerlink" title="Provisioning a Redis Instance"></a>Provisioning a Redis Instance</h3><h3 id="Provisioning-an-SMTP-Server"><a href="#Provisioning-an-SMTP-Server" class="headerlink" title="Provisioning an SMTP Server"></a>Provisioning an SMTP Server</h3><h3 id="Deploying-the-Application"><a href="#Deploying-the-Application" class="headerlink" title="Deploying the Application"></a>Deploying the Application</h3><h3 id="Populating-the-Database"><a href="#Populating-the-Database" class="headerlink" title="Populating the Database"></a>Populating the Database</h3><h3 id="Dockerizing-the-App"><a href="#Dockerizing-the-App" class="headerlink" title="Dockerizing the App"></a>Dockerizing the App</h3><h3 id="Course-Wrap-Up"><a href="#Course-Wrap-Up" class="headerlink" title="Course Wrap Up"></a>Course Wrap Up</h3>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2022/01/05/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch1-%E7%AE%97%E6%B3%95%E8%AF%84%E4%BB%B7%E6%A0%87%E5%87%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/05/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch1-%E7%AE%97%E6%B3%95%E8%AF%84%E4%BB%B7%E6%A0%87%E5%87%86/" class="post-title-link" itemprop="url">数据科学算法ch1-算法评价标准</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-01-05 21:19:34 / Modified: 21:58:32" itemprop="dateCreated datePublished" datetime="2022-01-05T21:19:34+08:00">2022-01-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="数据科学算法ch1-算法评价标准"><a href="#数据科学算法ch1-算法评价标准" class="headerlink" title="数据科学算法ch1-算法评价标准"></a>数据科学算法ch1-算法评价标准</h1><h3 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h3><p>以二分类为例，真实的样本标签有两类，分类器预测的标签也有两类，真是标签和预测标签的不同组合构成了一个<strong>混淆矩阵</strong>。其中：</p>
<ol>
<li>真阳性(true positive, TP): 正例样本被分类器预测为正例</li>
<li>假阳性(false positive, FP): 负例样本被分类器预测为正例</li>
<li>真阴性(true negative, TN): 负例样本被分类器预测为负例</li>
<li>假阴性(false negative,FN): 正例样本被分类器预测为负例</li>
</ol>
<p><img src="/2022/01/05/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch1-%E7%AE%97%E6%B3%95%E8%AF%84%E4%BB%B7%E6%A0%87%E5%87%86/1.jpeg"></p>
<ul>
<li>召回率：真阳性率(TPR), 反映模型的查全率，定义为：</li>
</ul>
<script type="math/tex; mode=display">
\text{Recall} = \frac{TP}{TP+FN}</script><p>可以这么理解，召回率是通过这个模型，有多少正例成功地被预测为正例，因为有一部分被预测称了负例。主要聚焦在正例样本上</p>
<ul>
<li>准确率：反映模型的查准率，定义为：</li>
</ul>
<script type="math/tex; mode=display">
\text{Precision } = \frac{TP}{TP+FP}</script><p>准确率用来衡量模型的精度，因为模型可能会把一些负例预测为正例，导致精度缺失</p>
<ul>
<li><strong>$F_\beta$</strong> 值：召回率或者准确率知识衡量了分类器某一方面的性能，综合召回率和准确率而定义 $F_\beta$ 值为：</li>
</ul>
<script type="math/tex; mode=display">
F_\beta = \frac{(\beta^2+1)PR}{\beta^2 P+R}</script><p>其中，$P$ 为准确率、 $R$为召回率。因此 $F_\beta$ 值是召回率把和准确率的加权调和平均数，通常取 $\beta$ 的值为1</p>
<script type="math/tex; mode=display">
F_1 = \frac{2PR}{P+R}</script><ul>
<li>ROC曲线下方的面积(AUC)：ROC所在的坐标系，其横坐标为假阳性率(FPR), 纵坐标是真阳性率(召回率) </li>
</ul>
<script type="math/tex; mode=display">
\text{假阳性率}~~FPR = \frac{FP}{FP+TN}\\</script><p>AUC值越大，代表ROC越靠近左上角，在相同假阳性率情形下，分类器可能具有更大的召回率，分类器表现更好。</p>
<p><img src="/2022/01/05/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch1-%E7%AE%97%E6%B3%95%E8%AF%84%E4%BB%B7%E6%A0%87%E5%87%86/2.png"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2022/01/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/" class="post-title-link" itemprop="url">机器学习-隐马尔可夫模型</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-01-04 19:49:15" itemprop="dateCreated datePublished" datetime="2022-01-04T19:49:15+08:00">2022-01-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-03-03 10:19:42" itemprop="dateModified" datetime="2022-03-03T10:19:42+08:00">2022-03-03</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="隐马尔可夫模型"><a href="#隐马尔可夫模型" class="headerlink" title="隐马尔可夫模型"></a>隐马尔可夫模型</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>首先，我们要了解，什么是隐马尔科夫模型？</p>
<p>隐马尔科夫模型，是关于时序的概率模型</p>
<p>它描述一个<strong>隐藏</strong>的马尔科夫链随机生成<strong>不可观测的状态随机序列(state sequence)</strong>，再由各个状态随机生成一个观测而产生<strong>观测随机序列</strong>的过程，序列的每一个位置又可以看做是一个<strong>时刻</strong></p>
<h3 id="三要素"><a href="#三要素" class="headerlink" title="三要素"></a>三要素</h3><p>隐马尔科夫模型由<strong>初始概率分布</strong>，<strong>状态转移概率分布</strong>，以及<strong>观测概率分布</strong>确定，隐马尔科夫模型的形式定义如下：</p>
<ul>
<li><p>$Q$ 所有可能的状态的集合 ，记为 $Q = {q_1,    q_2\cdots,q_N}$， $N$ 是可能的状态数</p>
</li>
<li><p>$V$ 所有可能观测的集合，记为 $V= {v_1,v_2\cdots,v_m}$ ,  $M$ 是可能的观测数</p>
</li>
<li><p>$I$ 长度为$T$ 的状态序列 ，记为 $I = (i_1,i_2\cdots,i_T)$</p>
</li>
<li><p>$O$ 对应的观测序列，记为 $O= (o_1,o_2,\cdots,o_T)$</p>
</li>
<li><p>$A$ 是状态转移概率矩阵，记为 $A = [a<em>{ij}]</em>{N\times N}$</p>
<p>其中， $a<em>{ij} = P(i</em>{i+1}=q_j|i_i=q_i),i=1,2\cdots,N;j=1,2\cdots,N$ 。这代表着，在时刻t，处于状态$q_i$  的条件下，在时刻 $t+1$ 转移到状态 $q_j$ 的概率</p>
</li>
<li><p>$B$ 是观测概率矩阵，记为 $B = [b<em>j(k)]</em>{N\times M}$ </p>
<p>其中，$b_{j}(k) = P(o_t=v_k|i_t=q_j),k=1,2\cdots M;j=1,2\cdots,N$</p>
<p>这代表着，在时刻 t， 处于状态$q_j$ 的条件下，生成观测 $v_k$ 的概率</p>
</li>
<li><p>$\pi$ 是初始状态概率向量，记为 $\pi = (\pi_i)$ </p>
<p>其中，$\pi_i = P(i_i=q_i),i=1,2\cdots,N$ ,这代表着，时刻$t=1$ 时，处于状态$q_i$ 的概率</p>
<p>它们的关系是：</p>
</li>
</ul>
<p>$\pi,A$ 确定了隐藏的马尔科夫链，生成不可观测的状态序列，$B$确定了如何从状态生成观测，与状态序列总和确定了如何产生观测序列。因此，隐马尔科夫模型$\lambda$可以由三元符号来表示：$\lambda = (A,B,\pi)$</p>
<p>$A,B,\pi$ 也被称为是马尔科夫模型三要素。</p>
<h3 id="两个基本假设"><a href="#两个基本假设" class="headerlink" title="两个基本假设"></a>两个基本假设</h3><p>从定义可知，隐马尔科夫模型做出了两个基本假设</p>
<ol>
<li><p>齐次马尔科夫性假设。即设 隐藏的马尔科夫链<strong>在任意时刻 t  的状态只依赖于前一时刻的状态</strong>，与其他时刻的状态及观测无关，也与时刻t 是无关的。(可以理解为无记忆性)</p>
<script type="math/tex; mode=display">
P(i_i|i_{i-1},o_{t-1},\cdots,i_1,o_1) = P(i_i|i_{i-1}) ,t=1,2\cdots,T</script></li>
<li><p>观测独立性假设。即假设<strong>任意时刻的观测</strong>只依赖于<strong>该时刻的马尔科夫链的状态</strong>，与其他观测及状态无关</p>
<script type="math/tex; mode=display">
P(o_t|i_T,o_T,\cdots,i_1,o_1) = P(o_i|i_i)</script></li>
</ol>
<p>隐马尔科夫模型可以用于标注，这时状态对应着标记。标注问题是给定观测序列预测期对应的标记序列. 我们可以假设，标注问题的数据是由隐马尔科夫模型生成的，这样，我们可以利用隐马尔科夫的学习与预测算法进行标注</p>
<h4 id="盒球模型"><a href="#盒球模型" class="headerlink" title="盒球模型"></a>盒球模型</h4><p>假设有四个盒子，每个盒子都有红白两种颜色共10颗球，如下</p>
<p><img src="/2022/01/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/1.png" style="zoom:100%;"></p>
<p>转移的规则是：</p>
<ul>
<li>盒子1 那么下一个一定是盒子2</li>
<li>盒子2或者盒子3，那么下一个0.4 是左边那个盒子，0.6是右边那个盒子</li>
<li>盒子4 下一个0.5是自己，0.5是盒子3</li>
</ul>
<p>我们可以通过观测每次去除球的颜色来得到观测序列，但是观测不到球是从哪个盒子取出来的，也就是说，我们观测不到盒子的序列。</p>
<p>在这个例子中，盒子的序列就是<strong>状态序列 I</strong>，球颜色的观测序列就是<strong>观测序列O</strong> </p>
<p>已知： </p>
<ul>
<li><p>状态集合 $Q =$ {盒子1，盒子2，盒子3，盒子4}，$N=4$ </p>
</li>
<li><p>观测集合 $V  =$ {红，白} $M=2$</p>
</li>
<li><p>观测序列和状态序列的长度 $T=5$</p>
</li>
<li><p>现在抽取五次，得到观测序列$O = ${红，红，白，白，红}</p>
</li>
<li><p>我们的初始概率分布为： $\pi = (0.25,0.25,0.25,0.25)$ 也就是抽到每个盒子的概率都是相同的</p>
</li>
<li><p>状态概率转移矩阵 $A$: </p>
<script type="math/tex; mode=display">
A = \begin{bmatrix}0&1&0&0\\0.4&0&0.6&0\\0&0.4&0&0.6\\0&0&0.5&0.5\end{bmatrix}</script><p>我们看到，这个矩阵记录的，就是上面说的三条转移规则</p>
</li>
<li><p>观测概率举证$B$:</p>
<script type="math/tex; mode=display">
A = \begin{bmatrix}0.5&0.5\\0.3&0.7\\0.6&0.4\\0.8&0.2\end{bmatrix}</script><p>我们看到，这个矩阵记录的，就是每个状态下，观测到红球或者是白球的概率</p>
</li>
</ul>
<h4 id="观测序列如何生成"><a href="#观测序列如何生成" class="headerlink" title="观测序列如何生成"></a>观测序列如何生成</h4><p>从盒球模型中，观测序列时直接给我们的，那么，它是怎么生成的呢？</p>
<p>通过 给定隐马尔科夫模型三要素 $\lambda = {A,B,\pi}$，以及观测序列长度 T(理解为要观测几次)，我们输出观测序列$O$</p>
<ol>
<li>安装初始状态分布$\pi$ 来产生 状态i</li>
<li>令$t = 1$</li>
<li>按照状态 $i<em>t$ 的观测概率分布 $b</em>{i_t}(k)$ 来生成$o_t$</li>
<li>按照状态 $i<em>t$ 的状态转移概率分布 ${a</em>{i<em>ti</em>{t+1}}}$ 来产生状态 $i_{t+1}$ </li>
<li>令 $t=t+1$ ,若 $t&lt;T$ ，跳转至(3), 否则终止</li>
</ol>
<h3 id="三个基本问题"><a href="#三个基本问题" class="headerlink" title="三个基本问题"></a>三个基本问题</h3><ul>
<li><p>概率计算问题。给定模型 $\lambda = (A,B,\pi)$ 和观测序列 $O=(o_1,\cdots,o_T)$ ,计算在模型$\lambda$ 下，观测序列$O$ 出现的概率$P(O|\lambda)$</p>
</li>
<li><p>学习问题。已知观测序列$O=(o_1,\cdots,o_T)$, 估计模型 $\lambda = (A,B,\pi)$ 参数，使得在该模型下观测序列概率$P(O|\lambda)$ 最大. 即用极大似然估计的方法估计参数</p>
</li>
<li>预测问题．已知模型$\lambda = (A,B,\pi)$ 和观测序列$O=(o_1,o_2,\cdots,o_T)$，求对给定观测序列条件概率$P(I|O)$ 最大的状态序列$I=(i_1,\cdots,i_T)$。 维特比算法应用动态规划高效地求解最优路径，即概率最大的状态序列</li>
</ul>
<h2 id="概率计算算法"><a href="#概率计算算法" class="headerlink" title="概率计算算法"></a>概率计算算法</h2><p>概率计算问题。给定模型 $\lambda = (A,B,\pi)$ 和观测序列 $O=(o_1,\cdots,o_T)$ ,计算在模型$\lambda$ 下，观测序列$O$ 出现的概率$P(O|\lambda)$</p>
<h3 id="前向算法"><a href="#前向算法" class="headerlink" title="前向算法"></a>前向算法</h3><p>首先，我们给出前向概率的定义：</p>
<p>给定隐马尔科夫模型$\lambda$ , 定义到时刻 t 部分观测序列为 $o_1,\cdots,o_t$ 且状态为$q_i$ 的概率为<strong>前向概率 $\alpha_t(i)$</strong>。记作：</p>
<script type="math/tex; mode=display">
\alpha_t(i) = P(o_1,\cdots,o_t,i_t = q_i|\lambda)</script><p>由此，我们可以递推地求得前向概率$\alpha_t(i)$ 以及观测序列概率$P(O|\lambda)$ </p>
<h4 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h4><p>输入：隐马尔科夫模型 $\lambda$, 观测序列 $O$:</p>
<p>输出 ：观测序列概率 $P(O|\lambda)$</p>
<ol>
<li>初值</li>
</ol>
<script type="math/tex; mode=display">
\alpha_1(i) = \pi_ib_i(o_1),i=1,2\cdots,N</script><p>这个步骤是初始化前向概率，是初始时刻的状态$i_t = q_i$ 和观测 $o_1$ 的联合概率</p>
<ol>
<li>递推，对于$t=1,2\cdots,T-1$</li>
</ol>
<script type="math/tex; mode=display">
\alpha_{t+1}(i)=\bigg[\sum_{j=1}^N\alpha_t(j)a_{ji}\bigg]b_i(o_{t+1}),i=1,2\cdots,N</script><ol>
<li>终止</li>
</ol>
<script type="math/tex; mode=display">
P(O|\lambda) = \sum_{i=1}^N \alpha_T(i)</script><h4 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h4><p><img src="/2022/01/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/2.png" style="zoom:100%;"></p>
<ul>
<li>步骤1，计算初值</li>
</ul>
<script type="math/tex; mode=display">
\alpha_1(1) = \pi_1\cdot b_1(o_1) = 0.10\\
\alpha_1(2) = \pi_2\cdot b_2(o_1) = 0.16\\
\alpha_1(3) = \pi_3\cdot b_3(o_1) = 0.28</script><ul>
<li>步骤2，递推</li>
</ul>
<p>我们要递推得到$\alpha_2$ 这一层的N个值(N=3)</p>
<script type="math/tex; mode=display">
\begin{align}
\alpha_2(1) = \bigg[\sum_{j=1}^3\alpha_1(j)\cdot a_{j1} \bigg]b_1(o_2)=[\alpha_1(1)\cdot a_{11}+\alpha_1(2)\cdot a_{21}+\alpha_1(3)\cdot a_{31}]\cdot b_1(o_2) = 0.154\times 0.5 = 0.077\\
\alpha_2(2) = \bigg[\sum_{j=1}^3\alpha_1(j)\cdot a_{j2} \bigg]b_2(o_2)=[\alpha_1(1)\cdot a_{12}+\alpha_1(2)\cdot a_{22}+\alpha_1(3)\cdot a_{32}]\cdot b_2(o_2) = 0.184\times 0.6 = 0.1104\\
\alpha_2(3) = \bigg[\sum_{j=1}^3\alpha_1(j)\cdot a_{j3} \bigg]b_3(o_2)=[\alpha_1(1)\cdot a_{13}+\alpha_1(2)\cdot a_{23}+\alpha_1(3)\cdot a_{33}]\cdot b_3(o_2) = 0.202\times 0.3 = 0.0606\\
\end{align}</script><p>因为 $T=3$,所以我们还要再推一层 $\alpha_3(i)$</p>
<script type="math/tex; mode=display">
\begin{align}
\alpha_3(1) = \bigg[\sum_{j=1}^3\alpha_2(j)\cdot a_{j1} \bigg]b_1(o_3)=[\alpha_3(1)\cdot a_{11}+\alpha_2(2)\cdot a_{21}+\alpha_2(3)\cdot a_{31}]\cdot b_1(o_3) = 0.154\times 0.5 = 0.04187\\
\alpha_3(2) = \bigg[\sum_{j=1}^3\alpha_2(j)\cdot a_{j2} \bigg]b_2(o_3)=[\alpha_3(1)\cdot a_{12}+\alpha_2(2)\cdot a_{22}+\alpha_2(3)\cdot a_{32}]\cdot b_2(o_3) = 0.184\times 0.6 = 0.03551\\
\alpha_3(3) = \bigg[\sum_{j=1}^3\alpha_2(j)\cdot a_{j3} \bigg]b_3(o_3)=[\alpha_3(1)\cdot a_{13}+\alpha_2(2)\cdot a_{23}+\alpha_2(3)\cdot a_{33}]\cdot b_3(o_3) = 0.202\times 0.3 = 0.05284\\
\end{align}</script><ul>
<li>步骤3，终止</li>
</ul>
<script type="math/tex; mode=display">
P(O|\lambda) = \sum_{i=1}^N\alpha_T(i) = 0.04187+0.03551+0.05284</script><h3 id="后向算法"><a href="#后向算法" class="headerlink" title="后向算法"></a>后向算法</h3><p>首先我们来定义后向概率：</p>
<p>给定隐马尔科夫模型$\lambda$ ,  定义在时刻t，状态为$q<em>i$ 的条件下，从$t+1$到$T$ 的部分观测序列为 $o</em>{t+1},\cdots,o_T$ 的概率为<strong>后向概率</strong>，记作</p>
<script type="math/tex; mode=display">
\beta_t(i) = P(o_{t+1},\cdots,o_{T}|i_t = q_i,\lambda)</script><p>由此，我们可以递推地求得后向概率$\beta_t(i)$ 以及观测序列概率$P(O|\lambda)$ </p>
<h4 id="算法步骤-1"><a href="#算法步骤-1" class="headerlink" title="算法步骤"></a>算法步骤</h4><ol>
<li>初值</li>
</ol>
<script type="math/tex; mode=display">
\beta_T(i) = 1,i=1,\cdots,N</script><ol>
<li>递推</li>
</ol>
<p>对于 $t=T-1,T-2,\cdots,1$， 有：</p>
<script type="math/tex; mode=display">
\beta_t(i) = \sum_{j=1}^Na_{ij}\cdot b_j(o_{t+1})\beta_{t+1}(j)&i=1,\cdots,N\\</script><ol>
<li><script type="math/tex; mode=display">
P(O|\lambda) = \sum_{i=1}^N\pi_ib_i(o_1)\beta_1(i)</script></li>
</ol>
<h3 id="一些概率与期望值的计算"><a href="#一些概率与期望值的计算" class="headerlink" title="一些概率与期望值的计算"></a>一些概率与期望值的计算</h3><p>利用前向和后向概率，我们可以得到关于单个状态和两个状态概率的计算公式</p>
<h4 id="单个状态"><a href="#单个状态" class="headerlink" title="单个状态"></a>单个状态</h4><p>给定模型$\lambda$和观测$O$, 我们可以通过前向后向概率计算求得 在时刻t 处于状态 $q_i$ 的概率：</p>
<script type="math/tex; mode=display">
\gamma_t(i) = P(i_t=q_i |O,\lambda)</script><p>由前向概率 $\alpha_t(i)$和 后向概率 $\beta_t(i)$ 的定义可知：</p>
<script type="math/tex; mode=display">
\alpha_t(i)\beta_t(i) = P(i_t=q_i,O|\lambda)</script><p>于是，我们得到</p>
<script type="math/tex; mode=display">
\gamma_t(i) = \frac{\alpha_t(i)\beta_t(i)}{P(O|\lambda)} = \frac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^N\alpha_t(j)\beta_t(j)}</script><h4 id="例题-1"><a href="#例题-1" class="headerlink" title="例题"></a>例题</h4><p><img src="/2022/01/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/3.jpg" style="zoom:100%;"></p>
<p>这就是上面所说的，求单个状态的概率值</p>
<ul>
<li>第一步，列出 $P(i_4 = q_3 |O,\lambda)$ 的式子：</li>
</ul>
<script type="math/tex; mode=display">
P(i_4=q_3|O,\lambda) = \frac{\alpha_4(3)\cdot\beta_4(3)}{\sum_{i=1}^N\alpha_4(i)\beta_4(i)} = \frac{\alpha_4(3)\cdot\beta_4(3)}{\alpha_4(1)\beta_4(1)+\alpha_4(2)\beta_4(2)+\alpha_4(3)\beta_4(3)}</script><ul>
<li>第二步，我们要求那些？</li>
</ul>
<p>首先，我们前向概率要从 $\alpha_1(i)$ 求到 $\alpha_4(i)$ 递推4次 ； 后向概率要从 $\beta_8(i)$求到 $\beta_4(i)$ ,递推5次</p>
<ul>
<li><p>第三步，前向计算</p>
<ul>
<li><p>初值: $\alpha_1(i) = \pi_ib_i(o_1)$</p>
<script type="math/tex; mode=display">
\alpha_1(1) = \pi_1\cdot b_1(o_1)=0.2\cdot0.5 = 0.10\\
\alpha_1(2) = \pi_2\cdot b_2(o_1)=0.3\cdot 0.4 = 0.12\\
\alpha_1(3) = \pi_3\cdot b_3(o_1)=0.5\cdot 0.7 = 0.35</script></li>
<li><p>递推: $\alpha<em>{t+1}(i) = \bigg[\sum</em>{j =1}^N a<em>{ji}\cdot \alpha_t(i)\bigg]\cdot b_i(o</em>{t+1})$</p>
<script type="math/tex; mode=display">
\begin{align}
&\alpha_2(1) = [\sum_{j=1}^3 a_{j1}\cdot\alpha_1(1)]\cdot b_1(o_{2}) = 0.078\\
&\alpha_2(2) = [\sum_{j=1}^3 a_{j2}\cdot\alpha_1(2)]\cdot b_2(o_{2})=0.084\\
&\alpha_2(3) = [\sum_{j=1}^3 a_{j3}\cdot\alpha_1(3)]\cdot b_3(o_{2})=0.0822\\

&\alpha_3(1) = [\sum_{j=1}^3 a_{j1}\cdot\alpha_2(1)]\cdot b_1(o_{3})=0.04032\\
&\alpha_3(2) = [\sum_{j=1}^3 a_{j2}\cdot\alpha_2(2)]\cdot b_2(o_{3})=0.026496\\
&\alpha_3(3) = [\sum_{j=1}^3 a_{j3}\cdot\alpha_2(3)]\cdot b_3(o_{3})=0.068124\\

&\alpha_4(1) = [\sum_{j=1}^3 a_{j1}\cdot\alpha_3(1)]\cdot b_1(o_{4})=0.00208668\\
&\alpha_4(2) = [\sum_{j=1}^3 a_{j2}\cdot\alpha_3(2)]\cdot b_2(o_{4})=0.01236192\\
&\alpha_4(3) = [\sum_{j=1}^3 a_{j3}\cdot\alpha_3(3)]\cdot b_3(o_{5})=0.004361112\\
\end{align}</script></li>
</ul>
</li>
<li><p>第四步，后向计算</p>
<ul>
<li><p>初值： $\beta_8(1)=\beta_8(2)=\beta_8(3) = 1$</p>
</li>
<li><p>递推 ： $\beta<em>t(i) = \sum</em>{j=1}^N a<em>{ij}\cdot b_j(o</em>{t+1})\cdot\beta_{t+1}(j) $</p>
<script type="math/tex; mode=display">
\begin{align}
&\beta_7(1) = \sum_{j=1}^3 a_{1j}\cdot b_j(o_8)\cdot\beta_8(j)=0.43\\
&\beta_7(2) = \sum_{j=1}^3 a_{2j}\cdot b_j(o_8)\cdot\beta_8(j)=0.51\\
&\beta_7(3) = \sum_{j=1}^3 a_{3j}\cdot b_j(o_8)\cdot\beta_8(j)=0.4\\
&\beta_6(1) = \sum_{j=1}^3 a_{1j}\cdot b_j(o_7)\cdot\beta_7(j)=0.1861\\
&\beta_6(2) = \sum_{j=1}^3 a_{2j}\cdot b_j(o_7)\cdot\beta_7(j)=0.2415\\
&\beta_6(3) = \sum_{j=1}^3 a_{3j}\cdot b_j(o_7)\cdot\beta_7(j)=0.1762\\
&\beta_5(1) = \sum_{j=1}^3 a_{1j}\cdot b_j(o_6)\cdot\beta_6(j)=0.105521\\
&\beta_5(2) = \sum_{j=1}^3 a_{2j}\cdot b_j(o_6)\cdot\beta_6(j)=0.100883\\
&\beta_5(3) = \sum_{j=1}^3 a_{3j}\cdot b_j(o_6)\cdot\beta_6(j)=0.111934\\
&\beta_4(1) = \sum_{j=1}^3 a_{1j}\cdot b_j(o_5)\cdot\beta_5(j)=0.04586531\\
&\beta_4(2) = \sum_{j=1}^3 a_{2j}\cdot b_j(o_5)\cdot\beta_5(j)=0.05280909\\
&\beta_4(3) = \sum_{j=1}^3 a_{3j}\cdot b_j(o_5)\cdot\beta_5(j)=0.4280618\\
\end{align}</script></li>
</ul>
<p><img src="/2022/01/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/3.png" style="zoom:100%;"></p>
<h2 id="预测算法"><a href="#预测算法" class="headerlink" title="预测算法"></a>预测算法</h2></li>
</ul>
<p>预测问题．已知模型$\lambda = (A,B,\pi)$ 和观测序列$O=(o_1,o_2,\cdots,o_T)$，求对给定观测序列条件概率$P(I|O)$ 最大的状态序列$I=(i_1,\cdots,i_T)$。 维特比算法应用动态规划高效地求解最优路径，即概率最大的状态序列</p>
<h3 id="维特比算法"><a href="#维特比算法" class="headerlink" title="维特比算法"></a>维特比算法</h3><p>维特比算法，是用动态规划来解决隐马尔科夫模型预测问题，即用动态规划求<strong>概率最大路径</strong>(最优路径)，这时一条路劲对应着一个状态序列。</p>
<p>根据动态规划原理，最优路径具有这样的特性：如果最优路径在时刻 $t$ 通过结点 $i_t^<em>$，那么这一路径从结点$i_t^</em>$ 到终点$i_T^<em>$ 的部分路径，对于从结点$i_t^</em>$ 到终点$i_T^*$ 的所有可能的部分路径来说，必须是最优的。</p>
<p>根据这一原理，我们只需要</p>
<ul>
<li>从时刻 $t=1$ 开始，递推地计算<strong>在时刻t</strong>、<strong>状态为i</strong>的各条部分路径的最大概率，直至得到时刻$t=T$ 、状态为i 的各条路径的最大概率。</li>
<li>时刻 $t=T$ 的最大概率即为最优路径的概率 $P^<em>$ ,最优路径的终结点 $i^</em>_T$ 也会同时得到。</li>
<li>之后，为了找到最优路径的各个结点，从终结点 $i^<em><em>T$ 开始，右后向前逐步求得点 $i</em>{T-1}^</em>,\cdots,i<em>{1}^<em>$ ， 得到最优路径 $I^</em> = (i_1^*,\cdots,i</em>{T}^*)$ </li>
</ul>
<p>输入：模型 $\lambda = (A,B,\pi)$ 和观测 $O=(o_1,o_2\cdots,o_T)$</p>
<p>输出：最优路径 $I^<em> = (i_1^</em> ,i^<em>_2\cdots,i_T^</em>)$</p>
<ol>
<li>初始化</li>
</ol>
<script type="math/tex; mode=display">
\begin{align}
&\delta_1(i) = \pi_ib_i(O_1)&i=1,\cdots,N\\
&\varPsi_1(i) = 0&i=1,\cdots,N\\
\end{align}</script><ol>
<li>递推</li>
</ol>
<script type="math/tex; mode=display">
\begin{align}
&\delta_t(i) = \max_{1\leq j\leq N}[\delta_{t-1}(j)a_{ji}]b_i(o_t)\\
&\varPsi_t(i) = \arg\max_{i\leq j\leq N}[\delta_{t-1}(j)a_{ji}]
\end{align}</script><ol>
<li>终止</li>
</ol>
<script type="math/tex; mode=display">
\begin{align}
&P^* = \max_{1\leq i\leq N}\delta_T(i)\\
&i_T^* = \arg\max_{1\leq i\leq N} [\delta_T(i)]
\end{align}</script><ol>
<li>最优路径回溯：对于 $t=T-1,T-2,\cdots ,1$ <script type="math/tex; mode=display">
i_t^* = \varPsi_{t+1}(i^*_{t+1})</script>求得最优路径 $I^<em> = (i_1^</em>,i_2^<em>,\cdots,i_T^</em>)$</li>
</ol>
<h4 id="例题-2"><a href="#例题-2" class="headerlink" title="例题"></a>例题</h4><p><img src="/2022/01/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/4.jpg" style="zoom:100%;"></p>
<ol>
<li>初始化 , 在 $t = 1$ 时，对每一个状态$i\in{1,2,3}$   ，求 <strong>状态为 i </strong>, <strong>观测</strong>$o_1$<strong>为红 </strong>的概率，记此概率为 $\delta_1(i)$.则</li>
</ol>
<script type="math/tex; mode=display">
\delta_1(i) = \pi_ib_i(o_1) =\pi_ib_i(o_1) ,~~~~ i=1,2,3</script><p>代入实际数据</p>
<script type="math/tex; mode=display">
\delta_1(1) = \pi_1b_1(o_1) = 0.2\times0.5 = 0.1\\
\delta_1(2) = \pi_2b_2(o_1) = 0.4\times0.4 = 0.16\\
\delta_1(3) = \pi_2b_2(o_1) = 0.4\times0.7 = 0.28\\
\varPhi_1(i) = 0,i=1,2,3</script><ol>
<li><p>递推</p>
<ul>
<li><p>在t = 2时</p>
<ul>
<li>对于每个状态$i\in{1,2,3}$,  记最大概率为  $\delta<em>2(i) = \max</em>{1\leq j\leq 3}[\delta<em>1(j)a</em>{ji}]b_i(o_2) $</li>
</ul>
<script type="math/tex; mode=display">
\delta_2(1) = \max\{0.1\times 0.5, 0.16\times0.3,0.28\times 0.2\} \times 0.5 = 0.028\\
\delta_2(2) = \max\{0.1\times 0.2, 0.16\times0.5,0.28\times 0.3\} \times 0.6 = 0.0504\\
\delta_2(3) = \max\{0.1\times 0.3, 0.16\times0.2,0.28\times 0.5\} \times 0.3  = 0.042</script><ul>
<li><p>同时，对于每个状态 $i\in {1,2,3}$ ，我们记录它的<strong>最大概率的前一个状态</strong> $j$. $\varPsi<em>2(i) = \arg\max</em>{1\leq j\leq 3}[\delta<em>1(j)a</em>{ji}]~~~i=1,2,3$</p>
<script type="math/tex; mode=display">
\varPsi_2(1) =\arg\max\{0.1\times0.5,0.16\times 0.3,0.28\times 0.2\} =3\\
\varPsi_2(2) =\arg\max\{0.1\times0.2,0.16\times 0.5,0.28\times 0.3\} = 3\\
\varPsi_2(3) =\arg\max\{0.1\times0.3,0.16\times 0.2,0.28\times 0.5\} = 3</script></li>
<li><p>在t = 3时</p>
<ul>
<li><p>$\delta<em>3(i) = \max</em>{1\leq j\leq3}[\delta<em>2(j)a</em>{ji}]\cdot b_i(o_3)$</p>
<script type="math/tex; mode=display">
\delta_3(1) = \max\{0.028\times 0.5, 0.0504\times0.3,0.042\times 0.2\} \times 0.5 = 0.00756\\
\delta_3(2) = \max\{0.028\times 0.2, 0.0504\times0.5,0.042\times 0.3\} \times 0.6 = 0.01008\\
\delta_3(3) = \max\{0.028\times 0.3, 0.0504\times0.2,0.042\times 0.5\} \times 0.3  = 0.0147</script></li>
<li><p>$\varPsi<em>3(i) = \arg\max</em>{1\leq j\leq 3}[\delta<em>2(j)a</em>{ji}]~~~i=1,2,3$</p>
<script type="math/tex; mode=display">
\varPsi_3(1) =\arg\max\{0.028\times 0.5, 0.0504\times0.3,0.042\times 0.2\}  =2\\
\varPsi_3(2) =\arg\max\{0.028\times 0.2, 0.0504\times0.5,0.042\times 0.3\}= 2\\
\varPsi_3(3) =\arg\max\{0.028\times 0.3, 0.0504\times0.2,0.042\times 0.5\} = 3</script></li>
</ul>
</li>
<li><p>在 t = 4 时</p>
<ul>
<li><p>$\delta<em>4(i) = \max</em>{1\leq j\leq3}[\delta<em>3(j)a</em>{ji}]\cdot b_i(o_4)$</p>
<script type="math/tex; mode=display">
\delta_4(1) = \max\{0.00756\times 0.5, 0.01008\times0.3,0.0147\times 0.2\} \times 0.5 = 0.00189\\
\delta_4(2) = \max\{0.00756\times 0.2,0.01008\times0.5,0.0147\times 0.3\} \times 0.6 = 0.003024\\
\delta_4(3) = \max\{0.00756\times 0.3, 0.01008\times0.2,0.0147\times 0.5\} \times 0.3  = 0.002205</script></li>
<li><p>$\varPsi<em>4(i) = \arg\max</em>{1\leq j\leq 3}[\delta<em>3(j)a</em>{ji}]~~~i=1,2,3$</p>
<script type="math/tex; mode=display">
\varPsi_4(1) =\arg\max\{0.00756\times 0.5, 0.01008\times0.3,0.0147\times 0.2\}  =1\\
\varPsi_4(2) =\arg\max\{0.00756\times 0.2,0.01008\times0.5,0.0147\times 0.3\}= 2\\
\varPsi_4(3) =\max\{0.00756\times 0.3, 0.01008\times0.2,0.0147\times 0.5\} = 3</script></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<ol>
<li>用 $P^*$ 来表示最优路径的概率，则：</li>
</ol>
<script type="math/tex; mode=display">
P^* = 3.025\times 10^{-3}</script><p>终点即 $i_4^*= \arg\max_i[\delta_4(i)] = 2$  </p>
<ol>
<li>由最优路径的终点 $i^<em>_4$ ,可以逆向找到 $i_3^</em>,i_2^<em>,i_1^</em>$</li>
</ol>
<script type="math/tex; mode=display">
\text{在t=3时},i^*_3 = \varPsi_4(i^*_4)=2\\
\text{在t=2时},i^*_2 =  \varPsi_3(i^*_3)=2\\
\text{在t=1时},i^*_1 =  \varPsi_2(i^*_2)=3\\</script><p>于是求得最优路径，即最优状态序列 $I^<em> = (i^</em>_1,i_2^<em>,i_3^</em>)= (3,2,2,2)$</p>
<p><img src="/2022/01/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/5.jpeg" style="zoom:100%;"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/5/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/44/">44</a><a class="extend next" rel="next" href="/page/7/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Jason</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">439</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">68</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jason</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

    

  

</body>
</html>
