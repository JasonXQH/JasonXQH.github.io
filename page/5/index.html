<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=">


  <link rel="mask-icon" href="/images/logo.svg?v=" color="#222">














<meta property="og:type" content="website">
<meta property="og:title" content="Jason‘s Blog">
<meta property="og:url" content="https://jasonxqh.github.io/page/5/index.html">
<meta property="og:site_name" content="Jason‘s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Jason">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '',
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    fancybox: false,
    tabs: ,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: 'undefined',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>







  <title>Jason‘s Blog</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', '[object Object]', 'auto');
  ga('send', 'pageview');
</script>





<meta name="generator" content="Hexo 4.2.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">
  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>
<a href="https://github.com/JasonXQH/JasonXQH.github.io" target="_blank" rel="noopener" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jason‘s Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/%20" rel="section">
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/%20" rel="section">
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/%20" rel="section">
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/%20" rel="section">
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/%20" rel="section">
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2022/03/31/%E4%BA%86%E8%A7%A3Spark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/%5Bobject%20Object%5D">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/03/31/%E4%BA%86%E8%A7%A3Spark/" itemprop="url">了解Spark</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-03-31T13:08:45+08:00">
                2022-03-31
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2022-05-09T18:22:54+08:00">
                2022-05-09
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="了解Spark"><a href="#了解Spark" class="headerlink" title="了解Spark"></a>了解Spark</h1><p>Spark 最初是基于内存计算的批处理系统，逐步发展成为<strong>内外存同时使用</strong>的批处理系统，并增加了Spark Streaming支持实时流计算，以及Structured Streaming 支持批流融合。</p>
<h2 id="设计思想"><a href="#设计思想" class="headerlink" title="设计思想"></a>设计思想</h2><h3 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h3><p>首先我们必须意识到，MapReduce虽然底层透明，部署简单，但是基础算子太少。比如，没有Join算子，需要自己实现。而且，Map段的结果需要先写入到本地磁盘，才能由Reduce来拉取。那么如果有多个MapReduce作业串行执行的话，就会使得数据不断在HDFS中写入读出，严重影响性能。</p>
<p>因此我们可以总结一下MapReduce的局限性：</p>
<ul>
<li>编程框架的表达能力有限，无法直接用Join操作</li>
<li>单个作业中需要Shuffle的数据以阻塞方式传输，磁盘IO开销大、延迟高。因为Shuffle数据需要先写磁盘</li>
<li>多个作业之间衔接设计IO开销，应用程序的延迟高<ul>
<li>特别是迭代计算，因为会导致迭代中间结果反复读写，使得整个应用程序的延迟非常高。</li>
</ul>
</li>
</ul>
<h3 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h3><p>在学MapReduce的时候，其数据结构是简单的键值对。而在Spark里面，数据模型是<strong>RDD(Resilient Distributed Dataset)</strong></p>
<p>RDD 是一个<strong>数据集(Dataset)</strong>: 与MapReduce不同，Spark操作对象是抽象的数据集，而不是文件</p>
<p>RDD是<strong>分布式的(Distributed)</strong>：每个RDD可分成多个分区，每个分区就是一个数据集片段，一个RDD的不同分区可以存到集群中的不同的节点上。</p>
<p>RDD具有<strong>弹性(Resilient)</strong>: 具有可恢复的容错特性，就好比一个弹力球，变形了以后还能恢复。</p>
<h4 id="RDD性质"><a href="#RDD性质" class="headerlink" title="RDD性质"></a>RDD性质</h4><p>RDD是<strong>只读的</strong>记录分区的集合</p>
<ul>
<li>其本质上就是一个只读的对象集合</li>
<li>RDD经创建后，不能进行修改</li>
</ul>
<p>RDD不可变(Immutable)</p>
<ul>
<li>只能通过在其他RDD上执行确定的转换操作(如map、join、group by) 来得到新的RDD, 而不是改变原有的RDD</li>
</ul>
<p>遵循了函数式编程的特性</p>
<ul>
<li>变量的值是不可变的</li>
</ul>
<h3 id="计算模型"><a href="#计算模型" class="headerlink" title="计算模型"></a>计算模型</h3><p>在MapReduce里面其实只有两个算子：Map和Reduce。也就是一个非常简单的<strong>有向无环图</strong>。 但是在Spark中，算子比较多，大致上可以分成三大类：创建类、转换类、行动类</p>
<h4 id="创建类算子"><a href="#创建类算子" class="headerlink" title="创建类算子"></a>创建类算子</h4><p>创建(create)类算子可以从本地内存或外部数据源创建RDD，提供了数据输入的功能</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>创建操作</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>parallelize(seq,[numSlices])</code></td>
<td>从内存集合创建RDD</td>
</tr>
<tr>
<td><code>textFile(path, [minPartitions])</code></td>
<td>读取HDFS兼容的文件系统下的文件来创建RDD</td>
</tr>
<tr>
<td><code>wholeTextFile(path, [minPartitions])</code></td>
<td>读取HDFS兼容文件系统下的文件夹中的所有文件创建RDD</td>
</tr>
<tr>
<td><code>hadoopFile(path,inputFormatClass,keyClass,valueClass,[minPartitions])</code></td>
<td>读取HDFS兼容的文件系统下拥有任意inputFormat的文件来创建RDD</td>
</tr>
</tbody>
</table>
</div>
<h4 id="转换类算子"><a href="#转换类算子" class="headerlink" title="转换类算子"></a>转换类算子</h4><p>转换(Transformation)类算子描述了RDD的转换逻辑，提供对RDD进行变换的功能。现在我们列举一部分：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>转换操作</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>map(func)</code></td>
<td>和MapReduce类似，每个记录都是用func进行转换，返回一个新的RDD</td>
</tr>
<tr>
<td><code>filter(func)</code></td>
<td>对滤除对RDD中的每个记录都是用func后返回值为true的记录，类似于数据库中的过滤</td>
</tr>
<tr>
<td><code>flatMap(func)</code></td>
<td>与map相似，但是对于RDD中的每个记录可以映射0个或多个新的参数，而map是1个</td>
</tr>
<tr>
<td><code>mapPartitions(func)</code></td>
<td>与map类似，但是是对每个分区进行操作</td>
</tr>
<tr>
<td><code>union(otherRDD)</code></td>
<td>两个RDD取并集得到一个新的RDD</td>
</tr>
<tr>
<td><code>intersect(otherRDD)</code></td>
<td>两个RDD取交集得到一个新的RDD</td>
</tr>
<tr>
<td><code>groupByKey([numPartitions])</code></td>
<td>类似于Shuffle，将键值对按键分组，返回一个<code>[K,Iterable&lt;V&gt;]</code> 组成的新RDD</td>
</tr>
<tr>
<td><code>reducedByKey(func,[numPartitions])</code></td>
<td>将键值对按键聚合在一起，对每一个键的所有值使用func，类似于Reduce操作</td>
</tr>
<tr>
<td><code>sortByKey([ascending],[numPartitions])</code></td>
<td>将键值对按键进行排序，返回一个新的RDD</td>
</tr>
<tr>
<td><code>join(otherRDD,[numPartitions])</code></td>
<td>join操作</td>
</tr>
<tr>
<td><code>cogroup(otherRDD,[numPartitions])</code></td>
<td>类似于笛卡尔积的操作</td>
</tr>
</tbody>
</table>
</div>
<h4 id="行动算子Action"><a href="#行动算子Action" class="headerlink" title="行动算子Action"></a>行动算子Action</h4><p>行动算子标志着转换结束，出发DAG生成。有点类似于输出的操作。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>行动操作</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>reduce(func)</code></td>
<td>对RDD中的记录按func聚合，这个func必须满足交换律和结合律</td>
</tr>
<tr>
<td><code>collect()</code></td>
<td>收集RDD中的所有记录到driver中，返回一个Array</td>
</tr>
<tr>
<td><code>count()</code></td>
<td>返回RDD中记录的个数</td>
</tr>
<tr>
<td><code>take(n)</code></td>
<td>返回RDD中的前n个记录</td>
</tr>
<tr>
<td><code>saveAsTextFile(path)</code></td>
<td>将RDD中的记录以文本文件的额形式写入本地文件系统，HDFS等兼容的文件系统</td>
</tr>
<tr>
<td><code>countByKey()</code></td>
<td>按key统计计数，返回一个由<code>[K,long]</code>组成的Map</td>
</tr>
<tr>
<td><code>foreach(func)</code></td>
<td>对RDD中的每个记录都使用func</td>
</tr>
</tbody>
</table>
</div>
<h4 id="Operator-DAG"><a href="#Operator-DAG" class="headerlink" title="Operator DAG"></a>Operator DAG</h4><p>在一个实际操作中，算子肯定不能像MapReduce那样组成一个非常简单的有向无环图，而是一个比较复杂的图,可能如下所示：</p>
<p><img src="/2022/03/31/%E4%BA%86%E8%A7%A3Spark/1.png"></p>
<p>DAG主要以算子的角度来描述整个操作的过程，每一个节点都是一个算子。因此DAG的核心主要是以操作算子为描述对象</p>
<h5 id="物理计算模型"><a href="#物理计算模型" class="headerlink" title="物理计算模型"></a>物理计算模型</h5><p>在分布式架构中，DAG中的操作算子实际上是由若干个实例任务(Task)来实现。</p>
<p><img src="/2022/03/31/%E4%BA%86%E8%A7%A3Spark/3.png"></p>
<p>如上图，每个全都是一个task，一共有22个task</p>
<h4 id="RDD-Lineage"><a href="#RDD-Lineage" class="headerlink" title="RDD Lineage"></a>RDD Lineage</h4><p>上面的DAG是以Operator为顶点、RDD为边的；现在我们可以将RDD作为顶点，Operator作为边，进行一个图的转换，这就是<strong>RDD Lineage</strong>，如下图所示：</p>
<p><img src="/2022/03/31/%E4%BA%86%E8%A7%A3Spark/2.png"></p>
<p>首先，RDD读取外部数据源进行创建</p>
<p>然后，RDD经过了一系列转换操作，每次都会产生不同的RDD，供给下一个转换操作使用。</p>
<p>最后一个RDD经过行动操作进行转换，并输出到外部数据源</p>
<h5 id="物理计算模型-1"><a href="#物理计算模型-1" class="headerlink" title="物理计算模型"></a>物理计算模型</h5><p>用RDD Lineage的角度来看更加清楚，每个Task通常负责处理RDD的一个分区，但事实上对于一个流水线的过程(如分区7-分区9-分区13),我们可以将其简化为一个task</p>
<p><img src="/2022/03/31/%E4%BA%86%E8%A7%A3Spark/4.png"></p>
<h2 id="体系架构"><a href="#体系架构" class="headerlink" title="体系架构"></a>体系架构</h2><h3 id="抽象架构图"><a href="#抽象架构图" class="headerlink" title="抽象架构图"></a>抽象架构图</h3><p><img src="/2022/03/31/%E4%BA%86%E8%A7%A3Spark/5.png"></p>
<p>上图是Spark的抽象架构图。我们看到里面有三种节点</p>
<h4 id="Cluster-Manager"><a href="#Cluster-Manager" class="headerlink" title="Cluster Manager"></a>Cluster Manager</h4><p>cluster manager是集群管理器，负责管理整个系统的资源、监控工作节点。就是说和MapReduce中的Job Tracker本质上是一样的。</p>
<p>Cluster Manager是一个抽象的概念，在部署完成后并没有一个进程叫做cluster manager。而且根据Spark的部署方式的不同，Cluster Manager也不一样</p>
<ul>
<li>在Standalone方式(即不适用Yarn或Mesos等其他资源管理系统)中，集群管理器包含 Master和Worker<ul>
<li>注：这个和MapReduce中的Standalone模式是不一样的，在MR中Standalone代表单机集中式部署</li>
</ul>
</li>
<li>在Yarn方式中集群管理器包括：Resource Manager和Node  Manager</li>
</ul>
<h4 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a>Executor</h4><p>Executor是执行器，负责任务的执行</p>
<p>Executor是运行在工作节点上的一个进程，它启动若干个线程Task或者线程组TaskSet来进行执行任务。</p>
<p>在Standalone部署方式下，Executor进程的名称为：CoarseGrainedExecutorBackend</p>
<p>这和MapReduce不一样，MapReduce中的Task是进程，因此其对资源的消耗往往要高于Spark</p>
<h4 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h4><p>Driver是驱动器，负责启动应用程序的主方法并管理作业运行</p>
<p>Spark的架构实现了资源管理和作业管理两大功能的分离</p>
<ul>
<li>Cluster Manager负责集群资源管理</li>
<li>Driver负责作业管理</li>
</ul>
<p>在MapReduce中，JobTracker既负责资源管理，又负责作业管理</p>
<h3 id="Standalone架构图"><a href="#Standalone架构图" class="headerlink" title="Standalone架构图"></a>Standalone架构图</h3><p>Standalone是不包含Yarn和Mesos的，其架构如下：</p>
<p><img src="/2022/03/31/%E4%BA%86%E8%A7%A3Spark/6.png"></p>
<p>我们将其和抽象架构图作一个比较</p>
<p><img src="/2022/03/31/%E4%BA%86%E8%A7%A3Spark/7.png"></p>
<h4 id="Standalone中的Driver"><a href="#Standalone中的Driver" class="headerlink" title="Standalone中的Driver"></a>Standalone中的Driver</h4><p>从图中我们看出，Standalone架构图中并没有出现Driver，那么Driver在哪里？</p>
<p>从逻辑上来说，Driver是独立于主节点、从节点以及客户端的</p>
<p>但是根据应用程序的Client或Cluster运行方式，Driver会以不同的形式存在</p>
<ul>
<li>Client方式：Driver和客户端以同一个进程存在</li>
<li>Cluster方式：系统将由某一Worker启动一个进程作为Driver</li>
</ul>
<p>客户端提交应用程序时可以选择Client或Cluster方式</p>
<ul>
<li>Standalone Client模式(默认)</li>
</ul>
<p><img src="/2022/03/31/%E4%BA%86%E8%A7%A3Spark/8.png"></p>
<ul>
<li>Standalone Cluster 模式</li>
</ul>
<p><img src="/2022/03/31/%E4%BA%86%E8%A7%A3Spark/9.png"></p>
<p>那么这两者有什么差别呢？</p>
<p>由于我们申请的四台虚拟机都是在同一个云平台中的， 这时候Cluster模式和Client没有什么太大的区别。</p>
<p>只有当客户端和Spark集群的物理距离非常远的时候，那么就应该使用Cluster模式，因为这样内网通信更快。如果使用Client模式，Driver管理不同设备的时候，就需要通过远程网络传输，这时候开销就很大了。</p>
<h4 id="Spark-vs-MapReduce"><a href="#Spark-vs-MapReduce" class="headerlink" title="Spark vs MapReduce"></a>Spark vs MapReduce</h4><p><img src="/2022/03/31/%E4%BA%86%E8%A7%A3Spark/10.png"></p>
<h2 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h2><p><img src="/2022/03/31/%E4%BA%86%E8%A7%A3Spark/11.png"></p>
<p>之前我们画的抽象结构图中，在driver部分并没有详细说。其实driver里面存在如上图的结构</p>
<p>首先RDD对象会经过解析器，然后得到一个有向无环图，然后DAG会被拆分成Stage，并交给task调度器去执行。我们可以将其和数据库查询引擎做一个对比，SQL语言经过解析器之后会变成语法树。我们可以把DAG看做是逻辑方面的模型，而Stage则是物理方面的模型</p>
<h3 id="Stage划分"><a href="#Stage划分" class="headerlink" title="Stage划分"></a>Stage划分</h3><h4 id="按依赖关系划分"><a href="#按依赖关系划分" class="headerlink" title="按依赖关系划分"></a>按依赖关系划分</h4><p><img src="/2022/03/31/%E4%BA%86%E8%A7%A3Spark/12.jpeg"></p>
<p>首先我们要搞清楚两种不同的依赖有何区别。宽依赖可以理解为多对多，类似于map reduce中的shuffle过程，窄依赖可以理解为子分区一对一、子分区一对多。</p>
<p>因此，我们可以通过分析各个RDD的偏序关系来生成DAG，再通过分析各个RDD中的分区之间的依赖关系来决定如何划分Stage。</p>
<p>具体划分方法如下：</p>
<ul>
<li>在DAG中进行反向解析，遇到宽依赖就断开</li>
<li>遇到窄依赖就把当前的RDD加入到Stage中</li>
</ul>
<p>比如：还是以展开后的DAG为例，我们看到有明显的两处宽依赖的部分，A与B之间、F与G之间。我们就根据这个将算子划分为三个stage</p>
<p><img src="/2022/03/31/%E4%BA%86%E8%A7%A3Spark/13.png" style="zoom:67%;"></p>
<p>我们把每个stage抽象出来，得到下面这张拓扑图，通过这张图可以还原出原来的DAG。</p>
<p><img src="/2022/03/31/%E4%BA%86%E8%A7%A3Spark/14.png" style="zoom:67%;"></p>
<h4 id="Stage类型"><a href="#Stage类型" class="headerlink" title="Stage类型"></a>Stage类型</h4><p><strong>ShuffleMapStage</strong></p>
<ul>
<li><p>输入/输出</p>
<ul>
<li><p>输入可以是从外部获取数据，也可以是另一个ShuffleMapStage的输出</p>
</li>
<li><p>以Shuffle为输出，作为另一个Stage开始</p>
</li>
</ul>
</li>
<li><p>特点</p>
<ul>
<li><p>不是最终的Stage，在它之后还有其他Stage</p>
</li>
<li><p>它的输出一定需要经过Shuffle过程，并作为后续Stage的输入</p>
</li>
<li><p>在一个DAG里可能有该类型的Stage，也可能没有该类型Stage</p>
</li>
</ul>
</li>
</ul>
<p><strong>ResultStage</strong></p>
<ul>
<li><p>输入/输出</p>
<ul>
<li><p>其输入可以是从外部获取数据，也可以是另一个ShuffleMapStage的输出</p>
</li>
<li><p>输出直接产生结果或存储</p>
</li>
</ul>
</li>
<li><p>特点</p>
<ul>
<li><p>最终的Stage</p>
</li>
<li><p>在一个DAG里必定有该类型Stage</p>
</li>
</ul>
</li>
</ul>
<p>判断方式也很容易，只要有输出结果的算子如saveAsTextFile，那么这个stage就是ResultStage。 因此，一个DAG含有一个或多个Stage，其中至少含有一个ResultStage</p>
<h3 id="Stage-内部数据传输"><a href="#Stage-内部数据传输" class="headerlink" title="Stage 内部数据传输"></a>Stage 内部数据传输</h3><p><strong>问题是，为什么要将窄依赖尽可能划分在同一个stage里，而在宽依赖的时候将DAG断开</strong>？接下来两节我们就来解决这个问题。</p>
<p>首先Spark和MapReduce是不一样的，他算子很多，自然不可能想MR一样每经过一个算子就将其压入文件系统，让下一个算子读取。因此Spark采用了<strong>流水线</strong>的方式,而不是阻塞方式：</p>
<p><img src="/2022/03/31/%E4%BA%86%E8%A7%A3Spark/15.png" style="zoom:67%;"></p>
<p>如上图，我们把目光聚焦到Stage1，这里有10个分区，如果按照最笨的方法，就需要开启10个task。但事实上，我们可以采用流水线的方式，这样只用开启4个ShuffleMapTask就可以了</p>
<p>那么流水线的过程是不是类似于MapReduce中的Shuffle过程呢？<strong>不同！</strong>，流水线方式不要求物化前序算子的所有计算结果</p>
<p>分区7通过map操作生成的分区9，并不需要物化分区9，而且可以不用等待分区8到分区10这个map操作的计算结束，而是继续进行union操作，得到分区13</p>
<p>如果采用MapReduce中的Shuffle方式，那么意味着分区7、8经map计算得到分区9、10并将这两个分区进行物化之后，才可以进行union</p>
<h3 id="Stage之间数据传输"><a href="#Stage之间数据传输" class="headerlink" title="Stage之间数据传输"></a>Stage之间数据传输</h3><p>这时候我们再来看stage之间的划分依据，发现窄依赖的stage之间用流水线方式很方便，但<strong>在宽依赖的stage之间，采用流水线方式就不适用</strong>了。因此我们就需要在宽依赖的时候断开DAG。</p>
<p>Stage之间的数据传输需要进行Shuffle，该过程与MapReduce中的Shuffle类。</p>
<ul>
<li><p>从Stage层面来看，Shuffle过程可能发生在两个ShuffleMapStage之间，或者ShuffleMapStage与ResultStage之间。</p>
</li>
<li><p>从Task层面来看，该过程表现为两组ShuffleMapTask之间，或一组ShuffleMapTask与一组ResultTask之间的数据传输</p>
</li>
</ul>
<p>下面是两个宽依赖的stage之间的数据传输过程，也就是spark shuffle</p>
<p><img src="/2022/03/31/%E4%BA%86%E8%A7%A3Spark/16.png" style="zoom:67%;"></p>
<p>其过程如下：</p>
<ul>
<li><p>在Shuffle Write阶段，会将数据进行Partition操作，ShuffleMapTask需要将输出RDD的记录按照partition函数划分到相应的bucket当中并<strong>物化到本地磁盘形成ShuffleblockFile</strong>，之后才可以在Shuffle Read阶段被拉取</p>
</li>
<li><p>在Shuffle Read阶段，ShuffleMapTask或ResultTask根据partiton函数读取相应的ShuffleblockFile，存入buffer并进行继续后续的计算 </p>
</li>
</ul>
<p>因此</p>
<ul>
<li>在stage内部的信息传递不需要物化，采用pipeline 的形式</li>
<li>在stage之间的信息传递是需要物化的，且是阻塞的，采用shuffle形式</li>
</ul>
<p>因此Spark相比于MapReduce，其改进也是有限的，特别是在Shuffle过程上，基本没有发生变化。因此从这一点来说，限制了Spark只能是一个批处理系统，而不能成为一个流处理系统</p>
<h3 id="应用与作业"><a href="#应用与作业" class="headerlink" title="应用与作业"></a>应用与作业</h3><p>现在我们对Spark中的应用和作业做一个梳理</p>
<p>Application:  用户编写的Spark应用程序</p>
<p>Job: 一个Job包含多个RDD及作用于响应RDD转换操作，其中最后一个为action</p>
<p>MapReduce VS Spark</p>
<ul>
<li>在MapReduce中，一个应用就是一个作业</li>
<li>Spark中，一个应用可以由多个作业来构成</li>
</ul>
<p><img src="/2022/03/31/%E4%BA%86%E8%A7%A3Spark/17.png" style="zoom:67%;"></p>
<p>那么在Spark中，应用、作业和任务的关系又是什么？</p>
<ul>
<li>从逻辑执行角度<ul>
<li>一个Application = 一个或者多个DAG</li>
<li>一个DAG = 一个或多个Stage</li>
<li>一个Stage = 若干窄依赖的RDD操作</li>
</ul>
</li>
<li>从物理执行角度<ul>
<li>一个Application = 一个或者多个Job(Job = DAG)</li>
<li>一个Job = 一个或者多个TaskSet<ul>
<li>一个Job会分为多组Task，每组Task被称为Stage，或者也被称为TaskSet</li>
</ul>
</li>
<li>一个TaskSet = 多个没有Shuffle关系的Task</li>
<li>一个Task：运行在Executor上的单元。</li>
</ul>
</li>
</ul>
<p>可总结为：</p>
<p><img src="/2022/03/31/%E4%BA%86%E8%A7%A3Spark/18.png" style="zoom:67%;"></p>
<h2 id="容错机制"><a href="#容错机制" class="headerlink" title="容错机制"></a>容错机制</h2><h3 id="故障类型"><a href="#故障类型" class="headerlink" title="故障类型"></a>故障类型</h3><p>首先，Spark里面每个部分都有可能出现故障。</p>
<ul>
<li>Master故障：可以利用ZooKeeper配置多个Master，但这不再讨论范围之内。</li>
<li>Worker故障</li>
<li>Executor故障</li>
<li>Driver故障： 这个无解，只能重启，因此不在讨论范围之内</li>
</ul>
<p>因此我们主要来解决Worker、Executor出现问题怎么办？主要有三种方式：RDD持久化、故障恢复、检查点</p>
<h3 id="RDD持久化"><a href="#RDD持久化" class="headerlink" title="RDD持久化"></a>RDD持久化</h3><p>由于计算过程中会不断地产生新的RDD， 所以系统不能将所有的RDD都存在内存 。因此， 一旦达到相应存储空间的阈值，Spark会使 用置换算法（例如，LRU）将部分RDD的 内存空间腾出 。如果不做任何声明，这些RDD会被直接丢弃。但是，某些RDD在后续很可能会被再次使用，那么这时候就需要让RDD持久化</p>
<p>在Spark里面提供了RDD持久化的接口</p>
<p><code>persist(StorageLevel)</code></p>
<ul>
<li>接受StorageLevel类型参数，可配置各种级别 </li>
<li>持久化后的RDD将会保留在工作节点的中，可重复使用</li>
</ul>
<p><code>cache()</code>： 缓存</p>
<ul>
<li>相当于<code>persist(MEMORY_ONLY)</code></li>
</ul>
<p>这边提供几个StorageLevel参数及其含义：</p>
<ul>
<li>MEMORY_ONLY： 在JVM中缓存Java的对象。如果内存不足，直接丢弃某些partition </li>
<li>MEMORY_AND_DISK： 在JVM中缓存Java的对象。如果内存不足，则 将某些partitions写入到磁盘中 </li>
<li><p>MEMORY_ONLY_SER： 在内存为每个partition存储一个byte数组，数组 内容为当前partition中Java对象的序列化结果</p>
<ul>
<li>序列化可以理解为对存储空间进行一个压缩</li>
</ul>
</li>
<li><p>MEMORY_AND_DISK_SER： 与MEMORY_AND_DISK类似，但每个分区存 储的是Java对象序列化后组成的byte数组 </p>
</li>
<li>DISK_ONLY: 将每个分区的数据序列化到磁盘中</li>
<li>MEMORY_ONLY_2： 与MEMORY_ONLY相同，但是每个分区备份 到两台机器上 </li>
<li>MEMORY_AND_DISK_2: 与MEMORY_AND_DISK相同，但是每个分区备份到两台机器上</li>
</ul>
<h3 id="故障恢复"><a href="#故障恢复" class="headerlink" title="故障恢复"></a>故障恢复</h3><p> 通常，一个Worker/Executor出了问题，常常是RDD出了问题，因此我们回到RDD Lineage，来看看出了问题后该怎么办？ </p>
<p>根据Lineage的机制， 如果是红色部分丢失，那么就需要重新计算紫色部分。这里宽依赖和窄依赖就有所不同了，对于窄依赖，只要能拿到其对应的一两个父RDD，就可以还原出来了。但是对于宽依赖，其涉及到的父RDD就很多了。</p>
<p>因此宽依赖的恢复代价高、窄依赖的恢复代价低。</p>
<p><img src="/2022/03/31/%E4%BA%86%E8%A7%A3Spark/19.png" style="zoom:67%;"></p>
<p>因此，基于RDD Lineage的恢复可被总结如下：</p>
<ul>
<li>利用RDD Lineage的故障恢复<ul>
<li>重新计算丢失分区</li>
<li>重算过程在不同节点之间可以并行 </li>
</ul>
</li>
<li>Lineage 状态存放在哪里？<ul>
<li>Lineage是存放在Driver里面的，Driver里面可以解析DAG得到Lineage。因此如果Driver出故障的话，那真的只能重启了</li>
</ul>
</li>
<li>与数据库恢复的比较 <ul>
<li>RDD Lineage：记录<strong>粗粒度</strong>的操作 ，并没有记录RDD那里做了修改，只是对RDD变化的<strong>过程</strong>做了记录</li>
<li>数据复制或者日志：记录<strong>细粒度</strong>的操作</li>
</ul>
</li>
</ul>
<h3 id="检查点机制"><a href="#检查点机制" class="headerlink" title="检查点机制"></a>检查点机制</h3><ul>
<li>前述机制的不足之处 <ul>
<li>Lineage可能非常长 </li>
<li>RDD持久化机制保存到集群内机器的磁盘，并 不完全可靠</li>
</ul>
</li>
<li>检查点机制将RDD写入外部可靠的（本身 具有容错机制）分布式文件系统，例如 HDFS，这样更加可靠<ul>
<li>在实现层面，写检查点的过程是一个独立的作业，在用户作业结束后运行</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    
    <div>
      
    </div>
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2022/03/31/%E4%BA%86%E8%A7%A3%E4%BB%A5%E5%A4%AA%E5%9D%8A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/%5Bobject%20Object%5D">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/03/31/%E4%BA%86%E8%A7%A3%E4%BB%A5%E5%A4%AA%E5%9D%8A/" itemprop="url">了解以太坊</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-03-31T07:12:07+08:00">
                2022-03-31
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2025-04-11T10:19:02+08:00">
                2025-04-11
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="了解以太坊"><a href="#了解以太坊" class="headerlink" title="了解以太坊"></a>了解以太坊</h1><h2 id="以太坊简介"><a href="#以太坊简介" class="headerlink" title="以太坊简介"></a>以太坊简介</h2><p>和比特币不一样，以太坊是一个<strong>可编程</strong>的区块链平台。与比特币相比，两者都属于公有链，但是以太坊侧重于<strong>可编程的智能合同</strong>即分布式应用程序。</p>
<p>也就是说，以太坊可以理解为一个<strong>永不宕机的世界计算机</strong>。 可想而知，这样的系统需要满足很多条件：其中最重要的就是它不能受平台的限制，必须实现跨平台通用，如Linux, Windows, mac等。因此，实现这个要求的最佳方案就是<strong>虚拟机</strong>——和JVM一样，以太坊也有一个自己的虚拟机环境 EVM。</p>
<p>此外，这个系统如何控制软硬件对计算机资源的消耗？因为可能存在恶意的软件不断消耗以太坊平台的资源。答案很简单，就是引入代币机制，而以太坊中的代币就是<strong>以太币</strong>。也就是说，以太币仅仅是驱动智能合约执行所需燃料的加密令牌。</p>
<h3 id="以太坊设计原则"><a href="#以太坊设计原则" class="headerlink" title="以太坊设计原则"></a>以太坊设计原则</h3><ul>
<li>简单性： 协议尽可能简单</li>
<li>普遍性： 没有“特征”</li>
<li>模块化：各个部分尽可能模块化和可分离</li>
<li>敏捷性：更改容易，允许通过硬分叉进行重大修改。硬分叉即不向前兼容，旧节点不能接收新的区块。通常在硬分叉之后，更新后的节点会得到一定的币的补偿，以激励节点更新到新的版本。</li>
<li>不歧视，不审查：<ul>
<li>协议不会限制特定类别的使用</li>
<li>监管机制不会反对具体的不可取的应用</li>
<li>能运行无限循环的脚本</li>
</ul>
</li>
</ul>
<h3 id="以太坊虚拟机"><a href="#以太坊虚拟机" class="headerlink" title="以太坊虚拟机"></a>以太坊虚拟机</h3><ul>
<li>智能合约在不同的操作系统、硬件平台都应该具有确定性和一致性的结果</li>
<li>可以执行任意算法复杂性的代码</li>
<li>开发人员可以使用基于现有熟悉的变成创建在EVM上运行的应用程序</li>
<li>每个运行EVM的节点都是共识协议的一部分，独立验证交易序列、运行交易触发的代码。</li>
</ul>
<h3 id="以太坊系统特点"><a href="#以太坊系统特点" class="headerlink" title="以太坊系统特点"></a>以太坊系统特点</h3><ul>
<li>可以部署基于分布式的大规模并行化计算</li>
<li>为DApp(去中心化的应用程序)提供了极端水平的容错能力，确保零停机时间</li>
<li>数据不可更改和可追溯</li>
</ul>
<p>所以说，对于一个DApp，我们在设计的时候只把关键的业务逻辑写到区块链上，多方面不需要共识的部分我们就放在链下。</p>
<h3 id="以太坊应用领域"><a href="#以太坊应用领域" class="headerlink" title="以太坊应用领域"></a>以太坊应用领域</h3><ol>
<li>对等实体之间的直接交互或者跨网络的组织之间组织协调。</li>
<li>基于点对点的应用领域或复杂金融合约自动化运行领域。例如：复杂的金融交流或交换；信任、 安全和持久性都很重要的环境一. 例如资产登记、投票、治理和物联网。</li>
</ol>
<h3 id="以太坊的升级和分叉"><a href="#以太坊的升级和分叉" class="headerlink" title="以太坊的升级和分叉"></a>以太坊的升级和分叉</h3><p><img src="/2022/03/31/%E4%BA%86%E8%A7%A3%E4%BB%A5%E5%A4%AA%E5%9D%8A/1.png" style="zoom:67%;"></p>
<p><img src="/2022/03/31/%E4%BA%86%E8%A7%A3%E4%BB%A5%E5%A4%AA%E5%9D%8A/2.png" style="zoom:67%;"></p>
<h2 id="以太坊框架"><a href="#以太坊框架" class="headerlink" title="以太坊框架"></a>以太坊框架</h2><h3 id="以太坊账户"><a href="#以太坊账户" class="headerlink" title="以太坊账户"></a>以太坊账户</h3><p>账户是以太坊的核心角色，以太坊中的交易就是账户之间价值和信息的转换。</p>
<p>账户的结构如下：</p>
<p><img src="/2022/03/31/%E4%BA%86%E8%A7%A3%E4%BB%A5%E5%A4%AA%E5%9D%8A/3.png" style="zoom:67%;"></p>
<p>账户也分为两类：外部账户和 合约账户</p>
<ul>
<li>外部账户(EOAs) 由公钥-私钥对 控制，没有相关代码</li>
<li>合约账户由交易类型、消息类型进行创建，由代码控制</li>
</ul>
<p>以太坊中，所有的交易发起者，只能是外部账户。但是，智能合约执行过程中，可以创建新的交易</p>
<h3 id="以太币"><a href="#以太币" class="headerlink" title="以太币"></a>以太币</h3><p>以太币是以太坊中货币的名称，它被用来支付以太坊虚拟机（EVM）计算的费用。它不是被直接消耗掉而是间接的通过购买gas 来实现的。</p>
<p>以太坊最小的面额被称为Wei，1 Eth = 1e18 Wei</p>
<p>获得以太币有三种方式：成为以太坊矿工，通过挖矿获得以太币；通过第三方担保和持有以太币的人进行交易；使用用户友好Mist以太坊钱包，用客户端API购买以太币</p>
<h3 id="交易"><a href="#交易" class="headerlink" title="交易"></a>交易</h3><p>以太坊中“交易”是指存储从外部账户发出的消息的签名数据包，数据包中包含从<strong>一个外部账户</strong>发往<strong>一个账户</strong>的消息。</p>
<p>一笔交易的组成部分如下图所示：</p>
<p><img src="/2022/03/31/%E4%BA%86%E8%A7%A3%E4%BB%A5%E5%A4%AA%E5%9D%8A/4.png" style="zoom:67%;"></p>
<p>value字段就是转账的金额，Startgas和Gasprice我们比较陌生，所依赖着重学习一下：</p>
<ul>
<li>Startgas 指该交易能消耗的最大的瓦斯使用量，如果交易完成还有得多，会将gas退回到账户</li>
<li>Gasprice 指的是一个gas的单价，这个单价是可以变化的。</li>
</ul>
<h3 id="消息"><a href="#消息" class="headerlink" title="消息"></a>消息</h3><p>消息可以理解为一种特殊的交易，是合约代码执行过程中对其他合约的功能调用（通过操作码“CALL”和“DELEGATECALL”）。</p>
<p><img src="/2022/03/31/%E4%BA%86%E8%A7%A3%E4%BB%A5%E5%A4%AA%E5%9D%8A/5.png" style="zoom:67%;"></p>
<p>消息与交易都会导致接收者的合约账户执行合约代码，区别在于交易是外部账户创建的，而消息则由合约也只能通过合约创建。</p>
<p>比如说，我要给A转10个以太币，那么我可以只发起一个交易；但是我也可以把这个操作写到一个智能合约，我可以调用这个智能合约实现转账，这时候合约发起的是一个消息。</p>
<p>但是消息不一定只用于转账，也可以用做消息传递。</p>
<h3 id="GAS"><a href="#GAS" class="headerlink" title="GAS"></a>GAS</h3><p>在以太坊中用瓦斯（GAS）作为合约代码执行成本基本单位。瓦斯由外部账户通过以太币从执行代码的矿工那里购买。矿工可以决定瓦斯价格，即设置最低价格的单价，低于该价格的交易会被矿工拒绝。</p>
<p>以太坊为每一个交易和合约的计算步骤设置费用，外部账号可以为交易设置以太币值、STARTGAS和GASPRICE等瓦斯限制，矿工也有权选择验证这个交易以获取费用或者拒绝。</p>
<p>在发送一个交易时根据预估交易费用设置相应的以太币是有必要的，其计算方法如下：</p>
<ul>
<li><p>预估交易费用 = 瓦斯使用量 * 瓦斯价格</p>
</li>
<li><p>瓦斯使用量 = 交易瓦斯使用量 + 消息瓦斯使用量</p>
</li>
<li><p>瓦斯价格 = 默认值 = 0.05e18 wei (0.05个以太币)</p>
</li>
</ul>
<h3 id="轻客户端"><a href="#轻客户端" class="headerlink" title="轻客户端"></a>轻客户端</h3><p>以太坊中的轻客户端和比特币网络中的轻节点是类似的</p>
<ul>
<li>轻客户端使用全节点作为中介，不需要直接与区块链交易</li>
<li>轻客户端只需要下载最新的区块头，速度快效率高</li>
<li>轻客户端能判断那些全节点是恶意全节点</li>
<li>轻客户端在分片技术中能够发挥重要作用。分片就是</li>
<li>轻客户端可以运行在移动设备上</li>
</ul>
<p>轻节点查询特定账户余额的过程如下，这时候就不是UTXO模型了</p>
<ul>
<li>用户连接到轻客户端，并发送查询特定账户余额的请求。</li>
<li>轻客户端调用连接的全节点转发账户查询请求 </li>
<li>验证全节点给出的答案是否与它们所拥有的“签名”相匹配</li>
<li>轻客户端返回结果给用户。</li>
</ul>
<h3 id="全节点运行网络"><a href="#全节点运行网络" class="headerlink" title="全节点运行网络"></a>全节点运行网络</h3><p>全节点最上层是DApp，就是在以太坊平台的基础上由许多开发团队所开发的各类分布式应用项目，如和以太坊基金会是独立的Augur, Digix, Maker。DApp和以太坊的关系与操作系统和应用软件的关系相似。</p>
<p>第二层是RPC，即远程调用接口，包含两个模块：智能合约和共识模块</p>
<p>第三层是区块链本体，包含事件、交易、区块、状态</p>
<p>最下面一层是区块链的基础，分为网络模块和安全模块</p>
<p><img src="/2022/03/31/%E4%BA%86%E8%A7%A3%E4%BB%A5%E5%A4%AA%E5%9D%8A/6.png" style="zoom:67%;"></p>
<h2 id="智能合约"><a href="#智能合约" class="headerlink" title="智能合约"></a>智能合约</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><ul>
<li><p>以太坊中的智能合约是具有特定地址（合约账户）的代码（合约功能）和数据（合约状态）的集合。</p>
</li>
<li><p>以太坊中的智能合约通常使用Solidity等高级语言编写，编译成基于EVM的字节代码后上传到合约账户。</p>
</li>
<li><p>合约的代码以基于EVM的二进制字节码存储在账户中，当被交易或消息触发时在EVM中运行。 </p>
</li>
<li><p>合约账户之间可以相互传递消息并实现图灵完备运算。</p>
</li>
</ul>
<h3 id="状态转换"><a href="#状态转换" class="headerlink" title="状态转换"></a>状态转换</h3><p>现在我们来学习智能合约调用过程中的状态转换</p>
<ol>
<li>首先检查交易的格式是否正确</li>
<li>计算交易费用 <code>fee = STARTGAS * GASPRICE</code>, 从发送者账户减去交易费用</li>
<li>设定初值GAS =STARTGAS，根据交易中的字节数减去一定量的瓦斯值。</li>
<li>从发送者的账户转移价值到接收者账户。如果接收账户是一个合约，则运行合约的代码。</li>
<li>如果价值转移失败，则恢复原来的状态，但仍需要支付交易费用。</li>
<li>否则，将所有剩余的瓦斯归还给发送者。</li>
</ol>
<p><img src="/2022/03/31/%E4%BA%86%E8%A7%A3%E4%BB%A5%E5%A4%AA%E5%9D%8A/7.png" style="zoom:67%;"></p>
<p>我们 来看这样一个例子，假设交易中STARTGAS为2000 gas，GASPRICE为0.001以太币</p>
<ul>
<li>检查交易是否有效、格式是否正确</li>
<li><p>检查交易发送者至少有2000*0.001=2个以太币。如果有，从发送者账户中减去2个以太币。- 先按预估瓦斯费用扣款，再返款</p>
</li>
<li><p>初始设定gas=2000,假设交易长为170字节，每字节的瓦斯使用量是减去850，所以还剩1150。</p>
</li>
<li><p>从发送者账户减去10个以太币，为合约账户增加10个以太币。10个以太币为转账金额</p>
</li>
<li><p>运行代码。在这个合约中，运行代码很简单：它检查合约存储器索引为2处是否已使用，注意到它未被使用，然后将其值置为CHARLIE。假设这消耗了187单位的瓦斯，于是剩余的瓦斯为1150 - 187 = 963。</p>
</li>
</ul>
<ul>
<li>智能合约运行瓦斯使用量</li>
<li>向发送者的账户增加963*0.001=0.963个以太币，这是剩下未使用的gas，因此返还给发送者，返回最终状态。 </li>
</ul>

          
        
      
    </div>
    
    
    
    <div>
      
    </div>
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2022/03/25/%E4%BA%86%E8%A7%A3MapReduce/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/%5Bobject%20Object%5D">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/03/25/%E4%BA%86%E8%A7%A3MapReduce/" itemprop="url">了解MapReduce</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-03-25T02:46:20+08:00">
                2022-03-25
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2022-03-31T14:00:20+08:00">
                2022-03-31
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="了解MapReduce"><a href="#了解MapReduce" class="headerlink" title="了解MapReduce"></a>了解MapReduce</h1><h2 id="MapReduce引入"><a href="#MapReduce引入" class="headerlink" title="MapReduce引入"></a>MapReduce引入</h2><p>首先，我们需要了解什么是MapReduce，它是用来做什么的？</p>
<p>比如说，我们有10TB的数据，放在一台机子上肯定是不现实的，因此需要存放在之前我们介绍的存储系统：HDFS中。但是，如果放在HDFS上，由于大文件被切成一个个小块，如果这时候要对这些文件建立倒排索引、统计词频的话，按照原来的逻辑，可能会出现一定的问题——如果按照原来的逻辑，我们可以写一个客户端，将所有DataNode上的数据读进来进行处理，但这样说到底还是一个单机程序，没有体现出分布式系统的优势。</p>
<p>这时候一个合理的逻辑是把我的程序发送到集群的每一台DataNode上去做统计，也就是<strong>把运算往数据靠近，而不是数据往运算靠近</strong>，数据在哪里我就去那里运算，最终得到了DataNode上的局部结果。因此，我们不但需要分布式存储, 还需要分布式运算.</p>
<p>但分布式运算的构想很丰满，显示却很“骨干”，这边有几个痛点问题需要解决：</p>
<ol>
<li>怎么把程序包<strong>分发</strong>到集群中的DataNode上？</li>
</ol>
<p>我们该用什么方法去保证每个节点上的环境都是相同的，该用什么系统去分发程序。总不可能用U盘一个一个拷贝过来。</p>
<ol>
<li>程序如何分发到<strong>有数据的</strong>Datanode上？</li>
</ol>
<p>假设有30个节点存放着我们要跑的数据，那么我希望我的程序就运行在这30个节点上，如果在其他节点上运行也不是不可以，只不过数据需要靠网络传输，又变成数据向计算靠拢的情况了。</p>
<ol>
<li>在DataNode上运行的结果，怎么将其汇总起来？</li>
</ol>
<p>如果只在一台机器上汇总，那么这台机器的负载肯定会很高。但如果在多台机器上汇总，逻辑就会变得异常复杂。比如说，如果要统计词频，应该如何设计那些单词分发到哪个节点上去统计。</p>
<p>这些问题是很难解决的，这时候就需要MapReduce了。对于程序员来说，我们<strong>只关心如何处理数据的逻辑</strong>，并不关心数据是如何被分配，又是如何被汇总的，因为这些我们不擅长却异常复杂。MapReduce就是将这些不擅长又必须克服的痛点给解决了，它将这些东西全部封装起来。因此，对程序员来说底层的分发、计算、汇总都是透明的。 </p>
<h3 id="MPI"><a href="#MPI" class="headerlink" title="MPI"></a>MPI</h3><p>其实在MapReduce出现以前，程序员可以使用MPI并行处理数据。但是这个接口库存在以下缺点：</p>
<ol>
<li>从用户编程角度快来看，程序员需要考虑到进程之间的并行问题，并且进程之间的通信需要用户在程序中<strong>显式表达</strong>，这增加了编程的复杂性，使得底层对用户不够透明。</li>
<li>从系统实现角度来看，MPI 以多进程的方式运行。因此除非在编写程序的时候加入了故障恢复功能，否则MPI不具备容错能力，系统鲁棒性很差。</li>
</ol>
<p>因此，分布式计算系统本身需要具备容错能力，这也是MapReduce系统与MPI的重要区别。</p>
<h3 id="MR数据模型"><a href="#MR数据模型" class="headerlink" title="MR数据模型"></a>MR数据模型</h3><p>上面可以看做是对MapReduce的抽象了解。其实，MR会将数据抽象为一系列键值对，这些键值对通常来自于存储在HDFS中的文件。如下图，我们看到MapReduce的输入是一组键值对，进行转换后的输出也是一组键值对</p>
<p><img src="/2022/03/25/%E4%BA%86%E8%A7%A3MapReduce/1.png" style="zoom:67%;"></p>
<h3 id="MR计算模型"><a href="#MR计算模型" class="headerlink" title="MR计算模型"></a>MR计算模型</h3><p>我们可以用做菜来比如MapReduce的计算过程：统合来看，MapReduce就是你有很多各种各样的蔬菜水果面包（Input），有很多厨师，不同的厨师分到了不同的蔬菜水果面包，自己主动去拿过来（Split），拿到手上以后切碎（Map），切碎以后给到不同的烤箱里，冷藏机里（Shuffle），冷藏机往往需要主动去拿，拿到这些东西存放好以后会根据不同的顾客需求拿不同的素材拼装成最终的结果，这就是Reduce，产生结果以后会放到顾客那边等待付费（Ticket），这个过程是Finalize。</p>
<p>从上图我们可以看出，Map转换前后的键值对的内容通常都会不同。MapReduce框架会将Map转换的道德键值对按照键进行分组，这就是Shuffle过程 (紫色的归为紫色，蓝色的归为蓝色)。之后，Reduce会对相同键的键值对进行计算，并可根据需要将计算结果进行一次键值对转换后输出。</p>
<p>从用户编程角度看，程序员不再需要像MPI一样关注节点之间的通信，可以像编写集中式程序那样便携MapReduce代码。</p>
<p>从系统角度看，系统可以并行启动一系列的进程去执行Map和Reduce操作，而且一旦这些进程出现故障，MapReduce框架可以自动进行容错处理，无需用户编写程序。</p>
<h2 id="MR体系架构"><a href="#MR体系架构" class="headerlink" title="MR体系架构"></a>MR体系架构</h2><p><img src="/2022/03/25/%E4%BA%86%E8%A7%A3MapReduce/2.png"></p>
<p>如上图所示，MapReduce系统采用<strong>主从</strong>架构，里面有主要工作部件：</p>
<ul>
<li><code>JobTracker</code>： 主节点运行的后台进程，负责整个系统的<strong>资源管理</strong>和<strong>作业管理</strong>。<ul>
<li>资源管理是指<code>JobTracker</code>通过监控<code>TaskTracker</code>来管理系统拥有的计算资源。</li>
<li>作业管理是指<code>JobTracker</code>负责将作业(Job)拆分成任务(Task),并进行任务调度以及跟踪任务的运行进度、资源使用量等信息。</li>
</ul>
</li>
<li><code>TaskTracker</code>: 从节点运行的后台进程，负责管理本节点的资源、执行<code>JobTracker</code>的命令并会报情况。<code>TaskTracker</code>使用slot等量划分本节点上的资源量(如CPU,RAM), 接收<code>JobTracker</code>发送过来的命令并执行，通过心跳将本节点上资源使用情况和任务运行进度汇报给<code>JobTracker</code></li>
<li><code>Task</code> : 从节点在应用程序执行过程中所启动的进程，负责任务执行。<code>JobTracker</code>根据<code>TaskTracker</code>回报的信息进行调度，命令存在空闲slot 的<code>TaskTracker</code>启动<code>Task</code>进程执行map或者reduce任务，即<code>MapTask</code>和<code>ReduceTask</code>。在Hadoop MapReduce的实现中，<code>Task</code>又被称为<code>Child</code></li>
<li>客户端：客户端所在节点为提交应用程序启动的进程，负责将用户编写的MapReduce程序提交给JobTracker。在Haddop MapReduce的实现中，客户端被称为<code>RunJar</code></li>
</ul>
<p>通常，MapReduce和HDFS会同时部署在一个节点，否则会导致MapReduce从远程DataNode中读入数据，而且需要通过网络将输出结果写入远程DataNode中，这样的代价是很高的。</p>
<p>因此，JobTracker和NameNode部署在同一个物理节点，而TaskTracker和DataNode部署在同一个物理节点。在这种方式下，可以最大程度避免远程写入的网络开销。如下图所示：</p>
<p><img src="/2022/03/25/%E4%BA%86%E8%A7%A3MapReduce/3.png"></p>
<p>这种部署方式使得MapReduce在输入输出数据的读写时，无需通过网络移动数据，体现了<strong>计算向数据靠拢</strong>的理念。</p>
<h3 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h3><p><img src="/2022/03/25/%E4%BA%86%E8%A7%A3MapReduce/4.jpg" style="zoom:150%;"></p>
<p>上图是MapReduce执行应用程序的流程：</p>
<ol>
<li>客户端将用户编写的MapReduce作业的配置信息，jar包等上传到共享文件系统，通常是HDFS</li>
<li>客户端提交作业给JobTracker，并告诉JobTracker作业信息的位置</li>
<li>JobTracker读取作业的信息，生成一系列Map和Reduce任务，调度给拥有空闲slot的TaskTracker</li>
<li>TaskTracker根据JobTracker的指令启动Child进程执行Map任务，Map任务将从共享文件系统读取输入数据。</li>
<li>JobTracker从TaskTracker处获得Map任务进度信息</li>
<li>一旦Map任务完成之后，JobTracker会将Reduce任务分发给TaskTracker</li>
<li>TaskTracker根据JobTracker的指令启动Child进程执行Reduce作用，Reduce任务将从Map任务所在节点的本地磁盘中拉取Map的输出结果</li>
<li>JobTracker从TaskTracker处获取Reduce任务进度信息</li>
<li>当Reduce任务运行结束并将结果写入共享文件系统，则意味着整个作业执行完毕。</li>
</ol>
<h2 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h2><p><img src="/2022/03/25/%E4%BA%86%E8%A7%A3MapReduce/5.png"></p>
<p>MapReduce 工作原理如上图所示：每个Map任务从分布式文件系统(如HDFS)读取需要处理的数据，解析出键值对。然后新生成的键值对经过Shuffle传给Reduce任务，键相同的键值对都会发送给相同的Reduce任务，Reduce任务针对这些键值对进行计算后将结果以新的键值形式写入到分布式文件系统当中。</p>
<p>在这个过程中主要有Map、Shuffle、Reduce三个阶段，如果加上输入和输出就是五个阶段</p>
<h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><p>通常一个Map任务需要开启多个MapTask，那么我们可不就可以把存放在HDFS中的每一个Block”投喂”给一个Map任务呢？听起来这是可行的，但实际上，由于一个键值对极有可能被分在两个物理块当中，因此采用这种方法会导致Map任务读到的输入数据不完整</p>
<p>因此，MapReduce有自己的输入格式，它提供了数据逻辑划分和键值对解析功能。我们从下图可以看出，文件实际上会被划分为一个个Split(分片)，每个分片会包含一些meta data(如数据长度、起始位置、所在节点)，而且Split不会存在夸块的键值对(因为重新划分了)。因此，Map任务读取的单位是分片而不是文件块, 而且分片的数量决定了map任务的个数。</p>
<p><img src="/2022/03/25/%E4%BA%86%E8%A7%A3MapReduce/6.png"></p>
<p>简单来说，输入格式通过定义分片和键值对当解析的方法完成了从<strong>物理存储(HDFS物理文件块)</strong>到<strong>MapReduce程序可处理的逻辑数据</strong>之间的映射。</p>
<h3 id="Map阶段"><a href="#Map阶段" class="headerlink" title="Map阶段"></a>Map阶段</h3><p><strong>第一步</strong>：对于一个键值对来说，Map的过程就是将其转换为一个或者多个键值对，即$[K_1,V_1]\longrightarrow \text{List}([K_2,V_2])$ . 例如，在词频统计的例子中，$[行偏移量，An,An]$ 或转变为 $[An,1],[An,1]$ </p>
<p><strong>第二步</strong>：接着会根据partition方式，确定$[K_2,V_2]$键值对所属的分区，而总的分区数目等于Reduce任务的数量。得到$[K_2,V_2,\text{partition ID}]$ ,并将其放入缓冲区。</p>
<p><strong>第三步</strong>：当缓冲区的数据到达阈值(如80%), Map任务将会锁定当前阈值以内的缓冲区，现将缓冲区内的$[K_2,V_2,\text{partition ID}]$ 按照partition ID进行排序，再将同一个分区内的键值对按照键进行排序。这样做的目的是让发往同一个Reduce任务的键值对聚集在一起，且拥有相同键的键值对也聚集在一起。然后，Map任务将排序后的结果写到磁盘形成文件。排序和写入的过程并不阻塞新的键值对写入缓冲区，因此同一Map任务一般会产生多个磁盘文件，属于同一分区的键值对也会分布在多个磁盘文件当中。</p>
<p>值得指出的是，入股哦此时定义了combine方法，那么此时就会对相同的键值对进行Combine.例如WordCount示例中的合并操作将两个相同的键值对$[An,1],[An,1]$ ，合并得到 $[An,2]$ </p>
<p><strong>第四步</strong>：随着磁盘中溢写到达阈值，Map任务会进一步归并形成一个文件，使得数以一个分区的所有键值对连续存储。这本质上是一个多路归并排序的过程，最终会形成一个连续的大文件形式存储在磁盘中。</p>
<p>问：为什么要把结果写入到磁盘中去呢？为了<strong>容错</strong>，但为了保证容错牺牲了一定的性能。</p>
<p><img src="/2022/03/25/%E4%BA%86%E8%A7%A3MapReduce/7.png"></p>
<h3 id="Shuffle阶段"><a href="#Shuffle阶段" class="headerlink" title="Shuffle阶段"></a>Shuffle阶段</h3><p>现在，Map阶段已经将键值对分为很多分区，但是有很多Map任务，因此在不同的DataNode上会存在相同名字的分区。SHuffle过程就是将这些位于不同DataNode但是分区相同的键值对发送给<strong>同一个Reduce任务</strong>。也就是将$List([K_2,V_2])\longrightarrow [K_2,List(V_2)]$ 。仍用$[An,1],[An,1]$ 为例，Shuffle的过程就是将两个键值对发送给同一个Reduce任务并形成 $[An,{1,1}]$ 。</p>
<p>Map任务完成后，会通知所在节点的TaskTracker，并将Map输出文件所在的位置告知TaskTracker，之后，TaskTracker会进一步通知JobTracker。当系统中的Map任务完成率到达设定阈值的时候，系统就会启动Reduce任务。比如说，当阈值设定为60%的话，100个Map任务只需要完成60个，就可以qidongReduce任务，而不必等到100个Map任务全部完成</p>
<p>所以说，Shuffle阶段是一个中间阶段。</p>
<p><img src="/2022/03/25/%E4%BA%86%E8%A7%A3MapReduce/8.png"></p>
<h3 id="Reduce-阶段"><a href="#Reduce-阶段" class="headerlink" title="Reduce 阶段"></a>Reduce 阶段</h3><p>Reduce阶段启动之后，会定期想JobTracker获取各个节点已经完成的Map任务的输出位置。要记住，<strong>Reduce任务虽然不会等到所有Map任务执行结束才拉取Map任务的输出结果，但是拉取的数据必然来自于已经完成运行的Map任务</strong>。</p>
<p>对于一键值对来说，Reduce过程是将其转换为一个或者多个键值对。即 $[K_2,\text{List}(V_2)]$键值转换为 $[K_3,V_3]$ . 例如，在词频统计的例子中，Reduce过程对$[An,{1,1,1,1,1}]$进行求和操作转变为 $[An,5]$ </p>
<p>Reduce和Map阶段非常类似，Reduce任务从磁盘拉取的任务也会先放入<strong>内存缓冲区</strong> ，其中存储的是$[K_2,V_2]$的键值对，当内存缓存区中的数据占用空间到达阈值之后，Reduce任务将对缓存区中的键值按键进行归并排序。如果此时用户定义了conbine方法，则此时Reduce任务先执行合并操作然后将内存缓存区中的数据溢写到磁盘文件。注意，此时Reduce任务会继续拉取数据，因此随着数据的拉取将不断形成多个存在磁盘上的溢写文件</p>
<p>因为存在多个溢写文件，因此拥有相同键的键值对可能分在在不同文件中，因而这些文件需要再次归并。注意，并不是将所有溢写文件一次性归并，而是在达到一定数量的时候就执行归并操作。与Map端不同的是，最后一次归并操作的结果无需写入磁盘，而是直接可以执行reduce方法处理，这样可以省去一次磁盘读写操作。</p>
<p><img src="/2022/03/25/%E4%BA%86%E8%A7%A3MapReduce/9.png"></p>
<p>Reduce的结果会写入到用户指定的存储系统中，通常是HDFS</p>
<h3 id="数据输出"><a href="#数据输出" class="headerlink" title="数据输出"></a>数据输出</h3><p>事实上，每个Reduce 任务都会输出一个文件。也就是说，MapReduce的输出结果是一组文件而不是一个文件。</p>
<h2 id="容错"><a href="#容错" class="headerlink" title="容错"></a>容错</h2><h3 id="JobTracker故障"><a href="#JobTracker故障" class="headerlink" title="JobTracker故障"></a>JobTracker故障</h3><p>JobTracker是MapReduce框架的”首脑“，一旦它发生故障，那么正在运行的所有作业内部状态信息将会丢失。因此，重新启动JobTracker之后，所有作业需要重新执行。</p>
<p>从而，MapReduce中JobTracker的单节点故障瓶颈是该架构设计的缺陷。</p>
<h3 id="TaskTracker故障"><a href="#TaskTracker故障" class="headerlink" title="TaskTracker故障"></a>TaskTracker故障</h3><p>当TaskTracker进程故障时，JobTracker将接收不到TaskTracker发来的心跳。因此，在该TaskTracker所在节点运行过的任务都会被标记为失败。JobTracker会将这些任务调度到别的TaskTracker所在节点重新执行。这个过程对用户来说是<strong>透明的</strong>，只会感觉到该作业执行变慢了而已</p>
<h3 id="Task故障"><a href="#Task故障" class="headerlink" title="Task故障"></a>Task故障</h3><p>Task进程故障比较常见，因其错误的原因通常有代码错误、磁盘损坏等。当TaskTracker检测到一个任务故障时，TaskTracker将在下一次心跳里JobTracker报告该故障，JobTracker收到报告的消息后会重新调度该任务。</p>
<p>重新调动的额任务可以再集群的任一节点重试，但是当一个任务经过最大尝试数的尝试运行后仍然失败，那么整个作业将被标记为失败。</p>
<p>对于Map和Reduce任务来说，重试的过程有所不同：</p>
<ul>
<li>对于重试的Map任务来说，将从输入路径重新读取数据。</li>
<li>对于重试的Reduce任务来说，将重新拉取Map端的输出文件，如果读取不到该文件，那么相应的Map任务也需要重新执行。因为Reduce只会读取Map任务结束后输出的文件，因此只要磁盘不损坏，Reduce都可以重新读取。</li>
</ul>
<h2 id="编程实例"><a href="#编程实例" class="headerlink" title="编程实例"></a>编程实例</h2><h3 id="代码框架"><a href="#代码框架" class="headerlink" title="代码框架"></a>代码框架</h3><h4 id="map方法的框架"><a href="#map方法的框架" class="headerlink" title="map方法的框架"></a>map方法的框架</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.edu.ecnu.mapreduce.example.java.wordcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 步骤1：确定输入键值对[K1,V1]和 输出键值对 [K2,V2]的数据类型 */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">K1</span>数据类型, <span class="title">V1</span>数据类型, <span class="title">K2</span>数据类型, <span class="title">V2</span>数据类型&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(K1数据类型 key, V1数据类型 value, Context context)</span></span></span><br><span class="line"><span class="function">            <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">/* 步骤2：编写处理逻辑将[K1,V1]转换为[K2,V2]并输出 */</span></span><br><span class="line">        <span class="comment">//.....逻辑.....</span></span><br><span class="line">            context.write(K2, V2);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="reduce方法的框架"><a href="#reduce方法的框架" class="headerlink" title="reduce方法的框架"></a>reduce方法的框架</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.edu.ecnu.mapreduce.example.java.wordcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 步骤1：确定输入键值对[K2,List(V2)] 和 输出键值对[K3,V3]的数据类型 */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">K2</span>数据类型, <span class="title">V2</span>数据类型, <span class="title">K3</span>数据类型, <span class="title">V3</span>数据类型&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(K2数据类型 key, Iterable&lt;V2数据类型&gt; values, Context context)</span></span></span><br><span class="line"><span class="function">            <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">/* 步骤2：编写处理逻辑将[K2,List(V2)]转换为[K3,V3]并输出 */</span></span><br><span class="line">        <span class="comment">//....逻辑.....</span></span><br><span class="line">        <span class="comment">// 输出计数结果</span></span><br><span class="line">        context.write(K3, V3);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="主方法框架"><a href="#主方法框架" class="headerlink" title="主方法框架"></a>主方法框架</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">/* 步骤1：设置作业的信息 */</span></span><br><span class="line">        Job job = Job.getInstance(getConf(), getClass().getSimpleName());</span><br><span class="line">        <span class="comment">// 设置程序的类名</span></span><br><span class="line">        job.setJarByClass(getClass());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置数据的输入输出路径</span></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置map和reduce方法</span></span><br><span class="line">        job.setMapperClass(自定义Map类);</span><br><span class="line">        job.setReducerClass(自定义Reduce类);</span><br><span class="line">        job.setCombinerClass(自定义Combine类);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置map方法的输出键值对数据类型 </span></span><br><span class="line">      	<span class="comment">// 如果不设置的话，默认为 TextInputFormat 和 TextOutputFormat</span></span><br><span class="line">        job.setMapOutputKeyClass(map方法的输出键数据类型);</span><br><span class="line">        job.setMapOutputValueClass(map方法的输出值数据类型);</span><br><span class="line">      </span><br><span class="line">        <span class="comment">// 设置reduce方法的输出键值对数据类型</span></span><br><span class="line">        job.setOutputKeyClass(reduce方法的输出键数据类型);</span><br><span class="line">        job.setOutputValueClass(reduce方法的输出值数据类型);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">/* 步骤2：运行作业 */</span></span><br><span class="line">        <span class="keyword">int</span> exitCode = ToolRunner.run(<span class="keyword">new</span> WordCount(), args);</span><br><span class="line">        System.exit(exitCode);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="词频统计"><a href="#词频统计" class="headerlink" title="词频统计"></a>词频统计</h3><p>词频统计是一个很简单但很经典的MapReduce的应用案例。</p>
<p>输入：是一个包含大量单词的文本文件</p>
<p>输出：文件中每个单词及其出现次数(频数)，每个单词和其频数占一行，单词和频数之间有间隔。</p>
<p>如下所示：</p>
<p><img src="/2022/03/25/%E4%BA%86%E8%A7%A3MapReduce/10.png"></p>
<p>总体来说，词频统计的流程如下：</p>
<p>Map过程：会把文本的每行内容转换为键值对：[单词,1]</p>
<p>Reduce过程：</p>
<ul>
<li>单词相同的键值对会被发送到同一个Reduce中(shuffle)</li>
<li>对单词相同的键值对进行计数</li>
<li>输出计数后的结果[单词, 频数]</li>
</ul>
<p><img src="/2022/03/25/%E4%BA%86%E8%A7%A3MapReduce/11.png"></p>
<p>具体的过程如下:</p>
<ol>
<li>每个Map任务通过map方法将 输入键值对$[\text{行偏移量,每行内容}]$ 转换为输出键值对 $[\text{单词,频数}]$ 。比如将$[\text{offset},An,An]$ 转换为$[An,1]$</li>
<li>Map任务在缓冲区占用量达到一定阈值的时候(这里阈值为2)将键值对溢写到本地磁盘文件中。并在溢写的过程中执行归并操作。例如，把三个$[An,1]$转换为 $[An,{1,1,1}]$</li>
<li>之后，Map任务将归并产生的键值对经过Shuffle过程发送给Reduce任务。</li>
<li>在Reduce阶段，Reduce任务会进行归并操作。例如将$[My,1],[My,1],[My,{1,1,1}]$ 转换为 $[My,{1,1,1,1,1}]$</li>
<li>在归并操作完成之后，Reduce任务会通过Reduce方法将归并产生的键值对$[\text{单词,List(频数)}]$ 转换为$[\text{单词,频数}]$ 并输出。例如：将$[My,{1,1,1,1,1}]$ 转化为 $[My,5]$</li>
</ol>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><ul>
<li>WordCountMapper</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function">            <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">/* 步骤2：编写处理逻辑将[K1,V1]转换为[K2,V2]并输出 */</span></span><br><span class="line">        <span class="comment">// 以空格作为分隔符拆分成单词</span></span><br><span class="line">        String[] datas = value.toString().split(<span class="string">" "</span>);</span><br><span class="line">        <span class="keyword">for</span> (String data : datas) &#123;</span><br><span class="line">            <span class="comment">// 输出分词结果</span></span><br><span class="line">            context.write(<span class="keyword">new</span> Text(data), <span class="keyword">new</span> IntWritable(<span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>WordCountReducer</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span></span></span><br><span class="line"><span class="function">            <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">/* 步骤2：编写处理逻辑将[K2,List(V2)]转换为[K3,V3]并输出 */</span></span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 遍历累加求和</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 输出计数结果</span></span><br><span class="line">        context.write(key, <span class="keyword">new</span> IntWritable(sum));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>WordCount 主方法</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">/* 步骤1：设置作业的信息 */</span></span><br><span class="line">        Job job = Job.getInstance(getConf(), getClass().getSimpleName());</span><br><span class="line">        <span class="comment">// 设置程序的类名</span></span><br><span class="line">        job.setJarByClass(getClass());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置数据的输入输出路径</span></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置map和reduce方法</span></span><br><span class="line">        job.setMapperClass(WordCountMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(WordCountReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setCombinerClass(WordCountReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置map方法的输出键值对数据类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="comment">// 设置reduce方法的输出键值对数据类型</span></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">/* 步骤2：运行作业 */</span></span><br><span class="line">        <span class="keyword">int</span> exitCode = ToolRunner.run(<span class="keyword">new</span> WordCount(), args);</span><br><span class="line">        System.exit(exitCode);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="关系表自然连接和优化"><a href="#关系表自然连接和优化" class="headerlink" title="关系表自然连接和优化"></a>关系表自然连接和优化</h3><h3 id="网页链接排名"><a href="#网页链接排名" class="headerlink" title="网页链接排名"></a>网页链接排名</h3><h3 id="K-Means聚类"><a href="#K-Means聚类" class="headerlink" title="K-Means聚类"></a>K-Means聚类</h3><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><h3 id="MapReduce中的Merge和Combine有什么区别"><a href="#MapReduce中的Merge和Combine有什么区别" class="headerlink" title="MapReduce中的Merge和Combine有什么区别"></a>MapReduce中的Merge和Combine有什么区别</h3><p>Combine函数需要程序员显示定义，如果定义了combine方法，那么</p>
<p>而Merge和程序员是没有关系的</p>
<h3 id="MapReduce中的分布式缓存机制有什么用"><a href="#MapReduce中的分布式缓存机制有什么用" class="headerlink" title="MapReduce中的分布式缓存机制有什么用"></a>MapReduce中的分布式缓存机制有什么用</h3>
          
        
      
    </div>
    
    
    
    <div>
      
    </div>
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2022/03/23/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F-%E6%9F%A5%E8%AF%A2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/%5Bobject%20Object%5D">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/03/23/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F-%E6%9F%A5%E8%AF%A2/" itemprop="url">高级数据库系统-查询</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-03-23T10:45:50+08:00">
                2022-03-23
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2022-06-25T14:21:36+08:00">
                2022-06-25
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h1><h2 id="执行引擎"><a href="#执行引擎" class="headerlink" title="执行引擎"></a>执行引擎</h2><p>数据库查询可以看做是对数据集合做运算，运算的基本单位是算子。比如投影、扫描、选择、连接、排序等</p>
<p>在<a href="https://jasonxqh.github.io/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/">关系数据库及其基本实现原理</a> 这篇博客中，我们初步了解了几种算子的功能以及如何实现的。</p>
<p>现在我们来介绍一下在执行查询的流程：</p>
<ol>
<li>首先SQL语言会被解析，并得到好几种不同的查询方案(plan)。 SQL$\rightarrow$ Plans 的过程被称为 Interpretation</li>
<li>然后引擎会找出最佳的哪个执行方案。Plans$\rightarrow$ Best Plan 的过程被称为Query  Optimization</li>
<li>最后引擎会执行这个方案，并返回结果。Best Plans $\rightarrow$ Results 的过程为成为Query Execution</li>
</ol>
<p>过程如下：</p>
<p><img src="/2022/03/23/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F-%E6%9F%A5%E8%AF%A2/2.png"></p>
<ul>
<li>从逻辑上讲：<ul>
<li>查询 $\rightarrow$ 语法树 $\rightarrow$ 逻辑优化$\rightarrow$ 物理优化 $\rightarrow$ 查询执行</li>
<li>逻辑优化是关系代数的等价变换</li>
<li>物理优化是访问路径的选择，算子执行路径的选择</li>
</ul>
</li>
<li>在实现中<ul>
<li>很可能发生耦合</li>
<li>查询 $\rightarrow$ 语法树和数据结构  $\rightarrow$ 逻辑优化/物理优化耦合 $\rightarrow$ 执行</li>
</ul>
</li>
</ul>
<p>下图是PostgreSQL中查询引擎的组成部分：</p>
<p><img src="/2022/03/23/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F-%E6%9F%A5%E8%AF%A2/1.png"></p>
<p>引擎主要可以分为几个组件：</p>
<h4 id="parser"><a href="#parser" class="headerlink" title="parser"></a>parser</h4><p>首先是parser，它进行的是编译过程，对SQL进行词法和语法分析 </p>
<p>SQL被编译过程中会形成这样一棵 Parse Tree(语法树)</p>
<p><img src="/2022/03/23/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F-%E6%9F%A5%E8%AF%A2/3.png"></p>
<h4 id="Analyzer"><a href="#Analyzer" class="headerlink" title="Analyzer"></a>Analyzer</h4><p>然后，Analyzer会对语法树做语义检查，一共是检查这几个方面：</p>
<ol>
<li>投影列是否存在与对应的关系中</li>
<li>属性是否明确？是否有歧义？是否存在？</li>
<li>类型检查(int ,double等)</li>
</ol>
<h4 id="Rewriter"><a href="#Rewriter" class="headerlink" title="Rewriter"></a>Rewriter</h4><p>Parse Tree经过 analyzer之后就变成了Query Tree, 在这里面有一个视图表 (蓝色的RTE_VIEW)。Rewriter的作用是展开视图表</p>
<p><img src="/2022/03/23/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F-%E6%9F%A5%E8%AF%A2/4.png"></p>
<h4 id="Planner"><a href="#Planner" class="headerlink" title="Planner"></a>Planner</h4><p>Planner是对上面的完整的Query Tree进行处理，形成查询计划</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> tb1_a <span class="keyword">WHERE</span> <span class="keyword">id</span> &lt; <span class="number">300</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="keyword">data</span>;</span><br></pre></td></tr></table></figure>
<p>比如说对上面这句SQL语言，会生成如下一个plan tree</p>
<p><img src="/2022/03/23/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F-%E6%9F%A5%E8%AF%A2/5.png"></p>
<p>在这个过程之后会进行逻辑优化和物理优化，这部分我们放到最后一节去说。</p>
<p>我们可以使用EXPLAIN功能，输出物理计划：</p>
<p><img src="/2022/03/23/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F-%E6%9F%A5%E8%AF%A2/6.png"></p>
<h4 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a>Executor</h4><p>最后交给执行器去执行，接下来我们就要学习查询执行过程中的火山模型</p>
<h3 id="查询引擎-火山模型"><a href="#查询引擎-火山模型" class="headerlink" title="查询引擎-火山模型"></a>查询引擎-火山模型</h3><p>我们从上面看出，物理计划是一颗树形的结构，那么以此提出了火山模型来实现查询引擎的功能。火山模型是一种通用的SQL执行引擎的实现方法，因此很多数据库都会使用。</p>
<ul>
<li>操作流：从树顶依次往孩子节点要数据，直到底层算子提供数据</li>
<li>数据流：从叶子依次往上层返回数据</li>
</ul>
<p>每个数据库操作，都会使用共同的结构：</p>
<ul>
<li>Open() ： 准备资源，准备获得第一个tuple</li>
<li>Next():  一次提供一个数据</li>
<li>Close(): 释放资源</li>
</ul>
<p><img src="/2022/03/23/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F-%E6%9F%A5%E8%AF%A2/7.png" style="zoom:50%;"></p>
<p>比如说上面这个执行树，投影、选择、扫描算子都有这三个接口。当我的SQL语句想要得到一行结果的时候， 首先会去调用投影的next接口，然后投影的next会去调选择的next接口，而选择的next又会去调扫描的next接口，只有满足了条件的数据才能够向上传递。</p>
<p>总而言之，就是父亲节点去调用孩子节点的next接口，直到获得一条数据为止。比如说第3条数据符合age&lt;25的条件，那么，<strong>选择算子</strong>会循环调用3次扫描算子的next接口，才会拿到一条符合条件的数据，并向投影算子传递</p>
<p><img src="/2022/03/23/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F-%E6%9F%A5%E8%AF%A2/8.png" style="zoom:50%;"></p>
<p>比如说我们要实现一个排序算子，我们可以这样来实现</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sqrt()&#123;</span><br><span class="line">	if(第一次)&#123; &#x2F;&#x2F; 这里需要判断是否为第一次执行，第一次执行需要将孩子节点的信息全部调用上来，后面则不用</span><br><span class="line">	  while(true)&#123;</span><br><span class="line">	    child.next</span><br><span class="line">	  &#125;</span><br><span class="line">	  store sort</span><br><span class="line">	  返回信息</span><br><span class="line">	&#125;else&#123;</span><br><span class="line">		store sort</span><br><span class="line">	 	返回信息</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="阻塞算子"><a href="#阻塞算子" class="headerlink" title="阻塞算子"></a>阻塞算子</h4><p>其实，上面的sort就是一种阻塞算子，我们也可以看到sort和扫描是完全两种不同的算子——sort需要拿到所有的数据，而扫描不需要。</p>
<p>因此，我们称 要把数据全部拿到再执行的算子称为<strong>阻塞算子</strong></p>
<p>常见的阻塞算子是：构建哈希表(哈希连接)和排序(合并连接)</p>
<h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4><p>我们可以首先来对比批处理执行和流水线执行</p>
<p>如果是用批处理的话，我们可以写几个for循环，然后几行代码就可以搞定。虽然效率很高，但这样的可扩展性极差。</p>
<p>但是使用流水线来执行的话，虽然效率很低，但是可扩展。</p>
<p>因此我们可以做出预测：</p>
<p>火山模型的优点：</p>
<ol>
<li>实现简单易扩展</li>
<li>节省内存资源</li>
</ol>
<p>缺点：</p>
<ol>
<li>冗余的流控指令</li>
<li>效率低：虚函数嵌套，CPU的分支预测不友好</li>
</ol>
<h4 id="流水线在内存数据库中的优化"><a href="#流水线在内存数据库中的优化" class="headerlink" title="流水线在内存数据库中的优化"></a>流水线在内存数据库中的优化</h4><p>这实际上是一个并行的概念，也就是一次next并不是只取一条数据，而是取一批数据：</p>
<p><img src="/2022/03/23/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F-%E6%9F%A5%E8%AF%A2/9.png" style="zoom: 67%;"></p>
<h2 id="Join-算子实现"><a href="#Join-算子实现" class="headerlink" title="Join 算子实现"></a>Join 算子实现</h2><p>这里我们主要学习连接算子join的实现过程。两张表的Join的效果如下</p>
<p><img src="/2022/03/23/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F-%E6%9F%A5%E8%AF%A2/10.png"></p>
<h3 id="连接算法"><a href="#连接算法" class="headerlink" title="连接算法"></a>连接算法</h3><p>连接算法一共有三种：Nest Loop Join、 Hash   Join、 Merge Join. 目的就是将相同属性的pair给找出来</p>
<h4 id="Nest-Loop-Join"><a href="#Nest-Loop-Join" class="headerlink" title="Nest Loop Join"></a>Nest Loop Join</h4><p>Nest Loop Join就是对 R 和 S 进行双循环匹配。如果 $R\bowtie S$  的话，R为内表，S为外表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Function : nljoin (R,S,p)</span><br><span class="line">&#x2F;* outer relation S *&#x2F;</span><br><span class="line">foreach record s in S do</span><br><span class="line">		&#x2F;* inner relation R *&#x2F;</span><br><span class="line">		foreach record r in R do</span><br><span class="line">			&#x2F;* &lt;s,r&gt; denotes record concatenation *&#x2F;</span><br><span class="line">        if &lt;s,r&gt; satisfies p then</span><br><span class="line">            append &lt;s,rs&gt; to result</span><br></pre></td></tr></table></figure>
<h4 id="Hash-Join"><a href="#Hash-Join" class="headerlink" title="Hash Join"></a>Hash Join</h4><p>如果采用Hash Join方法，可以对内表(这里是R表)构建哈希表，然后用S表去做哈希探测</p>
<p><img src="/2022/03/23/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F-%E6%9F%A5%E8%AF%A2/11.png" style="zoom: 50%;"></p>
<h4 id="Merge-Join"><a href="#Merge-Join" class="headerlink" title="Merge Join"></a>Merge Join</h4><p>如果采用Merge Join,需要分别对 R 和 S 进行排序，然后用归并算法得到Join结果</p>
<p><img src="/2022/03/23/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F-%E6%9F%A5%E8%AF%A2/12.png" style="zoom: 50%;"></p>
<h3 id="连接算法结合火山模型"><a href="#连接算法结合火山模型" class="headerlink" title="连接算法结合火山模型"></a>连接算法结合火山模型</h3><p>现在我们来看三种算法在火山模型下该如何实现。</p>
<h4 id="Nest-Loop的火山模型"><a href="#Nest-Loop的火山模型" class="headerlink" title="Nest Loop的火山模型"></a>Nest Loop的火山模型</h4><p>我们可以简单画出 Nest Loop Join的查询计划表</p>
<p><img src="/2022/03/23/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F-%E6%9F%A5%E8%AF%A2/15.png" style="zoom: 50%;"></p>
<p>我们看到，由于Join算子需要对两张表进行连接，那么他一定是由两个孩子算子的</p>
<p>事实上，在数据库系统中，有三种不同类型的算子：</p>
<ul>
<li>没孩子的，比如Scan算子</li>
<li>单孩子的，比如Sort算子、投影算子</li>
<li>双孩子的，比如Join算子</li>
</ul>
<p>首先我们列出比较简单的open接口和close接口</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Function: open()</span><br><span class="line">R.open();</span><br><span class="line">S.open();</span><br><span class="line">r &lt;- R.next;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Function: close()</span><br><span class="line">R.close()</span><br><span class="line">S.close()</span><br></pre></td></tr></table></figure>
<p>接下来给出next()接口的伪代码，本质上就是两重扫描：伪代码也非常直接</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">While (r !&#x3D; &lt;EOF&gt; )do</span><br><span class="line">    while ((s&lt;- S.next())!&#x3D;&lt;EOP&gt;) do</span><br><span class="line">        if p(r,s) then</span><br><span class="line">          &#x2F;*向上传递连接后的结果*&#x2F;</span><br><span class="line">          return &lt;r,s&gt;</span><br><span class="line">        &#x2F;*一遍循环完了，现在重置内表，继续循环*&#x2F;</span><br><span class="line">        S.close();</span><br><span class="line">        S.open();</span><br><span class="line">        r &lt;- R.next();</span><br><span class="line">return &lt;EOF&gt;;</span><br></pre></td></tr></table></figure>
<h4 id="Hash-Join的火山模型"><a href="#Hash-Join的火山模型" class="headerlink" title="Hash Join的火山模型"></a>Hash Join的火山模型</h4><p>Hash Join 用到了多个算子：Hash算子用来创建哈希表，Hash Join算子用来探索哈希表</p>
<p><img src="/2022/03/23/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F-%E6%9F%A5%E8%AF%A2/14.png" style="zoom: 50%;"></p>
<p>Hash  Join的火山模型有两种实现方式：</p>
<ul>
<li>写法1</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">open()&#123;</span><br><span class="line">		R.open();</span><br><span class="line">		while((r&#x3D;R.next())!&#x3D;EOF)</span><br><span class="line">				将 r 加入哈希表h (内表构造哈表h)</span><br><span class="line">		S.open();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Next()&#123;</span><br><span class="line">		while()&#123;</span><br><span class="line">				s &#x3D; S.next();</span><br><span class="line">				用s探索哈希表h；</span><br><span class="line">				if(找到一个匹配的&lt;r,s&gt;)</span><br><span class="line">						return &lt;r,s&gt;</span><br><span class="line">		&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在这中写法中，open接口其实是做一个准备工作，也就是将内表打开并将其扫入内存、构建哈希表了。这样在next接口中只需要做扫描即可。实际应用中这种方式更加常见</p>
<ul>
<li>写法2</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">open()&#123;</span><br><span class="line">		R.open();</span><br><span class="line">		r &#x3D; R.next();</span><br><span class="line">		S.open();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">next()&#123;</span><br><span class="line">		if(r是R的第一个元组)&#123;</span><br><span class="line">				将r加入哈希表h</span><br><span class="line">				while((r&#x3D;R.next())!&#x3D;EOF)</span><br><span class="line">						将r加入哈希表h</span><br><span class="line">		&#125;</span><br><span class="line">		while()&#123;</span><br><span class="line">				s &#x3D; S.next();</span><br><span class="line">				用s探索哈希表h；</span><br><span class="line">				if(找到一个匹配的&lt;r,s&gt;)</span><br><span class="line">						return &lt;r,s&gt;</span><br><span class="line">		&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果采用这种写法，需要在next的时候多加一层判断，实际上开销是差不多的。</p>
<h4 id="Merge-Join的火山模型"><a href="#Merge-Join的火山模型" class="headerlink" title="Merge Join的火山模型"></a>Merge Join的火山模型</h4><p>Merge Join 用到了多个算子：Sort、Merge Join 等，下面是其查询计划表：</p>
<p><img src="/2022/03/23/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F-%E6%9F%A5%E8%AF%A2/13.png" style="zoom: 50%;"></p>
<p>Merge Join 下面有两个Sort算子，负责对两张表的数据进行排序。Sort算子在之前我们说过会维护一个缓冲区，用一个while循环不断地向Scan算子要数据，然后在缓冲区中进行排序。</p>
<p>两个Sort都排序完成后，进行Merge Join算子的操作，也就是会从两个Sort算子各取一条数据，比较它们是否相等，如果相等说明满足条件，return，否则就继续取下面两条数据。</p>
<p>Join算子并不是阻塞算子，Sort算子才是阻塞算子</p>
<h3 id="基于块数据的连接算法"><a href="#基于块数据的连接算法" class="headerlink" title="基于块数据的连接算法"></a>基于块数据的连接算法</h3><p>现在很多数据库的底层是LSM Tree，在那种数据库环境下谈基于块数据的算法是没有意义的，这里我们讨论的是传统数据库中的基于块数据的连接方法</p>
<p>由于在传统数据库中，数据库以Block/Page为基本存储单位，因此缓冲区可能会存在内存不足的问题，导致无法将数据全部加载到内存中进行计算。</p>
<p>假设内存有B个Block用于Join，其中一个Block用于缓存Join的结果(在火山模型中并不需要缓存所有结果)。基于上面这种情况，我们再来讨论三种算法的实现</p>
<h4 id="Nest-Loop-Join-1"><a href="#Nest-Loop-Join-1" class="headerlink" title="Nest Loop Join"></a>Nest Loop Join</h4><p>首先我们来回顾一下Nest Loop Join算法：对于 $S\bowtie R$,  $S$ 为内表，$R$ 为外表。那么需要做一个双重循环，将$<R,S>$ 一一比较后得出结果</R,S></p>
<p>当块和Next Loop Join结合起来会变成什么样？</p>
<p>我们假定 R 和 S 的blocks数量分别为$N_R$ 和 $N_S$ ，那么对于外表来说，其每一个块都需要放到内存里面一遍，对于内表来说，外表的元素每改变一次，就要将所有的块放入内存中访问一遍，因此总的块访问次数为 $N_R+|R| \cdot N_S$ </p>
<h5 id="Index-Nest-Loop-join"><a href="#Index-Nest-Loop-join" class="headerlink" title="Index Nest Loop join"></a>Index Nest Loop join</h5><p>实际上我们可以对Nest Loop做一定的优化：改进成了 Indexed Nest Loop Join</p>
<p>对R表的每个数据，直接对S表做Index Scan，以此减少磁盘访问次数和访问数据量。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Function : index_nljoin(R,S,p)</span><br><span class="line">foreach record r in R do</span><br><span class="line">		scan S-index using (key value in) r </span><br><span class="line">		and concatenate r with all match tuples s;</span><br><span class="line">		</span><br><span class="line">		appand &lt;r,s&gt; to result;</span><br></pre></td></tr></table></figure>
<p>前提条件是，在S表上的那一列需要对其构建索引。然后，对于R表中的每个元素，对S表做索引的查询，这样复杂度就可以降下来了。对块的访问次数也没有之前多——不用再扫描全表的块$N_s$了</p>
<h5 id="Block-Nest-Loop-Join"><a href="#Block-Nest-Loop-Join" class="headerlink" title="Block Nest Loop Join"></a>Block Nest Loop Join</h5><p>我们可以再进行一次优化来减少磁盘的访问次数</p>
<p>假设缓冲区中 $b_r$ 和 $b_s$ 个块用于缓存R和S的数据，$b_r+b_s=B-1$(B为内存可分配的块数量), 剩下1个block是用来存放输出的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Function: block_nljoin (R, S, p)</span><br><span class="line">foreach b_r-sized block in R do</span><br><span class="line">		build an in-memory hash table H for the current R-block;</span><br><span class="line">		foreach b_s-sized block in S do</span><br><span class="line">				foreach record s in current S-block do</span><br><span class="line">						probe H and append matching (r, s) tuples to result</span><br></pre></td></tr></table></figure>
<p>也就是说，一次读入 $b_r,b_s$ 个块读入内存，对其进行比较、连接。</p>
<p>因此，总的磁盘访问次数为： $\lceil N_R/b_r\rceil\cdot\lceil N_S/b_s\rceil $ (默认块是连续的) </p>
<p>访问block数量为： $N_R\cdot N_S$</p>
<p>通常来说，我们会取 $b_r = B-2,b_s = 1$ ，内存中建立哈希表优化匹配 .(如果R表可以一次性全部放进内存，事实上和哈希连接比较相近)</p>
<p><img src="/2022/03/23/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F-%E6%9F%A5%E8%AF%A2/16.png"></p>
<h4 id="Block-Hash-Join"><a href="#Block-Hash-Join" class="headerlink" title="Block Hash Join"></a>Block Hash Join</h4><p>如果R表可以完全放入内存，那么R和S只访问一遍，和上面我们说的Nest Loop算法相近</p>
<p>那么如果R表无法完全放入内存的话，就需要<strong>对R表进行Block Hash 算法</strong>了：</p>
<p>我们可以建 B-1 个桶， 每个桶写满一个Block就刷盘，一个桶可能占多个Block，如下所示：</p>
<p><img src="/2022/03/23/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F-%E6%9F%A5%E8%AF%A2/17.png"></p>
<p>对S表也可以做这样一个Block Hash算法，此时，R表和S表都按照桶分配到磁盘上了。然后，对每个Partition做一趟哈希连接</p>
<p><img src="/2022/03/23/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F-%E6%9F%A5%E8%AF%A2/18.png"></p>
<p>伪代码如下：</p>
<p><img src="/2022/03/23/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F-%E6%9F%A5%E8%AF%A2/19.png"></p>
<p>当然，这种Block Hash Join算法存在一定的限制</p>
<p>如果每个Partition中R表的Block数量都少于B-1的话，那么算法需要对R、S表进行两趟访问：</p>
<ul>
<li>第一趟将他们读入内存然后进行 Partition, 再将它们刷盘</li>
<li>第二趟将每个Partition读入内存，进行Hash Join</li>
</ul>
<p>但是，如果有Partition中的R表的Block数量大于B-1的话，需要在此基础上继续进行Partition，因此需要多趟算法</p>
<p>如果按照正常的都小于B-1来说，Hash Join访问磁盘Block的次数为：$3N_R+3N_S$ 。 其中，第一趟算法读取和刷盘就包含了$2N_R+2N_S$ ，第二次读取包含了 $N_R+N_S$</p>
<h5 id="Hybrid-Hash-Join"><a href="#Hybrid-Hash-Join" class="headerlink" title="Hybrid Hash Join"></a>Hybrid Hash Join</h5><p>还可以对Block Hash Join做进一步优化，也就是对部分 Partition 做一趟算法，部分用两趟算法</p>
<ul>
<li>假设构建k个桶，对其中m个桶完全保留在内存中，其他k-m个桶只保留一个block</li>
</ul>
<p><img src="/2022/03/23/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F-%E6%9F%A5%E8%AF%A2/20.png"></p>
<h3 id="基于块的排序算法"><a href="#基于块的排序算法" class="headerlink" title="基于块的排序算法"></a>基于块的排序算法</h3><p>同样的，对于另外一个阻塞算子——排序，如果是基于块的情况下，是怎么样的？</p>
<p>首先，我们有$N_R$ 个块，但内存只有 B-1 个块，$N_R&gt;B-1$ ， 此时该如何借助内存进行排序</p>
<p>我们可以从$N_R$ 个块中每次取出 $B-1$ 个块，然后对其进行排序，排序完以后将其刷回磁盘，记为部分1；这样一直记录到部分x。</p>
<p>这样一共会得到 x个排好序的部分，如果$x<B-1 $， 那么就可以继续做多路归并排序； 如果$x>B-1$ ，那么再对一些部分做归并。</B-1></p>
<p>因此可能是两趟，也可能是多趟排序</p>
<h2 id="查询优化器"><a href="#查询优化器" class="headerlink" title="查询优化器"></a>查询优化器</h2><p>查询优化器的目标是在众多查询计划中选择一个高效的查询执行计划。</p>
<ul>
<li>在集中式数据库中，高效的指标取决于I/O代价、CPU代价和查询的内存开销。</li>
<li>在分布式数据库中，评估的总代价=I/O代价+CPU代价+通信代价</li>
</ul>
<p>之前我们说了有两种优化分类：</p>
<ul>
<li>逻辑优化主要是关系代数表达式的等价变化</li>
<li>物理优化更加细致，主要可分三点<ul>
<li>存取路径：索引、基本表</li>
<li>底层操作算子的选择：连接算子的选择、聚合算子选择等</li>
<li>多表连接顺序选择(其实也是一种关系代数表达式的变换)</li>
</ul>
</li>
</ul>
<h3 id="逻辑优化-关系表达式转换"><a href="#逻辑优化-关系表达式转换" class="headerlink" title="逻辑优化-关系表达式转换"></a>逻辑优化-关系表达式转换</h3><p>目标：通过对关系代数表达式的等价变换来提高查询效率</p>
<p>如果两个关系代数表达式在每个合法数据库实例上生成相同的元组集，则称这两个关系代数表达式是等价的</p>
<h4 id="逻辑优化案例"><a href="#逻辑优化案例" class="headerlink" title="逻辑优化案例"></a>逻辑优化案例</h4><p>下面是一个JOIN带选择的SQL语句</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> S.SN</span><br><span class="line"><span class="keyword">FROM</span> S,SC</span><br><span class="line"><span class="keyword">WHERE</span> S.S =SC.S  <span class="keyword">AND</span> SC.C =‘C2’;</span><br></pre></td></tr></table></figure>
<p>假定学生－课程数据库中有1000个学生记录，10000个选课记录，其中选修C2课程的选课记录为50个。</p>
<p><img src="/2022/03/23/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F-%E6%9F%A5%E8%AF%A2/24.png"></p>
<p>这三种方法的评估计划时间的成本差异可能很大，可能是秒级与天级的差异</p>
<p>从代价上看，第三种是最好的，也就是现在SC中做好选择，再去和S表做连接。这比前两种要快很多，几乎可以称为是一条规则了。</p>
<h4 id="常用的等价变换规则"><a href="#常用的等价变换规则" class="headerlink" title="常用的等价变换规则"></a>常用的等价变换规则</h4><ol>
<li>连接、笛卡尔积交换律</li>
</ol>
<p>设E1和E2是关系代数表达式，F 是连接运算的条件，则有</p>
<script type="math/tex; mode=display">
E_1\times E_2 ≡ E_2\times E_1\\
E_1\bowtie E_2 ≡ E_2\bowtie E_1\\
E_1\bowtie_F E_2 ≡ E_2\bowtie_F E_1</script><ol>
<li>连接、笛卡尔积的结合律，设E1，E2，E3是关系代数表达式，F1和F2是连接运算的条件，则有：</li>
</ol>
<script type="math/tex; mode=display">
(E_1\times E_2)\times E_3≡E_1\times(E_2\times E_3)\\
(E_1\bowtie E_2)\bowtie E_3 ≡ E_1\bowtie(E_2\bowtie E_3)\\

(E_1\bowtie_{F_1} E_2)\bowtie_{F_2} E_3 ≡ E_1\bowtie_{F_1}(E_2\bowtie_{F_2} E_3)\\</script><p>当然，这些只是改变表和表之间连接的顺序，可能需要放到物理优化中去，因为在逻辑优化中是看不出来哪个式子代价大的</p>
<ol>
<li>选择与笛卡尔积的交换律, 如果F中涉及的属性都是E1中的属性，则</li>
</ol>
<script type="math/tex; mode=display">
\sigma_F(E_1\times E_2)≡\sigma_F(E_1) \times E_2\\</script><p>如果$F=F_1\land F_2$ ，并且 $F_1$ 只涉及 $E_1$ 中的属性，$F_2$ 只涉及$E_2$ 中的属性，则由上面的等价变换规则可推出：</p>
<script type="math/tex; mode=display">
\sigma_F(E_1\times E_2) ≡ \sigma_{F_1}(E_1)\times \sigma_{F_2}(E_2)</script><p>若$F_1$涉及$E_1$ 中的属性，$F_2$ 涉及$E_1$和$E_2$两者的属性，则仍有下式，它使得部分选择在笛卡尔积前先做</p>
<script type="math/tex; mode=display">
\sigma_F(E_1\times E_2) ≡ \sigma_{F_2}(\sigma_{F_1}(E_1)\times E_2)</script><h4 id="实现逻辑优化—基于规则优化"><a href="#实现逻辑优化—基于规则优化" class="headerlink" title="实现逻辑优化—基于规则优化"></a>实现逻辑优化—基于规则优化</h4><p>在逻辑优化阶段，我们对查询树依次执行设置的启发式规则，如果满足，则执行规则</p>
<p>–  启发式规则。比如说，选择下推——编码时将选择操作绑定到扫描算子上</p>
<p>– 如何实现？</p>
<ul>
<li><p>通常可以调整查询树的数据结构</p>
</li>
<li><p>有些规则需要独特编码方式实现</p>
</li>
</ul>
<p>还有一些比较好用的规则，如下：</p>
<p><strong>把投影运算和其他运算结合</strong></p>
<p><strong>条件化简</strong></p>
<p>利用等式和不等式的性质，可以将WHERE、HAVING和ON条件化简</p>
<ul>
<li>常量传递：使得条件分离后有效实施“条件下推”<ul>
<li><code>Col_1 = Col_2 AND Col_2=3</code>化简为 <code>Col_1=3 AND Col2=3</code></li>
</ul>
</li>
<li>消除死码：化简条件，去除不必要的条件<ul>
<li><code>WHERE (0&gt;1 AND s1=5)</code>,条件恒假，则不必执行该SQL</li>
</ul>
</li>
<li>表达式计算：加快计算效率<ul>
<li><code>WHERE Col_1=1+2</code> 变换为 <code>WHERE Col_1=3</code></li>
</ul>
</li>
<li>不等式变换：化简条件，去除不必要的条件<ul>
<li><code>a&gt;10 AND b = 6 AND a&gt;2</code> 化简为<code>b=6 AND a&gt;10</code></li>
</ul>
</li>
<li>谓词传递闭包：加速计算，有效实施“条件下推”<ul>
<li><code>a&gt;b AND b&gt;2</code> 推导出 <code>a&gt;b AND b&gt;2 AND a&gt;2</code></li>
</ul>
</li>
</ul>
<p><strong>子查询展开</strong></p>
<p>又称子查询上拉，实质是把某些子查询重写为等价的多表连接操作</p>
<p>优势：连接方法和连接顺序选择更加灵活</p>
<p>比如说：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> t1,</span><br><span class="line">(<span class="keyword">SELECT</span> * <span class="keyword">FROM</span> t2 <span class="keyword">WHERE</span> t2.a2&gt;<span class="number">10</span>) v_t2</span><br><span class="line"><span class="keyword">WHERE</span> t1.a1&lt;<span class="number">10</span> <span class="keyword">AND</span> v_t2.a2&lt;<span class="number">20</span>;</span><br></pre></td></tr></table></figure>
<p>可以被优化为：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span></span><br><span class="line">t1,t2</span><br><span class="line"><span class="keyword">WHERE</span> t1.a1&lt;<span class="number">10</span> <span class="keyword">AND</span> t2.a2 &lt;<span class="number">20</span> <span class="keyword">AND</span> t2.a2&gt;<span class="number">10</span></span><br></pre></td></tr></table></figure>
<p>因为第一种的方法，相当于t2表给限制了一部分，那么在选择的时候就没那么灵活。但是第二种方法就可以在WHERE中更加灵活地组织条件。</p>
<p><strong>外连接消除</strong></p>
<ul>
<li>右表存在非Null条件</li>
</ul>
<p><code>SELECT * FROM T1 LEFT JOIN T2 ON T1.c1 = T2.c1 WHERE T2.c2 &gt; 0</code></p>
<p>注意这里SQL执行顺序是先做JOIN，再做选择。通常使用外连接的话可以降低失误率,但外连接的代价比非外连接要大很多。那么在做查询优化的时候就可以消除外连接，然后把选择可以优化为：</p>
<p><code>SELECT * FROM T1,T2 WHERE T1.c1 = T2.c1 and R.c2&gt;0</code></p>
<ul>
<li>满足传递性链式非NULL条件</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> T1 </span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> T2 <span class="keyword">ON</span> T1.c1 = T2.c1</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> T3 <span class="keyword">ON</span> T2.c2 = T3.c2</span><br><span class="line"><span class="keyword">WHERE</span> T3.c3 &gt; <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>可以优化为：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> T1,T2,T3</span><br><span class="line"><span class="keyword">WHERE</span> T1.c1 = T2.c1 </span><br><span class="line"><span class="keyword">AND</span> T2.c2 = T3.c2</span><br><span class="line"><span class="keyword">AND</span> T3.c3 &gt; <span class="number">0</span>;</span><br></pre></td></tr></table></figure>
<h3 id="物理优化"><a href="#物理优化" class="headerlink" title="物理优化"></a>物理优化</h3><p>逻辑优化改变查询语句中操作的次序和组合，不涉及底层的存取路径</p>
<p>对于一个查询语句的算子有很多实现方案，它们的执行效率不同</p>
<p>物理优化就是要选择高效合理的操作算子、数据访问路径和查询树结构，求得相对最优的查询计划</p>
<ul>
<li><p>基于规则的优化</p>
</li>
<li><p>基于代价的优化</p>
</li>
</ul>
<h4 id="优化案例-路径选择"><a href="#优化案例-路径选择" class="headerlink" title="优化案例-路径选择"></a>优化案例-路径选择</h4><p>在查询引擎中，选择操作通常由两种实现方法：</p>
<ol>
<li>简单的全表扫描方法</li>
</ol>
<p>对查询的基本表顺序扫描，逐一检查每个元组是否满足选择条件，把满足条件的元组作为结果输出。这是和小表操作，但并不适合大表</p>
<ol>
<li>索引(或散列)扫描方法</li>
</ol>
<p>适合条件中的属性上有索引(B+树索引/哈希索引)，通过索引先找到满足条件的元组主码或者元祖指针，再通过元祖指针直接在查询的基本表中找到元组</p>
<p>现在来看最典型的SQL查询： <code>SELECT * FROM student WHERE &lt;条件表达式&gt;</code> ，通常有这几种情况</p>
<ul>
<li>C1: 无条件</li>
<li>C2: Sno=’200215121’</li>
<li>C3: Sage&gt;20;</li>
<li>C4: Sdept=’CS’ AND Sage&gt;20</li>
</ul>
<p>如果以C2为例，Sno上如果有索引，那么就可以使用索引得到Sno为’200215121‘ 元组的指针，通过元组指针在student表中检索到该学生。如果没有索引的话，就需要全表扫描了</p>
<p>以C3为例，Sage&gt;20,并且Stage有B+树索引。那么，使用B+树索引可以找到Sage=20的索引项，以此为入口点在B+树的顺序集得到Sage&gt;20的所有元组指针。通过这些元组指针到student表中检索到所有年龄大于20的学生</p>
<p>那么查询优化器该选择哪种方式呢？全表扫描还是利用索引+回表的方式进行查询</p>
<ul>
<li>如果预先知道结果比较少，就应该使用索引+回表的方式。</li>
<li>如果最后的结果很多，甚至整张表都是，那么还是全表扫描比较快。因为找那么多索引再去点查的代价非常高</li>
</ul>
<p>这两种方法如果选的不好，可能是数十倍的差距，因此，代价评估就非常重要了。</p>
<p>现在来看看更复杂的情况：以C4为例，<code>Sdept＝‘CS’ AND Sage&gt;20</code>，如果Sdept和Sage上都有索引：</p>
<p><strong>算法一</strong>：分别用上面两种方法分别找到<code>Sdept＝‘CS’</code>的一组元组指针和<code>Sage&gt;20</code>的另一组元组指针。求这2组指针的交集，将其放到student表中检索，得到计算机系年龄大于20的学生</p>
<p><strong>算法二</strong>：找到<code>Sdept＝‘CS’</code>的一组元组指针，那么通过这些元组指针到student表中检索，然后 对得到的元组检查另一些选择条件(如<code>Sage&gt;20</code>)是否满足。最后把满足条件的元组作为结果输出。</p>
<p>在数据库中，第一种方法是不会去实现的，通常<strong>只会挑选一个属性去做索引查询</strong>。因此，问题被简化为三种情况：选择Sdept做索引查询、选择Sage做索引查询、全表扫描。<strong>最终选择那一种方法还是要看选择率的高低</strong></p>
<ul>
<li>如果表中只有一个通选学了CS，那么就应该选择Sdept作为索引</li>
<li>如果表中Sage&gt;20的同学只有一两个，那么就需要选Sage做索引查询</li>
<li>如果选择率都不是很低，那么应该做全表扫描</li>
</ul>
<h4 id="路径选择操作的启发式规则"><a href="#路径选择操作的启发式规则" class="headerlink" title="路径选择操作的启发式规则"></a>路径选择操作的启发式规则</h4><ul>
<li><p>对于小关系(&lt;1000行):</p>
<ul>
<li>使用全表顺序扫描，即使选择列上有索引</li>
</ul>
</li>
<li><p>对于大关系( 对于选择条件是主码＝值的查询)：</p>
<ul>
<li>查询结果最多是一个元组，可以选择主码索引</li>
</ul>
</li>
<li><p>对于选择条件是非主属性＝值的查询，并且选择列上有索引</p>
<ul>
<li>估算查询结果的元组数目:比例较小(&lt;10%)可以使用索引扫描方法, 否则全表顺序扫描</li>
</ul>
</li>
<li><p>对于选择条件是属性上的非等值查询或者范围查询，并且选择列上有索引</p>
<ul>
<li>估算查询结果的元组数目:比例较小(&lt;10%)可以使用索引扫描方法, 否则全表顺序扫描</li>
</ul>
</li>
</ul>
<h4 id="选择操作的启发式规则"><a href="#选择操作的启发式规则" class="headerlink" title="选择操作的启发式规则"></a>选择操作的启发式规则</h4><ul>
<li><p>对于用AND连接的多路选择条件</p>
<ul>
<li><p>如果有涉及这些属性的组合索引, 优先采用组合索引扫描方法</p>
</li>
<li><p>如果某些属性上有一般的索引, 选择选择率最低的一个索引</p>
</li>
</ul>
</li>
<li><p>对于用OR连接的析取选择条件，一般使用全表顺序扫描</p>
</li>
</ul>
<h4 id="连接操作的启发式规则"><a href="#连接操作的启发式规则" class="headerlink" title="连接操作的启发式规则"></a>连接操作的启发式规则</h4><ul>
<li><p>如果2个表都已经按照连接属性排序</p>
<ul>
<li>选用排序-合并方法</li>
</ul>
</li>
<li><p>如果前表较小，后表在连接属性上有索引</p>
<ul>
<li>选用Nest Loop索引连接方法</li>
</ul>
</li>
<li><p>如果上面2个规则都不适用，其中一个表较小</p>
<ul>
<li>选用Hash join方法或内存Nest Loop Join</li>
</ul>
</li>
<li><p>否则采用Block Nest Loop Join或Grace Hash Join</p>
</li>
</ul>
<h3 id="基于代价的优化"><a href="#基于代价的优化" class="headerlink" title="基于代价的优化"></a>基于代价的优化</h3><p>必要条件</p>
<ul>
<li>实现数据统计信息，用于帮助计算代价</li>
</ul>
<p>核心代价模型</p>
<ul>
<li><p>访问路径</p>
</li>
<li><p>多表连接顺序</p>
</li>
<li><p>执行算子(很多情况下使用规则代替)</p>
</li>
<li>连接、聚集</li>
</ul>
<p>那么统计信息哪里来？我们可以列出如下几种</p>
<h4 id="统计信息"><a href="#统计信息" class="headerlink" title="统计信息"></a>统计信息</h4><ol>
<li><p>对每个基本表</p>
<ul>
<li>该表的元组总数(N)</li>
<li>元组长度(l)</li>
<li>占用的块数(B)</li>
<li>占用的溢出块数(BO)</li>
</ul>
</li>
<li><p>对基表的每个列</p>
<ul>
<li><strong>该列不同值的个数(m)</strong>(用于估算Join结果)</li>
<li><p><strong>选择率(f)</strong></p>
<ul>
<li>如果不同值的分布是均匀的，f＝1/m</li>
<li>如果不同值的分布不均匀，则每个值的选择率＝具有该值的元组数/N</li>
</ul>
</li>
<li><p>该列最大值/最小值</p>
</li>
<li><p>该列上是否已经建立了索引</p>
</li>
<li><p>索引类型(B+树索引、Hash索引)</p>
</li>
</ul>
</li>
<li><p>对索引(如B+树索引)</p>
<ul>
<li><p>索引的层数(L)</p>
</li>
<li><p>不同索引值的个数</p>
</li>
<li><p>索引的选择基数S(有S个元组具有某个索引值)</p>
</li>
<li><p>索引的叶结点数(Y)</p>
</li>
</ul>
</li>
</ol>
<h4 id="访问路径代价模型"><a href="#访问路径代价模型" class="headerlink" title="访问路径代价模型"></a>访问路径代价模型</h4><ul>
<li><p>全表扫描算法的代价估算公式</p>
<ul>
<li><p>如果基本表大小为B块，全表扫描算法的代价cost＝B</p>
</li>
<li><p>如果选择条件是码＝值，那么平均搜索代价cost＝B/2</p>
</li>
</ul>
</li>
<li><p>索引扫描算法的代价估算公式</p>
<ul>
<li><p>如果选择条件是码＝值</p>
<ul>
<li>如［例-C2］，则采用该表的主索引<ul>
<li>若为B+树，层数为L，需要存取B+树中从根结点到叶结点L块，再加上基本表中该元组所在的那一块，所以<code>cost=L+1</code></li>
</ul>
</li>
</ul>
</li>
<li><p>如果选择条件涉及非码属性</p>
<ul>
<li>如［例-C3］，若为B+树索引，选择条件是相等比较，S是索引的选择基数(有S个元组满足条件，选择率)</li>
<li>最坏的情况下，满足条件的元组可能会保存在不同的块上，此时，cost=L+S</li>
</ul>
</li>
<li><p>如果比较条件是＞，＞＝，＜，＜＝操作</p>
<ul>
<li>假设有一半的元组满足条件就要存取一半的叶结点</li>
<li>通过索引访问一半的表存储块cost=L+B/2 </li>
<li>如果可以获得更准确的选择基数，可以进一步修正B/2</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="多表连接顺序搜索策略"><a href="#多表连接顺序搜索策略" class="headerlink" title="多表连接顺序搜索策略"></a>多表连接顺序搜索策略</h4><p>常常使用启发式+基于代价的搜索，比如说枚举、DP算法等，也可以用随机算法</p>
<p>连接顺序常常是树形结构(使用左深树),它有几个好处</p>
<ul>
<li>每个连接算子的右侧输入是一个关系（基本表），而不是连接后的中间结果</li>
<li>基于成本的优化是昂贵的，但对于大型数据集的查询是有价值的（典型的查询具有较小的n，通常小于10）</li>
</ul>
<p><img src="/2022/03/23/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F-%E6%9F%A5%E8%AF%A2/23.png"></p>
<p>左深树如上图所示，R和S做Join，然后其结果和T做Join，其结果在和U做Join</p>
<p>但是在一些比较新的数据仓库中，会让B和C分别建一个哈希表，然后用A去探索这两个哈希表，实际上用的是右深树</p>
<h3 id="几个难题"><a href="#几个难题" class="headerlink" title="几个难题"></a>几个难题</h3><p>为什么查询优化器对代价的估计可能估不准？查询优化器非常难设计。其实都集中在统计信息上。</p>
<ul>
<li>统计信息的选择自动收集与更新非常难，因为数据库在不断运行，数据在不断更新</li>
<li><p>选择条件下数据的条件分布，(满足A的条件下去选择 B),这样导致原先统计数据在这种情况下是没什么用的。而我们又无法为所有的分布创建一个直方图，因此通常导致估算不准确。</p>
</li>
<li><p>计划不准导致的查询超时无法从理论上避免</p>
</li>
</ul>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li><p>逻辑优化</p>
<ul>
<li>定义一些启发式规则</li>
</ul>
</li>
<li><p>物理优化</p>
<ul>
<li><p>先决定访问路径</p>
</li>
<li><p>启发式规则决定连接顺序与连接方式</p>
</li>
</ul>
</li>
</ul>
<p><img src="/2022/03/23/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F-%E6%9F%A5%E8%AF%A2/25.png"></p>

          
        
      
    </div>
    
    
    
    <div>
      
    </div>
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2022/03/08/AI-Searching/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/%5Bobject%20Object%5D">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/03/08/AI-Searching/" itemprop="url">AI-Searching</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-03-08T19:16:27+08:00">
                2022-03-08
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2022-04-03T15:25:34+08:00">
                2022-04-03
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Searching"><a href="#Searching" class="headerlink" title="Searching"></a>Searching</h1><p>搜索的方法有很多种，但是大体上可以分为两个大类——盲搜和有先验知识的搜索</p>
<h2 id="Uninformed-Search-Methods"><a href="#Uninformed-Search-Methods" class="headerlink" title="Uninformed Search Methods"></a>Uninformed Search Methods</h2><h3 id="Depth-First-Search"><a href="#Depth-First-Search" class="headerlink" title="Depth-First Search"></a>Depth-First Search</h3><h3 id="Breadth-First-Search"><a href="#Breadth-First-Search" class="headerlink" title="Breadth-First Search"></a>Breadth-First Search</h3><h4 id="Interative-Deepening"><a href="#Interative-Deepening" class="headerlink" title="Interative Deepening"></a>Interative Deepening</h4><h3 id="Uniform-Cost-Search"><a href="#Uniform-Cost-Search" class="headerlink" title="Uniform-Cost Search"></a>Uniform-Cost Search</h3><p>对于深搜和广搜我们都已经十分了解了，现在我们来学习另外一种搜索方式——代价一致性搜索，即Uniform-Cost Search(UCS). 其示意图如下：</p>
<p><img src="/2022/03/08/AI-Searching/1.png" style="zoom:67%;"></p>
<p>首先，如果要使用UCS进行搜索，我们需要先定义一个 $g(n)$, 它代表了根到第n个节点的代价。UCS的代价就是，每次选择$g(n)$最小的节点进行展开，不管它它的目标状态有有多远。因此，UCS</p>
<h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h4><p>(1）将起始点加入到frontier中，将与frontier相连的点进行探索，每个点的代价（后面用cost替代）是从起点到该点的距离，然后将所有的点加入到frontier里面</p>
<p>(2) 从frontier里面挑出cost最小的点A，判断该点是不是终点，如果是终点算法结束。如果不是终点，探索与该点相连的所有点，每个点的cost是A点的cost加上A点到B点的路径的权重。如果B点是已经出现在frontier里面了，比较一下原来的cost与新生成的cost哪个更小，然后将最小的cost赋值给B点，并更新它的父节点。</p>
<p>(3) 如果frontier为空还没有找到终点，则没有到达终点的最短路径</p>
<h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><p><img src="/2022/03/08/AI-Searching/2.png"></p>
<p>对于上面这张图，我们对其进行UCS搜索，找到S到G的最短路径。</p>
<ol>
<li>首先，S的邻接节点是d、e、p，他们的$g(n)$分别为： $g(d)=3,g(e)=9,g(p)=1$</li>
<li>我们选择$g(n)$最小的节点p，并把p的邻接节点q加入到队列。现在的队列为：$g(d)=3,g(e)=9,g(q)=16$ </li>
<li>选择$g(n)$最小的节点d，并把d的邻接节点b、c、e加入到队列。现在的队列为：$g(b)=4,g(c)=11,g(e)=5,g(q)=16$</li>
<li>选择$g(n)$最小的节点b，并把b的邻接节点a加入队列。现在的队列为：$g(a)=6,g(c)=11,g(e)=5,g(q)=16$</li>
<li>选择$g(n)$最小的节点e，并把e的邻接节点h、r 加入队列。现在的队列为：$g(a)=6,g(r)=7,g(c)=11,g(h)=13,g(q)=16$</li>
<li>选择$g(n)$最小的节点a,但a没有邻接节点。因此选择次小节点r，并把r的邻接节点f加入队列。现在的队列为：$g(f)=8,g(c)=11,g(h)=13,g(q)=16$</li>
<li>选择$g(n)$最小的节点f,把f的邻接节点加入队列。现在队列为$g(G)=10,g(c)=11,g(h)=13,g(q)=16$</li>
<li>找到了$S$ 到 $G$ 的道路</li>
</ol>
<p><img src="/2022/03/08/AI-Searching/3.png"></p>
<p>整个过程如上图所示。</p>
<h4 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h4><p>既然UCS算法是根据当前节点的$g(n)$去进行搜索的，那么就需要一个数据结构来帮助我们存储这些节点的代价。这里我们选择的就是Priority Queue(优先级队列)</p>
<p>由于我们的策略是选择优先队列中代价最小的节点，那么假设目标节点的代价为C*,每次搜索一条边的平均代价为$\varepsilon$.那么UCS的搜索深度就是 $C^*/\varepsilon$ .</p>
<p>由BFS的空间和空间复杂度为$O(b^s)$可推得，UCS要找到目标节点的时间复杂度为: $O(b^{C^<em>/\varepsilon})$ ，空间复杂度也为$O(b^{C^</em>/\varepsilon})$</p>
<p><img src="/2022/03/08/AI-Searching/4.png"></p>
<p>那么UCS是complete的吗?（能够找到最优解)</p>
<ul>
<li>如果$C^*$ 是有限且$\varepsilon&gt;0$的话, 那么UCS是可以找到最优解的</li>
</ul>
<p>那么UCS是optimal的吗?（能够以最小的代价找到最优解)</p>
<ul>
<li>是的，我们将在接下来的$A^*$Search中对其证明</li>
</ul>
<h2 id="Informed-Search-Methods"><a href="#Informed-Search-Methods" class="headerlink" title="Informed Search Methods"></a>Informed Search Methods</h2><p>和之前的盲搜不同，Informed Search Methods是有一些先验知识的搜索方式。在这里我们主要介绍3种方式：启发式搜索、贪心搜索和A*搜索。</p>
<h4 id="什么是启发-Heuristics"><a href="#什么是启发-Heuristics" class="headerlink" title="什么是启发(Heuristics)"></a>什么是启发(Heuristics)</h4><p>启发，是一个<strong>估计某一状态距离目标状态有多近的函数</strong>(不一定完全准确)， 它为特定的搜索问题而设计，无法保证最优性。</p>
<p>我们举一个例子：翻煎饼问题。现在有大小不同的四张煎饼，它们以一种混乱的方式排列，我们的目标是将其从小到大排列好。其中，完成目标的代价是每次翻的时候翻了多少煎饼。比如说初始的状态如下所示：</p>
<p><img src="/2022/03/08/AI-Searching/5.png"></p>
<p>从初始状态可以一步到位变成最终状态，只需要反动三张煎饼即可。</p>
<p>下图展示了在四张煎饼的情况下，各个状态互相转变所需要的代价：</p>
<p><img src="/2022/03/08/AI-Searching/6.png"></p>
<p>要找出某个状态到最终状态需花费的最小代价，可以用UCS，但是更好的方法是：告诉电脑以当前状态有多少煎饼不在正确的位置作为启发，并进行搜索。</p>
<ul>
<li><p>又比如，对于pacman小游戏来说，我要让吃豆人以最优的方法到达目标位置，可以告诉电脑当前吃豆人距离目标点的欧几里得距离或者是曼哈顿距离。</p>
</li>
<li><p>再比如，对于罗马尼亚旅行问题，我们可以以任意两个城市之间的直线距离作为启发。</p>
<p><img src="/2022/03/08/AI-Searching/7.png"></p>
</li>
</ul>
<h3 id="Greedy-Search"><a href="#Greedy-Search" class="headerlink" title="Greedy Search"></a>Greedy Search</h3><p>贪心就是一种启发式搜索，但是某些基于贪心的启发式可能会退化成dfs（如果这种贪心完全没有起效果，就等同于在dfs中任意选择下一个节点展开）</p>
<p>启发式的搜索展开节点时，选择可能离目标状态“最近”的节点展开。（例如旅行问题中，展开直线距离离目标节点最小的节点）</p>
<p>贪心法可能让我们获得一个比较好的解，但是无法保证获得的结果一定是最优的。如下图：机器人朝着一颗钻石挖去，挖到了，但是却忽视了更大的两颗钻石</p>
<p><img src="/2022/03/08/AI-Searching/8.png"></p>
<h3 id="A-Search"><a href="#A-Search" class="headerlink" title="A* Search"></a>A* Search</h3><p>A<em> 算法是UCS和贪心算法两者的结合。我们以龟兔赛跑为例，UCS是乌龟、贪心是兔子，结合两者的优点是A</em> 算法。</p>
<p><img src="/2022/03/08/AI-Searching/9.png"></p>
<p>比如说对于上面这张图，每条边都有权重。我们的目标就是，从S出发，找到一条到G的最佳路线。用树来表示，如下</p>
<p><img src="/2022/03/08/AI-Searching/10.png" style="zoom:50%;"></p>
<ul>
<li><p>如果采用UCS：每次展开的是当前已花费代价最小的节点，即<strong>回首过去</strong></p>
<ul>
<li>从s出发，首先展开a， 现在$g(b)=2,g(e)=9,g(d)=4$</li>
<li>展开$b$,现在$g(c)=3,g(d)=4,g(e)=9$ </li>
<li>展开$c$, 发现没路了，而且没到终点，现在$g(d)=4,g(e)=9$</li>
<li>展开$d$,$g(g) = 6,g(e)=9$</li>
<li>展开$g$ ,到达终点</li>
</ul>
<p>路线为：$s\rightarrow a\rightarrow b\rightarrow c \rightarrow d\rightarrow g$ , 虽然找到了最有线路，但是绕了一个弯</p>
</li>
<li><p>如果采用贪心法：每次展开启发函数最小的节点，即<strong>展望未来</strong>。</p>
<ul>
<li>从s出发，首先展开a，$h(e)=1,h(d)=2,h(b)=6$ ，注意，这里的$h(e)=1$ 是通过某种启发算出来的，这里只是举个例子。</li>
<li>展开$e$，$h(d)=2,h(b)=6$ </li>
<li>展开$d$, $h(g)=0,h(b)=6$</li>
<li>展开$g$, 到达终点</li>
</ul>
<p>路线为：$s\rightarrow a \rightarrow e \rightarrow d \rightarrow g$ ， 找错了线路</p>
</li>
<li><p>而使用A*算法，结合了ucs和greedy的优点，即回首过去+展望回来。令函数 $f(n) = g(n)+h(n)$ ,A* 算法中维护一个优先队列，按照$f(n)$ 的值从小到大进行维护。</p>
</li>
</ul>
<ul>
<li>从s出发，首先展开a，$,f(d)=4+2=6,f(b)=2+6 = 8,f(e) = 1+9=10$</li>
<li>展开d，$f(g) = 6,f(b)=8,f(e)=10$</li>
<li><p>展开g</p>
<p>路线为：$s\rightarrow a\rightarrow d\rightarrow g$, 找到了最佳线路，而且花费的时间也最短。</p>
</li>
</ul>
<h4 id="A-算法何时终止？"><a href="#A-算法何时终止？" class="headerlink" title="A*算法何时终止？"></a>A*算法何时终止？</h4><p>第一次遇见目标节点（遇见，就是指展开了目标节点的父节点，将目标节点加入队列）时不能退出（此时不一定最优）。只有把一个目标节点剔除队列的时候，才会让算法终止。</p>
<p>记忆： enqueue不止，dequeue才止</p>
<p><img src="/2022/03/08/AI-Searching/11.png" style="zoom: 100%;"></p>
<ul>
<li><p>从S出发，$f(B)=1+2=3, f(A) = 2+2=4$</p>
</li>
<li><p>展开B，如果之后选择G节点展开，但是s-b-g的代价比s-a-g的代价要大，所以不能退出</p>
</li>
<li>因为此时$f(A)&lt; f(G)$，所以这个时候展开A</li>
<li>然后从A到G，找到最优的路线</li>
<li>将G从队列中移除，停止算法</li>
</ul>
<h4 id="A-算法的最优性"><a href="#A-算法的最优性" class="headerlink" title="A*算法的最优性"></a>A*算法的最优性</h4><p>现在我们来讨论A*算法是不是最优的？</p>
<p>要考虑这个问题，首先我们要想，A*算法是不是始终是正确的？一个很朴素的想法就是，如果我们的启发函数错误地估计了节点到目标节点的距离(比真实值大了很多)，那么这时候A*算法肯定不会选择这个节点作为下个展开的节点。</p>
<p>这时候，我们要设定能使A*算法最优的一个前提：Admissible Heuristics（容许的启发）</p>
<script type="math/tex; mode=display">
0\le h\le h^*(n)</script><p>即启发式算法的估价应该在0和真实代价之间。如果h(n)大于真实代价，则可能找不到最优解，不能保证最优性。</p>
<h5 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h5><p>在满足Admissible Heuristics时，A*搜索算法的最优性是可以被证明的：</p>
<p><img src="/2022/03/08/AI-Searching/12.png"></p>
<p>假设：</p>
<ul>
<li>A是最优目标节点 optimal goal node</li>
<li>B是次优目标节点 suboptimal goal node</li>
<li>h is admissible，估值满足$0\le h\le h^*(n)$</li>
</ul>
<p>证明思路</p>
<ul>
<li>A会在B之前被弹出（fringe）</li>
<li>假设A的祖先n（n可以是A本身）、B，都在待展开的队列边缘（on the fringe），证明n会在B之前被展开</li>
</ul>
<p>证明过程：</p>
<ul>
<li>首先：我们要证明 $f(n)\leq f(A)$ 。 可以从定义入手：<ul>
<li>$f(n)=g(n)+h(n)$ </li>
<li>$g(A) = g(n)+h*(n)$ </li>
<li>$f(A)=g(A)+0 $  (因为$h(A) = 0$) </li>
<li>根据假设 $h(n)\leq h*(n)$ 可知， $f(n)\leq f(A)$ </li>
</ul>
</li>
<li>然后，我们要证明 $f(A)\leq f(B)$ <ul>
<li>$g(A)&lt;g(B)$  A是最优解，B是次优解。</li>
<li>$f(A)&lt;f(B)$ 因为对于AB来说 ，$h(A)=h(B)=0$</li>
</ul>
</li>
</ul>
<p>因此，所有A的祖先(包括A自己) 都会在B之前被展开。这可以证明，当任意目标节点被dequeue时，最优路径已经被找到了。</p>
<p>关于证明的细节：为什么不仅要证明A会在B之前被展开，还要证明A的祖先也会在B之前被展开？因为证明的假设是A和B都要在队列边缘（A和B都已经被遇见）。我们想证明的是，当B在队列边缘时，总有一个A或A的祖先也在队列边缘，并比B拥有更高的展开优先级。</p>
<p>如果只证明A自身，那么当B在队列边缘时A也在队列边缘这一前提是无法被保证的。</p>
<p>因此我们证明A或A的祖先在队列边缘的情况，应为这能保证在B弹出之前至少会有A或者A的祖先(甚至是根)在队列边缘</p>
<h4 id="A-和UCS的比较"><a href="#A-和UCS的比较" class="headerlink" title="A*和UCS的比较"></a>A*和UCS的比较</h4><p><img src="/2022/03/08/AI-Searching/13.png" style="zoom: 50%;"></p>
<p>然后我们来看一下在树形状态下，UCS和A*算法的比较。 </p>
<p><img src="/2022/03/08/AI-Searching/15.png"></p>
<p>最后我们看一下pacman游戏中，三种不同算法的查询过程比较：</p>
<p><img src="/2022/03/08/AI-Searching/14.png"></p>
<p>感性理解：A*算法避免了很多没有必要的搜索，A*算法是一种“对冲”，即基于启发式估计h对UCS算法进行“瘦身”。称之为“对冲”是因为启发式估计本身是不确定的，相信这个估计本身就是存在风险的。</p>
<h2 id="怎么设计Heuristic"><a href="#怎么设计Heuristic" class="headerlink" title="怎么设计Heuristic"></a>怎么设计Heuristic</h2><p>原始问题通过松弛 relax 来进行处理。例如把整数解问题转换成实数解问题。</p>
<p>设计heuristic的时候也可以先对原问题进行简单的松弛，例如：</p>
<ul>
<li>在旅行问题中，直接使用两点间距离</li>
<li>在迷宫问题中，不考虑阻隔，直接使用曼哈顿距离</li>
</ul>
<h3 id="是否必须admissible"><a href="#是否必须admissible" class="headerlink" title="是否必须admissible"></a>是否必须admissible</h3><p>admissible：估值小于等于真实代价</p>
<p>但如果估值比真实代价小太多，也不好。</p>
<p><strong>如果admissible不满足呢？考虑真实代价为9，估值为0.01和估值为10两种情况。</strong></p>
<p><strong>即使10不是admissible的，但起到的效果更好。</strong></p>
<h3 id="具体案例"><a href="#具体案例" class="headerlink" title="具体案例"></a>具体案例</h3><h4 id="迷宫问题中的Heuristic"><a href="#迷宫问题中的Heuristic" class="headerlink" title="迷宫问题中的Heuristic"></a>迷宫问题中的Heuristic</h4><p><img src="/2022/03/08/AI-Searching/16.png"></p>
<p>对于如何设计启发函数，我们可以提出这几个问题：</p>
<ul>
<li>状态是什么：可以当做由1-9九个数构成的向量（把灰色格子也看成一个数，比如9）。但是要考虑到每个数的位置，每个数只出现一次。</li>
<li>状态数有多少: 整个状态数是 $9!$</li>
<li>行为是什么：将一个数移动到相邻的灰色格子</li>
<li>后继状态有多少：<ul>
<li>灰色格子在角落：2</li>
<li>灰色在边界上但不在角落：3</li>
<li>灰色格子在中间：4</li>
</ul>
</li>
<li>真实cost：隔板的真实移动次数。 </li>
</ul>
<p>我们并不知道真实值是多少，但我们可以用松弛的方法去进行估计。下面介绍了两种启发式 ，每周都是admissible的，因为估计的值小于真实值。</p>
<p><strong>松弛1</strong>：想象直接把板子拆下来，再直接放到正确的位置上面，一共需要几次，<strong>也就是看有多少个隔板被错误放置</strong>。我们可以想象，这种方法是将原本需要多步移动的实际情况压缩成一步到位的估计情况。</p>
<p>我们观察下面这个表格，在同样移动4步的情况下，UCS需要展开112个节点，而用TILES启发只需要展开13个节点。这种启发式估计相较于UCS展开的节点数明显减少了。</p>
<p><img src="/2022/03/08/AI-Searching/17.png"></p>
<p><strong>松弛2</strong>：不考虑其他隔板的影响（也就是假设移动的时候不会被挡住），基于曼哈顿距离。这种松弛比第一种松弛效果更好了，因为这是一个更接近真实值的松弛。因为用曼哈顿距离估计得到的步数肯定要大于等于1步.</p>
<p>如下图，要把1移到它正确的位置，用曼哈顿距离作为启发的话，是3步，而实际上，肯定不止3步。因为先要移动6这个阻挡块。</p>
<p><img src="/2022/03/08/AI-Searching/19.png"></p>
<p><img src="/2022/03/08/AI-Searching/18.png"></p>
<p>最理想的状态是把真实代价作为启发，但问题是不知道真实代价。</p>
<p>对冲（hedge）：A*需要平衡【计算估计】和【展开节点】之间的代价，需要做一个平衡。</p>
<h2 id="Semi-Lattice-of-Heuristics"><a href="#Semi-Lattice-of-Heuristics" class="headerlink" title="Semi-Lattice of Heuristics"></a>Semi-Lattice of Heuristics</h2><p>现在我们要学习启发式函数的<strong>半格</strong>特点</p>
<p>启发式函数所满足的特点：</p>
<ul>
<li>支配性Dominance：如果</li>
<li>Trivial heuristics：有一些状态的估计值是显然的。例如，目标节点的估计值就是0。</li>
<li><strong>semi-lattice半格原理：$h(n)=\max(h_a(n),h_b(n))$，取max后依然admissible</strong></li>
</ul>
<p>我们用一个例子来说明一下这个半格原理：</p>
<p><img src="/2022/03/08/AI-Searching/20.png"></p>
<p>比如说，我现在设计了三个admissible的启发式函数$h(a),h(b),h(c)$ ，且 $h_a$ 是 Dominance $h_c$的，那么在这个情况下我们取一个新的启发式函数，即对于每个节点n，都取 $h(n) = \max(h_a(n),h_b(n))$ . 在这种情况下，$h(n)$ 依然是admissible的</p>
<h2 id="Tree-Search-和-Graph-Search-对比"><a href="#Tree-Search-和-Graph-Search-对比" class="headerlink" title="Tree Search 和 Graph Search 对比"></a>Tree Search 和 Graph Search 对比</h2><p><strong>Tree search</strong>缺点：遇到有环的状态图时，会有重复展开的情况（不知道怎么规避已经展开过的节点）。如下图，我们看到e、a、p、q都是重复访问的节点。</p>
<p><img src="/2022/03/08/AI-Searching/21.png"> </p>
<p><strong>Graph search</strong>：不会展开同一个节点两次</p>
<ul>
<li><p>实现：tree search + 状态集合（记录已展开节点的集合）</p>
<ul>
<li>用集合记录是为了O(1)查询是否被展开过。使用列表的话，会额外增加搜寻所需要的时间。</li>
</ul>
</li>
<li><p>A*用Graph search来做检索会有什么问题？</p>
<p>重复状态不再展开。因此这时候就出问题了，如下图：</p>
<p><img src="/2022/03/08/AI-Searching/22.png"></p>
<ol>
<li>因为$f(B) = 1+1=2&lt;f(A)=1+4$, 所以先展开 B</li>
<li>B后面只有一个节点C，所以展开C，此时总体代价为3</li>
<li>C后面只有一个节点G，所以展开G，此时总体代价为6</li>
<li>第一次访问 <code>G(6+0)</code> 时，由于已经访问过一次，所以 <code>G</code> 就不会再被展开。</li>
<li>然后发现，$f(a) = 1+4&lt;f(G)=6+0$ ，且A未被展开过，因此访问A</li>
<li>A后面有一个节点C，且$f(C) = 2+1&lt;f(G)$ ，因此展开C</li>
<li>现在C后面是G，但是G已经被访问过了，因此A* Graph Search在这个时候就会弹出G，算法结束，返回$g(G)=6$ ,我们发现这并不是个最优解。<ul>
<li>但是使用A*  Tree Search的话，仍然会访问G,并返回 $g(G)=5$ ,此时是最优解</li>
<li>A<em> Graph Search <em>*只保证完整性，但不保证最优性。</em></em></li>
</ul>
</li>
</ol>
</li>
</ul>
<h2 id="Consistency-和Admissbility"><a href="#Consistency-和Admissbility" class="headerlink" title="Consistency 和Admissbility"></a>Consistency 和Admissbility</h2><p>我们知道Admissible可以保证A<em>树搜索方法可以得到最优解。其中心思想就是：<strong>启发函数的估计值</strong> $\leq$ <em>*真实的代价</em></em></p>
<p>如下图：$h(A) = 4$ ，它是小于等于A到G的真实代价4的。</p>
<p><img src="/2022/03/08/AI-Searching/23.png"></p>
<p> 那么为了保证A<em> Graph Search也可以保证得到最优解，我们提出Consistency。<em>*这是一个比Admissible更加严格的Heuristic</em></em>。其中心思想是：</p>
<script type="math/tex; mode=display">
h(A)-h(C)\leq \text{cost}(A\rightarrow C)</script><p>用汉语来说，就是估值的边权，需要小于等于真实的变权。我们举个例子：下图中，$h(A)=4,h(C)=1$ , 但是 ， $h(A)-h(C)=3&gt; 1$ 这就不满足 consistency的标准</p>
<p><img src="/2022/03/08/AI-Searching/24.png"></p>
<p>因此，如果要满足一致性，$h(A) $ 至少是小于等于2的。</p>
<h3 id="Consistency-的最优性证明"><a href="#Consistency-的最优性证明" class="headerlink" title="Consistency 的最优性证明"></a>Consistency 的最优性证明</h3><p>前提：Consistency</p>
<p><img src="/2022/03/08/AI-Searching/25.png"></p>
<p>情况如上图，n和n’ 对应的是同一个点的不同路径状态，n比n’更优。</p>
<p><strong>假设</strong>：p是n的祖先且p在队列中，n’已经被访问了。在n’被弹出时，p在队列但未弹出。</p>
<p><strong>目标</strong>：通过反证法 证明n要先于n’被弹出</p>
<ol>
<li>因为$f(p) = g(p)+h(p)$, $f(n)=g(n)+h(n)$    $f(n) -f(p) = g(n)-g(p)+h(n)-h(p)$ </li>
</ol>
<p>我们知道，$g(n)-g(p)$ 就是三角形上n点到p点的距离.   另外，由Consistency可知，$h(p)-h(n)\leq Cost(p\rightarrow n)=g(n)-g(p)$ 。 </p>
<p>因此$f(n)-f(p)&gt;0$ ,$f(p)&lt;f(n)$ .</p>
<p>于是，由A*算法可知，p会先于n节点展开</p>
<ol>
<li>又因为n’是次优节点，因此$f(n)&lt;f’(n)$，所以$f(p)&lt;f’(n)$</li>
</ol>
<p>这种情况有矛盾出现：$f(p)&lt;f’(n)$，说明p应该在n’前被展开。</p>
<p>这与假设里n’先于p被展开相悖，所以假设不成立。</p>
<p><strong>这证明了对于任何一个点，它的最优节点一定是最先被展开的。</strong></p>
<h2 id="A-算法总结"><a href="#A-算法总结" class="headerlink" title="A* 算法总结"></a>A* 算法总结</h2><h2 id="A-star算法总结"><a href="#A-star算法总结" class="headerlink" title="A-star算法总结"></a>A-star算法总结</h2><ul>
<li>Tree search<ul>
<li>当启发是admissible时，A*保证最优性</li>
<li>当h=0时，A*退化成UCS算法</li>
</ul>
</li>
<li>Graph search<ul>
<li>当启发是consistent时，A*保证最优性</li>
<li>当h=0时，同样A*退化成UCS算法</li>
</ul>
</li>
<li><p>Consistency 包含了admissibility</p>
</li>
<li><p>从实践经验来说，大多数admissible的估计都可以做到consistency。</p>
</li>
</ul>
<h2 id="Local-Search-algrithms"><a href="#Local-Search-algrithms" class="headerlink" title="Local Search algrithms"></a>Local Search algrithms</h2><p>在很多优化问题中，我们只要求找到目标状态，对找到目标状态的路径没有要求。比如：八皇后问题，TSP问题。</p>
<p>局部搜索(local search)是解决这类问题的一个方法：从一个初始解出发，搜索当前解的邻域，如果有更优的解则移动至该解并继续搜索，迭代这个过程直到找到局部最优解。</p>
<h3 id="Hill-climbing-爬山算法"><a href="#Hill-climbing-爬山算法" class="headerlink" title="Hill-climbing 爬山算法"></a>Hill-climbing 爬山算法</h3><p>代表性算法——爬山算法：</p>
<ul>
<li>找最好的邻居状态节点。如果比当前节点状态要大，就将其设为新的当前节点。</li>
</ul>
<p>怎么避免陷入局部最优？</p>
<ul>
<li>random restarts，多试几种起始点。</li>
<li>random sideways moves，允许平移。可以逃离shoulder，但是逃离不了local maximum, flat local maximum。</li>
<li>first choice，不一定要选择最高的邻居节点，而是随便选一个比当前状态高的。</li>
</ul>
<p><strong>总结：引入随机性</strong></p>
<p><img src="/2022/03/08/AI-Searching/26.png"></p>
<p><strong>爬山法用于八皇后问题</strong></p>
<p>举例，八皇后问题中如果用“冲突数”评价状态会存在很多shoulder（邻域状态与当前状态冲突数相同）。因此有必要引入sideway move。</p>
<h3 id="Simulated-annealing-模拟退火法"><a href="#Simulated-annealing-模拟退火法" class="headerlink" title="Simulated annealing 模拟退火法"></a>Simulated annealing 模拟退火法</h3><p>模拟退火本身的动机，在于让金属凉下来的方式达到一个low-energy的状态。</p>
<p>基本想法：在一开始的时候允许它往值更低的方向走，一开始温度高允许算法出现错误的移动，希望它能够跳出局部的最优。随着时间的推移，温度下降 ，减少允许它错误移动的机会和次数。</p>
<p>一句话总结，开始允许犯错，目的是为了跳出局部的最优，渐渐地不允许犯错了，希望找到全局的最优。</p>
<p>t指的是温度，按照指数级别的方式降低。按照某一个概率允许向下移动，该概率随t减小概率逐渐降低。</p>
<p>保证最优解，只要温度降低速度“足够慢”（按照指数级别下降exponentially slowly），就可以保证收敛到最佳状态（证明思路是模拟马尔可夫链）。</p>
<h3 id="Local-Beam-Search集束搜索"><a href="#Local-Beam-Search集束搜索" class="headerlink" title="Local Beam Search集束搜索"></a>Local Beam Search集束搜索</h3><p>对local search算法进行k次拷贝，对当前的k个状态生成状态的所有后继节点进行展开，在其中选择最好的k个后继节点。</p>
<p>与运行k次local search算法的区别在于Beam search每次只选择k个状态进行展开，选择的时候有做过对比，而退火算法是用纯粹random的方式进行选择。</p>
<h3 id="Genetic-algorithms-遗传算法"><a href="#Genetic-algorithms-遗传算法" class="headerlink" title="Genetic algorithms 遗传算法"></a>Genetic algorithms 遗传算法</h3><p>仿照基因进化原理，保留最优状态。</p>
<p>遗传算法实际上是随机束搜索的变形, 通过把两个父状态结合生成后继。</p>
<p>种群：种群中的每个个体都是潜在解 ，可以看成所求问题解的集合。</p>
<p>个体表示： 染色体， 实际就是状态的表示 ，每条染色体也可以看成所求解问题对应的一组解。</p>
<p>适应度函数fitness：表示解的好坏程度 。</p>
<p>选择selection（利用）：根据适应度选取比较好的解优先进行两两繁殖 。</p>
<p>交叉cross-over（利用为主+探索）： 选取一个杂交点， 两边染色体互相交换 。</p>
<p>变异mutation（探索）：每个位置都会小概率发生变异 。</p>
<p>类比成兔子，遗传算法是一群吃了失忆药片随机分布在地球上的某些地方的兔子们。他们不知道自己的使命是什么。但是，如果你过几年就杀死一部分海拔低的兔子，多产的兔子们自己就会找到珠穆朗玛峰。这就是遗传算法。</p>
<h3 id="在连续性的空间做局部搜索算法"><a href="#在连续性的空间做局部搜索算法" class="headerlink" title="在连续性的空间做局部搜索算法"></a>在连续性的空间做局部搜索算法</h3>
          
        
      
    </div>
    
    
    
    <div>
      
    </div>
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2022/03/03/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%9F%BA%E7%A1%80-%E5%93%88%E5%B8%8C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/%5Bobject%20Object%5D">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/03/03/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%9F%BA%E7%A1%80-%E5%93%88%E5%B8%8C/" itemprop="url">区块链基础-哈希</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-03-03T19:43:58+08:00">
                2022-03-03
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2025-04-11T10:19:10+08:00">
                2025-04-11
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="区块链基础-哈希"><a href="#区块链基础-哈希" class="headerlink" title="区块链基础-哈希"></a>区块链基础-哈希</h1><h2 id="区块链中常见的Hash函数"><a href="#区块链中常见的Hash函数" class="headerlink" title="区块链中常见的Hash函数"></a>区块链中常见的Hash函数</h2><p>首先，我们要给出Hash函数的定义以及特征：</p>
<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>Hash函数是一种从<strong>任何种类、任何大小的数据中创建固定大小值</strong>的方法，这个值也被称为散列值。通常，用一个短的随机字母和数字组成的字符串来代表。</p>
<p>比如说对于sha 256算法来说，不管输入的值是什么类型、值有多大，它出来的一定是一个256bits的字符串。</p>
<h4 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h4><ul>
<li>Hash函数是逆向难解的。根据给定的散列值，以目前的计算资源很难在有限的时间内推出原文</li>
<li>强抗碰撞性。对于任意两个不同的明文x和y，$H(x)$ 不等于$H(y)$ </li>
<li>雪崩效应。原文的微小变更会导致散列值巨大的变化，使得通过明文来反推原文的难度是非常困难的</li>
</ul>
<h4 id="Hash函数常见算法"><a href="#Hash函数常见算法" class="headerlink" title="Hash函数常见算法"></a>Hash函数常见算法</h4><ul>
<li>MD </li>
</ul>
<p>MD系列有MD4和MD5，输出位128位。MD5比MD4更加安全，但计算速度较慢。</p>
<ul>
<li>SHA </li>
</ul>
<p>SHA是一个Hash函数族。主要有SHA-0,SHA-1,SHA-2,SHA-3 这几个版本。 SHA-2版本包括SHA-224、SHA-256、SHA-384和SHA-512，其中SHA-256是应用最广泛的Hash函数。</p>
<p>现在 MD5、SHA-0和SHA-1已经发现碰撞，存在安全问题，已经不适于商业应用。</p>
<h3 id="哈希函数在区块链中的应用"><a href="#哈希函数在区块链中的应用" class="headerlink" title="哈希函数在区块链中的应用"></a>哈希函数在区块链中的应用</h3><p>首先我们要思考在P2P网络中，如何保证数据的准确性？比如说我用P2P下载一部电影，那么这时候电影文件会被拆分成一个一个片段，由不同的用户传给我。那么怎么保证我收到的电影片段是完整的呢？答案就是利用哈希。因为哈希的雪崩效应和强抗碰撞性，只要输入的片段有所偏差，那么输出的值肯定是不对的。因此我们可以用哈希函数对数据的正确性进行验证。</p>
<p>应用在区块链上：</p>
<h4 id="区块链的地址与标识。"><a href="#区块链的地址与标识。" class="headerlink" title="区块链的地址与标识。"></a><strong>区块链的地址与标识</strong>。</h4><p>每个交易的标识、区块的标识都是一个 Hash值。区块链通过区块链头中的PrevBlockHash或ParentHash连接相邻的块，最终形成链状结构。 </p>
<h4 id="区块体相关数据的树根hash-——-交易树。"><a href="#区块体相关数据的树根hash-——-交易树。" class="headerlink" title="区块体相关数据的树根hash —— 交易树。"></a><strong>区块体相关数据的树根hash —— 交易树</strong>。</h4><p>在区块的头部存储Merkle树的根散列值，这个根散列值是由两两交易逐层生成的，因此这个根散列值也代表了区块中的所有交易数据的数字摘要。</p>
<p><img src="/2022/03/03/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%9F%BA%E7%A1%80-%E5%93%88%E5%B8%8C/1.png" style="zoom:67%;"></p>
<p>比如说，我想知道L3这块数据的正确性，我只需要Hash0、Hash1-1这两个哈希值(Top Hash已知)，我们就能通过Hash函数验证L3是否正确。这就是Merkle Proof</p>
<h4 id="签名与验证"><a href="#签名与验证" class="headerlink" title="签名与验证"></a><strong>签名与验证</strong></h4><p>要理解签名和验证，首先要搞明白公钥和私钥的区别。比特币账户是以一个$2^{160}$的数字地址形式存在的，该账户能拥有、转出和转入比特币。因为地址够长，从而可以确保任意两个账户不会重复。</p>
<h5 id="公钥和私钥"><a href="#公钥和私钥" class="headerlink" title="公钥和私钥"></a>公钥和私钥</h5><p>那么为了标识一个唯一的身份，就出现了公有密钥(公钥)和私有密钥(私钥)的概念。</p>
<ul>
<li>私有密钥是随机生成的，而共有密钥根据私有密钥产生</li>
<li>公有密钥是交易方对外公布的地址，别人都可以知道。用于接收转入</li>
<li>私有密钥则用于支付，这个是不能告诉任何人的。</li>
</ul>
<h5 id="对称加密和非对称加密"><a href="#对称加密和非对称加密" class="headerlink" title="对称加密和非对称加密"></a>对称加密和非对称加密</h5><p>了解了公钥和私钥的概念，还需要了解<strong>对称加密</strong>和<strong>非对称加密</strong>的概念。</p>
<ul>
<li>对称加密是最快速、最简单的一种加密方式，加密（encryption）与解密（decryption）用的是同样的密钥（secret key）。但是在对称加密中，如何给别人提供密钥呢？如果广播的话，每个人都知道密钥是什么，那么会造成数据的不安全。</li>
<li>非对称加密为数据的加密与解密提供了一个非常安全的方法，它使用了一对密钥，公钥（public key）和私钥（private key）。非对称加密使用这对密钥中的一个进行加密，而解密则需要另一个密钥。比如，你向银行请求公钥，银行将公钥发给你，你使用公钥对消息加密，那么只有私钥的持有人—银行才能对你的消息解密。与对称加密不同的是，银行不需要将私钥通过网络发送出去，因此安全性大大提高。</li>
</ul>
<h5 id="数字签名与验证"><a href="#数字签名与验证" class="headerlink" title="数字签名与验证"></a>数字签名与验证</h5><ol>
<li>数字签名的签名过程</li>
</ol>
<p>数字签名的操作过程需要有发方的签名数字证书的私钥及其验证公钥。</p>
<p>具体过程如下：</p>
<ul>
<li>首先对代发文件做哈希算法<strong>形成数字摘要</strong></li>
<li>再对数字摘要用签名私钥做非对称加密形成数字签名；</li>
<li>之后将以上的签名和电子文件原文以及签名证书的公钥加在一起进行封装，形成签名结果发送给收方待收方验证。</li>
</ul>
<ol>
<li>数字签名的验证过程</li>
</ol>
<p>接收方收到发方的签名结果进行签名验证</p>
<p>具体过程如下： </p>
<ul>
<li>接收方收到数字签名的结果，其中包括数字签名、电子原文和发方公钥，即待验证的数据。</li>
<li>接收方进行签名验证：接收方首先用发方公钥解密数字签名，导出数字摘要；</li>
<li>并对电子文件原文做同样哈希算法得出一个新的数字摘要，将两个摘要的哈希值进行结果比较，相同签名得到验证，否则无效。</li>
</ul>
<p>示意图如下：</p>
<p><img src="/2022/03/03/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%9F%BA%E7%A1%80-%E5%93%88%E5%B8%8C/3.jpeg" style="zoom: 150%;"></p>
<p>数字签名的意义就是，在传输一份文件的时候，除了文件本身还附带了一个我的签名，这个签名的作用就是证明，我本人发了这个文件，是赖不掉的(不可抵赖性)；而且保证了文件的安全性，因为如果被篡改会导致验证失败</p>
<ol>
<li><strong>防篡改</strong>。由哈希函数和默克尔树的特性可知，区块中的某个交易的修改会导致交易的散列值发生变化，区块头部中的根散列值也会发生变化。区块的校验方法会校验区块头部中的根散列值和区块中交易生成的新的根散列值是否一致</li>
<li><strong>数据编码</strong></li>
</ol>
<h2 id="加密解密算法"><a href="#加密解密算法" class="headerlink" title="加密解密算法"></a>加密解密算法</h2><h2 id="数字签名"><a href="#数字签名" class="headerlink" title="数字签名"></a>数字签名</h2><h2 id="数字证书及PKI体系"><a href="#数字证书及PKI体系" class="headerlink" title="数字证书及PKI体系"></a>数字证书及PKI体系</h2>
          
        
      
    </div>
    
    
    
    <div>
      
    </div>
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/%5Bobject%20Object%5D">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/" itemprop="url">高级数据库-存储</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-03-02T19:25:30+08:00">
                2022-03-02
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2022-06-25T01:41:14+08:00">
                2022-06-25
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="高级数据库-存储"><a href="#高级数据库-存储" class="headerlink" title="高级数据库-存储"></a>高级数据库-存储</h1><p>参考博客：<a href="https://blog.csdn.net/helloworld_ptt/article/details/105801262" target="_blank" rel="noopener">https://blog.csdn.net/helloworld_ptt/article/details/105801262</a></p>
<p>衡量或者评价一个数据库的性能，通常我们会从四个方面来进行：计算、事务、存储、高可用。现在，我们主要来学习数据库架构中存储这一块的知识。</p>
<h2 id="传统数据库的存储"><a href="#传统数据库的存储" class="headerlink" title="传统数据库的存储"></a>传统数据库的存储</h2><h3 id="KVS接口与设计需求"><a href="#KVS接口与设计需求" class="headerlink" title="KVS接口与设计需求"></a>KVS接口与设计需求</h3><p>KVS(Key Value System) 可以被认为属关系型数据库的基础。如果可以实现KVS，那么其中的Key就相当于关系型数据表中的主键，而Value则相当于数据表中的非主键列。最后，在KVS基础上封装一层Schema，就可以实现关系数据库的基本功能了。</p>
<p>那么在设计KVS的时候，我们需要 考虑哪几方面的内容？</p>
<ul>
<li><strong>需求</strong></li>
<li><strong>功能接口</strong></li>
</ul>
<p>这两点是最重要的，决定了整个项目的成败</p>
<ul>
<li>架构</li>
<li>模块</li>
<li>模块内数据结构、算法</li>
<li>模块间接口</li>
<li>测试</li>
</ul>
<p>上面几方面都和数据库性能有关</p>
<h4 id="功能需求"><a href="#功能需求" class="headerlink" title="功能需求"></a>功能需求</h4><p>那么，在KVS中，我们需要实现什么需求？</p>
<ul>
<li>点操作，也就是 Put和Get操作，即通过put把数据存储到该系统中，再通过get从系统中取出数据</li>
<li>范围操作，即一次需要操作多个数据，比如说用get读取一片连续的数据。因此key的需要存储在连续的空间当中。</li>
<li>容错，因为可能会出现程序错误、宕机，因此在这种情况下，我们需要对已经写入的数据，能确保其不会丢失</li>
<li>一致性，即指事务执行的结果必须是使数据库从一个一致性状态变到另一个一致性状态，在单机状态下实现难度比较小。</li>
</ul>
<h4 id="基本接口"><a href="#基本接口" class="headerlink" title="基本接口"></a>基本接口</h4><p>最终来说，我们需要KVS支持：</p>
<p>单点读和写</p>
<ul>
<li><code>db-&gt;Get(leveldb::ReadOptions(),key1,&amp;value);</code></li>
<li><code>if(s.ok())s=db-&gt;Put(leveldb::WriteOptions(),key2.value)</code></li>
<li><code>if(s.ok())s=db-&gt;Delete(leveldb::WriteOptions(),key1s)</code></li>
</ul>
<p>对范围的支持，我们可以用迭代器来实现：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">leveldb::Iterator* it = db-&gt;Newlterator(leveldb::ReadOptions));</span><br><span class="line"><span class="keyword">for</span> (it-&gt; Seek(start);</span><br><span class="line">  it-&gt; Valid( &amp;&amp; it-&gt;key0.ToString0 &lt; limit;</span><br><span class="line">  it-&gt;Next) &#123;...&#125;</span><br></pre></td></tr></table></figure>
<h3 id="B-Tree"><a href="#B-Tree" class="headerlink" title="B+ Tree"></a>B+ Tree</h3><p>我们学过很多树形的数据结构，但是可能只有B+树可以满足KVS的需求。在 <a href="https://jasonxqh.github.io/2020/06/17/B树/#B-树">这篇博客</a> 中，我们学习了B+树的增删改查。</p>
<p>那么为什么B+树可以满足KVS的设计呢？这就要联系到计算机体系结构的相关知识了。</p>
<p>我们知道计算机的存储都是以块为单位的，最底层的块设备(ssd或者磁盘)的块大小是4KB(不同存储介质不同)。因此，在这种情况下使用二叉树，是不可能使得一个节点充满整一个块的。</p>
<p>我们现在来看B+树的结构：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/1.png"></p>
<p>现在可以发现B+树的优点了 </p>
<ul>
<li>B+树的一个节点可以容纳多个数据，而且可以和底层的块一一对应</li>
<li>B+树和红黑树不一样，它是平衡的。这意味访问任何一个页所需要的深度是相等的，这样使得整体访问代价比较均衡。而且通常来说，一个三层的B+树就可以存储很多数据了，因此一颗B+树的并不会很高</li>
<li>B+树支持范围查询，因为倒数第二层节点之间存在指针(严格意义上B+树并不是树)</li>
</ul>
<h3 id="append-only-btree"><a href="#append-only-btree" class="headerlink" title="append-only btree"></a>append-only btree</h3><p>但是如果我们要做内存中的存储，那么B+树就会显得性能不够，因此我们来学习一种改进后的数据结构： COW(copy on write) B+ Tree</p>
<p>考虑这个 3-level 的b+ 树。有两级分支页面（根为分支页面）和5个叶子节点。数据存储在叶子节点中。</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/2.png" style="zoom:50%;"></p>
<p>这个数据结构不支持叶子与叶子之间的指针(B+树支持) 以便于顺序访问，因为它需要在每次更新时重写整个树。</p>
<p>我们看到，实际上，这些节点是按照顺序存储在数据库文件中的，meta10节点包括指向根页面的指针，SHA1和统计计数器，当文件打开时，它被逐页向后扫描以找到有效的元页面，从而找到树的根。如下所示：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/3.png"></p>
<p> 在COW B+ Tree中，叶子(数据块)是存放在连续空间上的，当我对某一个块进行修改的时候，并不是就地修改页面，而是新建一个页面。并更改索引。如下所示：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/4.png"></p>
<p>当我更新了 leaf 8 中的值，不是就地更改页面，而是生成一个 leaf12。</p>
<p>因为页面的位置发生了变化，所以每个父页面都需要更新以指向新的位置：</p>
<ul>
<li>branch6是 leaf7和leaf8的父页面，因此需要生成一个新的branch11指向leaf7(保持不变)和新生成的页面leaf12</li>
<li>root9是树根，自然需要发生变化，因此需要生成一个root13，指针指向branch1.branch4和branch11。需要注意，root9并不是被删除了，图表中的虚线和指针仍然存在与文件中，只是并不是当前版本罢了。</li>
</ul>
<p>因此，使用这种数据结构，通过将新页面附加到原有文件之后来写入页面。使得已经写入的页面永远都不会被修改。</p>
<p>将树写成顺序形状，如下所示：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/5.png"></p>
<p>那么有人就要问了：每一次都要生成一个新版本的树，不是很浪费空间吗？但事实上，这是一种以空间换时间的方法，将<strong>连续</strong>页面写入磁盘比<strong>随机</strong>写入位置更加有效，而且不需要记录事务日志，因为数据库文件就是事务日志。</p>
<p>但是我们要知道，无论使用B+树还是COW B+树，都会导致<strong>写放大</strong>。因为在写入的时候，除了更新页数据之外，还要更新元数据。因此实际写入的物理数据量是写入数据量的多倍。</p>
<h3 id="Database-Buffer"><a href="#Database-Buffer" class="headerlink" title="Database Buffer"></a>Database Buffer</h3><p>我们知道，如果所有数据都存放在磁盘，每次数据库访问的时候都需要到磁盘里读取，那么，数据访问就讲受限于磁盘的IO，严重影响访问性能。因此，就引入了缓冲区架构：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/7.png" style="zoom:67%;"></p>
<p>缓冲区相当于在数据库和磁盘中间的一层，由缓存管理器管理，它将决定那些Page需要放到缓冲区。这样一来，任何数据库的请求都需要经过缓冲区才能实现。</p>
<p>缓冲区内部结构可以具象为下图：我们把缓冲区中的每一格称为一个Frame，每个Frame可以放一个Page</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/6.png"></p>
<p>问题是，我们要设计一个高效的缓冲区的话，从管理上必须考虑这几个问题：</p>
<ul>
<li>怎么知道哪些Frame是空的？<ul>
<li>扫描</li>
<li>维护一个空的Frame列表</li>
<li>使用一个Bitmap</li>
</ul>
</li>
<li><p>知道Dist page存放在哪个Frame当中</p>
<ul>
<li>使用哈希表，将 page id 映射到frame id</li>
</ul>
</li>
<li><p>如果缓冲区已经满了，该怎么办？</p>
<ul>
<li>在CSAPP中，我们学习过LRU策略，就是淘汰最近没有使用的页，其实还有很多替换策略</li>
</ul>
</li>
</ul>
<h3 id="Implement-LRU-Cache"><a href="#Implement-LRU-Cache" class="headerlink" title="Implement LRU-Cache"></a>Implement LRU-Cache</h3><p>上面我们说LRU策略是比较常用的淘汰机制。那么LRU有哪些缺点呢？</p>
<ol>
<li>LRU对循环读取的场景不太友好，比如我们循环读取第1、2、3、4页，那么我们刚把1淘汰掉，就需要去读取第1页了。</li>
<li>LRU会引入很冷的数据。比如某一页只出现了一次，也会出现在缓冲区当中，并在相当一段时间里面占有一个位置。</li>
</ol>
<p>因此，我们可以对LRU做一个优化——LRU-K</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/8.png" style="zoom: 50%;"></p>
<p>LRU-K需要多维护一个队列或者更多，用于记录所有缓存数据被访问的历史。只有当数据的访问次数达到K次的时候，才将数据放入缓存。当需要淘汰数据时，LRU-K会淘汰第K次访问时间距当前时间最大的数据。</p>
<ul>
<li>第一步添加数据照样放入第一个队列的头部<ul>
<li>如果数据在该队列里访问没有达到K次（该数值根据具体系统qps来定）则会继续到达链表底部直至淘汰；</li>
<li>如果该数据在队列中时访问次数达到了K次，那么它会被加入到接下来的2级(具体需要几级结构也同样结合系统分析)链表中，按照时间顺序在2级链表中排列</li>
<li>接下来2级链表中的操作与上面算法相同，链表中的数据如果再次被访问则移到头部，链表满时，底部数据淘汰</li>
</ul>
</li>
</ul>
<p>相比LRU，LRU-K需要多维护一个队列，用于记录所有缓存数据被访问的历史，所以需要更多的内存空间来用来构建缓存，但优点也很明显，较好的降低了数据的污染率提高了缓存的命中率，对于系统来说可以用一定的硬件成本来换取系统性能也不失为一种办法。</p>
<h4 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h4><p>我们要实现一个LRU Cache的话，需要满足什么要求，又要采用什么数据结构？</p>
<p>首先我们要考虑，怎么能让页号在内存里的位置，然后需要考虑LRU淘汰机制</p>
<ul>
<li>如何判断数据是否在缓存里，是否命中？<ul>
<li>使用哈希表</li>
</ul>
</li>
<li>如何筛选LRU的数据？<ul>
<li>我们可以使用链表，把新插入的数据放在表头，更新的数据也放在表头，这样不访问的数据会慢慢聚集到表尾</li>
</ul>
</li>
</ul>
<p>因此，最后的数据结构应该是一个哈希表和链表的结合——用哈希链地址法，另外做一个链表，把哈希表中所有的节点链起来</p>
<p>如下所示：</p>
<ul>
<li>Lru ： 链表的头指针</li>
<li>Prev/next: 链表的前驱后继指针</li>
<li>Hash_next:哈希表的后继指针，用来指向value</li>
</ul>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/9.png" style="zoom: 50%;"></p>
<h2 id="bitcask结构"><a href="#bitcask结构" class="headerlink" title="bitcask结构"></a>bitcask结构</h2><h3 id="Log-structured-store"><a href="#Log-structured-store" class="headerlink" title="Log-structured store"></a>Log-structured store</h3><p>事实上，对于数据该如何存储在磁盘中，也非常有门道——之前我们在学习COW B+树时知道顺序写入的性能是要优于随机写入的性能的。</p>
<p>这里我们再介绍一种存储方法——Log-structured 存储，也就是说，将数据像记录日志一样存储在磁盘当中。近些年来，这种存储方式被越来越多的用在数据库存储中。</p>
<p>Log-structured 存储系统的基本单位就是一个只能附加在原有数据之后的日志。每当我想写入新的数据的时候，只需要将其附加到日志的末尾，而不是在磁盘中随变找一个位置。同样的，对meta data也采用这种方式，它的更新也被附加到日志中。这可能看起来效率低下，实际上，每次写入时我们需要更新的索引节点的数量通常非常少。</p>
<p>我们来看一个例子，下面是一个仅包含单个数据项的日志 Foo以及它的索引节点A:</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/10.png"></p>
<p>现在，我们要添加第二个元素Bar，将其添加到日志的末尾，然后更新索引条目A’,并将更新后的版本也附加到日志中：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/11.png"></p>
<p>我们看到，旧索引A仍然在日志文件当中，但是不再被使用了。因为他已经被新的索引A’替代。新索引指向Foo和新文件Bar。当上层想要读取我们的文件系统的时候，首先会找到索引的根节点(即A’),然后从根节点出发去寻找其它节点。</p>
<p>接下来我们看看当修改文件的时候会怎么样, 比如说我们修改的是Foo:</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/12.png"></p>
<p>那么首先，修改过后的Foo’ 会被写入日志末尾。然后，我们再次更新索引节点A’ ,变成一个新的A’’,并将其也写在日志末尾。Foo的旧副本会保留在日志当中，但是不会再被新的索引引用.</p>
<p>可以意识到，这个系统并不是可以一直运行下去的，因为无用的文件始终在累计，总有一个时刻我们将耗尽存储空间。因此，在文件系统中，我们将整个磁盘作为一个可以循环的缓冲区，当放不下新的数据的时候，会将旧的日志数据覆盖掉。但在上层看来，新数据就好像是新写入的，而不是覆盖原本数据保存下来的。在常规的文件系统中，随着磁盘越来越满，文件系统需要花费越来越多的时间进行垃圾收集，并将数据写回到日志头部。实际上，当磁盘消耗的存储空间到达80%的时候，文件系统就会停止运行。</p>
<p>但是，我们可以不用这种数据结构来管理文件，我们可以将Log-structured存储用于数据库引擎。也就是说，我们不再底层应用这种模式，而在文件系统的上层来实现它。比如说，我们可以将数据库拆分为多个固定长度的块，那么当我们需要回收一点空间的时候，将有用的数据写在一个块里，删除其他块。比如说对于上面那张图，占用了6页，假设块的大小为4页，我们就可以将其缩短为一个块的大小：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/13.png"></p>
<p>对于上图，第一个日志段完全是空的，因此可以删除。</p>
<h4 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h4><p>和底层的文件系统比起来，使用Log-structured存储有以下几个优势：</p>
<ol>
<li>首先，使用这种存储方式在删除时不需要首先删除最旧的数据——如果中间段几乎是空的话，我们可以对其进行垃圾收集。这对一些需要长时间存放数据的数据库非常有用。<ul>
<li>对于何时进行垃圾收集，我们通常可以等到一个段的大部分已经”过期”以后再进行垃圾收集，进一步减少了额外工作量</li>
</ul>
</li>
<li>可以避免重写相同的未修改的数据，因此在这方面可以节省时间，提高性能。</li>
<li>使用这种方法备份也更容易：在传统的B tree上，如果我更新了数据，那么旧的页同时也丢失了；但是使用这种数据结构我们可以通过在完成时将每个新日志段复制到备份介质来连续、增量地备份我们的数据库。如果要恢复，我们根据备份来恢复即可。</li>
<li>使用这个系统的最后一个优势与更好的并发性。我们知道，为了提供事务的一致性(consistency), 大多数数据库会使用复杂的锁系统。根据所需的一致性级别，使用不同的锁。使用锁的缺点就是：当并发量很高时，写入数据相对较低，性能显著下降。<ul>
<li>因此，可以使用MVCC(Multiversion Concurrency Control) 来解决这个问题。当一个节点想从数据库读取数据的时候，它会去查找当前的根索引，并将其用于这个节点的事务的剩余部分。因为在一个log-based 系统中，存在的数据永远不会被修改，所以当我们读取了根节点的时候，相当于获取了整个数据库的快照(当前根节点有指向所有可用的数据页的指针)。比如说，我们读取到了A’’’,我们就能读取Foo’和Bar,而其他并发的事务如果对数据进行修改的话，是追加在A’’’之后的，对当前事务并无影响。因此，从这个角度看，我们实现了无锁读取。</li>
<li>在写回数据的时候，我们可以使用Optimistic concurrency(OCC,在<a href="https://jasonxqh.github.io/2022/05/11/高级数据库-事务/">高级数据库-事务</a>中有详细介绍)，顾名思义，就是乐观的并发，这和使用锁并等待的悲观机制有所不同。在写入对数据的更改的时候，我们获取写入锁。然后我们要验证当前数据和我们在读取阶段的数据是一致的——我们可以读取索引，判断索引指向的文件是否与之前的相同，地址是否相同。如果相同，说明在这段时间里面并没有发生写入，因此我们可以对其进行追加写；如果不同的话，就发生了事务冲突，我们只需要回滚并从读取阶段重新开始。</li>
</ul>
</li>
</ol>
<h3 id="bitcask"><a href="#bitcask" class="headerlink" title="bitcask"></a>bitcask</h3><p>学习了上面基于日志结构的存储系统，我们现在来学Bitcask架构，也就是<strong>日志型的基于哈希表结构的键值对存储系统</strong>。比如说Berkeley DB，BeansDB，Ranmcloud都采取了这种架构。 它有以下几个特点：</p>
<ol>
<li>通过内存中哈希表的key来索引磁盘中数据的位置</li>
<li>数据文件采用追加写的方式</li>
<li>比较低的读写时延。</li>
<li>比较高的随机写吞吐率。</li>
<li>能够控制更大的数据库。</li>
<li>容易备份和恢复。</li>
<li>相对简单，容易理解。</li>
<li>可预计的高访问压力情况。</li>
</ol>
<p>bitcask只支持追加操作(Append-only)，即所有的写操作只追加而不修改老的数据，每个文件都有一定的大小限制，当文件增加到相应的大小，就会产生一个新的文件，老的文件只读不写。在任意时刻，只有一个文件是可写的，用于追加数据，被称为<strong>活跃数据文件</strong>(active data file)，其他已经达到文件大小限制的，被称为老数据文件(older data file)。</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/15.png"></p>
<p>活跃数据文件仅支持追加写入，因此所有的写入操作都是串形化的而不用磁盘随机定位。写入的键值对格式如下：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/14.png"></p>
<ol>
<li>crc用于验证</li>
<li>timestamp 时间戳</li>
<li>key size : 键的长度</li>
<li>value size:值的长度</li>
<li>key 数据</li>
<li>Value 数据</li>
</ol>
<h4 id="哈希索引"><a href="#哈希索引" class="headerlink" title="哈希索引"></a>哈希索引</h4><p>在bitcask模型中，使用了Hash表的索引结构。在内存中有一张哈希表，通过哈希表中的key值可以快速地定位到磁盘中的数据，大致结构如下所示：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/16.png"></p>
<p>hash表对应的这个结构中包括了三个用于定位数据value的信息，分别是文件id号(<code>file_id</code>)，value值在文件中的位置（<code>value_pos</code>）,value值的大小（<code>value_sz</code>），于是我们通过读取<code>file_id</code>对应文件的<code>value_pos</code>开始的<code>value_sz</code>个字节，就得到了我们需要的value值。整个过程如下图所示：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/14.jpeg"></p>
<p>在一个具体的哈希索引例子中，我们可以更清楚的看到数据是如何被找到的：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/19.png"></p>
<h4 id="文件的merge"><a href="#文件的merge" class="headerlink" title="文件的merge"></a>文件的merge</h4><p>合并操作是定时对所有的旧数据文件进行扫描并生成新的数据文件（其本质是将同一个Key的多个操作进行合并。）在这个过程中，并没有包含active date file，因为它还还在不停写入。</p>
<p>键值对的删除也是追加写的方式写入活动数据文件中，真正的删除会在下一次的数据合并中进行。</p>
<p>这里的merge其实就是将对同一个key的多个操作以<strong>只保留最新</strong>原则进行删除。每次merge之后，新生成的数据文件就不再有冗余了。如下图所示：对于yawn,保留最新的511；对于scratch，保留最新的252；对于mew，保留最新的1087；对purr，保留最新的2114</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/18.png"></p>
<h4 id="容量限制"><a href="#容量限制" class="headerlink" title="容量限制"></a>容量限制</h4><p>我们假设内存是有限的，磁盘空间是无限的。可以给出一个元数据的格式：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//BeansDB文件元数据格式</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">t_item</span> &#123;</span></span><br><span class="line">	<span class="keyword">unit32_t</span> pos;	<span class="comment">//数据记录在</span></span><br><span class="line">	<span class="keyword">int32_t</span> ver;	<span class="comment">//数据记录版本号</span></span><br><span class="line">	<span class="keyword">unit16_t</span> hash;<span class="comment">//数据记录hash</span></span><br><span class="line">	<span class="keyword">unit8_t</span> length;<span class="comment">//key的长度</span></span><br><span class="line">	<span class="keyword">char</span> key[<span class="number">1</span>];	<span class="comment">//key的实际值</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>beansDB将pos的低8位用来存储<strong>文件的id</strong>，这样一便可以最多存储256个文件。然后用剩下的24位用来存储文件内偏移。每个文件的大小一般被定义为2GB,因此该系统最大支持的存储空间为512GB。每个文件元数据占据内存的总存储空间为：4B+4B+2B+1B+size(key), 按照size(key) = 20B来计算。 100M(20*100*1024)个文件也只会占用2GB的内存。</p>
<h4 id="容错"><a href="#容错" class="headerlink" title="容错"></a>容错</h4><p>从上面我们可以知道，索引的Hash表存放在内存中，如果发生系统重启，则须要扫描磁盘中的数据重建Hash表，如果数据量非常大，这个过程是非常耗时的。也就是说，<strong>bitcask启动速度是很慢的</strong>。 因此，bitcask模型中还要对older data file生成一个hint file，在这处文件中，数据结构与磁盘中的数据文件非常相似，不同的是 , 它不存储具体的value值，而是存储value的位置信息。相当于对旧数据哈希表的物化。其结构如下图：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/17.png"></p>
<h2 id="LSM-Tree-架构存储"><a href="#LSM-Tree-架构存储" class="headerlink" title="LSM Tree 架构存储"></a>LSM Tree 架构存储</h2><h3 id="Skiplist"><a href="#Skiplist" class="headerlink" title="Skiplist"></a>Skiplist</h3><p>在简单的链表中，如果我们要在里面找到一个元素的话，需要将整个链表都遍历一次。因此我们可以设计一种新型的链表——skiplist(跳表), 来提高查询的效率。跳表的平均期望的查找、插入、删除时间复杂度都是$O(\log n)$ ，n代表跳表的长度。</p>
<p>许多知名的开源软件（库）中的数据结构均采用了跳表这种数据结构。</p>
<ul>
<li>Redis中的有序集合zset</li>
<li>LevelDB、RocksDB、HBase中Memtable</li>
<li>ApacheLucene中的TermDictionary、Posting List</li>
</ul>
<h4 id="skiplist的构建"><a href="#skiplist的构建" class="headerlink" title="skiplist的构建"></a>skiplist的构建</h4><p>首先我们不满足只有一层的链表，那么我们是不是可以在原有链表的基础上增加一层？加入我们每相邻两个节点来增加一个指针，让指针去指向下下个节点，如下图：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/21.png"></p>
<p>这样一来，第二层包含的节点数只有原来的一半。现在，当我们想要查找数据的时候，可以先沿着第二层的链表进行查找。当碰到比待查数据大的节点时，再回到原来的链表中进行查找。比如我想查找23，就是这样一个查找路径：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/22.png"></p>
<ul>
<li>首先，23和7比较，再和19比较，比它们都大；继续向后比较，发现比26小，因此要掉到下面的链表(L0)</li>
<li>在L0中，从19开始，23比22要大；那么继续向后比较</li>
<li>发现23比26小，说明待查数据23在原链表中不存在</li>
</ul>
<p>我们发现，在这个查找过程中，由于新增加的指针，我们不需要与链表中的每个节点去逐个进行比较了。需要比较的节点数大概只有原来的一半。</p>
<p>因此，我们可以采用同样的方式再往上加一层，产生第三层链表L2</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/23.png"></p>
<p>在这个新的三层链表结构上，如果我们还是查找23，那么：</p>
<ul>
<li>沿着最上层链表首先要比较的是19，发现23比19大，但19已经是L2的最后一层了，因此跳到L1</li>
<li>查找方式如二层跳表。</li>
</ul>
<p>可以想象，当链表足够长的时候，这种多层链表的查找方式能让我们跳过很多下层节点，大大加快查找的速度</p>
<p>Skiplist就是根据这种思想而设计出来的。但是，如果我们按照上面的这种方法来构建跳表，<strong>在新插入一个节点之后，就会打乱上下相邻两层链表上节点个数严格2:1的对应关系。如果要维持这种对应关系，就必须把新插入的节点后面的所有节点（也包括新插入的节点）重新进行调整，这会让时间复杂度重新退化成O(n)</strong>。</p>
<p>因此跳表引入了一个很好的机制。为了避免这种情况的发生，它不要求上下两层链表之间的节点个数严格满足1:2, 而是在生成上层链表的时候，会给每一个节点”抛硬币”——假设抛到正面就会上升一层，抛到反面就按兵不动。</p>
<p>一般来说，跳表的高度是$\log(n)$ ，n为链表中元素个数。最终结果如下：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/20.png"></p>
<h4 id="skiplist的查找"><a href="#skiplist的查找" class="headerlink" title="skiplist的查找"></a>skiplist的查找</h4><p>Skiplist的查找和上面所说的方法基本是一样的，即从高层向底层查找，不断缩小范围，最后锁定目标。</p>
<h4 id="skiplist的插入"><a href="#skiplist的插入" class="headerlink" title="skiplist的插入"></a>skiplist的插入</h4><p>skiplist的插入是需要一定的技巧的</p>
<p>首先是进行查找，比如对上面这张跳表，我想插入15。那么我们首先要查找到15该插入的位置，同时记录每一层的小于插入值的最大节点，比如在L2，15处于6和25之间，我们就将6保存下来，之后有用。</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/24.png"></p>
<p>在这里我们保留每个红色箭头的起始节点:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>层数</th>
<th>保留节点</th>
</tr>
</thead>
<tbody>
<tr>
<td>L3</td>
<td>6</td>
</tr>
<tr>
<td>L2</td>
<td>6</td>
</tr>
<tr>
<td>L1</td>
<td>9</td>
</tr>
<tr>
<td>L0</td>
<td>12</td>
</tr>
</tbody>
</table>
</div>
<p>经过了查找，我们知道15应该插入到12到17 中间，那么问题来了——15节点的层数应该是多少？</p>
<p>这里我们还是采用随机的方法，对15节点抛硬币，逻辑如下</p>
<ul>
<li>如果是正面，那么就加一层，并继续抛硬币；<ul>
<li>如果还是正面，就继续抛，一直到最高层</li>
<li>如果是反面，就立刻停止</li>
</ul>
</li>
<li>如果是反面，那么就不再增加层数了。</li>
</ul>
<p>最后，假设15节点的层高为3，那么就需要先断开三层链表，并进行重连该节点的前后指针。</p>
<p>伪代码如下：</p>
<ul>
<li>首先，每个节点肯定都有第1层指针（每个节点都在第1层链表里）</li>
<li>如果一个节点有第i层(i&gt;=1)指针（即节点已经在第1层到第i层链表中），那么它有第(i+1)层指针的<strong>概率为p</strong></li>
<li>节点最大的层数不允许超过一个最大值，记为<strong>MaxLevel</strong><br>这个计算随机层数的伪码如下所示：</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">randomLevel()</span><br><span class="line">    level := <span class="number">1</span></span><br><span class="line">    <span class="comment">// random()返回一个[0...1)的随机数</span></span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">random</span>() &lt; p <span class="keyword">and</span> level &lt; MaxLevel <span class="keyword">do</span></span><br><span class="line">        level := level + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> level</span><br></pre></td></tr></table></figure>
<h4 id="skiplist的删除"><a href="#skiplist的删除" class="headerlink" title="skiplist的删除"></a>skiplist的删除</h4><p>删除和插入一样，首先要进行查找，同时记录每个前驱结点，因为在删除L0节点之后需要断开上面几层的指针</p>
<h4 id="skiplist每个点的结构"><a href="#skiplist每个点的结构" class="headerlink" title="skiplist每个点的结构"></a>skiplist每个点的结构</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">nodeStructure</span> *<span class="title">node</span>;</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">nodeStructure</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">  keyType key; 			<span class="comment">// key值</span></span><br><span class="line">  valueType value;  <span class="comment">// value值</span></span><br><span class="line">  <span class="comment">// 向前指针数组，根据该节点层数的不同指向不同大小的数组</span></span><br><span class="line">  node forward[<span class="number">1</span>]; 	<span class="comment">// 这是一个指针数组首地址，实际分配内存大小时多于一个</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// newNodeOfLevel生成一个nodeStructure结构体，同时生成l个node* 数组指针</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> newNodeOfLevel(l) (node)malloc(sizeof(struct nodeStructure)+(l)*sizeof(node *));</span></span><br></pre></td></tr></table></figure>
<p>注意，节点其实只有一层，而不是多层链表</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/25.png"></p>
<h4 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h4><p>在这一部分，我们来简单分析一下skiplist的时间复杂度和空间复杂度，以便对于skiplist的性能有一个直观的了解。</p>
<p>我们先来计算一下<strong>每个节点所包含的平均指针数目（概率期望）</strong>。节点包含的指针数目，相当于这个算法在空间上的额外开销(overhead)，可以用来度量空间复杂度</p>
<h5 id="平均指针数目"><a href="#平均指针数目" class="headerlink" title="平均指针数目"></a>平均指针数目</h5><p>根据前面randomLevel()的伪码，我们很容易看出，<strong>产生越高的节点层数，概率越低</strong>。定量的分析如下：</p>
<ul>
<li>节点层数至少为1。而大于1的节点层数，满足一个概率分布</li>
<li>节点层数恰好等于1的概率为$1-p$</li>
<li>节点层数大于等于2的概率为$p$，而节点层数恰好等于2的概率为$p(1-p)$</li>
<li>节点层数大于等于3的概率为$p^2$，而节点层数恰好等于3的概率为$p^2(1-p)$</li>
<li>节点层数大于等于4的概率为$p^3$，而节点层数恰好等于4的概率为$p^3(1-p)$ </li>
</ul>
<p>一般来说我们取$p=0.5$</p>
<p>因此，一个节点的平均层数（也即包含的平均指针数目），计算如下：</p>
<script type="math/tex; mode=display">
1\times(1-p)+2p(1-p)+3p^2(1-p) +...=(1-p)\sum_{k=1}^{+\infty}kp^{k-1}\\
\sum_{k=1}^{+\infty}\int kp^{k-1}d_k = \sum_{k=1}^{\infty}p^k =p(\frac{1-p^k}{1-p}) = \frac{1}{1-p}-1\\</script><p>再对积分求导数，可得:</p>
<script type="math/tex; mode=display">
1\times(1-p)+2p(1-p)+3p^2(1-p) +...=(1-p)\sum_{k=1}^\infty kp^{k-1} = (1-p)\frac{1}{(1-p)^2} = \frac{1}{1-p}</script><p>现在很容易计算出：</p>
<ul>
<li>当p=1/2时，每个节点所包含的平均指针数目为2；</li>
<li>当p=1/4时，每个节点所包含的平均指针数目为1.33。这也是Redis里的skiplist实现在空间上的开销。</li>
</ul>
<h5 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h5><p>接下来，为了分析时间复杂度，我们计算一下skiplist的<strong>平均查找长度</strong>。查找长度指的是查找路径上跨越的跳数，而查找过程中的比较次数就等于查找长度加1。</p>
<p>为了计算查找长度，这里我们需要利用一点小技巧。我们注意到，每个节点插入的时候，它的层数是由随机函数randomLevel()计算出来的，而且随机的计算不依赖于其它节点，每次插入过程都是完全独立的。所以，<strong>从统计上来说，一个skiplist结构的形成与节点的插入顺序无关</strong></p>
<p>这样的话，为了计算查找长度，我们可以将查找过程倒过来看，从右下方第1层上最后到达的那个节点开始，沿着查找路径向左向上回溯，类似于爬楼梯的过程。我们假设当回溯到某个节点的时候，它才被插入，这虽然相当于改变了节点的插入顺序，但从统计上不影响整个skiplist的形成结构</p>
<p>现在假设我们从一个层数为i的节点x出发，需要向左向上攀爬k层。这时我们有两种可能：</p>
<ul>
<li>如果节点x有第(i+1)层指针，那么我们需要向上走。这种情况概率为p</li>
<li>如果节点x没有第(i+1)层指针，那么我们需要向左走。这种情况概率为(1-p)</li>
</ul>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/27.png"></p>
<p>用C(k)表示向上攀爬k个层级所需要走过的平均查找路径长度（概率期望），那么：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">C(0)&#x3D;0C(k)&#x3D;(1-p)×(上图中情况b的查找长度) + p×(上图中情况c的查找长度)</span><br><span class="line">1</span><br></pre></td></tr></table></figure>
<p>代入，得到一个差分方程并化简：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">C(k)&#x3D;(1-p)(C(k)+1) + p(C(k-1)+1) </span><br><span class="line">C(k)&#x3D;1&#x2F;p+C(k-1) </span><br><span class="line">C(k)&#x3D;k&#x2F;p</span><br></pre></td></tr></table></figure>
<p>这个结果的意思是，我们每爬升1个层级，需要在查找路径上走 1/p 步。而我们总共需要攀爬的层级数等于整个skiplist的总层数-1。</p>
<p>那么接下来我们需要分析一下当skiplist中有n个节点的时候，它的总层数的概率均值是多少。这个问题直观上比较好理解。根据节点的层数随机算法，容易得出：</p>
<ul>
<li>第1层链表固定有$n$个节点；</li>
<li>第2层链表平均有$n\times p$个节点；</li>
<li>第3层链表平均有$n\times p^2$个节点；</li>
</ul>
<p>所以，从第1层到最高层，各层链表的平均节点数是一个指数递减的等比数列。容易推算出，总层数的均值为$\log\frac{1}{pn}$，而最高层的平均节点数为$1/p$。</p>
<p>综上，粗略来计算的话，平均查找长度约等于 $C(\log\frac{1}{pn-1}) = \frac{\log 1/(pn-1)}{p}$ ，也就是时间复杂度为 $O(log(n))$</p>
<h4 id="Skiplist和B树以及哈希表的对比"><a href="#Skiplist和B树以及哈希表的对比" class="headerlink" title="Skiplist和B树以及哈希表的对比"></a>Skiplist和B树以及哈希表的对比</h4><ul>
<li>skiplist和各种平衡树（如AVL、红黑树等）的元素是有序排列的，而哈希表不是有序的。因此，<strong>在哈希表上只能做单个key的查找，不适宜做范围查找。</strong> 所谓范围查找，指的是查找那些大小在指定的两个值之间的所有节点</li>
<li>在做范围查找的时候，平衡树比skiplist操作要复杂。<ul>
<li>在平衡树上，我们找到指定范围的小值之后，还需要以中序遍历的顺序继续寻找其它不超过大值的节点。如果不对平衡树进行一定的改造，这里的中序遍历并不容易实现。</li>
<li>而在skiplist上进行范围查找就非常简单，只需要在找到小值之后，对第1层链表进行若干步的遍历就可以实现</li>
</ul>
</li>
<li>平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而<strong>skiplist的插入和删除只需要修改相邻节点的指针</strong>，操作简单又快速</li>
<li>从内存占用上来说，skiplist比平衡树更灵活一些。一般来说，平衡树每个节点包含2个指针（分别指向左右子树），而skiplist每个节点包含的指针数目平均为1/(1-p)，具体取决于参数p的大小。如果像Redis里的实现一样，取p=1/4，那么平均每个节点包含1.33个指针，比平衡树更有优势</li>
<li>查找单个key，skiplist和平衡树的时间复杂度都为$O(\log n)$，大体相当；而哈希表在保持较低的哈希值冲突概率的前提下，查找时间复杂度接近O(1)，性能更高一些。所以我们平常使用的各种Map或dictionary结构，大都是基于哈希表实现的</li>
<li>从算法实现难度上来比较，skiplist比平衡树要简单得多</li>
</ul>
<h3 id="Log-structured-Merge-Tree-Model"><a href="#Log-structured-Merge-Tree-Model" class="headerlink" title="Log-structured Merge Tree Model"></a>Log-structured Merge Tree Model</h3><p> LSM的基本思想是将修改的数据保存在内存，达到一定数量后在将修改的数据批量写入磁盘，在写入的过程中与之前已经存在的数据做合并。同B树存储模型一样，<strong>LSM存储模型也支持增、删、读、改以及顺序扫描操作。LSM模型利用批量写入解决了随机写入的问题，虽然牺牲了部分读的性能，但是大大提高了写的性能。</strong></p>
<p>下图是LSM 树存储模型的基本架构：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/28.png" style="zoom:67%;"></p>
<p>我们需要了解三个最重要的组件：MemTable，Immutable MemTable和SSTable</p>
<h4 id="Memtable"><a href="#Memtable" class="headerlink" title="Memtable"></a>Memtable</h4><p> LSM本身由MemTable,Immutable MemTable,SSTable等多个部分组成，其中<strong>MemTable位于内存</strong>，用于记录最近修改的数据，<strong>一般用跳跃表来组织(也可以用平衡树)</strong>。当MemTable达到一定大小后，将其冻结起来变成Immutable MemTable，然后开辟一个新的MemTable用来记录新的记录。而Immutable MemTable则等待转存到磁盘。</p>
<h4 id="Immutable-MemTable"><a href="#Immutable-MemTable" class="headerlink" title="Immutable MemTable"></a>Immutable MemTable</h4><p>顾名思义，就是只能读不能写的内存表。内存部分已经有了MemTable，为什么还要使用Immutable MemTable？个人认为其原因是<strong>为了不阻塞写操作</strong>。因为转存的过程中必然要保证内存表的记录不变，否则如果新插入的记录夹在两条已经转存到磁盘的记录中间，处理上会很麻烦，转存期间势必要锁住全表，这样一来就会阻塞写操作。所以不如将原有的MemTable变成只读Immutable MemTable,再开辟一个新的MemTable用于写入，即简单，又不影响写操作。</p>
<h4 id="SSTable"><a href="#SSTable" class="headerlink" title="SSTable"></a>SSTable</h4><p>SSTable是本意是指<strong>有序的键值对集合</strong>( a set of sorted key-value pairs )。是一个简单有用的集合，正如它的名字一样，它存储的就是一系列的键值对。当文件较大的时候，还可以为其建立一个键-值的位置的索引，指明每个键在SSTable文件中的偏移距离。这样可以加速在SSTable中的查询。 </p>
<p>使用MemTable和SSTable这两个组件，可以构建一个最简单的LSM存储模型。这个模型与Bitcask模型相比，<strong>不存在启动时间长的问题</strong>，<strong>但是这个模型的读性能非常差</strong>。在之后会详细展开</p>
<h4 id="读写操作"><a href="#读写操作" class="headerlink" title="读写操作"></a>读写操作</h4><ul>
<li>读取操作</li>
</ul>
<p>之前我们说了在LSMT中，读取数据的性能会很差。因为一但在MemTable找不到相应的键，则需要在根据SSTable文件生成的时间，从最近到较早在SSTable中寻找，如果都不存在的话，则会遍历完所有的SSTable文件。因此，如果SSTable文件个数很多或者没有建立SSTable的文件内索引的话，读性能则会大大下降。</p>
<p>除了在对SSTable内部建立索引外，还可以使用Bloom Filter,  提高Key不在SSTable的判定速度。同样，定期合并旧的SSTable文件，在减少存储的空间的同时，也能提高读取的速度。 正如上图所示，SSTable分为好多层，Level1就是由Level0合并而来的，而Level2也是由Level1合并而来的。</p>
<ul>
<li>写入操作</li>
</ul>
<ol>
<li>首先，将这条KV记录以顺序写的方式追加到log文件末尾</li>
<li>如果写入log文件成功，那么这条KV记录插入内存中的Memtable中</li>
<li>SklipList先查找合适的插入位置，然后修改相应的链接指针将新纪录插入即可</li>
</ol>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/29.png" style="zoom:67%;"></p>
<h4 id="Compation操作"><a href="#Compation操作" class="headerlink" title="Compation操作"></a>Compation操作</h4><ul>
<li>Minor compaction：当内存中的memtable大小到了一定值的时候，将内容保存到磁盘文件中</li>
<li>Major compaction:  对多个文件中的所有记录重新进行排序。也就是Level0向Level1，Level1向Level2的转变过程</li>
</ul>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/30.png" style="zoom:67%;"></p>
<p>在最本章的最后一节我们会详细学习两种操作的具体内容。</p>
<h3 id="LevelDB-的-SSTable结构"><a href="#LevelDB-的-SSTable结构" class="headerlink" title="LevelDB 的 SSTable结构"></a>LevelDB 的 SSTable结构</h3><p>上面整体整体了一下LSMT中的SSTable，现在我们来看一下其内部结构：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/31.png" style="zoom:67%;"></p>
<p>从逻辑布局角度看：可以将<code>.sst</code>文件划分为数据存储区和数据管理区，数据存储区存放实际的kv数据，数据管理区则提供一些索引指针等管理数据，目的是更快速便捷地查找相应记录。</p>
<ul>
<li>DataBlock: 存储 Key-Value记录，分为data,type, CRC 三部分</li>
<li>MetaBlock：索引DataBlock的元数据块</li>
<li>MetaBlock_index： 记录 Filter的相关信息</li>
<li>IndexBlock：描述一个DataBlock，存储着对应DataBlock的最大Key值，DataBlock在<code>.sst</code>文件中的偏移量和大小</li>
<li>Footer：索引的索引，记录IndexBlock和MetaIndexBlock在SSTable中的<strong>偏移量</strong>和<strong>大小</strong></li>
</ul>
<h4 id="Footer"><a href="#Footer" class="headerlink" title="Footer"></a>Footer</h4><p>我们先从索引的索引Footer说起，它记录IndexBlock和MetaIndexBlock在SSTable中的偏移量和大小。其结构如下：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/32.png" style="zoom:67%;"></p>
<p>Footer位于SSTable的尾部，占用48个字节。 其中，最主要的就是<code>metaindex_handle</code>和<code>index_handle</code>两个信息。</p>
<p><code>metaindex_handle</code>与<code>index_handle</code>物理上占用了40个字节。每一个handle的结构如右图，逻辑上就是offset+size(大小和偏移量)，在内存中占用16个字节。因此，两个handle的实际存储可能不到32个字节，剩下是一些padding填充。</p>
<h4 id="DataBlock"><a href="#DataBlock" class="headerlink" title="DataBlock"></a>DataBlock</h4><p>数据块主要分为<strong>数据、重启点、尾部</strong>三个部分</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/33.png" style="zoom:67%;"></p>
<ul>
<li>每个Record的格式如下所示：</li>
</ul>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/34.png" style="zoom:67%;"></p>
<p>我们发现，里面有key共享长度和key非共享长度，这是为什么？</p>
<p>事实上LevelDB做了一些存储上的优化：在DataBlock中，key是有序存储的，相邻的key之间是可能有重复的，因此存储时采用前缀压缩，后一个key只存与前一个key不同的部分。因此出现了共享长度和非共享长度。比如说，Record1是 abcd，Record2是abce，那么共享长度就是abc, 非共享长度就是d和e。</p>
<ul>
<li><p>重启点我们看到是一个数组的形式，其指向的位置就表示该key不按前缀压缩，而是完整存储的key值。除了减少压缩空间之外，重启点的第二个作用就是加速读取，<strong>更快的读取</strong>！一般来说一个Block大小为4K，里面可能包含了很多Record。重启点可以通过二分的方法来定位具体的重启点位置，进一步减少了需要读取的数据。如果不设重启点，那么将Block读进内存之后，要找一个特定的record只能通过顺序搜索实现，影响性能</p>
</li>
<li><p>尾部主要分为type和crc。这里的CRC即循环冗余校验码 ( Cyclic Redundancy Check ）：是数据通信领域中最常用的一种查错校验码，其特征是信息字段和校验宇段的长度可以任意选定。循环元余检查（CRC）是一种数<br>据传输检错功能，对数据进行多项式计算，并将得到的结果附在帧的后面，接收设备也执行类似的算法 ，<strong>以保证数据传输的正确性和完整性</strong>。在Sstable中， 写数据时生成CRC， 读时进行校验</p>
</li>
</ul>
<h4 id="Data-Index-Block"><a href="#Data-Index-Block" class="headerlink" title="Data Index Block"></a>Data Index Block</h4><p>因为一个DataBlock的大小只有4KB，但是一个SSTable的大小可能有16MB，因此里面包含了许多DataBlock。将其全部加载到内存里并顺序找过来肯定是不现实的。 因此SSTable中设计了Data Index Block,然后用来实现对DataBlock的二分查找。在查找时，先将Data Index Block其加载到内存中，然后再去磁盘中读取想要的DataBlock</p>
<p>因为我们在DataBlock中，Key都是按照顺序存储的。理论上，只要保存了每个块中最大的Key，就可以实现二分查找，但是LevelDB在这里做了个优化：</p>
<p>它并非保存最大的Key，而是保存了一个能分隔两个DataBlock的<strong>最短Key</strong>。比如，假定DataBlock1的最后一个Key为 <code>abcdefg</code> ,  DataBlock2的第一个Key为 <code>abzxcv</code> , 那么，index就可以记录DataBlock1的索引Key为：<code>abd</code>。这样的分割串可能有很多，只要保证DataBlock1中的所有key都小于等于此索引，DataBlock2中所有Key都大于此索引即可。<strong>这种优化缩减了索引长度，查询时可以有效减少比较次数</strong></p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/35.png" style="zoom:67%;"></p>
<p>我们再举一个例子，如上图，DataBlock1 的最后一个索引是world, DataBlock2的第一个索引是www，我们可以令DataBlock1的索引为 word ，因为 <code>world&lt;word&lt;www</code></p>
<p> 最后，我们将上面讲的综合起来，写成一个细致的SSTable结构：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/36.png"></p>
<h3 id="SSTable文件管理"><a href="#SSTable文件管理" class="headerlink" title="SSTable文件管理"></a>SSTable文件管理</h3><p>在这一块我们来介绍文件合并的相关流程。</p>
<h4 id="文件管理模块"><a href="#文件管理模块" class="headerlink" title="文件管理模块"></a>文件管理模块</h4><p>首先，我们要知道<strong>每个SSTable</strong>都是一个文件。因此有一个文件信息的结构 <code>FileMetaData</code></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">FileMetaData</span> &#123;</span></span><br><span class="line">    <span class="keyword">int</span> refs:								</span><br><span class="line">    <span class="keyword">int</span> allowed seeks:			<span class="comment">// Seeks allowed until compaction</span></span><br><span class="line">    uint64 + number;</span><br><span class="line">    uint64t file <span class="built_in">size</span>;			<span class="comment">// File size in bytes</span></span><br><span class="line">    Internalkey smallest;		<span class="comment">// Smallest internal key served by table</span></span><br><span class="line">    Internalkey largest;		<span class="comment">// Largest internal key served by table</span></span><br><span class="line"></span><br><span class="line">	FileMetaData() : refs(0), allowed seeks (1 &lt;&lt; 30), file size(0) &#123; &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>文件在写入的时候会存在许许多多个版本，因此一个可以进行文件管理的模块是非常重要的</p>
<p>我们引入Version，Version Edit 和Version Set 三个概念：</p>
<h5 id="Version"><a href="#Version" class="headerlink" title="Version"></a>Version</h5><p>每次的文件融合，都会使得SSTable发生变化，每个SSTable的状态，我们称之为一个Version.</p>
<p>Version包含了什么东西？其实，只要记录当前版本的文件信息即可：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span> &lt;FileMetaData*&gt; <span class="comment">//有几层数组就有多长</span></span><br><span class="line">files_[<span class="built_in">config</span>::kNumLevels];</span><br></pre></td></tr></table></figure>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/38.png"></p>
<p>我们看到，其实version就是存储了文件信息的二维数组。为什么是数组？因为SSTable有好几层，每一层的状态都需要被记录下来。</p>
<h5 id="Version-Edit"><a href="#Version-Edit" class="headerlink" title="Version Edit"></a>Version Edit</h5><p>既然Version是保留文件信息的，那么当SSTable发生变化的时候，就需要这样一个文件来记录变化的内容。这个文件就是Version Edit.</p>
<p>其主要元素 就是：删除的文件，以及新产生的文件</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DeletedFileSet deleted_files_;<span class="comment">//删除的文件</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="built_in">std</span>::pair&lt;<span class="keyword">int</span>, FileMetaData&gt;&gt;new_files_;<span class="comment">//新产生的文件</span></span><br></pre></td></tr></table></figure>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/39.png"></p>
<p>Version之间是通过增量的方式演进的，而两个Version之间的差异就称之为一个Version Edit ： <code>Version0+VersionEdit = Version1</code></p>
<h5 id="Version-Set"><a href="#Version-Set" class="headerlink" title="Version Set"></a>Version Set</h5><p>从旧的到新的所有Version都存储在Version Set当中，所以，Version Set可以理解为一个Version的链表。生成新Version并加入到Version Set的流程示意图如下：</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/37.jpeg"></p>
<p>此外，VersionSet需要做一个<strong>持久化</strong>，因为，如果数据库重新启动或者恢复的时候，没有VersionSet持久化会导致需要将所有的SSTable文件全部遍历一遍，严重降低启动速度。</p>
<h4 id="Minor-Compaction-流程"><a href="#Minor-Compaction-流程" class="headerlink" title="Minor Compaction 流程"></a>Minor Compaction 流程</h4><p>之前我们说过，Minor Conpaction就是当内存中的immutable memtable大小到了一定值的时候，将其保存到SSTable的Level 0中</p>
<p>每次插入操作，都会检查是否memtable超过限制，超过了就会触发函数，进行compaction</p>
<p>合并流程：</p>
<ul>
<li>构建SSTable</li>
<li>寻找合适的Level</li>
<li>生成Version Edit</li>
<li>应用到当前Version</li>
</ul>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/40.jpeg" style="zoom:67%;"></p>
<h4 id="Major-Compaction-流程"><a href="#Major-Compaction-流程" class="headerlink" title="Major Compaction 流程"></a>Major Compaction 流程</h4><p>大合并的过程就是将Level n层合并成Level n+1层的过程。</p>
<p>首先我们要判定什么时候会触发Major Compaction</p>
<ul>
<li>某个Level的 SSTable太多</li>
<li>某个Levl的某个SSTable不成功seek次数太多。</li>
</ul>
<p>现在开始进行合并，需要确定参与compaction的文件。怎么理解呢？比如说下面这个SSTable，Level1中间的.sst 触发了大合并，和这个文件相关的，是level2中的两个.sst文件，这两个文件和Level1有交集。</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/41.png"></p>
<p>合并文件结束的时候，需要更新Version</p>
<h2 id="并发索引"><a href="#并发索引" class="headerlink" title="并发索引"></a>并发索引</h2><p>在数据库中有两处需要并发：1. 上层的事务的并发2. 数据层面的索引并发。这两者是没有关联的。</p>
<h3 id="Concurrent-Linklist"><a href="#Concurrent-Linklist" class="headerlink" title="Concurrent Linklist"></a>Concurrent Linklist</h3><p>我们用的数据结构就是有序列表， 假定有序链表不允许重复元素。</p>
<p>我们要实现3种方法的并发：</p>
<ul>
<li><code>add(x)</code> 增加一个元素</li>
<li><code>remove(x)</code> 删除一个元素 </li>
<li><code>contains(x)</code> 查找一个元素</li>
</ul>
<p>在链表中，每个节点包括：</p>
<ul>
<li>Key</li>
<li>一个指向后面节点的指针 next</li>
</ul>
<h4 id="并发控制的异常"><a href="#并发控制的异常" class="headerlink" title="并发控制的异常"></a>并发控制的异常</h4><p>要实现并发控制常常从可能出现的异常出发。常见的异常就是：</p>
<ul>
<li>丢失更新</li>
</ul>
<p>丢失更新常常发生在两个插入操作之间的，比如下图，6号先插入，但是后来插入的7号把六号给覆盖了，那么此时就发生了丢失更新</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/43.png"></p>
<ul>
<li>假删除</li>
</ul>
<p>假删除发生在两次remove的时候,如下图，同时删除3和5的情况。</p>
<p>此时对于节点5来说，Prev指针指向3，cur指针指向5；对于节点3来说，Prev指针指向1，cur指针指向5。</p>
<p>如果先删掉5，那么就让 <code>prev-&gt;next = cur-&gt;next</code></p>
<p>在并发时删掉3的话，由于此时对于3号节点的cur先于删掉节点5前获得，因此在执行<code>prev-&gt;next = cur-&gt;next</code> 的时候还是会指向节点5。也就是说5号节点被恢复了。</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/44.png"></p>
<ul>
<li><p>插入节点被删除。</p>
<p>比如我想删除b然后插入c. 但此时，由于b已经删掉了，而c在插入时是连接在b上面的，因此相当于c也被删掉了。</p>
</li>
</ul>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/45.png"></p>
<h4 id="并发控制的方法"><a href="#并发控制的方法" class="headerlink" title="并发控制的方法"></a>并发控制的方法</h4><p>寻求并发控制方法就是防止上述异常的发生。通常来说有这么几种方法：</p>
<h5 id="粗粒度锁"><a href="#粗粒度锁" class="headerlink" title="粗粒度锁"></a>粗粒度锁</h5><p>粗粒度锁就是对add(), remove(), contains()三个操作 加锁，锁住整个数据结构</p>
<ul>
<li>访问链表时加锁，操作完成后释放锁 </li>
<li>最安全  </li>
<li>效率最差</li>
</ul>
<h5 id="细粒度锁-hand-over-hand-locking-或-lock-coupling"><a href="#细粒度锁-hand-over-hand-locking-或-lock-coupling" class="headerlink" title="细粒度锁(hand over hand locking 或 lock coupling)"></a>细粒度锁(hand over hand locking 或 lock coupling)</h5><p>细粒度锁其实这是用一对锁。链表向前推进的过程中，先获取 curr锁，然后再释放prev锁 。数据操作时，<strong>同时锁住前驱和后继</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">head.lock();</span><br><span class="line">pred = head:</span><br><span class="line"><span class="keyword">try</span>&#123;</span><br><span class="line">		curr = pred.next;</span><br><span class="line">		curr.lock();</span><br><span class="line">		<span class="keyword">try</span>&#123;</span><br><span class="line">      <span class="comment">// 推进过程</span></span><br><span class="line">      <span class="keyword">while</span>(curr.key &lt; key)&#123;</span><br><span class="line">          pred.unlock();</span><br><span class="line">          pred = curr;</span><br><span class="line">          curr = curr.next;</span><br><span class="line">          curr.lock();</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="keyword">if</span>(curr.key == key) &#123;</span><br><span class="line">		    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//插入操作</span></span><br><span class="line">    NodeWithLock&lt;T&gt; node = <span class="keyword">new</span> NodeWithLock&lt;T&gt;(item);</span><br><span class="line">    node.next = curr:</span><br><span class="line">    pred.next = node;</span><br><span class="line">    <span class="comment">//插入操作停止</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;<span class="keyword">finally</span>&#123;</span><br><span class="line">		    curr.unlock();</span><br><span class="line">		&#125;</span><br><span class="line">&#125;<span class="keyword">finally</span>&#123;</span><br><span class="line">	pred.unlock();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>比如说我想在节点8和节点10之间加入节点9。那么首先curr指针会指向节点10，pred节点会指向8。在停止推进时，pred和curr都没有解锁。只有当插入操作停止值之后，才依次放开两个锁</p>
<p>使用了这种细粒度锁，可以避免上述异常的发生，我们以并发删除两个节点为例：</p>
<ol>
<li>首先，<code>remove(c)</code> 先操作，锁住哨兵节点</li>
</ol>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/54.png"></p>
<ol>
<li><code>remove(c)</code> 向前推进，同时释放前面的锁。这里红色方锁住了b和a。与此同时，remove(b) 代表的绿色方也开始遍历链表。</li>
</ol>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/47.png"></p>
<ol>
<li>绿色方继续遍历链表，要删除b必须获取b节点上的锁，但是该锁现在在红色方手中。因此绿色方只能等待</li>
</ol>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/48.png"></p>
<ol>
<li>红色方继续遍历链表，获取了b节点和c节点上的锁</li>
</ol>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/49.png"></p>
<p>从这里也可以看出，如果只锁住了一个节点是行不通的。加入红方只锁了c，前面b没有锁，那么绿色方就可能获取到b的锁。那么此时红方和绿方的删除操作就可以同时进行了</p>
<p>如果我们只锁了b，那么如果存在另外一个删除操作transact2想要删除d节点，transct2就会锁上C。如果tran2先执行，那么它会让 <code>c-&gt;next = d-&gt;next</code> 也就是e节点；然后再执行我们的删除c操作：<code>b-&gt;next = c-&gt;next</code> ,也就是d节点。事实上，最终的结果并不是 b节点指向e节点，而是b指向d，c指向e。</p>
<ol>
<li>红色方执行删除操作</li>
</ol>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/50.png"></p>
<ol>
<li>红色方完成删除操作，释放锁，因此绿色方可以继续遍历</li>
</ol>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/51.png"></p>
<ol>
<li>绿色方获得了a和b节点的锁，可执行删除操作</li>
</ol>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/52.png"></p>
<ol>
<li><p>删除完成</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/53.png"></p>
</li>
</ol>
<h5 id="乐观锁"><a href="#乐观锁" class="headerlink" title="乐观锁"></a>乐观锁</h5><p>之前的Lock Coupling需要不停的去获得和释放锁 </p>
<p>那么我们可不可以这样？<strong>只在需要加锁的时候再加锁</strong></p>
<ol>
<li>只有在寻找到要加锁位置的时候才加锁，之前不加锁 </li>
<li>需要加锁时，先加锁，再进行验证是否现场已经被修改 </li>
<li>如果验证失败就需要从头开始重试</li>
</ol>
<h5 id="Lazy-List"><a href="#Lazy-List" class="headerlink" title="Lazy List"></a>Lazy List</h5><p>对于乐观锁，虽然很乐观但还是需要加锁，有可能在验证时失败了需要重试，但是对于Lazy List，它使用<strong>标记删除 </strong>法</p>
<p>标记删除法的意思就是先做逻辑标记，然后在标记后进行删除。</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/56.png"></p>
<h5 id="无锁编程"><a href="#无锁编程" class="headerlink" title="无锁编程"></a>无锁编程</h5><p>这几种方法并发能力越来越强，但是实现起来复杂度也越来越高</p>
<h3 id="Concurrent-Index"><a href="#Concurrent-Index" class="headerlink" title="Concurrent Index"></a>Concurrent Index</h3><p>现在我们来看看B+树上并发控制的一些技术。</p>
<ul>
<li><p>正确的读操作：</p>
<ul>
<li><p>Read Case1:  <strong>不会读到一个处于中间状态的键值对</strong>：读操作访问中的键值对正在被另一个写操作修改 </p>
</li>
<li><p>Read Case2: <strong>不会找不到一个存在的键值对</strong>：读操作正在访问某个树节点，这个树节点上的键值对同时被另一个写操作（分裂/ 合并操作）移动到另一个树节点，导致读操作没有找到目标 键值对 </p>
</li>
</ul>
</li>
<li><p>正确的写操作： </p>
<ul>
<li>Write Case1 两个写操作不会同时修改同一个键值对</li>
</ul>
</li>
</ul>
<p>读和写的第一种情况是常见的读写冲突和写写冲突，如果是单版本存储，那么几乎任何时候都会存在。但是对于读的第二种情况，是B+树特殊的并发控制需求。</p>
<p>场景： 考虑一个读操作刚拿到一个叶子节点的指针，打算访问这个节点后半部分的数据，此时一个写操作发生使 得节点分裂，那么后半部分数据将分裂到另一个节点上， 因此读操作的指针无法获得目标数据。<br>这时候，简单的对数据加锁无法保证R2的需求。</p>
<p>如下图，P1是一个查找操作，需要找的内容是15；P2是一个插入操作，需要插入9。这两者毫不相关，但是在某些情况下却会造成冲突。</p>
<p>在P1已经拿到了指针，需要读取15的时候，却被P2操作“捷足先登”了，但是因为插入了9号，导致下一层节点发生裂变。这时候原指针指向的区域就没有15了，因为15到可新的孩子节点中去了。</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/57.png"></p>
<p>思路和之前的有向链表是类似的。</p>
<h5 id="粗粒度锁："><a href="#粗粒度锁：" class="headerlink" title="粗粒度锁："></a>粗粒度锁：</h5><p>对于整个索引上锁(对整个B+树数据结构上锁)</p>
<h5 id="细粒度锁"><a href="#细粒度锁" class="headerlink" title="细粒度锁"></a>细粒度锁</h5><p>不再对整个索引加锁，使用节点粒度的S/X锁(S锁：共享锁，用于读操作；X锁: 排他锁，用于写操作)</p>
<ul>
<li>写操作</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">current &lt;= root</span><br><span class="line">XL(current)</span><br><span class="line">While current is not leaf <span class="keyword">do</span> &#123;</span><br><span class="line">    XL(current-&gt;son)</span><br><span class="line">    current &lt;= current-&gt;son</span><br><span class="line">    If current is safe <span class="keyword">do</span> &#123; <span class="comment">//如何判断？</span></span><br><span class="line">        <span class="comment">/* Unlocked ancestors on stack. */</span></span><br><span class="line">        XU(locked ancestors)</span><br><span class="line">    &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="comment">/* Already lock the modified branch. */</span></span><br><span class="line"> Modify the leaf and upper nodes</span><br><span class="line"> XU(current) <span class="function">and <span class="title">XU</span><span class="params">(locked ancestors)</span></span></span><br></pre></td></tr></table></figure>
<p>其实这个操作和链表中的细粒度锁就很像了，在链表中是前后节点上锁，在B+树中就是对父子节点上锁</p>
<ul>
<li>读操作</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Algorithm4. 读操作 */</span></span><br><span class="line">current &lt;= root</span><br><span class="line">SL(current)</span><br><span class="line">While current is not leaf <span class="keyword">do</span> &#123;</span><br><span class="line">    SL(current-&gt;son)</span><br><span class="line">    SU(current)</span><br><span class="line">    current &lt;= current-&gt;son</span><br><span class="line">&#125;</span><br><span class="line">Read the leaf node</span><br><span class="line">SU(current)</span><br></pre></td></tr></table></figure>
<h5 id="乐观读"><a href="#乐观读" class="headerlink" title="乐观读"></a>乐观读</h5><p>使用一个节点的版本标记判断节点是否更新 – 减少R1带来的读操作带来的锁或latch开销.</p>
<p>B+树乐观读和链表中的乐观读</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">procedure <span class="title">traverse</span><span class="params">(root, key)</span></span>&#123;</span><br><span class="line">		node = root;</span><br><span class="line">    <span class="keyword">while</span> (node is not a leaf)&#123;</span><br><span class="line">        t = turn-off-LSB (node.ccinfo);</span><br><span class="line">        next = <span class="function">find <span class="title">next</span> <span class="params">(node, key)</span></span></span><br><span class="line"><span class="function">        <span class="title">if</span> <span class="params">(node.ccinfo = t)</span> node </span>= next</span><br><span class="line">  &#125;</span><br><span class="line">		<span class="keyword">return</span> node ; <span class="comment">//判断节点是否被修改过</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="在多核系统下加锁方式效率低"><a href="#在多核系统下加锁方式效率低" class="headerlink" title="在多核系统下加锁方式效率低"></a>在多核系统下加锁方式效率低</h4><p>在多核系统下，CPU的性能是主导的，IO性能反而没那么重要. 因为多核情况下每个核有缓存，每次加锁解锁都需要读入内存中去。而且每个核的缓存中的变量需要要求保持一致，因此，会出现 Cache Coherence的问题, 由此导致CPU效率低下。</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/58.png"></p>
<p>繁加锁操作在多核处理器上会产生Coherence Cache Miss过高的问题。以上图为例， 假设有4个处理器（p1/p2/p3/p4），每个处理器分别有自己的private cache （c1/c2/c3/c4）。假设有4个线程（p1/p2/p3/p4），与处理器一一绑定。下文中的 n1/n2/n3/n4/n5/n6/n7可以指的是树节点的锁，也可以指代树节点。为什么频繁加 锁会引入较高的Coherence Cache Miss开销？ </p>
<ul>
<li>p1访问树节点n1/n2/n4，然后将它们放在缓存c1； </li>
<li>p2访问树节点n1/n2/n5，然后将它们放在缓存c2； </li>
<li>p2修改的S锁会导致缓存c1中的n1/n2失效； </li>
<li>注意即使缓存c1中有足够大的空间，这些缓存缺失操作依然会发生；</li>
<li>p3访问树节点n1/n3/n6，然后导致缓存c2中的n1失效；</li>
<li>p4访问树节点n1/n3/n7，然后导致缓存c3中的n1/n3失效；</li>
</ul>
<h4 id="消除R2所带来的加锁-Blink-Tree"><a href="#消除R2所带来的加锁-Blink-Tree" class="headerlink" title="消除R2所带来的加锁-Blink Tree"></a>消除R2所带来的加锁-Blink Tree</h4><ul>
<li>这个Blink Tree和普通B+树不同，在每个节点添加一个 右指针，指向右兄弟节点</li>
<li>一旦search的key比节点最高值大，则使用右指针访问右兄弟而不是孩子 ，可以在这一层中查找。</li>
<li>假定节点写操作是原子的，那么search操作无需加锁</li>
</ul>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/59.png"></p>
<h4 id="SkipList上的并发控制问题"><a href="#SkipList上的并发控制问题" class="headerlink" title="SkipList上的并发控制问题"></a>SkipList上的并发控制问题</h4><p>我们仅关注插入和查找这两个问题。</p>
<p>首先我们讨论并发操作对数据结构本身的正确 性是否会产生破坏</p>
<ul>
<li><p>插入/插入操作 </p>
<ul>
<li>丢失更新的问题 </li>
</ul>
</li>
<li><p>查找/插入操作 </p>
<ul>
<li>插入一个数据，有没有可能让另一个已经存在数 据读取失败</li>
</ul>
</li>
</ul>
<p>其次，考虑查找/插入操作的一致性</p>
<p>比如说，在下面这个跳表中，我们插入17，有无可能让19或者12的查找失败？</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/60.png"></p>
<p>那就要看，新插入的节点的指针，是从下层往上层更新还是从上层往下层更新了。</p>
<p>如果是从上往下更新。那么查找12的时候，还是能找到的。</p>
<p>但是如果需要查找19，此时17的第二层指针已经更新，因此判断出19应该在17-25之间，因此下沉继续查找。但是下层指针还没有更新，那么就找不到19了。</p>
<p>如果从下往上更新，那么12、19都是可以找到的。</p>
<p>我们在来看看插入、查找操作的一致性问题。如果考虑插入17和查找17两者的并发，那么理论上，线性一致性可以原子化，称为时间轴上的一个点，以timestamp的形式存储。</p>
<p>假设插入17的timestamp是A, 查找是B </p>
<ul>
<li>如果A&gt;B , 那么查找17将返回找不到 </li>
<li>如果A&lt;B, 那么查找17能找到</li>
</ul>
<p>假定我们定插入/查找的timestamp分配时 间是他们操作开始的时候，是否能做到线性一致？</p>
<ul>
<li><p>如果A&gt;B, 一定能做到，因为可通过版本号直接判断 </p>
</li>
<li><p>如果A&lt;B(插入在前)，有可能无法保证，因为实际插入操作可能迟迟没有发生</p>
</li>
</ul>
<p>因此，上述定timestamp的方式无法做到线性一致</p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="SnapShot"><a href="#SnapShot" class="headerlink" title="SnapShot"></a>SnapShot</h3><p>我们需要了解SnapShot的概念：即系统在某个时间点的一个<strong>全量状态</strong>。比如我需要出2011的资产负债表，这需要数据保持在2011年12月31日零点时的状态。</p>
<p>快照,其本质类似于数据库的照片,也就是在某个特定时间点(创建快照的时间点)给数据库拍个照放在那儿.但是这个照片是一个新的数据库,可以应用SQL语句. </p>
<p>快照数据库里的数据是不变的.创建快照后,系统会对原数据库的所有数据页做个标识,如果数据页在创建快照后被修改,会复制一个数据页出来,没有修改的数据页则不会有快照(原数据库和快照数据库共用该数据页).</p>
<p>从这样来看,快照存在的时间越长,对系统的压力会越大(要维护的变化数据页太多). </p>
<h3 id="Memory-Allocation"><a href="#Memory-Allocation" class="headerlink" title="Memory Allocation"></a>Memory Allocation</h3><p>在系统实现的时候还需要考虑<strong>分配内存</strong>这个重要因素。</p>
<p>数据库里的内存分配和算法题中的内存分配完全不一样，通常会有一个完整的内存管理模块来统一管理内存。这个模块会一次申请大块的内存，多次分给客户，这样可以避免new/delete多次的申请和释放引起的内存碎片。</p>
<p>在一个内存管理模块中，通常需要两个函数：Allocate函数和AllocateAligned函数</p>
<ul>
<li>Allocate函数</li>
</ul>
<p>如下所示，管理器会先申请一大块内存，默认每次申请的大小为4KB，同时记录下剩余指针和剩余内存字节数量。这样可以让申请到的内存在一个连续的空间里面。</p>
<p>然后，每当有函数向管理器发送申请，如果当前剩余的字节能满足需要，则直接返回给用户；如果不能，对于超过1KB的请求，直接new返回(左边第二个红块)，小于1KB的请求，管理器则会向内存申请一个新的4KB块并从中分配一部分给用户。</p>
<p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/42.png"></p>
<ul>
<li>AllocateAligned函数</li>
</ul>
<p>这个函数的作用就是做到地址对齐，让内存首地址是4或者8的倍数</p>
<h3 id="Bloomfilter"><a href="#Bloomfilter" class="headerlink" title="Bloomfilter"></a>Bloomfilter</h3><p>可参考这篇文章： <a href="https://jasonxqh.github.io/2021/09/24/数据科学与工程算法-哈希/#布隆过滤器">数据科学与工程算法-哈希：布隆过滤器</a></p>
<h3 id="Second-Index"><a href="#Second-Index" class="headerlink" title="Second Index"></a>Second Index</h3><p>什么是二级索引？二级索引就是定义在非主键上的索引，这在数据库中有很广泛的应用。</p>
<p>二级索引的作用是什么？为非主键提供数据访问的有效路径</p>
<p>二级索引的几种选择：可以是B+树，Block Index等</p>
<p>其实有一种更简单的方式：把二级索引看做是一张特殊的表——索引表</p>
<ul>
<li>比如说有数据表&lt;主键，非主键列1，非主键列2，非主键列3&gt;, 我们要在非主键列1上构建二级索引。我们就可以建一张索引表：&lt;非主键列1(作为主键)，主键列(作为 非主键)&gt;。这样，非主键列1自动就会有索引效果了。</li>
<li>当我们要查找非主键列的时候，先查询索引表，然后依靠索引表上的主键去找到主键列对应的数据，再根据主键列的值去原本的数据表中查找。</li>
</ul>
<p><strong>回表查询VS非回表查询</strong></p>
<ul>
<li>非回表查询：<strong>只查询索引表</strong>就能得到想要的结果。比如说只查找非主键列1</li>
<li>回表查询：<strong>需要通过索引表</strong>再返回数据表查询才能得到查询结果。比如说根据非主键列1去找到非主键列2</li>
</ul>
<h4 id="二级索引的更新"><a href="#二级索引的更新" class="headerlink" title="二级索引的更新"></a>二级索引的更新</h4><ul>
<li>同步更新<ul>
<li>在主表更新时一起更新</li>
</ul>
</li>
<li>异步更新<ul>
<li>在主表更新之后，查询之前更新索引表</li>
</ul>
</li>
</ul>
<p>这就引入了一致性问题，通常大多数数据库都是采用同步更新的方式。因此索引越多，数据库写入的速度也就会越慢</p>
<h3 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h3><p>压缩在数据库中是很常见的技术。可以参考博客：<a href="https://devopedia.org/database-compression#:~:text=While most compression algorithms are,techniques used in database applications." target="_blank" rel="noopener">Compression</a></p>
<p>压缩可以通过两种主要方式实现：</p>
<ul>
<li><strong>无损</strong>：可以从压缩数据中完全重建原始数据。</li>
<li><strong>有损</strong>：由于故意降低质量而减小数据</li>
</ul>
<p>我们用几个问题来学习一下数据库中的压缩技术</p>
<h4 id="数据库压缩有什么好处"><a href="#数据库压缩有什么好处" class="headerlink" title="数据库压缩有什么好处?"></a>数据库压缩有什么好处?</h4><p>大多数压缩算法都会构建一个<strong>内部编码字典</strong>来管理压缩关键字。<strong>当数据库大小较小</strong>时，由于创建了额外的字典，压缩文件可能比未压缩文件大。</p>
<p>对于任何数据库，<strong>压缩和解压缩都是</strong>高于其常规 DML/DDL 操作的任务开销，它消耗额外的 CPU/内存。所以必须是只有当优化页面读取所带来的CPU/内存收益远远大于压缩开销时，才可以尝试压缩。</p>
<p>虽然压缩可能作为后台任务并行发生，但<strong>解压缩会引入客户端延迟</strong>，因为它在客户端查询后会在前端进行解压缩。但是，像 Oracle 这样的商业数据库支持高级压缩技术，无需解压缩即可读取数据库。</p>
<p><strong>数据基数较差时</strong>不推荐压缩。对于数字数据和非重复字符串，不要压缩数据。对于数据<code>BLOB</code>（图像、音频）等数据类型，根据压缩算法，存储大小可以减少或增加。</p>
<p>总之，在实时数据库上实施之前，估计任何算法可能的存储和性能优势非常重要。</p>
<h4 id="压缩对常见数据类型有什么影响"><a href="#压缩对常见数据类型有什么影响" class="headerlink" title="压缩对常见数据类型有什么影响?"></a>压缩对常见数据类型有什么影响?</h4><p>压缩是一种 DDL 功能，可以使用<code>CREATE</code>,<code>ALTER</code>和<code>BACKUP</code>命令选择性地应用于表、索引或分区。</p>
<p>数据压缩适用于这些数据库对象——堆、聚集索引、非聚集索引、分区、索引视图。</p>
<p>行级压缩将固定长度数据类型转换为可变长度类型。因此，这适用于固定长度的文本和数字字段 ( <code>char</code>, <code>int</code>, <code>float</code>)。</p>
<ul>
<li>例如，将 23 存储在一<code>int</code>列中，压缩时只需要 1 个字节，而不是分配的全部 4 个字节。</li>
<li>char(100) 也不会占用100个字符空间</li>
<li>NULL 或 0 值不占用空间。</li>
</ul>
<h4 id="有哪些流行的数据库压缩技术"><a href="#有哪些流行的数据库压缩技术" class="headerlink" title="有哪些流行的数据库压缩技术?"></a>有哪些流行的数据库压缩技术?</h4><p>数据库通常避免在后端进行有损压缩。在无损技术中，所有压缩数据都以二进制形式存储。</p>
<ul>
<li>Run-Length Encoding。在这种技术中，会扫描顺序数据以查找重复符号，例如图像中的像素，并用称为“运行”的短代码替换。比如说，对于灰度图像， run-length code 可以表示为 ${S_i,C_i}$,其中<code>Si</code>是像素的符号或强度，<code>Ci</code>是 的计数或重复次数<code>Si</code>。</li>
<li>Prefix Encoding。之前我们在SSTable中介绍过这种方式，是另一种<strong>仅适用于同类数据的基本技术</strong>。数据字段的第一部分匹配通用性，而不是整个字段。因此我们将相同的前缀存储起来，做为编码。</li>
<li>Dictionary compression。字典压缩是另一种应用于页面的压缩技术。该算法计算对列的单个属性进行编码所需的位数 (X)，然后会计算这X位中有多少可以放入1、2、3、4个字节中，进行压缩。</li>
</ul>
<h4 id="数据库压缩与通用数据压缩技术有何不同"><a href="#数据库压缩与通用数据压缩技术有何不同" class="headerlink" title="数据库压缩与通用数据压缩技术有何不同?"></a>数据库压缩与通用数据压缩技术有何不同?</h4><p>通用数据压缩可以使用于字段，比如说，将单个数据项占用的空间最小化；但是，数据库中的压缩方法是一种<strong>aggregate technique(聚合技术)</strong>  ，它是将跨行、跨列、跨页面的数据进行压缩。</p>
<p>除了在数据库级别应用压缩外，在字段级别应用数据压缩也很常见：</p>
<ul>
<li>对于图像压缩，可以使用BMP到JPG的有损压缩。</li>
<li>又如可以用无损方法在DB级别压缩BLOB数据字段。</li>
<li>虽然NULL值无法单独进一步压缩，但是当DB中出现一系列NULL值时，Sparse Column等压缩技术可以共同优化NULL的存储</li>
</ul>
<p>数据库压缩技术不仅压缩实际数据，它们还作用于派生实体，如索引、视图、集群和堆。但是，对于索引等派生实体，在索引查找或扫描期间重建键列值的 CPU 开销很小。这可以通过在块中本地保留前缀来最小化。</p>
<h4 id="关系数据库和分层数据库的压缩技术"><a href="#关系数据库和分层数据库的压缩技术" class="headerlink" title="关系数据库和分层数据库的压缩技术"></a>关系数据库和分层数据库的压缩技术</h4><p>应用于关系数据库和Level DB的压缩算法没有很大的区别，只是命名方式有所不同。</p>
<p>Level DB 具有类似于 RDBMS 中的表的段。段中的字段与列进行比较。</p>
<p>但是，在分层数据库中，段之间是隐式连接的。因此，I/O 分页过程会有所不同，因此算法必须考虑页面大小。</p>
<h4 id="大型软件应用程序如何处理数据库压缩"><a href="#大型软件应用程序如何处理数据库压缩" class="headerlink" title="大型软件应用程序如何处理数据库压缩?"></a>大型软件应用程序如何处理数据库压缩?</h4><ul>
<li>Facebook 使用具有超快速解码器的实时压缩算法<strong>Zstandard</strong>以及用于小数据的字典压缩。</li>
<li>Google 在其 BigTable 和 MapReduce 系统中使用<strong>Snappy</strong>，这是一种高速压缩算法。</li>
<li>像SQL Server这样的商业 DBMS使用行、列和页面级别的压缩。SQL Server 使用专有的<strong>XPRESS</strong>算法进行备份压缩。</li>
<li><strong>Oracle 使用称为Oracle Advanced Compression</strong>的内部压缩技术。</li>
<li>开源 MySQL 使用<strong>LZ77</strong>算法进行 InnoDB 表压缩。</li>
</ul>
<h3 id="Design-Consideration"><a href="#Design-Consideration" class="headerlink" title="Design Consideration"></a>Design Consideration</h3><ul>
<li>Should be aware of the storage hierarchy </li>
</ul>
<p>对存储的层次结构(CPU缓存，内存，磁盘)要有很深刻的概念。因为数据库是数据密集型的应用，而不是计算密集型的应用。如何提高IO性能是我们要研究的方向。</p>
<ol>
<li>Database is usually data-intensive</li>
<li>cache miss is important 缓存命中率是很重要的</li>
</ol>
<ul>
<li>Pay attention to the hardware properties </li>
</ul>
<ol>
<li>Sequential access and random access is usually different</li>
<li>bandwith and IOPS</li>
</ol>
<ul>
<li>Design Issues 设计时的问题</li>
</ul>
<ol>
<li>choose a proper index</li>
<li>support variable length data</li>
<li>for most secondary storage, try avoid random writes</li>
<li>reduce the write amplication</li>
<li>group your writes</li>
</ol>
<ul>
<li>Optimization On LSM-tree 优化LSMT的性能</li>
</ul>
<ol>
<li>optimize write amplication</li>
<li>optimize read performance</li>
<li>other optimizations: cache, special workload</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="/2022/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%AD%98%E5%82%A8/61.png"></p>

          
        
      
    </div>
    
    
    
    <div>
      
    </div>
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2022/03/02/%E5%BD%93%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BDLab1-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/%5Bobject%20Object%5D">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/03/02/%E5%BD%93%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BDLab1-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" itemprop="url">当代人工智能Lab1-文本分类</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-03-02T13:48:16+08:00">
                2022-03-02
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2022-03-08T22:34:06+08:00">
                2022-03-08
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="当代人工智能Lab1-文本分类"><a href="#当代人工智能Lab1-文本分类" class="headerlink" title="当代人工智能Lab1-文本分类"></a>当代人工智能Lab1-文本分类</h1><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><ul>
<li>python3.8</li>
<li>Tensorflow-text 2.8.*</li>
<li>Tf-models-offiial 2.7.0</li>
</ul>
<h2 id="模型选择与构建"><a href="#模型选择与构建" class="headerlink" title="模型选择与构建"></a>模型选择与构建</h2><h3 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h3><p>这里我选择的是BERT模型，它是最近几年比较流行的用来做自然语言处理的模型。BERT主要采用了双向编码器架构，因此相较于RNN，LSTM模型来说，有着更强的并发能力、能在多个不同层次提取关系特征，进而更全面地反映出句子的语义。</p>
<p>此外，BERT对TensorFlow的支持也很全面(都是谷歌开发的)，甚至可以直接通过API来下载预训练模型。因此只要选择合适的模型与环境，就能迅速地实现数据训练，事实上我们所做的就是对预训练模型中的权重进行微调。</p>
<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>首先我们拿到的训练数据是 json格式的文档。为了能让Pandas、TensorFlow导入方便,我们需要将其转换为csv文件。因为python比较缺乏成熟的自动化转换库，因此我们采用了比较偷懒的方法——把json导入mongodb，再以csv的格式导出。</p>
<p>此外，测试集由于给的是txt格式的文档，这里我用了python程序将其转换为一个csv文件。</p>
<p>这两个文件分别为<code>lab1_test.csv,lab1_train.csv</code> ，在根目录下可以找到。</p>
<p><img src="/2022/03/02/%E5%BD%93%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BDLab1-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/2.png"></p>
<p>接下来，我们需要对label进行独热编码，因为我们不能用连续的数字来表示标签，否则会导致语义上的一些差别。可以用<code>tf.keras.utils.to_categorical()</code>这个函数来操作</p>
<p>最后，我们使用<code>train_test_split</code> 对数据集和标签进行 分割，变成训练集和测试集(实际上是验证集)。</p>
<h3 id="模型的构建"><a href="#模型的构建" class="headerlink" title="模型的构建"></a>模型的构建</h3><ul>
<li><p>首先，我们需要用到Bert的文本预处理层: <a href="https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3" target="_blank" rel="noopener">下载地址</a>。这个预处理层从维基百科和BooksCorpus中提取英文单词并可以将每一行输入文字转换为一个<code>shape=(1,128)</code>的向量。这个向量将被作为下一层Transformer Encoder的输入。</p>
</li>
<li><p>然后，我们需要用到Bert的预训练模型：<a href="https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1" target="_blank" rel="noopener">下载地址</a> 。由于我们要处理的数据集的数据量并不是很大(8000条数据)，因此我们选择的是一个小型的Bert模型——具有4个隐藏层，每个隐藏层大小为512；8个注意力头。</p>
<p>这一层有两个输出：<code>pooled_output</code>和<code>sequence_output</code>,前者是句子的向量表示，大小为[batch size,512]; 后者代表句子中每个词语的向量表示，大小为[batch size,seq_length,512]. 在这里，我们选择前者作为下一层的输入。</p>
</li>
<li><p>接下来的Drop out主要是用来正则化的，来让模型简化，减小冗余，我们不希望很多神经元去学习同一个特征，我们希望神经网络中的神经元有自己的“特长”，各自负责各自的特征。这样就可以减少过拟合发生的概率。我们将随机丢失神经元的概率设置为20%</p>
</li>
<li><p>最后，我们添加一个全连接层。由于这个任务要求单标签多分类，一共有10类文本，因此我们要将输出向量设为(1,10)。同时设置激活函数为softmax</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_classifier_model</span><span class="params">()</span>:</span></span><br><span class="line">    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name=<span class="string">'text'</span>)</span><br><span class="line">    <span class="comment"># 文本预处理</span></span><br><span class="line">    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name=<span class="string">'preprocessing'</span>)</span><br><span class="line">    <span class="comment"># Bert模型编码器</span></span><br><span class="line">    encoder_inputs = preprocessing_layer(text_input)</span><br><span class="line">    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=<span class="literal">True</span>, name=<span class="string">'BERT_encoder'</span>)</span><br><span class="line">    outputs = encoder(encoder_inputs)</span><br><span class="line">    net = outputs[<span class="string">'pooled_output'</span>]</span><br><span class="line">    <span class="comment">#Dropout层</span></span><br><span class="line">    net = tf.keras.layers.Dropout(<span class="number">0.2</span>)(net)</span><br><span class="line">    <span class="comment">#全连接层</span></span><br><span class="line">    net = tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>, name=<span class="string">'output'</span>)(net)</span><br><span class="line">    <span class="keyword">return</span> tf.keras.Model(text_input, net)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/03/02/%E5%BD%93%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BDLab1-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/1.png" style="zoom:67%;"></p>
<p>在这里我们就可以利用预处理层和编码器层来查看不同类别的文本之间的相似程度：我选择了来自0、1、2三类各两条数据绘制成热力图。我们发现，相同类之间的颜色确实会更深一些——(1,1)-(2,2)这一个正方形；(3,3)-(4,4)这一个正方形，都比周围的色块要深。因此可以定性得说明经过bert模型编码之后，可以有效地反映出文本之间的相似程度。</p>
<p><img src="/2022/03/02/%E5%BD%93%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BDLab1-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/heatmap.png" alt="heatmap" style="zoom:67%;"></p>
<h3 id="模型的训练"><a href="#模型的训练" class="headerlink" title="模型的训练"></a>模型的训练</h3><h4 id="Optimizer选择与策略"><a href="#Optimizer选择与策略" class="headerlink" title="Optimizer选择与策略"></a>Optimizer选择与策略</h4><p>现在表现较好、较为流行的梯度下降策略就是adam算法了，在这里我没有使用<code>Tensorflow.train.AdamOptimizer</code>   。而使用的是<code>official.nlp</code>中的<code>optimization</code>模块。相比前者，这个模块创建的优化器可以提供更多的功能，而且对Bert模型有良好的支持。</p>
<p>在这个模块中，我们除了需要设置最基本的初始学习率之外，还要输入总的训练步长<code>num_train_steps</code>，以及预热步数<code>num_warmup_steps</code>。学习率预热是指在训练初期选择一个较小的学习率，然后在训练一定步数之后使用预先设置好的学习率。因为刚开始的时候模型是随机初始化权重，如果使用较大的学习率，会让模型不稳定，所以可以在一定训练步数内，使用较小的学习率，模型可以慢慢稳定，然后使用之前设定的学习率。 </p>
<p>把学习率预热和adam算法结合起来，学习率走势如下所示：</p>
<p><img src="/2022/03/02/%E5%BD%93%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BDLab1-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/3.jpeg" style="zoom:67%;"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">epochs = <span class="number">10</span></span><br><span class="line">steps_per_epoch = <span class="number">120</span></span><br><span class="line">num_train_steps = steps_per_epoch * epochs</span><br><span class="line">num_warmup_steps = int(<span class="number">0.1</span>*num_train_steps)</span><br><span class="line">init_lr = <span class="number">3e-5</span></span><br><span class="line">optimizer = optimization.create_optimizer(init_lr=init_lr,</span><br><span class="line">                                          num_train_steps=num_train_steps,</span><br><span class="line">                                          num_warmup_steps=num_warmup_steps,</span><br><span class="line">                                          optimizer_type=<span class="string">'adamw'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="引入早停机制"><a href="#引入早停机制" class="headerlink" title="引入早停机制"></a>引入早停机制</h4><p>为了避免模型过拟合，这里我设置了早停。对val_loss进行观测，如果连续三轮没有降低0.01、或者连续三轮没有降低到0.25以下的话，训练就会自动停止，并将val_loss最低(表现最好)的那轮的结果保存下来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置早停</span></span><br><span class="line">earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor = <span class="string">'val_loss'</span>,</span><br><span class="line">                                                      min_delta=<span class="number">0.01</span>, </span><br><span class="line">                                                      baseline=<span class="number">0.25</span>,</span><br><span class="line">                                                      patience=<span class="number">3</span>,</span><br><span class="line">                                                      restore_best_weights=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h4 id="评价函数的设置"><a href="#评价函数的设置" class="headerlink" title="评价函数的设置"></a>评价函数的设置</h4><p>此外，在训练过程中我们还需要记录一些实时观测的值。也就是设置metrics。首先，我们要看的一个指标就是预测的准确率，即<code>tf.keras.metrics.CategoricalAccuracy</code></p>
<p>此外，我们还要保留召回率(recall)、精确率(precision)、以及 f1值</p>
<p>召回率计算公式：$R = \frac{TP}{TP+FN}$,  TP+FN: 也就是全体完全满足文本标注的文本的条数 ; TP：被预测正确的文本个数</p>
<p>精确率计算公式：$P = \frac{TP}{TP+FP}$ ,TP+FP: 也就是全体被预测为正确的文本个数，TP: 被预测正确的文本个数</p>
<p>F1 值是precision和recall调和平均值：$F_1 = \frac{2TP}{2TP+FP+FN}$</p>
<h4 id="模型的编译"><a href="#模型的编译" class="headerlink" title="模型的编译"></a>模型的编译</h4><p>这里我们要用<code>model.compile()</code>方法, 其作用就是在训练之前告知模型训练时用的优化器、损失函数、准确率评测标准(metrics)。在这里，我们使用多分类的交叉熵作为损失函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">METRICS = [</span><br><span class="line">      tf.keras.metrics.CategoricalAccuracy(name=<span class="string">"accuracy"</span>),</span><br><span class="line">      balanced_recall,</span><br><span class="line">      balanced_precision,</span><br><span class="line">      balanced_f1_score</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">classifier_model.compile(optimizer=optimizer,</span><br><span class="line">                         loss=<span class="string">"categorical_crossentropy"</span>,</span><br><span class="line">                         metrics=METRICS)</span><br></pre></td></tr></table></figure>
<h4 id="模型的训练-1"><a href="#模型的训练-1" class="headerlink" title="模型的训练"></a>模型的训练</h4><p>在训练阶段，我们需要用到<code>model.fit</code> ，除了传入训练集、测试集之外，还需要设置训练集和测试集batch的大小、训练的代数、每一代训练的步数、以及是否开启多核处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">history = classifier_model.fit(x_train,  </span><br><span class="line">                               y_train, </span><br><span class="line">                               validation_data = (x_test, y_test),</span><br><span class="line">                               steps_per_epoch=steps_per_epoch,</span><br><span class="line">                               batch_size=<span class="number">10</span>,</span><br><span class="line">                               validation_batch_size = <span class="number">10</span>,</span><br><span class="line">                               callbacks = [earlystop_callback],</span><br><span class="line">                               use_multiprocessing=<span class="literal">True</span>, <span class="comment"># 使用多核，能让CPU占用率达到90%以上</span></span><br><span class="line">                               epochs=epochs)</span><br></pre></td></tr></table></figure>
<h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><p>模型训练完之后，我们可以对测试集进行预测，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预测函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_class</span><span class="params">(model,reviews)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [np.argmax(pred) <span class="keyword">for</span> pred <span class="keyword">in</span> model.predict(reviews)]</span><br><span class="line"><span class="comment"># 对text进行预测</span></span><br><span class="line">y_pred = predict_class(classifier_model,test_data[<span class="string">"text"</span>])</span><br></pre></td></tr></table></figure>
<p>最后，为了模型能继续使用，我们可以把模型保存下来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">classifier_model.save(<span class="string">"./models/text_classifier_v1"</span>)</span><br></pre></td></tr></table></figure>
<p>如果我们想继续预测别的测试集，可以这样来导入模型，并做训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">new_model = tf.keras.models.load_model(<span class="string">"./models/text_classifier_v1"</span>,</span><br><span class="line">                                       custom_objects=&#123;<span class="string">'balanced_recall'</span>:balanced_recall,</span><br><span class="line">                                                      <span class="string">'balanced_precision'</span>:balanced_precision,</span><br><span class="line">                                                      <span class="string">'balanced_f1_score'</span>:balanced_f1_score,</span><br><span class="line">                                                       <span class="string">'KerasLayer'</span>:hub.KerasLayer,</span><br><span class="line">                                                       <span class="string">'AdamWeightDecay'</span>: optimizer</span><br><span class="line">                                                      &#125;)</span><br><span class="line">[np.argmax(pred) <span class="keyword">for</span> pred <span class="keyword">in</span> new_model.predict(reviews)]</span><br></pre></td></tr></table></figure>
<h2 id="问题与困境"><a href="#问题与困境" class="headerlink" title="问题与困境"></a>问题与困境</h2><ol>
<li>环境的安装</li>
</ol>
<p>首先是TensorFlow那数不清的版本，因为现在TensorFlow1.x已经没啥人用了，但是我原来的python3.7里面装的是TensorFlow1.x的版本。重装以后又发现TensorFlow2.8所依赖的库python3.7并不支持。因此干脆直接新建了一个python3.8的环境专门用来跑项目。</p>
<p>其实性价比更高的方法是创建虚拟环境，管理卸载起来也并不麻烦。但由于我选择在jupyter下跑，因此在anaconda中新建环境更加方便。</p>
<ol>
<li>预处理</li>
</ol>
<p>在一开始我选择的是用<code>tf.data.experimental.make_csv_dataset</code>将csv文件直接转换为OrderedDict，但是我却怎么也不能将其放到模型中进行训练，会报奇奇怪怪的错误。</p>
<p>在调试了两天无果之后，作罢，采用最原始的方式——导入为dataframe之后进行切割，并放到fit函数当中。虽然代码更长了些，但少了很多调试的煎熬。</p>
<h2 id="超参数选择"><a href="#超参数选择" class="headerlink" title="超参数选择"></a>超参数选择</h2><p>由于我一开始就采用了官方文档中的初始学习率3e-5,发现效果就非常好，训练几代以后再验证集上的正确率都能达到90%以上，此外，囿于电脑算力的限制，训练一个模型的时间普遍长达2小时，因此我只是对初始学习率和batch大小进行了微调。</p>
<p>由于调试对模型最终的预测准确率影响不大，我把选择合适超参的重心放在正确率增长速度、正确率达到平稳值所需的时间上。我发现，当调大batch的值以后，每一轮所需要的步数变少了，虽然每步所需的时间有所增长，但总体而言，在内存能负担的情况下，调高batch的值可以有效地加快模型的训练时间。</p>
<p>epoch=10 batch=32 lr = 1e-5 每轮训练时间15-16分钟</p>
<p><img src="/2022/03/02/%E5%BD%93%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BDLab1-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/model1.png" style="zoom:150%;"></p>
<p>Epoch=6 batch=32 lr = 3e-5 每轮训练时间 10-12分钟</p>
<p><img src="/2022/03/02/%E5%BD%93%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BDLab1-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/model2.png" style="zoom:150%;"></p>
<p>Epoch=6 batch = 50   lr = 3e-5  每轮训练9-10分钟</p>
<p><img src="/2022/03/02/%E5%BD%93%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BDLab1-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/model3.png" style="zoom:150%;"></p>
<p>epoch=8 batch = 100 lr = 3e-5每轮训练时间 8-9 分钟</p>
<p><img src="/2022/03/02/%E5%BD%93%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BDLab1-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/model4.png" style="zoom:150%;"></p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>根据图片和一些保存的数据，最后我们选择的是将batch设置为100，初始学习率设置为 3e-5, 训练10轮。模型在训练8轮后由于早停机制停止训练，结果如下图所示。</p>
<p><img src="/2022/03/02/%E5%BD%93%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BDLab1-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/final.png" style="zoom:150%;"></p>

          
        
      
    </div>
    
    
    
    <div>
      
    </div>
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/%5Bobject%20Object%5D">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/" itemprop="url">了解区块链</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-02-24T13:37:21+08:00">
                2022-02-24
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2025-04-11T10:19:05+08:00">
                2025-04-11
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="了解区块链"><a href="#了解区块链" class="headerlink" title="了解区块链"></a>了解区块链</h1><p>部分内容引用自： <a href="https://www.zhihu.com/question/37290469" target="_blank" rel="noopener">https://www.zhihu.com/question/37290469</a></p>
<h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>首先我们要问自己三个问题：</p>
<h3 id="一个去中心化的数字货币系统是什么样的"><a href="#一个去中心化的数字货币系统是什么样的" class="headerlink" title="一个去中心化的数字货币系统是什么样的"></a>一个去中心化的数字货币系统是什么样的</h3><p>说到数字货币，我们首先想到的就是三个最明显的特征：防篡改、可追溯以及去中心化。那么什么是去中心化？简单来说就是P2P(peer to peer)模式，在这个模式中，没有中心式服务器，用户和用户之间是对等的。用户之间采用Gossip协议来传播消息。</p>
<p>这里岔开来简单介绍一下<a href="https://zhuanlan.zhihu.com/p/41228196" target="_blank" rel="noopener">Gossip协议</a>的执行过程：Gossip 过程是由种子节点发起，当一个种子节点有状态需要更新到网络中的其他节点时，它会随机的选择周围几个节点散播消息，收到消息的节点也会重复该过程，<strong>直至最终网络中所有的节点都收到了消息</strong>。这个过程可能需要一定的时间，由于不能保证某个时刻所有节点都收到消息，但是理论上最终所有节点都会收到消息，因此它是一个<strong>最终一致性协议</strong>。</p>
<p>那么，在这样一个去中心化的数字货币系统，在没有国家信用背书的情况下，应该设计什么？</p>
<ul>
<li>第一个问题是开户，在这样一个数字货币系统中开户，需要什么？</li>
</ul>
<p>这就要我们了解公钥和私钥的概念：我们可以把公钥理解为我们的身份证，把私钥理解为要支取货币、进行交易时候需要输入的密码。</p>
<ul>
<li>第二个问题是当没有了一个中心化的服务器，各个结点应该如何存储数据？</li>
</ul>
<p>答案是，每一份节点都需要存储全量的数据。因为当没有中心服务器的时候，如果每个人只存放了部分数据，一些人可能会篡改交易记录，导致违背了防篡改的原则。</p>
<h3 id="一个永不停机的世界计算机系统是什么样的"><a href="#一个永不停机的世界计算机系统是什么样的" class="headerlink" title="一个永不停机的世界计算机系统是什么样的"></a>一个永不停机的世界计算机系统是什么样的</h3><p>既然说，这个系统要永不停机，而且还要是P2P的。就要求这个系统有高可用、高可靠的特性。因此，这个问题的答案和上一个问题有一定的重合——要求每一个节点要有全量的数据。</p>
<p>为了解释清楚为什么要这样做，我们可以说一个故事：</p>
<p><strong>【1：一个公共账本】</strong></p>
<p>假如你现在在上大学，你们寝室是标准的四人寝，除了你之外还有<strong>小王，小黄和小白</strong>三个室友。平时你们亲是内部的活动很多，于是经常会有人垫付饭钱，车费，还有水电费。你们大家发现，如果每次消费后，都要一一计算交结非常麻烦，于是你们决定采用记账的方案。</p>
<p>于是乎，你们买了一个公共的账本，本次产生消费后，就由付钱的人在账本上记清楚，谁应付给自己相应的金额。如此一来，只要每月月末统一结算即可，大大节省了时间精力。</p>
<p>时间一长，你们发现在纸上记账还是麻烦。于是你们决定”升级“这个账本，改成在电脑种建立一个excel表格，比如这个样子： </p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/1.jpeg"></p>
<p>但问题是，如果你们寝室里有个人不厚道，偷偷修改账本怎么办呢？</p>
<p>例如小王把自己要付钱记在了小白头上。如果这个问题不能得以解决，那这个账本的信用就将会大大折扣。</p>
<p><strong>【2：每个人都有账本】</strong></p>
<p>他们采用的解决方案就是，给四个人每人都配备一个账本。需要记录时，就由对应的操作人高喊交易内容，广播给寝室里的所有人。</p>
<p>例如，小王高喊，”<strong>小王需要支付给小白30元  “。</strong>然后寝室里其他人听到了，就在各自的小本本上记下，”小王需要支付给小白30元 “。</p>
<p>如此一来，就算小王故意使坏，把自己要付的钱记在别人身上，那也只能是篡改自己的账本。这样到月底时，小王的账本和其余三个人对应不上，便能知道小王的账本有问题。</p>
<p>从上面这个故事可以很清楚的了解，为什么每一个节点都需要保存全量的数据。但是我们还有一个很重要的问题没有解决：我们知道，在分布式数据库中，为了解决宕机、故障时，需要使用Paxos、raft协议来保证各个分布式数据库的数据不发生错误。但是，在这种区块链系统中，Paxos是否还能达到这种效果呢？</p>
<p>答案是不行的，因为在这个系统里面存在背叛者。也就是经典的拜占庭将军问题——如果小王恶作剧，不负责任的乱喊“<strong>小白需要支付给小王100元</strong>”，如此一来，很可能会有不明真相的舍友记录下来；或者说，付了钱而不喊出来。那么这个账本就乱套了。</p>
<p>因此，可以解决拜占庭容错的方案需要被提出，也就是<strong>数字签名</strong></p>
<p> <strong>【3：在交易记录后签名】</strong></p>
<p>这个问题在纸质帐本里很好解决，那就是在每一条记录后，由需付款的一方加上自己的手写签名，以示自己认可这笔记录。这个思路换到计算机中就是<strong>数字签名，</strong>所以我们要求每一笔记录后面，都要由需付款的一方加上自己的数字签名。</p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/1.jpeg"></p>
<h3 id="元宇宙中支撑虚拟资产交易的基础设施是什么"><a href="#元宇宙中支撑虚拟资产交易的基础设施是什么" class="headerlink" title="元宇宙中支撑虚拟资产交易的基础设施是什么"></a>元宇宙中支撑虚拟资产交易的基础设施是什么</h3><p>在中心化系统中，DBA具有至高无上的权利，但是在去中心化的场景下，是一个无政府主义情况，没有一个绝对的权威。那么，它的基础设施就是我们今天要学习的——区块链。</p>
<h2 id="比特币"><a href="#比特币" class="headerlink" title="比特币"></a>比特币</h2><p>上面这个账本还存在一些问题，使得只适合小范围使用，如果扩大到更大的范围，比如整个学校使用，这个时候交易量和用户数剧增，记录就会变得非常麻烦。</p>
<p>比特币的正是为了解决这个问题而对前面介绍的账本系统的改进。</p>
<h3 id="改进一：交易单位为比特币BTC"><a href="#改进一：交易单位为比特币BTC" class="headerlink" title="改进一：交易单位为比特币BTC"></a>改进一：交易单位为比特币BTC</h3><p>我们之前用的账本里面的交易单位是人民币，但在比特币系统中，我们的交易单位变为比特币。</p>
<h3 id="改进二：将记账改为事实交易支付"><a href="#改进二：将记账改为事实交易支付" class="headerlink" title="改进二：将记账改为事实交易支付"></a>改进二：将记账改为事实交易支付</h3><p>之前的账本是在月底结账，而现在我们把交易单位改成了虚拟的比特币，交易也由月底统一交割改为事实交割。而比特币就像是账本上的数字，随时都可以视大家的意见进行”套现“，即在现实中交割。</p>
<p>那么要完成比特币交易，就必须要求用户的账本里的比特币余额是足够多的。在传统的银行系统中(中心化系统)，银行会记录储户的账余额，判断能否进行交易。但是在比特币系统中我们并没有这样一个中心，所以不能像银行这样来进行操作。</p>
<p>因此比特币提出的解决方案是：<strong>每笔交易不余额为基础，而是以以前的交易为基础</strong>。在这里我们就要介绍比特币的记账模型：<strong>UTXO(Unspent Transaction Out)</strong>，字面意思就是尚未被花出去的交易额</p>
<h4 id="UTXO"><a href="#UTXO" class="headerlink" title="UTXO"></a>UTXO</h4><h5 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h5><p>这个记账模型的理念就是：一切的交易，都是<strong>可追溯的</strong>， 交易与交易之间组成了网状关系，一个交易的输出，成为了下一个交易的输入；下一个交易的输出，又成了下下一个交易的输入。</p>
<p>如下图：</p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/7.png" style="zoom:67%;"></p>
<p>我们可以举一个例子：现在user1和user2达成了一笔交易，user1需要支付给user2 一共37个BTC，那么首先user1的账户余额里面的BTC个数是要大于37个的。</p>
<ul>
<li>问题1：怎么计算user1账户中的余额？<ul>
<li>因为在比特币系统中，并不像银行一样直接存储账号里有多少钱。而存储的是一笔笔的交易，也就是一笔笔的UTXO。然后，某个人的钱包的余额就等于他的UTXO的总和。比如说在这个user1的账户中，一共有3笔UTXO，金额分别是5,15和20，加起来一共是40BTC。余额是大于37BTC的，因此这笔交易可以进行</li>
</ul>
</li>
<li>问题2：为什么不直接支付37元，而是要用更复杂的先支付40BTC，再让user2找user1 钱呢？<ul>
<li>因为在UTXO中，每笔交易的金额是不能够被拆分的。对user1来说，他将T2、T3转给user2之后，是不能将T1交易额中的5BTC拆出2BTC转给user2的，要给就一起给。</li>
<li>打个比方，就好比你拿了一堆纸币去买东西，你必须从这堆纸币中选出几张，是的纸币的面值之和大于这笔交易的金额。你只能要求商家将多余的钱找给你，而不能说把纸币撕成两半花</li>
</ul>
</li>
<li>问题3：在真实的比特币交易中，这种方法难道不会徒增麻烦吗？<ul>
<li>这只是比特币交易的底层实现逻辑。事实上在上层有比特币钱包这种软件可以帮你计算UTXO、选择哪几个UTXO进行支付等。对用户来说这是完全透明的</li>
</ul>
</li>
</ul>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/8.png"></p>
<p>总结：</p>
<ul>
<li>任何一笔Transaction ，汇划费多个UTXO(Input), 同时也会产生多个新的 UTXO(Output), 属于多个不同的收款人</li>
<li>1个UTXO  = 1个Transaction ID + Output Index</li>
<li>旧的UTXO不断消亡新的UTXO不断产生，所有的UTXO组成了UTXO Set的数据库，存在于每个节点。</li>
<li>任何一笔UTXO，有且尽可能被一个交易花费1次，也就是说具有不可拆分性。(以纸币类比)</li>
</ul>
<h5 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h5><p>UTXO（Unspent TX Output）译名为未花费的交易输出，UTXO模型的设计基于一种思路：除了铸币（Coinbase）交易生成的比特币，任意一笔钱不会凭空产生，也不会凭空消失。UTXO由四部分组成；</p>
<ul>
<li><p>address: 拥有此UTXO的地址；</p>
</li>
<li><p>amount: 此UTXO的金额；</p>
</li>
<li>Signature Script: UTXO解锁脚本，使用交易发送私钥加密交易内容得到的签名；</li>
<li>Pubkey Script: UTXO锁定脚本，包含交易获得方的公钥哈希。</li>
</ul>
<p>上面提及了比特币中的解锁脚本（Signature Script）与锁定脚本（PubKey Script）。比特币脚本是一种基于栈结构的无状态脚本语言，只能进行有限的操作，图灵不完备。虽然简单，但在数字货币领域，也意味着更少的金融风险。</p>
<p>在UTXO 模型中，交易只是代表了 UTXO 集合的变更。而账户和余额的概念是在 UTXO 集合上更高的抽象，账号和余额的概念只存在于钱包中。以比特币钱包为例，钱包管理的是一组私钥，对应的是一组公钥和地址。要查看钱包余额，必须从创世区块开始扫描每一笔交易，如果：</p>
<ol>
<li><p>遇到某笔交易的某个Output是钱包管理的地址之一，则钱包余额增加；</p>
</li>
<li><p>遇到某笔交易的某个Input是钱包管理的地址之一，则钱包余额减少。</p>
</li>
</ol>
<p>所以从狭义上来说，比特币钱包中并没有比特币，只有钱包地址关联的所有UTXO之和，代表着钱包地址拥有这些比特币的所有权。</p>
<p>现在我们举一个实际情况中转账交易的例子：假设Alice有amount个比特币</p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/16.png"></p>
<ol>
<li>Alice使用公钥解锁自己的UTXO，构造新的UTXO和一笔转账交易。</li>
<li>Alice使用私钥对UTXO的交易内容进行签名：将UTXO的输出地址为Bob的钱包地址，并把Bob的公钥Hash作为解锁此UTXO的凭据</li>
<li>经过6个区块确认(证明该笔交易极大概率合法)之后，这amount个比特币就属于Bob了。实际上，Bob拥有的是这笔转账交易的UTXO</li>
</ol>
<p>从比特币的记账模式来看，所有的交易记录都是可以被追踪的。因此，在一些灰色产业(如洗钱)中，用户为了抹去自己的交易记录，会采用<strong>洋葱路由</strong>的方式，将交易洗牌以后重新组合来规避法律风险。</p>
<h3 id="改进三：不留证明，改留比特币账号"><a href="#改进三：不留证明，改留比特币账号" class="headerlink" title="改进三：不留证明，改留比特币账号"></a>改进三：不留证明，改留比特币账号</h3><p>如上图，用户A并不用留下自己的名字，而是用一个字符串来代表自己进行交易：</p>
<ul>
<li><strong>1H6ZZpRmMnrw8ytepV3BYwMjYYnEkWDqVP</strong></li>
</ul>
<p>同理，用户B和C也是如此，只留下了一个字符串。所以你们室友在帐本里都不写名字，而是写下跟各自对应的字符串，即比特币账户。虽然对于你们四个人的寝室这纯属脱裤子放屁，但如果对于规模更大的系统，比如前文提到的整个学校而言，这种操作可以极大提升隐私性。</p>
<p>我们只能知道每个账户，而无法知道谁拥有这个账户，这就保证了隐私性。</p>
<h2 id="区块链"><a href="#区块链" class="headerlink" title="区块链"></a>区块链</h2><h3 id="区块链简介"><a href="#区块链简介" class="headerlink" title="区块链简介"></a>区块链简介</h3><p>前面我们提到了这个账本是分布式存储的，每个人都有一个自己独立管理的账本。</p>
<p>当这个账本系统变得很大时，一致性问题就必须要考虑。比如，如果你有室友在交易记录发布时不在寝室，那么他就错了这次消息，使得这次消息不会出现在他的帐本里。</p>
<p>换回比特币系统，也就是部分电脑可能处于关机或者未联网状态，会错过部分交易。此外，还可能会有黑客入侵部分电脑，篡改交易记录。</p>
<p>此外，还有一个更严重的问题就是，实际网络拓扑非常复杂，链路质量的随机性很大。因此，如果用户A（假设账户里有10BTC）连续广播两条相互矛盾的消息，比如：</p>
<ul>
<li><strong>交易信息1：用户A支付10BTC给B；</strong></li>
<li><strong>交易信息2：用户A支付10BTC给C；</strong></li>
</ul>
<p>有的读者应该会觉得，那我们就采信先收到的交易信息1，忽略与之矛盾的脚印信息2不就行了。</p>
<p>但问题是，因为网络链路的复杂性，所以很可能存在部分用户先收到交易信息1，又有部分用户先收到交易信息2。如果依靠先后顺序辨别有效性，那么就会存在不同用户记录的交易信息不一致。</p>
<p><strong>为了解决这个问题，中本聪提出了区块链的概念。</strong></p>
<p>每个用户如果愿意，都可以整理自己从网络中接收到的交易信息，然后检查其是否合理（每笔交易是否由足够余额？数字签名是否正确？）后，再将交易记录打包成一个区块。每个区块有多笔交易(两三千笔)。</p>
<p>因此每个交易记录都是以区块的形式存储，然后再广播到系统中的其他用户中。而区块之间相互连接，形成一条由系统内全体用户共同维护的区块链。 </p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/4.jpeg"></p>
<p>因此其他用户收到广播的区块时，就会把这个区块加到自己维护的账本，也就是区块链的尾部。</p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/5.jpeg"></p>
<p>但如果只是这样，并没有解决任何问题。互联网节点遍布全球，广播过程也需要时间，因此肯定会存在不同节点收到不同区块存储的问题。</p>
<p>以此图为例，设备A和设备B几乎同时发布了自己的区块，两者的区块并不相同。很好理解：</p>
<ul>
<li><strong>红色区域中的用户离设备A近，会先收到设备A发的区块甲；</strong></li>
<li><strong>蓝色区域中的用户离设备B近，会先收到设备B发的区块乙；</strong></li>
</ul>
<p><strong>然后，不同的用户会将不同的区块加入到自己维护的区块链尾部，生产不同的区块链。</strong></p>
<p>如此一来这个网络就乱套了。为了降低传播时间的影响，一个简单粗暴的方式就是从系统设计中就限制区块生成的速度。比特币系统的核心思想采就是<strong>用算力限制区块的生成速度。</strong></p>
<p>比特币系统要求，每个用户在发布新区块前，必须先完成一个任务。这个任务就是：</p>
<ul>
<li><strong>根据前一区块的一些信息加上新区块的一些信息，生成一个字符串S；</strong></li>
<li><strong>选择一个字符串B，与S合并成“BC”，且要求“BC”的哈希映射满足某个条件，比如映射结果的前72位为0（概率：2^(-72)）；</strong></li>
</ul>
<p>上面的第2步，除了一个个试以外是没有捷径的，而且结果是否满足要求可以快速试出。举个例子就相当于给你一个银行卡，让你挨个试密码。那你没有任何办法，只能000000-999999挨个试验，最后虽然能试出来，但也会<strong>花费大量时间</strong>。而且你把试出来的结果告诉别人后，别人可以很快验证你的结果是否正确</p>
<p>时间恰好是我们宇宙中最稀缺的资源。因此我们可以调节这个难度，比如使得系统中所有平均<strong>每20分钟</strong>才会有一台设备完成要求的任务，猜出符合要求的字符串B。如此一来，撞车的概率就会大大降低。补充一句，这个难度是不断调节的，以适应硬件算力的提升。</p>
<h3 id="区块链分类"><a href="#区块链分类" class="headerlink" title="区块链分类"></a>区块链分类</h3><p>我们可以大致把联盟链分为三类：公有链、私有链、联盟链</p>
<ul>
<li>公有链：Public blockchains<ul>
<li>公有链没有官方组织及管理机构，没有中心服务器，参与的节点按照系统规则自由接入网络、不受控制，节点间基于共识机制开展工作</li>
</ul>
</li>
</ul>
<p>比特币就是一个典型的公有链</p>
<ul>
<li>私有链：Private blockchains<ul>
<li>建立在某个企业内部，系统的运作规则根据企业要求进行设定，仅有少数节点拥有修改、读取的权限，同时仍保留着区块链的真实性和部分去中心化的特性。</li>
</ul>
</li>
<li>联盟链<ul>
<li>联盟链由若干机构联合发起，介于公有链和私有链之间，兼具部分去中心化的特性。联盟管理员可以根据业务要求限制某些用户的读取权限。并限制某些节点参与共识计算</li>
<li>可以理解为一个特殊的分布式数据库</li>
</ul>
</li>
</ul>
<p>私有链和联盟链可以被统称为<strong>许可链</strong>，公有链又可以被称为<strong>非许可链</strong>。要进入许可链，需要一个<strong>身份证</strong>。</p>
<h3 id="挖矿-POW"><a href="#挖矿-POW" class="headerlink" title="挖矿(POW)"></a>挖矿(POW)</h3><h4 id="比特币机制"><a href="#比特币机制" class="headerlink" title="比特币机制"></a>比特币机制</h4><p>系统为了鼓励大家生成新的区块，于是在开始时就定下规则：</p>
<ul>
<li><strong>每当一个新区块加入主链，这个区块的发行者就会被赠与50个BTC；</strong></li>
<li><strong>每21万个区块后，奖励额度缩水一半；</strong></li>
</ul>
<p>这也就解释了，为什么BTC的发行上限是：</p>
<script type="math/tex; mode=display">
21000\times50\times(1+\frac{1}{2}+\frac{1}{4}+\cdots) = 21000000</script><p>也就是2100万个。</p>
<p>在 这个网站 中 <a href="https://blockexplorer.one/bitcoin/mainnet，我们可以看到最新被挖出的区块的编号，可以看到最新被挖出的编号是725025,每个区块的奖励是6.25个BTC，已发行的BTC的数量是18562811个" target="_blank" rel="noopener">https://blockexplorer.one/bitcoin/mainnet，我们可以看到最新被挖出的区块的编号，可以看到最新被挖出的编号是725025,每个区块的奖励是6.25个BTC，已发行的BTC的数量是18562811个</a></p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/3.png"></p>
<h3 id="区块链的结构"><a href="#区块链的结构" class="headerlink" title="区块链的结构"></a>区块链的结构</h3><p>区块链的内部结构如下：</p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/1.png" style="zoom:150%;"></p>
<p>Header 中包含的信息主要是下面几项：</p>
<ul>
<li>Version 。区块的版本号。</li>
<li>之前一个区块的 header 哈希。</li>
<li>Merkle根哈希。这个哈希是由区块中包含的所有交易的哈希值运算得出的，如果有人篡改了任何交易，这个哈希值就会变。header 中保留了这个值意味着只要 header 安全了，整个区块就安全了，这解释了为何计算 nouce 的时候，只需要计算 header 的哈希，而忽略区块主体数据。</li>
<li>时间。矿工开始运算 header 哈希的那个时间点。</li>
<li>nBits 。压缩格式的 target 值。这一项是跟 nonce 紧密相关的，后面我们还会详细介绍。</li>
<li>Nonce 。简单来说 nonce 就是每个区块 header 之中都会保存的一个数,下面详细介绍。</li>
</ul>
<p>计算 nonce 值的过程就是对区块 header 不断的运算哈希，直至找到能使区块哈希小于 target 的 nonce。</p>
<p>Target 就是一个 256-bit 的数。因为计算 header 哈希使用的<a href="https://link.zhihu.com/?target=https%3A//happypeter.github.io/binfo/hash">哈希算法</a>是 Sha256 ，运算得到的哈希值是 256-bit 的一个数，所以恰好可以用来和 target 对比。所有比特币客户端在计算 nonce 的时候<strong>都使用相同的 target</strong> ，并且这个 target 值会被记录到 header 中，保存到了 nbits 这一项。nBits 是一个 32-bit 的数，所以保存的是 target 的压缩形式，具体换算关系可以参考<a href="https://link.zhihu.com/?target=https%3A//bitcoin.org/en/developer-reference%23target-nbits">这篇文档</a>。Target 要小于这个数 0x00000000FFFF0000000000000000000000000000000000000000000000000000 。</p>
<p>每次计算 header 哈希就让 nonce 值加1。具体 nonce 的运算过程就是不断的修改 nonce 值，然后对这个 header 重新运算哈希的过程。每得到一个哈希值就去跟 target 对比，如果哈希小于或者等于 target ，那么运算过程就结束了，当前 nonce 值会被最终记录到 header 中。否则，就把 nonce 值加1，再次计算 header 哈希。</p>
<p>显然，各个比特币客户端开始了一场寻找 nonce 的比赛，谁的硬件速度快，就有更大的概率率先找到 nonce 值，也就是宏观意义上抢到的记账权，当然，哈希的运算过程非常随机，所以这场比赛也跟抓彩票一样有很强的运气成分。</p>
<p>以上就是 nonce 的生成步骤了</p>
<h4 id="Merkle树"><a href="#Merkle树" class="headerlink" title="Merkle树"></a>Merkle树</h4><p>从上面的结构可知，比特币的区块体就是由Merkle树组成的。那么，为什么我们需要Merkle树呢？</p>
<p>首先，我们要了解Merkle树的生成过程。由区块链的结构可知，每个区块由前一个区块的一些信息和新区块的一些信息组成的。其中我们要计算的这个Merkle根就属于新区块的一些信息</p>
<p>如何计算Merkle根？</p>
<ul>
<li>首先，对于每一个区块，节点收到了一批交易以后，它们会组成Merkle树的叶子节点。</li>
<li>然后，我们要给每一项交易都生成一个哈希值。</li>
<li>接着，将树根中的哈希值两两哈希，生成倒数第二层节点。</li>
<li>重复对每一层节点进行两两哈希，生成它们的父亲节点。</li>
<li>直到生成Merkle树的树根，这个最后的哈希值就是默克尔树的根哈希。</li>
</ul>
<h5 id="SPV验证"><a href="#SPV验证" class="headerlink" title="SPV验证"></a>SPV验证</h5><p>SPV的目标是验证某个支付是否真实存在，并得到了多少个确认。</p>
<p>首先我们要来了解三种节点：</p>
<ul>
<li>轻节点指的是：节点本地只保存与其自身相关的交易数据（尤其是可支配交易数据），但<strong>并不保存完整区块链信息</strong>的技术。轻节点的目标不仅是支付验证，而且是用于管理节点自身的资产收入、支付等信息。因此，<strong>轻节点仍需下载每个新区块的全部数据</strong>并进行解析，获取并在本地存储与自身相关的交易数据，只是无须在本地保存全部数据而已。</li>
<li>全节点是指：<strong>维持包含全部交易信息</strong>的完整区块链的节点。更加准确地说，这样的节点应当被称为完整区块链节点。</li>
<li>SPV节点是指：和轻节点不同，<strong>SPV节点不需要下载新区块的全部数据</strong>，只需保存区块的头部信息即可。虽然轻节点部分借鉴了SPV的理念，但和SPV是完全不同的。</li>
</ul>
<p>Merkle树结构，可以用来验证或确保一个数字货币交易已经在对应区块链的一个区块中。这就是SPV验证。</p>
<p>在验证某一个交易的真实性的时候，SPV节点只需要把该交易哈希值向网络中连接的全节点发起询问。网络里面的全节点只需要回复最小量必要数据给SPV节点，即可验证交易的真实性。如果SPV节点不信任提供交易验证数据的全节点，还可以同时发起多个全节点的询问，来确保交易验证的最大可靠性。</p>
<p>以下是SPV节点的验证过程</p>
<ul>
<li>从网络上获取并保存最长链的所有block header到本地</li>
<li>计算 该交易的hash值 ：tx_hash</li>
<li><p>定位到包含该tx_hash所在的区块，验证block header是否包含在已知的最长链中</p>
<ul>
<li>怎么定位？最简单的方法是顺序查找。</li>
</ul>
</li>
<li><p>从区块中获取构建merkle tree所需的hash值</p>
</li>
<li>根据这些hash值计算merkle_root_hash</li>
<li>若计算结果与block header中的merkle_root_hash相等，则交易真实存在</li>
<li>根据该block header所处的位置，确定该交易已经得到多少个确认。</li>
</ul>
<h4 id="Target"><a href="#Target" class="headerlink" title="Target"></a>Target</h4><p> <strong>通过调整 target 值，可以来调整挖矿难度</strong>，挖矿难度每两周会调整一次。</p>
<p>Target 的值越小，挖矿难度就越大。POW 算法下的挖矿难度就体现在找到 nonce 值的难度，挖矿难度越大找到 nonce 值所需要的时间就越长。寻找 nonce 意味着找到一个满足特定范围的区块哈希，这个范围越大找到满足条件的 nonce 就越容易，反之则越难。而这个范围就是大于零小于 target ，所以 target 越小，找到合适的 nonce 就越难。</p>
<p>挖矿难度每两周会调整一次。首先说，为什么要调整呢？由于寻找 nonce 的过程带有一定的随机性，每次找到 nonce 的时间都不一定是网络所期待的十分钟，所以如果时间大于十分钟，说明挖矿难度有可能设置的太高了，那么下一次我们就需要把挖矿难度降低，从而尽量保持平均找到 nonce 的时间是十分钟。而实际中，并不是每次出块后都会调整挖矿难度，比特币网络规定挖矿难度每2016个区块调整一次，2016个十分钟恰好是两周，所以每次难度调整的时间间隔是大约两周。</p>
<p>总之，伴随 target 的调整，挖矿难度也随着调整，可以保证平均的每次找到 nonce 的时间尽量趋近十分钟。</p>
<h3 id="安全性"><a href="#安全性" class="headerlink" title="安全性"></a>安全性</h3><p>采用了挖矿机制之后，虽然概率大大降低，但仍然无法排除存在撞车的概率。因此，区块链验证中最核心的思想就是——<strong>相信最长的区块链</strong></p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/6.jpeg"></p>
<p>我们拿上图举例，在原有的到区块链N的情况后，短时间内出现了两个不同的区块，分别用蓝色和红色表示。</p>
<p>如此一来，就会有：</p>
<ul>
<li><strong>部分用户的区块链：原区块链+蓝色区块</strong></li>
<li><strong>部分用户的区块链：原区块链+红色区块</strong></li>
</ul>
<p>这个时候，我们需要做的只有一件事情，那就是『等待』。因为同时产生区块的小概率事件，总不可能连续发生。这样我们只需要等下一个区块产生，看这个新区块是连在蓝色后还是红色后。</p>
<p>图例里是有个新区块连在了蓝色区块后面，这个时候：</p>
<ul>
<li><strong>链长度（原区块链+蓝色区块+新区块）&gt;链长度（原区块链+红色区块）</strong></li>
</ul>
<p><strong>因此，我们选择长度更长的“原区块链+蓝色区块+新区块”作为大家共同维护的主链。</strong></p>
<h4 id="双花攻击"><a href="#双花攻击" class="headerlink" title="双花攻击"></a>双花攻击</h4><p>双花攻击(Double Spend attach)，就是说攻击者几乎同时将同一笔钱(UTXO)用作不同交易。那么如何避免双花攻击？</p>
<ul>
<li>虽然确实可以几乎同时发起两笔交易，这些交易单可以任意顺序进行广播，但是它们被加入区块时必定呈现一定的顺序。我们只要认定第一笔交易合法，剩余的交易非法即可。</li>
</ul>
<p>那么，如果这个黑客向两个不同的网络，用一个UTXO发起两笔交易，分别加入两个区块。会怎么样呢?</p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/10.png"></p>
<ul>
<li>这也是不影响的。因为在比特币中我们最终认的是最长链。即使通过双花生成了两个分叉，但这些交易也是暂时的，最后只有在最长链的那笔交易会被保留下来。</li>
</ul>
<h4 id="女巫攻击"><a href="#女巫攻击" class="headerlink" title="女巫攻击"></a>女巫攻击</h4><p>女巫攻击(Sybil attach)也是很常见的攻击手段，如果投票机制存在，那么就可能出现一个人分饰多角给某一个人投票，出现作弊的现象。</p>
<p>但是在区块链中，几乎不可能出现女巫攻击的情况 </p>
<p>如果有人想对区块链造假，那么他就需要一直抢先生成假的新区块，并广播出去。</p>
<p>这就要求造假者生产新区块的速度，要快于系统中的其他所有用户的生成能力之和。换句话说，造假者需要用户有和其他用户算力之和匹敌的算力。</p>
<p>假设这个造假者拥有系统总算力的80%，而剩下用户拥有20%（注：这个假设显然不可能）。如此一来，在造假者广播了一个假的区块后，就有80%的概率先于其他用户生成新区块，然后连在自己之前生成的假区块后。此时造假才有可能成功。</p>
<p>但实际是，每个用户用的算力相比系统总算力都是微不足道的。而且随着比特币网络的逐渐壮大，系统就会越来越鲁棒。这就使得造假变得不可能，也使得造假的成本变得无法接受。</p>
<h4 id="可锻性攻击"><a href="#可锻性攻击" class="headerlink" title="可锻性攻击"></a>可锻性攻击</h4><p>首先，我们需要知道密码学中的一个常识：在椭圆曲线数字签名算法中，会生成两个大整数$r$和$s$并组合起来作为签名。同时 ,$r$ 和 $BN-s$ 也同样可以作为签名来验证交易，(BN是一个很大的数)。</p>
<p>因此出现了这样一个状况:同样的输入和输出，可以修改签名，使得交易ID出现不同。同时可以通过验签。</p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/18.png"></p>
<p>那么这时候，攻击者就可以侦听比特币网络中的交易，利用交易签名算法的特征修改原交易中的input签名，生成拥有一样input和output的新交易，然后广播导网络中形成双花</p>
<p>攻击过程如下：</p>
<ol>
<li>首先需要有足够多的比特币矿机接入网络，以增加伪造的请求被优先处理的可能性</li>
<li>攻击者在第三方交易平台提交一个提款请求，并获得一个交易ID。第三方交易平台只根据TransactionID来判断交易是否成功。根据交易信息伪造一个签名，同时生成一个完全不同的交易ID，并将伪造的请求发出</li>
<li>若伪造的交易被优先处理，则<strong>原始交易失败</strong></li>
<li>攻击者可以再次提交现请求，第三方交易平台确认之前的交易失败后会再次发送提现交易，至此，攻击成功</li>
</ol>
<p>也就是说，我可以给自己转一笔钱，成功了，但是我可以伪装成自己失败了并从交易所获得退款。所以真正受到攻击的是交易所。</p>
<ul>
<li>如何防范？</li>
</ul>
<ol>
<li>当使用TXID查找不到对应的交易时，需要用TXIN_OUTPOINT再查一下；</li>
<li>如果确实需要重发，引用的UTXO一定要与原来保持一致，把这种麻烦事儿交给比特币网络来处，系统规则确保了最终只会确认其中的一个（不会出现双花）。</li>
</ol>
<h4 id="挖矿的意义"><a href="#挖矿的意义" class="headerlink" title="挖矿的意义"></a>挖矿的意义</h4><p>挖矿的意义是什么？有些人会对挖矿机制产生质疑——这不是在浪费电力资源吗？</p>
<p>其实，在比特币这样的公有链中，挖矿是一个绝顶聪明的发明。通过挖矿其实是实现了一个<strong>共识机制</strong>，也就是一个<strong>选主的过程</strong>。因为在区块链中，每一个节点都会收到一批交易，那么到底认谁收到的那批交易是”正统“的呢？</p>
<p>答案很简单，谁的算力强，先算出来nonce的值，谁就被认可。而且通过挖矿和认可最长链的机制，使得比特币系统可以抵御很多种类的攻击</p>
<h3 id="区块链的应用场景"><a href="#区块链的应用场景" class="headerlink" title="区块链的应用场景"></a>区块链的应用场景</h3><p>当某一个场景符合以下几点特征的时候，运用区块链技术可以很好的解决这些困境。</p>
<ul>
<li>多方参与</li>
<li>互不信任</li>
</ul>
<p>上面两点我们很熟悉，正式因为互不信任我们才会使用P2P网络。否则相信中心化就好了</p>
<h4 id="链下数据可信的保证——Oracle-预言机"><a href="#链下数据可信的保证——Oracle-预言机" class="headerlink" title="链下数据可信的保证——Oracle 预言机"></a>链下数据可信的保证——Oracle 预言机</h4><p>这两点就比较难以理解了，链上数据的安全性能够保证，但是涉及到链下数据的时候，我们怎么能保证其上链之前没有被篡改过呢？有一种好方法就是 Oracle预言机。</p>
<p>比如说我和同学A打赌，明天是否下雨，并每人给100元作为赌注。如果下雨我赢，如果不下雨则他赢。那么怎么判断明天到底是否下雨呢？——存在一个公证人C会将明天的天气情况输入到系统中。但是这样存在一个问题，即我可以收买公证人C，使其输入到系统中的天气始终为下雨。这就存在链下数据遭篡改的可能。这时，就需要Oracle预言机了。我们可以在以太坊官网学习Oracle的相关知识 <a href="https://ethereum.org/zh/developers/docs/oracles/" target="_blank" rel="noopener">https://ethereum.org/zh/developers/docs/oracles/</a></p>
<h4 id="自治"><a href="#自治" class="headerlink" title="自治"></a>自治</h4><p>自治就相当于在一个无政府主义的社会，每个人都回去遵循大家一起定下来的规则去行事。</p>
<p>我们可以来看一个例子：航空公司要做一个延误险，保险双方分别为保险公司和乘客。如果航班延误，那么保险公司就会赔付相应的钱给顾客，如果航班准点到达，那么乘客投保的钱就给了保险公司。</p>
<p>问题就是，我们是怎么知道航班延误了？很自然的想法是，到航空公司的网站上去查找飞机到达时间，但这显示是从外部获取的信息，属于链下数据。既然是链下数据就存在一定的不安全性，因此需要Oracle预言机来保证输入数据的正确性，但是引入Oracle预言机会使得镇各系统变得复杂。因此有没有内部的方法，不引用外部的信息源，仅通过共识来保证数据的安全性、正确性呢？</p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/11.jpeg"></p>
<p>我们要了解区块链中的一个原则：<strong>一个不信任，永远在验证</strong> 。在这种情况下，仅用”1个“外部见证人肯定是不行的。因此我们可以这样——我让下一个航班买了延误险的顾客来对这个航班的晚点情况进行投票(人数够多，对这些人来说航班是否晚点是客观事实)。</p>
<ul>
<li>如果一个顾客投了延误，实际上真的延误了，那么他就会获得一定的以太币奖励；</li>
<li>如果顾客投了没有延误，实际上确实没有延误，那么也会获得一定的以太币奖励；</li>
<li>如果一个顾客投了延误，但实际上并没有延误，就说明他说谎，因此我们可以取消他延误险的有效性， 即使延误了他也无法获得赔付 ，以此作为惩罚。</li>
</ul>
<p>在这个系统中，就使用了<strong>自治</strong>的方法，对于个人来说，他说假话并不会得到任何的好处，而说实话则会得到奖励。因此大家都会倾向于说真话。基于此，就相当于通过<strong>人肉筛选</strong>来避免了直接从外部源获得信息，从而使整个系统运行起来。</p>
<h2 id="区块链2-0"><a href="#区块链2-0" class="headerlink" title="区块链2.0"></a>区块链2.0</h2><p>区块链2.0最重大的特征：支持智能合约(smart contract)</p>
<p>那么智能合约是什么？在比特币系统中，它支持的脚本是非常有限的，主要以转账为主。但是到了2.0，我们就可以在这个平台上编程了，我可以在上面开发应用。我们把这些应用统称为智能合约</p>
<p>智能合约可以对接收到的信息进行回应，可以接收和存储价值，还可以对外发送信息和价值。</p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/12.png"></p>
<p>注意：智能合约程序不只是一个可以自动执行的计算机程序，其自己就是一个<strong>系统参与者</strong>。这意味着智能合约可以调用另一个智能合约</p>
<h3 id="软件架构"><a href="#软件架构" class="headerlink" title="软件架构"></a>软件架构</h3><p>下面是区块链2.0的软件架构</p>
<ul>
<li>在共识层，需要用到挖矿算法以及拜占庭容错的方法</li>
<li>在智能合约引擎中，包括智能合约的代码以及其存储的数据。</li>
</ul>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/13.png"></p>
<h3 id="业务流程"><a href="#业务流程" class="headerlink" title="业务流程"></a>业务流程</h3><p>开发人员可以写一个自己的智能合约，然后我们调用系统提供的SDK，并将数据存储在数据库当中。如下图所示：</p>
<p><img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/14.png"></p>
<h2 id="区块链小结"><a href="#区块链小结" class="headerlink" title="区块链小结"></a>区块链小结</h2><ol>
<li>区块链是一个分布式的账本，一种通过<strong>去中心化、去信任</strong>的方式由参与各方维护一个<strong>可靠的、不可篡改交易记录的</strong>技术方案。</li>
<li>从数据库角度，可以将区块链比作一种<strong>分布式数据库</strong>技术，通过维护数据块的链式结构，可以维持持续增长的、不可篡改的数据库记录。</li>
</ol>
<h3 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h3><ul>
<li>匿名：交易参与方与网络中的相关节点都是匿名的</li>
<li>不可信：开放的环境需要通过<strong>数字签名</strong>技术进行验证，按照系统既定的规则运行。</li>
<li><p>去中心化：基于P2P，不存在中心化的设备和管理机构</p>
<ul>
<li>任何能都可以参与到区块链网络，每一台设备都能作为一个节点，每个节点都允许获得一份完整的数据库拷贝</li>
<li>节点间基于一套共识机制，通过竞争计算共同维护整个区块链</li>
<li>任一节点失效，其余节点仍能正常工作</li>
</ul>
</li>
<li><p>不可篡改性</p>
<ul>
<li>批量交易一旦达成共识，就会作为一个区块添加到区块链中，会被永久地存储起来</li>
<li>单个甚至多个节点对交易数据的修改是无效的，除非能控制整个网络中超过51%的节点同时修改，但这几乎是不可能发生的。</li>
</ul>
</li>
<li><p>可追溯</p>
<ul>
<li>区块链中每一笔交易都通过密码学方法与相邻两个区块串联(UTXO)，因此可以追溯到任何一笔交易的前世今生.</li>
</ul>
</li>
<li><p>共识</p>
<ul>
<li>采用协商一致的规范和协议(POW) 来保证数据质量，越多人在这个系统中，这个区块链系统就会越鲁棒。</li>
</ul>
</li>
</ul>
<h3 id="应用情况"><a href="#应用情况" class="headerlink" title="应用情况"></a>应用情况</h3><h4 id="金融服务"><a href="#金融服务" class="headerlink" title="金融服务"></a>金融服务</h4><p>金融服务是区块链最早的应用领域之一，也是区块链应用数量最多、普及程度最高的领域之-</p>
<ul>
<li>防金融欺诈、资产托管交易、金融审计、跨境支付、对账与清结算、供应链金融以及保险理赔等；</li>
<li>典型案例：基于区块链的机构间对账平台、差异账检查系统，以及通过区块链技术改造的跨境直联清算业务系统等。</li>
</ul>
<h4 id="供应链管理"><a href="#供应链管理" class="headerlink" title="供应链管理"></a>供应链管理</h4><p>供应链核心企业、商业银行、电商平台等相关力量不断加强区块链在供应链管理领域的应用探索。</p>
<ul>
<li>防伪溯源：京东、蚂蚁金服、众安科技等科技企业；</li>
<li>供应链金融：央行数宇货币研究所、央行深圳市中心支行推动“粤港澳大湾区贸易金融区块链平合”，万向区块链、乎安壹账通、京东、腾讯等众多企业；</li>
<li>防伪溯源和物流等领域：更注重与物联网、人工智能等技术的融合发展。</li>
</ul>
<h4 id="智能城市"><a href="#智能城市" class="headerlink" title="智能城市"></a>智能城市</h4><p>区块链在建设智慧城市中的应用涵盖智薏园区、智薏物联网、智慧资产、智慧交通、能源电力、电子政务、法律应用等广阔领域。</p>
<ul>
<li>采用分布式点对点的网络结构，可以使设备之问保持共识，实现,点对点传输数据，減少甚至无需与中心服务器的数据库进行验证，避免对中心化设施的依赖；</li>
<li>利用区块链数据防篡改、可追溯的特性可以帮助打通政府各部门数据孤岛，为公众提供更加可信和有价值的服务。</li>
</ul>
<h4 id="公共服务"><a href="#公共服务" class="headerlink" title="公共服务"></a>公共服务</h4><p>部分地方政府大力推进“区块链＋政务”服务，以满足公共服务在信息共享、权限控制和隐私保护等方面的高要求。</p>
<ul>
<li>鉴证确权：将公民财产、数宇版权相关的所有权证明存储在区块链账本中，大大优化权益登记和转让流程，减少产权交易过程中的欺诈行为；</li>
<li>身份验证：将身份证、护照、驾照、出生证明等存储在区块链账本中，实现无需任何物理签名即可在线处理繁琐的流程，并能实时控制文件的使用权限：</li>
<li>信息共享：区块链技术用于机构内部以及机枸之问的信息共享和实时同步，能有效解决各行政部门间协同工作中流程繁琐、信息孤立等问题。</li>
</ul>
<h3 id="技术发展"><a href="#技术发展" class="headerlink" title="技术发展"></a>技术发展</h3><h4 id="区块链系统架构"><a href="#区块链系统架构" class="headerlink" title="区块链系统架构"></a>区块链系统架构</h4><p> <img src="/2022/02/24/%E4%BA%86%E8%A7%A3%E5%8C%BA%E5%9D%97%E9%93%BE/15.png"></p>
<h4 id="安全技术"><a href="#安全技术" class="headerlink" title="安全技术"></a>安全技术</h4><p>主要包括：数宇摘要算法、数字签名和加密算法。<br>经典算法：</p>
<ul>
<li>SHA256、SM3等主流的数宇摘要算法</li>
<li>RSA、 ECDSA、SM2等常用的数宇签名算法</li>
<li>AES、SM4等对称加密箅法</li>
<li>RSA、 SM2等非对称加密算法。</li>
</ul>
<h4 id="隐私保护技术"><a href="#隐私保护技术" class="headerlink" title="隐私保护技术"></a>隐私保护技术</h4><p><strong>挑战</strong></p>
<ul>
<li>量子计算的技术演进对现有的密码学安全机制的巨大影响；</li>
<li>根据Shor算法，经典非对称算法（基于大数分解、离散对数等算法，如RSA、 ECDSA和SM2等）可以被稳定、可用的量子计算机攻破；</li>
<li>密码学家正积极探索能够抵抗量子计算机攻击的密码机制，如基于格的密码机制、基于纠错码的密码机制、多变量密码机制等。</li>
</ul>
<p><strong>目标</strong>：</p>
<ul>
<li>“身份的隐私性”和“数据的机密性”</li>
<li>身份的隐私性”主要是对区块链参与者身份的保护；</li>
<li>“数据的机密性”主要是对记录内容、合约逻辑等数据的保护。</li>
</ul>
<p><strong>相关技术：环签名、同态加密、零知识证明和安全多方计算等。</strong></p>
<ul>
<li>环签名允许一个成员代表一个群组进行签名而不泄漏签名者信息，可以实现签名者完全匿名。</li>
<li>同态加密除了具有一般的加密操作之外，还能实现直接对密文的计算操作，通常分为加法同态、乘法同态、全同态等类型。</li>
<li>零知识证明是指一方（证明者）向另一方（验证者）证明某个事实的论断，同时不透露该事实的其他信息的方法。</li>
<li>安全多方计算能够在保证输入数据隐私的前提下，为缺之信任的参与方提供协同计算功能。</li>
</ul>
<h4 id="跨链技术"><a href="#跨链技术" class="headerlink" title="跨链技术"></a>跨链技术</h4><h4 id="分片技术"><a href="#分片技术" class="headerlink" title="分片技术"></a>分片技术</h4><p>分片技术本身是一种传统数据库技术，此前主要用于将大型数据库分成更小、更快、更容易管理的数据碎片。在区块链中，可将区块链网络分成很多更小的部分，即进行“分片”处理。</p>
<p>分片技术主要有网络分片、交易分片和状态分片三类</p>
<ul>
<li>网络分片是利用随机函数随机抽取节点形成分片，支持更海量的共识节点。</li>
<li>交易分片分为同账本分片和跨账本分片，主要思想是确保双花交易在相同的分片中或在跨分片通信后得到验证。</li>
<li>状态分片关键是将整个存储区分开，让不同的碎片存储不同的部分，每个节点只负责托管自身的分片数据，而不是存储完整的区块链状态。</li>
</ul>
<h4 id="数据存储"><a href="#数据存储" class="headerlink" title="数据存储"></a>数据存储</h4><p>区块的存储由链式结构发展为有向无环图 (DAG)，DAG 区块链在并行性、可扩展性上有较大改善。</p>
<p>链外数据的存储，除了传统集中的数据中心存储、云存储以外，产生了新的互联网点对点文件系统</p>
<ul>
<li>如融合Git、自证明文件系统(SFS)、BitTorrent 和DHT 等技术的星际 文件系统（IPFS)</li>
</ul>
<h4 id="共识机制"><a href="#共识机制" class="headerlink" title="共识机制"></a>共识机制</h4><p>主要集中在：提高系统吞吐量及降低网络带宽</p>
<ul>
<li><p>CFT (崩溃容错)</p>
<ul>
<li>Paxos, Raft</li>
</ul>
</li>
<li><p>BFT (拜占庭容错)</p>
<ul>
<li>POW系列：POW、POS、DPOS</li>
<li>PBFT, DBFT</li>
<li>DAG</li>
</ul>
</li>
</ul>
<h4 id="智能合约"><a href="#智能合约" class="headerlink" title="智能合约"></a>智能合约</h4><p>主要方面：形式化验证框架与通用型合约编程语言安全性是智能合约的关键性问题</p>
<ul>
<li>智能合约编程语言逐渐从脚本型语言向通用型语言演变</li>
<li>智能合约的执行逐渐从显式调用执行向由链上触发器（如预言机机制）自动触发执行的方向发展</li>
</ul>

          
        
      
    </div>
    
    
    
    <div>
      
    </div>
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2022/02/22/Hadoop%E5%AE%9E%E8%B7%B5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/%5Bobject%20Object%5D">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/02/22/Hadoop%E5%AE%9E%E8%B7%B5/" itemprop="url">Hadoop实践</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-02-22T22:17:28+08:00">
                2022-02-22
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2022-08-22T11:17:18+08:00">
                2022-08-22
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Hadoop实践"><a href="#Hadoop实践" class="headerlink" title="Hadoop实践"></a>Hadoop实践</h1><h3 id="Installing-the-MovieLens-Dataset"><a href="#Installing-the-MovieLens-Dataset" class="headerlink" title="Installing the MovieLens Dataset"></a>Installing the MovieLens Dataset</h3><p>首先，我们可以进入HDFS的管理界面，我们看到在这这有一个NameNode，一个SNameNode (Secondary NameNode,即NameNode的备份，当NameNode发生故障的时候，可以使用SNameNode进行修复) ,还有一个DataNode，负责存储文件。 </p>
<p><img src="/2022/02/22/Hadoop%E5%AE%9E%E8%B7%B5/1.png">然后，我们可以点击右上角的图标进入File View</p>
<p><img src="/2022/02/22/Hadoop%E5%AE%9E%E8%B7%B5/3.png"></p>
<p>进入文件管理系统后，我们可以点击右上角的图标新建文件夹，或者上传数据文件</p>
<p><img src="/2022/02/22/Hadoop%E5%AE%9E%E8%B7%B5/2.png"></p>
<p>我们选中了 .data文件进行上传</p>
<p><img src="/2022/02/22/Hadoop%E5%AE%9E%E8%B7%B5/4.png"></p>
<p>使用好以后可以对其进行删除。</p>
<h3 id="Install-the-MovieLens-dataset-into-HDFS-using-the-command-line"><a href="#Install-the-MovieLens-dataset-into-HDFS-using-the-command-line" class="headerlink" title="Install the MovieLens dataset into HDFS using the command line"></a>Install the MovieLens dataset into HDFS using the command line</h3><h3 id="Installing-Python-MRJob-and-nano"><a href="#Installing-Python-MRJob-and-nano" class="headerlink" title="Installing Python, MRJob, and nano"></a>Installing Python, MRJob, and nano</h3><h3 id="Code-up-the-ratings-histogram-MapReduce-job-and-run-it"><a href="#Code-up-the-ratings-histogram-MapReduce-job-and-run-it" class="headerlink" title="Code up the ratings histogram MapReduce job and run it"></a>Code up the ratings histogram MapReduce job and run it</h3><h3 id="Rank-movies-by-their-popularity"><a href="#Rank-movies-by-their-popularity" class="headerlink" title="Rank movies by their popularity"></a>Rank movies by their popularity</h3><h3 id="Note-Sorting-will-only-work-by-partition"><a href="#Note-Sorting-will-only-work-by-partition" class="headerlink" title="Note Sorting will only work by partition"></a>Note Sorting will only work by partition</h3><h3 id="Check-your-results-against-mine"><a href="#Check-your-results-against-mine" class="headerlink" title="Check your results against mine"></a>Check your results against mine</h3>
          
        
      
    </div>
    
    
    
    <div>
      
    </div>
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/">&lt;i class&#x3D;&quot;fa fa-angle-left&quot;&gt;&lt;&#x2F;i&gt;</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/44/">44</a><a class="extend next" rel="next" href="/page/6/">&lt;i class&#x3D;&quot;fa fa-angle-right&quot;&gt;&lt;&#x2F;i&gt;</a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/%5Bobject%20Object%5D"
                alt="Jason" />
            
              <p class="site-author-name" itemprop="name">Jason</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%20%7C%7C%20fa%20fa-archive">
              
                  <span class="site-state-item-count">435</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">61</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart" aria-hidden="true"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jason</span>
 <!--
  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>







-->
        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>




















  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v="></script>

  <script type="text/javascript" src="/js/src/motion.js?v="></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v="></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
