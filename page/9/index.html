<!DOCTYPE html>
<html lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jasonxqh.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Jason‘s Blog">
<meta property="og:url" content="https://jasonxqh.github.io/page/9/index.html">
<meta property="og:site_name" content="Jason‘s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Jason">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://jasonxqh.github.io/page/9/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Jason‘s Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Jason‘s Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2021/11/15/%E6%95%B0%E6%8D%AE%E6%AD%A3%E7%A1%AE%E6%80%A7%E4%B8%8E%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/15/%E6%95%B0%E6%8D%AE%E6%AD%A3%E7%A1%AE%E6%80%A7%E4%B8%8E%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/" class="post-title-link" itemprop="url">数据正确性与事务处理</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-15 15:00:34" itemprop="dateCreated datePublished" datetime="2021-11-15T15:00:34+08:00">2021-11-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-06-23 20:07:20" itemprop="dateModified" datetime="2022-06-23T20:07:20+08:00">2022-06-23</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="数据正确性与事务处理"><a href="#数据正确性与事务处理" class="headerlink" title="数据正确性与事务处理"></a>数据正确性与事务处理</h1><p>数据库除了提供数据的存储和查询功能，还提供<strong>事务处理</strong>功能。本章节就要来谈谈事务处理的相关细节。</p>
<h3 id="OLTP和OLAP"><a href="#OLTP和OLAP" class="headerlink" title="OLTP和OLAP"></a>OLTP和OLAP</h3><p>首先我们要了解应用的两种模式：OLTP和OLAP</p>
<p>OLTP 即事务处理，OLAP即分析处理，两者在数据库里存储的数据以及对数据库的操作都有不同。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>OLTP(事务型应用)</th>
<th>OLAP(分析型应用)</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据</td>
<td>状态型</td>
<td>历时型</td>
</tr>
<tr>
<td>例子</td>
<td>账户余额、购物车、课程表等</td>
<td>购物历史、记录等</td>
</tr>
<tr>
<td>对数据库的要求</td>
<td>稳定性、正确性</td>
<td>注重处理能力</td>
</tr>
<tr>
<td>对数据库的主要操作</td>
<td>更新</td>
<td>复杂SQL查询</td>
</tr>
</tbody>
</table>
</div>
<p>因此，对于事务型应用，我们要求数据库能保证<strong>数据正确性</strong>的同时提供<strong>事务处理</strong>功能。 </p>
<h3 id="数据的正确性问题"><a href="#数据的正确性问题" class="headerlink" title="数据的正确性问题"></a>数据的正确性问题</h3><p>导致数据出现错误的的原因主要有两个</p>
<p>一个是在数据并发的时候若不使用锁的时候，会发生LOST UPDATES，导致数据丢失</p>
<p>另一个是在处理事务的时候数据库突然发生故障导致数据丢失</p>
<p>我们举一个很简单的例子：</p>
<p><img src="/2021/11/15/%E6%95%B0%E6%8D%AE%E6%AD%A3%E7%A1%AE%E6%80%A7%E4%B8%8E%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/1.jpeg" style="zoom:50%;"></p>
<p>在数据库中，A所在的一列是索引，因此当我们修改的A的值的时候需要修改表和索引两个地方，使其保持一致。假设甲和乙几乎同时提交修改请求，如果没有锁的存在，可能表中收到的指令是：先把A修改成2再把A修改成3；而索引收到的指令是：先把A修改成3再把A修改成2. 这样的后果就是索引和表中的值都无法保持一致</p>
<p>此外，数据库系统还可能在已经修改了表但还没有修改索引的时候突然奔溃，这也会使两个值不一样。</p>
<p>为了规避这些风险，我们需要一种机制去保证数据的正确性 </p>
<h3 id="数据库操作的原子性"><a href="#数据库操作的原子性" class="headerlink" title="数据库操作的原子性"></a>数据库操作的原子性</h3><p>理论上，只要我们对数据库的操作满足原子性，我们就能保证数据的正确性。那么什么是原子性？</p>
<p>我们可以把原子性理解为：不管多少事务是有重合的，我们要把他们实现成好像一瞬间完成的一样(具体实现方法之后再讲)，投影在一个数轴上，这样在数轴上这些点是不会重合的，每一个操作都有先后顺序，不会互相干扰</p>
<p><img src="/2021/11/15/%E6%95%B0%E6%8D%AE%E6%AD%A3%E7%A1%AE%E6%80%A7%E4%B8%8E%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/2.jpeg" style="zoom:50%;"></p>
<p>那么，怎么样实现原子性呢？ 我们提出日志机制</p>
<h4 id="undo日志"><a href="#undo日志" class="headerlink" title="undo日志"></a>undo日志</h4><p><img src="/2021/11/15/%E6%95%B0%E6%8D%AE%E6%AD%A3%E7%A1%AE%E6%80%A7%E4%B8%8E%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/3.jpeg" style="zoom:50%;"></p>
<p>现在有一个O1操作如上图所示，一共要执行6个步骤。为了保证数据的准确性，我们在硬盘上另外开了一片区域来记录日志</p>
<ul>
<li>在O1操作开始前，会记录日志<code>log(O1,start)</code> 代表O1操作已经开始</li>
<li>当我们发现要对硬盘上的数据进行修改的时候，也就是在第二步和第三步之间。则会记录A原始的值，即 <code>log(O1,A,3)</code> </li>
<li>同理在第五和第六步之间记录 <code>log(O1,B,3)</code></li>
<li>最后在O1操作全部完成之后，记录<code>log(O1,end)</code> 代表整个操作结束</li>
</ul>
<p>这样的当运行到第三句话，发生了故障导致机器重启。那么当数据库系统再次启动的时候会执行恢复程序，即扫描一遍日志，查看是否有异常。当扫描到O1操作的日志的时候，发现<code>log(O1,start)</code>却没有结束日志。因此判定O1操作没有执行完成，执行回滚操作。由于我们记录了A的原始数据，因此回滚起来也非常方便。</p>
<h4 id="redo日志"><a href="#redo日志" class="headerlink" title="redo日志"></a>redo日志</h4><p>上面的这种记录日志的方式叫做undo日志,其主要的功能就是在操作到一半的情况下能够回滚回去。</p>
<p>还有一种记录日志的方式叫做redo日志。其原理如下图所示</p>
<p><img src="/2021/11/15/%E6%95%B0%E6%8D%AE%E6%AD%A3%E7%A1%AE%E6%80%A7%E4%B8%8E%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/4.jpeg" style="zoom:50%;"></p>
<p>运用这种形式操作的步骤也是不同的，是先把数据取出来再修改好，然后一并写回。不是像undo一样改一个写回一个。那么如果在3和4之间发生中断，此时还没有写回到数据表当中，在日志里只记录了两行，那么此时系统不需要做任何事情。数据表中A和B仍然为3</p>
<p>那么如果在5和6之间发生中断，这时候A的值已经被改成5，B的值仍然为3，而此时在日志里，<code>log(O1,A,5)</code>和<code>log(O1,B,5)</code>已经被记录下来，<code>log(O1,end)</code>也记录了下来。系统会判断操作已经结束，然后去检查数据表中的数据是否与日志匹配，结果发现B仍然是3，因此判定出现了异常现象，此时只要把数据表中的B修改成日志当中记录的5即可</p>
<p>所以undo是把数据表恢复成操作开始前的状态(回滚)，redo是把数据表恢复成操作结束后的状态</p>
<p>那么undo和redo有什么优缺点呢？</p>
<p>redo：</p>
<ul>
<li>优点： 效率比较高，性能比较好，对数据的修改可以推迟到所有操作结束之后，记录日志和回写硬盘分开进行</li>
<li>缺点：对内存消耗比较高，当内存满了以后再添加会导致数据丢失。</li>
</ul>
<p>undo:</p>
<ul>
<li>优点：对内存的消耗比较小，没修改一个值内存就会直接释放</li>
<li>缺点：需要同步记录日志和修改数据的值，导致记录日志和回写硬盘交错进行，是随机访问硬盘的情况，性能较差。</li>
</ul>
<p>在数据库中，我们会把这两种日志结合在一起，称为 undo/redo 日志。这种日志不但包含了每个元素修改之前的值，还包含了元素修改之后的值。这样就可以规避undo/redo日志各自的缺点。如下：</p>
<p><img src="/2021/11/15/%E6%95%B0%E6%8D%AE%E6%AD%A3%E7%A1%AE%E6%80%A7%E4%B8%8E%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/5.jpeg" style="zoom:50%;"></p>
<p>一旦发生故障，就有两种选择，如果发现日志已经完整了，我们可执行redo操作；如果日志并不完整，可以执行undo操作。</p>
<p>运用undo/redo日志，我们看到每个操作都可以看做是时间轴上的某一个点，即使发生故障，故障也可以看做是时间轴上的一个点，只不过之后会做undo/redo罢了，也就是我们利用日志可以保证在发生系统故障的时候数据操作的原子性</p>
<h4 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h4><p><img src="/2021/11/15/%E6%95%B0%E6%8D%AE%E6%AD%A3%E7%A1%AE%E6%80%A7%E4%B8%8E%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/10.png" style="zoom:50%;"></p>
<p>这个是Undo日志，O2已经完成了但是O1还没有完成，因此undo会把A回滚成原来的样子，即3</p>
<p><img src="/2021/11/15/%E6%95%B0%E6%8D%AE%E6%AD%A3%E7%A1%AE%E6%80%A7%E4%B8%8E%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/11.png" style="zoom:50%;"></p>
<p>对于B，在Undo日志下，我们要 先记录元素原本的值，然后再讲修改后的值写回硬盘</p>
<p><img src="/2021/11/15/%E6%95%B0%E6%8D%AE%E6%AD%A3%E7%A1%AE%E6%80%A7%E4%B8%8E%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/12.png" style="zoom:50%;"></p>
<p>谁最后结束，磁盘上的值就是谁操作的值。</p>
<p>对于D， 操作o1 是最后结束的，因此最后应该<code>write(A=5)</code>而不是<code>write(A=6)</code></p>
<p><img src="/2021/11/15/%E6%95%B0%E6%8D%AE%E6%AD%A3%E7%A1%AE%E6%80%A7%E4%B8%8E%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/13.png" style="zoom:50%;"></p>
<p>对于C，由于undo的存在，在执行到一般的时候，可能会发生回滚导致已经结束的CRUD操作撤销。不一定会顺利完成</p>
<p><img src="/2021/11/15/%E6%95%B0%E6%8D%AE%E6%AD%A3%E7%A1%AE%E6%80%A7%E4%B8%8E%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/14.png" style="zoom:50%;"></p>
<h3 id="并发控制机制"><a href="#并发控制机制" class="headerlink" title="并发控制机制"></a>并发控制机制</h3><p>然而，我们还没有解决如何在并发时保持数据的原子性。因此当两个线程同时处理两个操作的时候，可能会出现数据错误的情况，如下：</p>
<p><img src="/2021/11/15/%E6%95%B0%E6%8D%AE%E6%AD%A3%E7%A1%AE%E6%80%A7%E4%B8%8E%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/6.jpeg" style="zoom:50%;"></p>
<p>假设CPU一开始收到O2的前半部分，修改了A的值为5，这时候发生了中断，转而去执行操作O2的所有指令，这时A变成了10，B变成了6；执行完O2后回过来执行O1，最终B为8</p>
<p>我们发现本应该是相同的A和B，但是当线程之间发生调度的时候，会出现AB不相等的情况。为了解决这个问题，可以使用<strong>锁</strong>或者<strong>时间戳</strong>机制</p>
<p><img src="/2021/11/15/%E6%95%B0%E6%8D%AE%E6%AD%A3%E7%A1%AE%E6%80%A7%E4%B8%8E%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/7.jpeg" style="zoom:50%;"></p>
<p>如上图，在操作1读取A之前，就要给A上锁，这样即使发生了线程调度，在运行了三句指令后去执行操作O2，CPU会发现A上的锁还没有解开，因此无法读取A的值。</p>
<p>这样一来，只有当操作O1完全结束，将锁完全释放之后，才能继续执行操作O2</p>
<p>上面所讲的日志和锁保证了数据库内单步操作的原子性，但是在事务型应用中，事务是由一系列的访问构成的，我们要保证整个事务的正确性，因此需要数据库提供事务处理接口。接下来我们我们就来学习这方面的知识。</p>
<h3 id="应用层面的数据正确性"><a href="#应用层面的数据正确性" class="headerlink" title="应用层面的数据正确性"></a>应用层面的数据正确性</h3><p>我们首先用一个例子来说明为什么应用中的事务逻辑会出现正确性问题：</p>
<p><img src="/2021/11/15/%E6%95%B0%E6%8D%AE%E6%AD%A3%E7%A1%AE%E6%80%A7%E4%B8%8E%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/8.jpeg" style="zoom:50%;"></p>
<p>那么当我们要修改账户的名称为Bob的时候，编写的程序应该如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Update —&gt; User.ID &#x3D; &#39;3679&#39;</span><br><span class="line">Set name &#x3D; &#39;Bob&#39;;</span><br><span class="line">For(Doc.AuthorID &#x3D;&#39;3679&#39;)&#123;</span><br><span class="line">	set Doc.AuthorName &#x3D; &#39;Bob&#39;,&#125;</span><br><span class="line">END for</span><br></pre></td></tr></table></figure>
<p>这时候，如果在第二行和第三行之间计算机发生了故障，软件重启后，就发现仅仅修改了User文档中的名字，而没有吧博客文章文档里面的每个名字都进行修改。这就出现异常了，但是这和数据库本身并没有问题，因此我们要提出新的机制来解决应用层面的数据正确性问题。</p>
<h4 id="用标志位防止数据异常"><a href="#用标志位防止数据异常" class="headerlink" title="用标志位防止数据异常"></a>用标志位防止数据异常</h4><p>和记录日志类似，我们也有一种机制，能让程序记起来自己在哪里中断，已经执行了什么，还未执行什么。</p>
<p>因此我们可以在User文档中新增一个标志位namesync,当user名字被修改的时候，会将其设为ToDo，用于告诉系统还有工作没做完。当所有博客文章的作者名称都修改完之后，才会更新标志位为 Done，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Update —&gt; User.ID &#x3D; &#39;3679&#39;</span><br><span class="line">Set name &#x3D; &#39;Bob&#39;;</span><br><span class="line">Set namesync &#x3D; &#39;ToDo&#39;	&#x2F;&#x2F;设置标志位为To DO</span><br><span class="line">For(Doc.AuthorID &#x3D;&#39;3679&#39;)&#123;</span><br><span class="line">	set Doc.AuthorName &#x3D; &#39;Bob&#39;,&#125;</span><br><span class="line">END for</span><br><span class="line">Update -&gt;User.ID &#x3D; &#39;3679&#39; Set namesync &#x3D;&#39;Done&#39; &#x2F;&#x2F;重新更新标志位为Done</span><br></pre></td></tr></table></figure>
<p>这样，当故障发生，系统重启之后，会去检查namesync的值，如果是todo，那么就会重新执行一遍改名操作。最后再将namesync置为Done即可</p>
<h4 id="用消息队列防止数据异常"><a href="#用消息队列防止数据异常" class="headerlink" title="用消息队列防止数据异常"></a>用消息队列防止数据异常</h4><p>之前我们是在一个现有文档里面添加一个标志位来保证正确性。现在我们可以新建一个任务文档来存放将要做的任务，我们称之为消息队列。示意图如下：</p>
<p><img src="/2021/11/15/%E6%95%B0%E6%8D%AE%E6%AD%A3%E7%A1%AE%E6%80%A7%E4%B8%8E%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/9.jpeg" style="zoom:50%;"></p>
<p>当我们要做这个互相加好友的操作的时候，消息队列中新插入一个任务，把status置为 Todo，意为还未完成。然后，当双方都完成更新操作，才将status置为Done </p>
<p>注意了，若要用消息队列来保证应用的数据正确性，首先要满足一个前提——操作的幂等性，也就是不管操作几次，最终的结果是一样的。</p>
<p>比如，向集合里插入一个元素、从集合里删除一个元素、对一个元素进行赋值，都属于幂等操作。但是对一个数据进行自增，这就不幂等了因为每次操作和原来的值都不一样</p>
<h3 id="事务处理的概念"><a href="#事务处理的概念" class="headerlink" title="事务处理的概念"></a>事务处理的概念</h3><p>上面两种操作是在APP层面保证应用的数据正确性，但是这给程序员增加了不少负担。</p>
<p>此外，可能有多个程序员编写多个程序在访问同一张表。一个程序员可能能让自己的程序保持数据的正确性，但是他不能要求别的程序员也这样。因此程序会变得非常复杂。</p>
<p>因此，我们想能不能把这些工作交给数据库去做呢? 这就叫做事务处理。</p>
<p>什么是事务(Transaction)？我在mysql博客中已经介绍过了，事务就是一段可以实现特定功能的SQL代码，在执行失误的时候，所有的语句都必须成功执行，否则事务就会失败。</p>
<p>Transaction有几个特性：</p>
<p>第一个是原子性(Atomicity)，也就是不可分割的，一个事务不管其包含多少语句都是一个完整的整体。除非事务中所有的语句都成功执行，Transaction才算执行成功，否则所有修改都会回滚。</p>
<p>第二个特性是一致性(Consistency)，也就是说我们使用了Transaction之后，我们的数据库永远保持一致。 </p>
<p>第三个特性是隔离性(Isolation)。也就是说 Transaction之间是相互隔离的，特别是他们要修改同一个数据的时候。他们之间不受影响。如果多个Transaction要修改同一个数据，这条记录就会被锁定，每次只能有一个Transaction有权修改。其他的Transaction需要等待这个Transaction执行完毕</p>
<p>最后一个是持久性(Durability), 意思是一但一个Transaction被提交，它的修改就是永久性的，无法撤销，其他任何崩溃的情况（停电、宕机),也不会影响数据的修改</p>
<p>我们称这四个属性为 ACID</p>
<p>使用了事务，就好像每个事务都只是数轴上的一个点，不会互相干扰。 </p>
<h3 id="合理使用事务"><a href="#合理使用事务" class="headerlink" title="合理使用事务"></a>合理使用事务</h3><p>现在我们举几个例子来说明事务的功能</p>
<p>首先是订电影票的流程:</p>
<ul>
<li>获取空闲座位</li>
<li>展示座位</li>
<li>用户选择座位</li>
<li>用户提交订票请求</li>
<li>将座位分配给用户</li>
</ul>
<p>一种朴素的思想是将这五步都包含在一个Transaction里面，虽然能保证事务的完整性，但是这就使得该订票平台每次只能有一个人订票。因为数据库在一开始就会把所有的位置上锁，当整个订票过程结束后才解锁。这样就无法满足高并发的情况</p>
<p>因此更好的方法是在第四步和第五步之间 Beigin Transaction。但当电影特别火的时候，还是有可能同时多个人抢一个座位的情况。因此我们不能简单的将座位直接分配给客户，而是在分配前做一个判断</p>
<blockquote>
<ul>
<li>获取空闲座位</li>
<li>展示座位</li>
<li>用户选择座位</li>
<li>用户提交订票请求</li>
</ul>
<p>Begin Transaction</p>
<ul>
<li>座位是否空闲<ul>
<li>IF空闲，分配给用户</li>
<li>ELSE不空闲，数据库abort，前端return SORRY</li>
</ul>
</li>
</ul>
<p>End Transaction¢</p>
</blockquote>
<p>因此，我们引出了一个创建事务的原则：事务应该是短小的。否则会导致整个程序的性能下降</p>
<hr>
<p>现在我们用另外一个例子：</p>
<p>购买商品的过程：</p>
<ul>
<li>IF 账户余额 &lt;= 商品价格，Then 取消</li>
<li>调用商品运输服务</li>
<li>账户余额  -= 商品价格 </li>
</ul>
<p>我们当然可以把这三步都放在一个Transaction里面，但是我们会发现，好像第二步和第一步、第三步的关系没有那么紧密，他并没有处理的用户账户相关的问题。因此更好的解决办法如下：</p>
<blockquote>
<p>Begin Transaction</p>
<ul>
<li>IF 账户余额 &lt;= 商品价格，Then 取消</li>
<li>账户余额  -= 商品价格 </li>
</ul>
<p>End Transaction</p>
<ul>
<li>调用商品运输服务</li>
</ul>
</blockquote>
<p>我们要根据不同的情境来设计不同的事务</p>
<h4 id="例题-1"><a href="#例题-1" class="headerlink" title="例题"></a>例题</h4><p><img src="/2021/11/15/%E6%95%B0%E6%8D%AE%E6%AD%A3%E7%A1%AE%E6%80%A7%E4%B8%8E%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/15.png" style="zoom:50%;"></p>
<p>我们要注意一句话：在拿到所有锁之前，不能释放任何一个锁。比如下面这个例子</p>
<p><img src="/2021/11/15/%E6%95%B0%E6%8D%AE%E6%AD%A3%E7%A1%AE%E6%80%A7%E4%B8%8E%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/18.jpeg" style="zoom:50%;"></p>
<p>当O1运行到 <code>x-&gt;A</code>后，如果释放了锁，这时候如果发生了线程调度，调到O2，马上锁住并全部运行完成。这就会造成数据不同步。因此即使手上有1000把锁，如果事务中还有锁没拿到，那么就一把锁也无法释放。</p>
<p>因此，这也告诉我们事务要设计的短一些。</p>
<p><img src="/2021/11/15/%E6%95%B0%E6%8D%AE%E6%AD%A3%E7%A1%AE%E6%80%A7%E4%B8%8E%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/16.png" style="zoom:50%;"></p>
<p>对于A，在读取A元素之前必须先给A上锁，因此错误</p>
<p>对于C，同理</p>
<p>对于D，我们要记住：先Start再锁，先End再释放。因为如果先锁再记录日志，这种逻辑很怪，因为在锁的时候我并不知道我的目标元素是什么</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="/2021/11/15/%E6%95%B0%E6%8D%AE%E6%AD%A3%E7%A1%AE%E6%80%A7%E4%B8%8E%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/1.png"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2021/11/14/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/14/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/" class="post-title-link" itemprop="url">关系数据库设计</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-14 13:26:42" itemprop="dateCreated datePublished" datetime="2021-11-14T13:26:42+08:00">2021-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-23 14:19:34" itemprop="dateModified" datetime="2022-11-23T14:19:34+08:00">2022-11-23</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="关系数据库设计"><a href="#关系数据库设计" class="headerlink" title="关系数据库设计"></a>关系数据库设计</h1><h3 id="关系数据库设计-1"><a href="#关系数据库设计-1" class="headerlink" title="关系数据库设计"></a>关系数据库设计</h3><p>怎么做好数据库的设计？</p>
<ol>
<li>需求分析, 解决存什么</li>
<li>概念设计，用何种模式</li>
<li>数据库结构设计，解决怎么存</li>
</ol>
<h4 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h4><p>下图是一个博客系统的4个界面</p>
<p><img src="/2021/11/14/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/1.png" style="zoom:67%;"></p>
<p>这个界面的功能如上图所示：</p>
<ul>
<li><p>第一个登录界面，用户能看见自己的个人信息以及关注的人写的文章</p>
</li>
<li><p>第二个界面，是用户点击特定一篇文章后显示的文章具体信息</p>
</li>
<li>第三个界面是用户点击新建按钮之后显式的，用于新创建一个界面</li>
<li>第四个界面是其他博主的主页，我们可以看到他写了那些文章，它关注的人之类的信息。</li>
</ul>
<h4 id="概念设计"><a href="#概念设计" class="headerlink" title="概念设计"></a>概念设计</h4><p>现在我们解决了需求分析，现在我们来做概念设计</p>
<p>对于这个 博客系统，我们需要创建三个对象：用户、文章、评论，其关系如下：</p>
<p><img src="/2021/11/14/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/2.jpeg" style="zoom:67%;"></p>
<h4 id="结构设计"><a href="#结构设计" class="headerlink" title="结构设计"></a>结构设计</h4><p>我们要用关系来表示这三个对象，用表格来呈现，每一列代表对象的一个属性，如下图所示：</p>
<p><img src="/2021/11/14/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/3.jpeg" style="zoom:67%;"></p>
<p>首先，黑色部分代表的是用表来展示对象的基本属性的部分。比如对于用户，有名字、信息等；比如对文档，有标题和内容等</p>
<p>但是怎么用表格来表示对象与对象之间的关系呢？</p>
<p>首先，对于文章和作者、评论和作者这类多对1关系，可以直接在后面加上一列。比如说，对于Doc表格，我们可以在后面加上一列Author，来记录UID，这样就能记录这篇文章的作者了</p>
<p>对于评论，我们可以在表后添加两列，分别记录代表作者的UID和代表文章的DID。用来表示这条评论所在的文章及其作者</p>
<p>但是对于用户和用户之间的关注关系，因为是多对多的，我们发现没有办法通过添加一列的方式来呈现，因此我们可以另外新建一张表，里面有Followee和Follower两列，分别记录被关注者和关注者的UID即可。</p>
<p>这是我们比较朴素的设计思路，接下来我们介绍一种更系统的设计方法——ER图</p>
<h3 id="ER图"><a href="#ER图" class="headerlink" title="ER图"></a>ER图</h3><p>ER图的表现形式有点类似于前面面向对象的模型的概念设计。但是在表达对象与对象之间的关系有一些更细致的方式。</p>
<p>ER图中的E 代表 Entity(实体)，R代表Relationship(联系)，ER图也就是用来刻画实体与实体之间关系的示意图 </p>
<p>比如说员工参与项目这个模型我们可以画出其ER图</p>
<p><img src="/2021/11/14/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/4.jpeg" style="zoom:67%;"></p>
<h4 id="形状的意义"><a href="#形状的意义" class="headerlink" title="形状的意义"></a>形状的意义</h4><p>其中，方块代表实体；圆圈代表实体中的属性；而菱型则代表连接对象之间的关系。</p>
<p>我们从这张ER图中可以很清楚的看到：员工实体有<strong>工号、名字、技能</strong>这三个属性；项目有<strong>项目号、经费</strong>；此外，参与这个关系也有一个<strong>工时</strong>属性，代表每个人需要花多少时间在这个项目上。</p>
<h4 id="箭头的意义"><a href="#箭头的意义" class="headerlink" title="箭头的意义"></a>箭头的意义</h4><p>现在，我们要了解不同类型的属性</p>
<ul>
<li><strong>唯一属性</strong>，就是可以唯一识别一个实体的属性。比如员工里的工号，项目中的项目号。通常我们可以把唯一属性看做是这个实体中的ID。</li>
<li><strong>单值属性</strong>，名字和经费这类属性可以被称为单值属性，因为一个员工只能对应一个名字，一个项目只能对应一笔经费</li>
<li><strong>多值属性</strong>，员工的技能并不是单值属性，一个员工可以由多个技能。</li>
</ul>
<p>不同的属性反映在不同的箭头上，唯一属性用双向箭头表示，单值属性用单箭头表示，双值属性用两个同一朝向的箭头表示 </p>
<h4 id="菱形两变m和1的意义"><a href="#菱形两变m和1的意义" class="headerlink" title="菱形两变m和1的意义"></a>菱形两变m和1的意义</h4><p>在上面这个ER图中，我们看到员工和项目之间的参与关系是一个多对多的关系，一个员工可以参加多个项目，同样一个项目也需要多个员工共同完成.因此参与关系的两变是m和m</p>
<p>还有两种关系：一对多/多对一，一对一</p>
<p>比如员工和部门之间就是多对一的关系，一个员工只能属于一个部门，而部门可以有多个员工</p>
<p>比如经理和部门之间的就是一对一关系，一个员工只能管理一个部门</p>
<h4 id="不同关系的表现形式"><a href="#不同关系的表现形式" class="headerlink" title="不同关系的表现形式"></a>不同关系的表现形式</h4><p>上面我们所介绍的都是两个实体之间的二元联系，其实联系可以有更多种不同的方式，比如：</p>
<p><img src="/2021/11/14/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/6.jpeg" style="zoom:67%;"></p>
<p>比如说，同样是员工，上级对下级是一个一对多的关系。我们可以用左边这张图来表示</p>
<p>对于用户、运营商、手机这样一个三边关系，很难说是一对多还是一对一的关系，我们直接用语言描述即可：一个用户可以有多台手机，一个手机只能属于一位用户，而且一个手机只能对应一个运营商</p>
<h4 id="延伸"><a href="#延伸" class="headerlink" title="延伸"></a>延伸</h4><p>ER图还有其他更细枝末节的刻画，比如说商品和食品、电子产品之间其实是父类和子类的关系。</p>
<p>还有弱实体的概念，比如说一个房间的存在依附于一座楼房的存在，一座城市依附于一个国家的存在</p>
<h3 id="从ER图到关系模型设计"><a href="#从ER图到关系模型设计" class="headerlink" title="从ER图到关系模型设计"></a>从ER图到关系模型设计</h3><p>我们之前设计出来了ER图，现在我们要介绍一种固定的方式来将设计出来的ER图转换成一张或者几张关系表。我们还是以上面员工和项目的ER图为例</p>
<ul>
<li>首先要将实体类中的唯一属性和单值属性选出来创建一张表</li>
</ul>
<p><img src="/2021/11/14/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/7.jpeg" style="zoom:67%;"></p>
<ul>
<li>对于多值属性，要单独拿出来刻画，比如说对于技能，要新建一张skills表来存放。</li>
</ul>
<p><img src="/2021/11/14/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/8.jpeg" style="zoom:67%;"></p>
<ul>
<li><p>最后我们来处理联系，对于一对一或者一对多联系，我们可以将其归并到一个实体当中。但是对于多对多联系，我们需要新建一张表来存放这个联系</p>
<ul>
<li>我们假设员工和项目是一对一的联系,我们可以任意选择一个实体关系表，在后面加上一列。比如：</li>
</ul>
<p><img src="/2021/11/14/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/9.jpeg" style="zoom:67%;"></p>
<ul>
<li>我们假设员工和项目是多对一的联系，我们在多的那一个实体关系表后加一列，即可，这里还是选择加在员工表后</li>
</ul>
<p><img src="/2021/11/14/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/9.jpeg" style="zoom:67%;"></p>
<ul>
<li>事实上，员工和项目是多对多的联系，因此我们要新建第四张表，这张表有三列，前两列分别存放员工的ID和项目的ID，第三列存放工时</li>
</ul>
<p><img src="/2021/11/14/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/10.jpeg" style="zoom:67%;"></p>
</li>
</ul>
<h3 id="ER图在博客系统中的应用"><a href="#ER图在博客系统中的应用" class="headerlink" title="ER图在博客系统中的应用"></a>ER图在博客系统中的应用</h3><p><img src="/2021/11/14/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/11.jpeg" style="zoom:67%;"></p>
<p>我们这里忽略了每一个实体的内部属性，把目光聚焦到关系上. 首先在这个类似于三角形的关系中，一共有三个一对多关系，因此在Doc表后加一列，在Comment表后加一列。此外，我们还要处理”关注”这个关系，新建一张表即可。</p>
<p><img src="/2021/11/14/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/12.jpeg" style="zoom: 50%;"></p>
<p>我们看到，从ER图到关系表，是一个机械化的过程。只要按部就班来即可</p>
<h4 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h4><p><img src="/2021/11/14/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/16.png"></p>
<p><img src="/2021/11/14/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/17.png"></p>
<p>一般联系要么可以附着在一张现有的表上(1 to1,1 to m) ，如果单提取出来新建一张表，说明这是m to m的关系，那么这一行是由关系两边的实体的ID共同确定的，即A</p>
<h3 id="关系数据库中的冗余"><a href="#关系数据库中的冗余" class="headerlink" title="关系数据库中的冗余"></a>关系数据库中的冗余</h3><p>我们在数据库中随处可见冗余，如下表所示：</p>
<p>这里第二行和第三行出现了两个May，然后Price，P_name 也出现了重复的情况</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>UID</th>
<th>U_name</th>
<th>PID</th>
<th>P_name</th>
<th>Price</th>
<th>Quant</th>
<th>Date</th>
</tr>
</thead>
<tbody>
<tr>
<td>U001</td>
<td>Jason</td>
<td>P003</td>
<td>Pencil</td>
<td>10</td>
<td>1</td>
<td>3/10/2020</td>
</tr>
<tr>
<td>U002</td>
<td>May</td>
<td>P002</td>
<td>Soap</td>
<td>5</td>
<td>2</td>
<td>4/10/2020</td>
</tr>
<tr>
<td>U003</td>
<td>May</td>
<td>P003</td>
<td>Pencil</td>
<td>10</td>
<td>2</td>
<td>8/10/2020</td>
</tr>
<tr>
<td>U004</td>
<td>Bob</td>
<td>P002</td>
<td>Soap</td>
<td>5</td>
<td>1</td>
<td>10/10/2020</td>
</tr>
</tbody>
</table>
</div>
<p>这是因为这张表是由 UID和PID确定的，但是U_name是单独由UID确定的，P_name和Price则是有PID单独确定的。在这张表里主键是UID+PID，因此在一张表里面当UID或PID重复出现多次的话，会造成数据的冗余。</p>
<p>冗余的副作用很多，最直观也最易理解的就是当我们修改一个值的时候需要将整个表扫描一遍然后修改所有冗余值。如何规避掉这些冗余，需要在我们设计数据库的时候下功夫。</p>
<p>现在我介绍一下<strong>函数依赖</strong>的含义：</p>
<p>比如u_name 是依赖于UID的，p_name,price是依赖于PID的。在这张表中有这两个函数依赖</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">UID -&gt; u_name</span><br><span class="line">PID -&gt; p_name,price</span><br></pre></td></tr></table></figure>
<p>当函数依赖的两个决定属性(UID和PID)不等于这个表的主键的时候，就会出现冗余的情况</p>
<h3 id="数据库设计的规范化"><a href="#数据库设计的规范化" class="headerlink" title="数据库设计的规范化"></a>数据库设计的规范化</h3><p>为了规避冗余，我们需要把我们的数据库设计的规范化，只要我们按照ER图的设计理念，构造出来的数据表一般都是规范的。就像上面这张表，以规范化的方式我们要将其拆分为三张小表，如下：</p>
<p>User</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>UID</th>
<th>U_name</th>
</tr>
</thead>
<tbody>
<tr>
<td>U001</td>
<td>Jason</td>
</tr>
<tr>
<td>U002</td>
<td>May</td>
</tr>
<tr>
<td>U003</td>
<td>Bob</td>
</tr>
</tbody>
</table>
</div>
<p>Product </p>
<div class="table-container">
<table>
<thead>
<tr>
<th>PID</th>
<th style="text-align:left">Pname</th>
<th>Price</th>
</tr>
</thead>
<tbody>
<tr>
<td>P002</td>
<td style="text-align:left">soap</td>
<td>5</td>
</tr>
<tr>
<td>P003</td>
<td style="text-align:left">pencil</td>
<td>10</td>
</tr>
</tbody>
</table>
</div>
<p>Order</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>UID</th>
<th>PID</th>
<th>Quant</th>
<th>Date</th>
</tr>
</thead>
<tbody>
<tr>
<td>U001</td>
<td>P003</td>
<td>1</td>
<td>3/10/2020</td>
</tr>
<tr>
<td>U002</td>
<td>P002</td>
<td>2</td>
<td>4/10/2020</td>
</tr>
<tr>
<td>U002</td>
<td>P003</td>
<td>2</td>
<td>8/10/2020</td>
</tr>
<tr>
<td>U003</td>
<td>P002</td>
<td>1</td>
<td>10/10/2020</td>
</tr>
</tbody>
</table>
</div>
<p>我们要记住宽表拆分的原则：</p>
<p>让同时被使用到的属性(即出现在同一个SQL中的属性)尽可能位于拆分后的一张表中。这样子就不用再做连接，增加开销了</p>
<h3 id="冗余带来的好处"><a href="#冗余带来的好处" class="headerlink" title="冗余带来的好处"></a>冗余带来的好处</h3><p>上面我们介绍了两种模式(大表和拆分后的三张表)现在我们来讨论哪一种模式更好。</p>
<p>比如说对于查询</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SELECT u_name,p_name</span><br><span class="line">FROM XXX</span><br><span class="line">WHERE date &#x3D; ? AND price*quant &gt;1000</span><br></pre></td></tr></table></figure>
<p>虽然大表有冗余，但是能很高效的来处理这段查询。反观拆分的三张表格，我们需要对其做连接后才能完成查询，代价比较大。</p>
<p>那么在规范化和非规范化之间应该怎么选择呢？我们要看看那些冗余的属性被修改的几率是否很高。</p>
<p>如果修改的几率很大，那么对于大表来说每次修改的开销就很大，就不划算；反之则可以选择大表来存储数据</p>
<p>但是对于这张表来说，我们在商城注册了之后，一般就不回去更改名字；且商品一旦创建之后，也不太会修改它的名字。价格这列见仁见智了。因此总体来说对于这套数据库系统，非规范化导致的数据冗余带来的负面作用是比较小的，但正面效果是比较好的。</p>
<p>所以说，非规范化和规范化都有其使用场景。</p>
<h3 id="如何利用冗余"><a href="#如何利用冗余" class="headerlink" title="如何利用冗余"></a>如何利用冗余</h3><p>现在我们有两张表：<code>Product(PID,P_name,Price)</code> 和 <code>Order(BID,UID,PID,Quant,Date)</code></p>
<p>然后我们用这个查询</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">Sum</span>(Quant)*Price </span><br><span class="line"><span class="keyword">FROM</span> Product,<span class="keyword">Order</span></span><br><span class="line"><span class="keyword">WHERE</span> Product.PID = Order.PID</span><br><span class="line"><span class="keyword">AND</span> Product.PID = ?</span><br></pre></td></tr></table></figure>
<p>这个查询非常普遍，就是要我们去计算某一件特定商品的总销售额。如果我们采用标准化的关系表，那么会非常慢，因为Product和Order会连接成一张大表，我们需要扫描这张表中所有的内容，然后再统计计算才能得到结果。因此这时候我们可以用冗余来帮助我们简化查询，方法也很简单就是给Product加一个TotalSales属性</p>
<p><code>Product(PID,P_name,Price,TotalSales)</code> </p>
<p>这样，我们就可以直接获得TotalSales了，而我们所需要付出的代价，仅仅是每次下订单的时候额外更新一下Product表格即可</p>
<p>最后我们对非规范化和冗余做一个延伸和总结。冗余经常会给我们的程序开发带来一些好处。最具有代表性的就是缓存机制。比如web server会专门开一片区域当缓存来存放数据库中经常被调用的那些值。 但是缓存是冗余的值，没了缓存还是可以正常运行，但是有缓存会大大提升程序的运行速度。</p>
<p>还有比如数据库内部的索引，这也是冗余的一种表现。</p>
<h3 id="网上购物场景实例"><a href="#网上购物场景实例" class="headerlink" title="网上购物场景实例"></a>网上购物场景实例</h3><ul>
<li>网站陈列了各种各样的商品，提供商品的详细信息，包括商品名称、种类、价格、仓储地址等。</li>
<li>用户可以登录到网站，浏览并选择商品。</li>
<li>用户提交购买请求，包括：购买商品的种类和数量、购买客户的姓名和地址等。</li>
<li>系统审核用户的购买请求，完成购买，整个过程包括调用外部系统实现付款和送货，以及记录用户的购买历史。</li>
</ul>
<h4 id="创建ER图"><a href="#创建ER图" class="headerlink" title="创建ER图"></a>创建ER图</h4><p>我们对于用户和商品之间的购买关系，有两种处理方式，第一种是用一个简单关系带描述(菱形)，第二种则是通过用户下订单、订单中包含商品的方式来呈现，如下图所示。</p>
<p><img src="/2021/11/14/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/13.jpeg" style="zoom: 50%;"></p>
<p>那么应该采用哪种方式呢？</p>
<p>选择标准就是这种关系能否由两个实体的ID去确定。比如UID和PID能唯一确定一个购买关系吗？答案是不能的，因为一个用户可以重复下单同一件商品，仅用UID和PID是反映出来的</p>
<p><img src="/2021/11/14/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/14.jpeg" style="zoom: 50%;"></p>
<h3 id="例题-1"><a href="#例题-1" class="headerlink" title="例题"></a>例题</h3><ol>
<li><p><img src="/2021/11/14/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/19.png"></p>
</li>
</ol>
<p>因为只有少量员工有两个电话号码，那么我们没有必要新开一张表去存储多值属性的关系，直接新建一列即可，这一列大多数都是空白的。不会让这张表变得非常宽</p>
<ol>
<li><p><img src="/2021/11/14/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/18.png"></p>
</li>
<li><p>某关系表R的外键是指：</p>
</li>
</ol>
<p>A. 其他关系表的键，可以是R的任意属性</p>
<p>B. 该关系表R除主键之外的另一个键</p>
<p>C. 其它关系表的键，同时必须作为R的主属性(即R的键包含的属性)</p>
<p>D. 其他关系表的键，同时必须作为R的非主属性(即不被R的键包含)</p>
<p>在了解这个问题之前，我们首先要学习 主码、候选码、主属性、非主属性 的定义</p>
<ol>
<li>候选码的定义：如果关系中的某一<strong>属性组</strong>的值能唯一地标识一个元组，则称该属性组为候选码；</li>
<li>主码的定义：如果一个关系有多个候选码，则选定其中一个为主码；</li>
<li>主属性定义：候选码的诸属性称为主属性；</li>
<li>非主属性定义：不包含在任何候选码中的属性称为非主属性；</li>
</ol>
<p>因为我们可以通过两张表的主键来定义一张表格，这两个主键都是这张表的外键。因此，外键在表中也是用来标识一个元组的，是主属性。所以选择C</p>
<ol>
<li><p>请针对以下需求设计ER图，并构建相应的关系模式：</p>
<p>一个关于电影、制作人员和演员的网站（类似一个简易的IMDB网站）。用户可以浏览每一部电影的简介和相关信息（出品年份、电影类型、时长、评级），以及其导演、编剧和演员的列表。用户还可以浏览每一位导演、编剧或演员的信息（姓名、性别、年龄、简介），以及他们参与过哪些电影作品。导演、编剧或演员只是职位，一个人可以身兼数职。</p>
<p>每一位演员在其参演的电影中都扮演一定的角色。用户在浏览电影时，除了能看到演员信息，还能看到每位演员扮演了什么角色。用户在浏览演员时，除了能看到他（或她）参演的电影，还能看到他（或她）在每一部电影中扮演的角色。（注意：一位演员可以在一部电影中扮演多个角色。同一个角色也可能由多名演员扮演，比如，年少时由一位演员扮演，年老时由另一位演员扮演。）</p>
<p>用户登录后还可以针对每一部电影、每一位导演、编剧或演员进行评价和打分，供别人参考。</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2021/11/09/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch10-%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/09/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch10-%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/" class="post-title-link" itemprop="url">数据科学算法ch10-矩阵分解</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-09 19:04:53" itemprop="dateCreated datePublished" datetime="2021-11-09T19:04:53+08:00">2021-11-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-12-02 14:37:04" itemprop="dateModified" datetime="2021-12-02T14:37:04+08:00">2021-12-02</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2021/11/09/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch9-%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3%E4%B8%8E%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/09/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch9-%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3%E4%B8%8E%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/" class="post-title-link" itemprop="url">数据科学算法ch9-奇异值分解与主成分分析</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-09 19:04:26" itemprop="dateCreated datePublished" datetime="2021-11-09T19:04:26+08:00">2021-11-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-01-06 23:48:34" itemprop="dateModified" datetime="2022-01-06T23:48:34+08:00">2022-01-06</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="奇异值分解与主成分分析"><a href="#奇异值分解与主成分分析" class="headerlink" title="奇异值分解与主成分分析"></a>奇异值分解与主成分分析</h1><p>在机器学习中，随着数据维度的升高，分类器的性能会先增加后降低。这可能是因为维度过高会导致过拟合问题。为了缓解过拟合，我们可以用数据降维。因此现在我们来学习两种常用的数据降维方法：<strong>奇异值分解</strong>(SVD) 和 <strong>主成分分析</strong>(PCA)</p>
<p>对于是方阵的数据，我们可以用特征值分解，但如果数据不是方阵，只是普通的方阵，那么就可以使用奇异值分解和主成分分析</p>
<h2 id="对角化与特征值分解"><a href="#对角化与特征值分解" class="headerlink" title="对角化与特征值分解"></a>对角化与特征值分解</h2><blockquote>
<p>相似矩阵</p>
</blockquote>
<p>对于矩阵 $\boldsymbol {A,B}\in\boldsymbol R^{n\times n}$ ，若存在$n\times n$ 的可逆矩阵$\boldsymbol P$，使得 $\boldsymbol A=\boldsymbol {PBP}^{-1}$,则称矩阵$\boldsymbol {A}$和$\boldsymbol B$ 是相似的</p>
<blockquote>
<p>正交矩阵</p>
</blockquote>
<p>对于矩阵$\boldsymbol P$ ,若$\boldsymbol P^{-1} = \boldsymbol P^T$ 则称$ \boldsymbol P$ 为正交矩阵。</p>
<p>由于$\boldsymbol {PP}^T = \boldsymbol I$​ ，因此正交矩阵是一类特殊的线性变换，它是保持原点不动，长度不变的旋转变换 </p>
<blockquote>
<p>对角化</p>
</blockquote>
<p>若方阵$\boldsymbol A$ 相似于一个对角阵，即存在一个可逆矩阵P和对角矩阵 $\Sigma$ ，使得 $\boldsymbol A=\boldsymbol {P\Sigma P}^{-1}$ ，则称$\boldsymbol  A$是可对角化的</p>
<p>矩阵对角化的重要性在于，若矩阵 $\boldsymbol A$ 是可对角化的，那么会满足以下两条性质：</p>
<script type="math/tex; mode=display">
\boldsymbol A^k = \boldsymbol {P\Sigma}^k\boldsymbol P^{-1}\\
\boldsymbol A^{-1} = \boldsymbol {P\Sigma}^{-1}\boldsymbol P^{-1}</script><p>接下来给出对角化的相关定理</p>
<ul>
<li><p>矩阵 $\boldsymbol A \in \boldsymbol {R}^{n\times n}$ 可对角化的充分必要条件是$\boldsymbol A$ 有 n 个线性无关的特征向量</p>
</li>
<li><p>矩阵 $\boldsymbol A \in \boldsymbol {R}^{n\times n}$ 可对角化的充分必要条件是，该矩阵每一个特征值的<strong>几何重数</strong> 等于其 <strong>代数重数</strong> </p>
</li>
</ul>
<blockquote>
<p>正交对角化</p>
</blockquote>
<p>若存在对角阵 $\boldsymbol \Sigma $ 、正交矩阵 $\boldsymbol P$ ，使得$\boldsymbol A = \boldsymbol P\boldsymbol \Sigma\boldsymbol P^{T}$​ 则称A是可正交对角化的</p>
<p>正交对角化也是一种矩阵分解方法，该方法又被称为特征值分解。若矩阵$\boldsymbol A$ 是可正交对角化的，意味着矩阵 $\boldsymbol A$ 对应的线性变换可以分解成三步：</p>
<ol>
<li>将空间向量经过正交矩阵$\boldsymbol P^T$ 进行旋转变换</li>
<li>通过对角矩阵$\boldsymbol \Sigma$​ 进行伸缩变换</li>
<li>经过正交矩阵$\boldsymbol P$ 进行旋转变换，其中 $\boldsymbol P$ 和 $\boldsymbol P^T$ 互为逆变换</li>
</ol>
<p>接下来给出对角化的相关定理：<strong>若$\boldsymbol A$是实对称矩阵</strong>，那么$\boldsymbol A$具有一下性质</p>
<ol>
<li>不同特征值对应的特征向量之间是正交的</li>
<li>$\boldsymbol A$ 有n个实特征值(如果包含重数)</li>
<li>对于任一特征值，其对应的特征向量的集合重数等于特征值的代数重数</li>
<li>特征空间是相互正交的</li>
<li>$\boldsymbol A$​是可正交对角化的</li>
</ol>
<h3 id="特征值分解"><a href="#特征值分解" class="headerlink" title="特征值分解"></a>特征值分解</h3><p>求解对称方程 $A\in \mathbb R^{n\times n}$ 的特征分解步骤</p>
<ul>
<li><p>计算矩阵A的特征值$\lambda_1\cdots,\lambda_n$ 即 求特征方程 $|A-\lambda I| = 0$  的n个根</p>
</li>
<li><p>求特征值对应的n个相互正交的特征向量 $q_1,\cdots,q_n$即求解方程组并单位化</p>
<script type="math/tex; mode=display">
Aq_i = \lambda_iq_i,i=1,\cdots,n</script></li>
<li><p>记矩阵 $Q=(q_1,\cdots,q_n)$ </p>
</li>
<li><p>最终得到矩阵A的特征分解为 </p>
<script type="math/tex; mode=display">
A = Q\begin{pmatrix} \lambda_1 &  &  \\  & \cdots &  \\ & & \lambda_n \end{pmatrix}Q^T</script></li>
</ul>
<p>例：求实对称矩阵的特征分解</p>
<script type="math/tex; mode=display">
A =\begin{bmatrix} 2& 1\\ 1 & 2 \end{bmatrix}</script><ol>
<li>首先我们计算特征向量</li>
</ol>
<script type="math/tex; mode=display">
|\lambda I-A| =\begin{bmatrix}\lambda-2& -1\\ -1 & \lambda-2 \end{bmatrix}=0\\</script><p>特征值：$\lambda_1=3,\lambda_2=1$</p>
<p>带入求解并单位化： $q_1 = (\frac{1}{\sqrt 2},\frac{1}{\sqrt 2})^T,q_2 = (-\frac{1}{\sqrt2},\frac{1}{\sqrt2})^T$ </p>
<ol>
<li>写出特征向量方阵Q和特征值方阵 $\Lambda$ </li>
</ol>
<script type="math/tex; mode=display">
Q =[q_1,q_2] =\begin{bmatrix} \frac{1}{\sqrt2}& -\frac{1}{\sqrt 2}\\ \frac{1}{\sqrt 2} & \frac{1}{\sqrt2} \end{bmatrix},\Lambda =\begin{bmatrix} \lambda_1 & \\  & \lambda_2 \end{bmatrix} = \begin{bmatrix} 3 &\\ & 1 \end{bmatrix}</script><ol>
<li>$p=(\alpha_1,\alpha_2,\alpha_3),(p^{-1})^T=(\beta_1,\beta_2,\beta_3)$ </li>
</ol>
<script type="math/tex; mode=display">
G_1 = \alpha_1\beta_1^T+\alpha_2\beta_2^T\\~\\
G_2 = \alpha_3\beta_3^T</script><p>A和$G_1,G_2$的关系就是特征分解</p>
<h2 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h2><p>给定矩阵 $\boldsymbol A\in \boldsymbol R^{m\times n}$ ,则矩阵 $\boldsymbol A$ 的奇异值分解为：</p>
<script type="math/tex; mode=display">
\boldsymbol A = \boldsymbol U\boldsymbol\Sigma\boldsymbol V^T</script><h3 id="分解过程"><a href="#分解过程" class="headerlink" title="分解过程"></a>分解过程</h3><ol>
<li>计算矩阵 $\boldsymbol {AA}^T$ 和 $\boldsymbol A^T\boldsymbol A$ </li>
<li>分别计算矩阵  $\boldsymbol {AA}^T$ 和 $\boldsymbol A^T\boldsymbol A$ 的特征值和对应的特征向量</li>
<li>用矩阵  $\boldsymbol {AA}^T$ 的特征向量组成矩阵 $\boldsymbol U$;用 $\boldsymbol A^T\boldsymbol A$  的特征向量组成$\boldsymbol V$</li>
<li>对矩阵 $\boldsymbol {AA}^T$ 和 $\boldsymbol A^T\boldsymbol A$ 的非零特征值求算术平方根，并对应特征向量的位置填入$\boldsymbol \Sigma$ 的<strong>对角元</strong></li>
</ol>
<h2 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2021/11/09/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch8-%E7%89%B9%E5%BE%81%E5%80%BC%E8%AE%A1%E7%AE%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/09/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch8-%E7%89%B9%E5%BE%81%E5%80%BC%E8%AE%A1%E7%AE%97/" class="post-title-link" itemprop="url">数据科学算法ch8-特征值计算</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-09 19:03:35" itemprop="dateCreated datePublished" datetime="2021-11-09T19:03:35+08:00">2021-11-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-01-07 09:28:22" itemprop="dateModified" datetime="2022-01-07T09:28:22+08:00">2022-01-07</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="数据科学算法ch8-特征值计算"><a href="#数据科学算法ch8-特征值计算" class="headerlink" title="数据科学算法ch8-特征值计算"></a>数据科学算法ch8-特征值计算</h1><h2 id="幂法"><a href="#幂法" class="headerlink" title="幂法"></a>幂法</h2><p>如果说 低阶方阵我们可以用搞死消元法来计算，但是在高阶方阵中，用这种方法显然效率太低。因此，我们需要找到快速的、高效的特征值和特征向量计算方法</p>
<p>幂法就是这样一种方法，是计算举证<strong>最大的特征值和对应特征向量</strong>的一种<strong>向量迭代法</strong></p>
<p>算法过程：</p>
<ol>
<li>首先，令初始向量 $x_0$ </li>
<li>向前递推，公式：当 $k=1,2\cdots$</li>
</ol>
<script type="math/tex; mode=display">
\begin{cases}
y_k = Ax_{k-1}\\
m_k = \max(y_k)\\
x_{k} = y_k/m_k\\
\end{cases}</script><ol>
<li>收敛到 $x_k^T,m$ 不再变化的时候，停止</li>
</ol>
<h2 id="反幂法"><a href="#反幂法" class="headerlink" title="反幂法"></a>反幂法</h2><p>反幂法是用来求解矩阵A 绝对值最小的非零特征值$\lambda_n$ 的。</p>
<p>算法如下：</p>
<ol>
<li>任取初始向量 $x_0\neq 0$</li>
<li>当 $k=1,2\cdots$ 时</li>
</ol>
<script type="math/tex; mode=display">
\begin{cases}
y_k = A^{-1}x_{k-1}\Rightarrow Ay_k = x_{k-1}\\
m_k = \max(y_k)\\
x_{k} = y_k/m_k\\
\end{cases}</script><p>此外，反幂法的优势就是，可以通过修改幂法算法来求解方程的任意特征值。</p>
<p>算法如下：</p>
<ol>
<li>取 $x_0 = (1,\cdots,1)$， 和一个常数 $\lambda$ </li>
<li>对于 $k=1,2\cdots$</li>
</ol>
<script type="math/tex; mode=display">
\begin{cases}
(A-\lambda I)y_k = x_{k-1}\\
m_k = \max(y_k)\\
x_{k} = y_k/m_k\\
\end{cases}</script><p>最终我们可以得到，$\boldsymbol A$ 最接近于 $\lambda$ 的特征值等于： $1/m_k +\lambda$ ， 然后，标准化 $x_k$ 即为该特征值的对应特征向量</p>
<h2 id="瑞丽商迭代法"><a href="#瑞丽商迭代法" class="headerlink" title="瑞丽商迭代法"></a>瑞丽商迭代法</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/" class="post-title-link" itemprop="url">统计方法ch5-聚类1</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-02 09:47:18" itemprop="dateCreated datePublished" datetime="2021-11-02T09:47:18+08:00">2021-11-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-01-03 22:25:42" itemprop="dateModified" datetime="2022-01-03T22:25:42+08:00">2022-01-03</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="统计方法ch5-聚类1"><a href="#统计方法ch5-聚类1" class="headerlink" title="统计方法ch5-聚类1"></a>统计方法ch5-聚类1</h1><p>首先我们要来了解一下聚类思想。<strong>以类识物</strong>是人类认识世界的一种重要方式，但是人类自身是没有办法处理眼睛所捕获的大量信息的，因此我们通常会对个体的特征进行归纳，并将相似的个体归并为一类，一次来达到信息的整体性认识。比如说我们看到头上有”王”字的动物会将其归类与老虎，其实捕捉的是老虎脸上的特征。</p>
<p>那么聚类分析有什么作用呢？</p>
<ul>
<li>识别从属特定总体的个体</li>
</ul>
<p>比如说，研究消费者行为从而将市场进行细分，对消费者精准投放广告或者商品推荐</p>
<ul>
<li>识别异常个体</li>
</ul>
<p>比如说，检测用户的上网行为从而判断其行为是否正常，对政府企业等重要数据库进行保护以防止黑客攻击</p>
<p>在这一章中，我们主要来解决一下几个问题：</p>
<ol>
<li>如何定义个体之间的相似性</li>
<li>如何确定类别的数目</li>
<li>如何选取个体的特征</li>
<li>如何评价聚类方法的结果</li>
</ol>
<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><h4 id="基本定义"><a href="#基本定义" class="headerlink" title="基本定义"></a>基本定义</h4><p>我们知道在一些场合下，获取足够多数量的负样本其实是比较困难的。而聚类研究的就是那些无标签的数据集。因此，聚类分析是<strong>无监督学习</strong>中最为常用且重要的方法之一。数据集可以写成矩阵形式如下：</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/1.png" alt="1"></p>
<p>我们可以将每一行看做是一个样本，第 i 个样本$\boldsymbol{x<em>i} = (x</em>{i1},x<em>{i2},\cdots,x</em>{ip})’$ 可以看做是p维空间中的一个点。那么，按行进行聚类的话，我们可以将相似的个体聚成一类，由此在数据集$\boldsymbol X $​中进行<strong>集群发现</strong>。</p>
<p>我们可以将每一列看做是一个特征，第j个特征$x^*<em>{j} = (x</em>{1j},x<em>{2j}\cdots,x</em>{nj})’$ 可以看做是n维空间中的一点​，如果按列进行聚类，可以将相似的变量聚成一类，从而对数据集$X$ 进行降维</p>
<p>在这一章我们主要来了解<strong>集群发现</strong>，也就是按行聚类</p>
<h1 id="距离的意义"><a href="#距离的意义" class="headerlink" title="距离的意义"></a>距离的意义</h1><h2 id="点间距离"><a href="#点间距离" class="headerlink" title="点间距离"></a>点间距离</h2><h3 id="连续变量的点间距离"><a href="#连续变量的点间距离" class="headerlink" title="连续变量的点间距离"></a>连续变量的点间距离</h3><h4 id="欧氏距离"><a href="#欧氏距离" class="headerlink" title="欧氏距离"></a>欧氏距离</h4><p>说起距离，最容易想到的就是欧氏距离了，即对两个样本$\boldsymbol{x<em>k}=(x</em>{k1},\cdots,x<em>{kp})’$ 和 $\boldsymbol{x_l}=(x</em>{l1},\cdots,x_{lp})’$​ 的欧氏距离为：</p>
<script type="math/tex; mode=display">
||x_k-x_l||_2 = \sqrt{\sum_{j=1}^p(x_{kj}-x_{lj})^2}</script><p>我们如果要计算p维空间的两个点之间的距离，相当于求矩阵中第k行和第l行的距离，那么我们要对每个对应的点求平方和在进行开方。</p>
<p>在二维空间中，如下图所示</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/2.png" alt="1"></p>
<p><strong>欧式距离度量的是欧几里得空间中的直线距离</strong></p>
<h4 id="平方欧氏距离"><a href="#平方欧氏距离" class="headerlink" title="平方欧氏距离"></a>平方欧氏距离</h4><script type="math/tex; mode=display">
||x_k-x_l||_2^2  = \sum_{j=1}^p(x_{kj}-x_{lj})^2</script><h4 id="闵式距离"><a href="#闵式距离" class="headerlink" title="闵式距离"></a>闵式距离</h4><p>闵式距离是包含欧氏距离的</p>
<script type="math/tex; mode=display">
(\sum_{j=1}^p(x_{kj}-x_{lj})^q)^{\frac{1}{q}}</script><p>当q=2的时候，闵式距离和欧氏距离是等价的</p>
<p>还有一写特殊的闵式距离——绝对距离和最大距离</p>
<h4 id="曼哈顿距离-绝对距离"><a href="#曼哈顿距离-绝对距离" class="headerlink" title="曼哈顿距离(绝对距离)"></a>曼哈顿距离(绝对距离)</h4><p>当q取1的时候，闵式距离变成了曼哈顿距离又称为绝对距离</p>
<script type="math/tex; mode=display">
||\boldsymbol{x_k}-\boldsymbol{x_l}||_1 = \sum_{i=1}^p|x_{kj}-x_{lj}|</script><h4 id="切比雪夫距离-最大距离"><a href="#切比雪夫距离-最大距离" class="headerlink" title="切比雪夫距离(最大距离)"></a>切比雪夫距离(最大距离)</h4><p>当q取无穷大的时候，闵式距离变成了最大距离</p>
<script type="math/tex; mode=display">
||\boldsymbol{x_k}-\boldsymbol{x_l}||_\infty = \max_{j}|x_{kj}-x_{lj}|</script><p>用来度量所有维度中差异最大的那个维度</p>
<h4 id="兰氏距离"><a href="#兰氏距离" class="headerlink" title="兰氏距离"></a>兰氏距离</h4><script type="math/tex; mode=display">
\sum_{j=1}^p \frac{|x_{kj}-x_{lj}|}{|x_{kj}|+|x_{lj}|}</script><p>兰氏距离可以看做是加权的曼哈顿距离，它对接近于零的值的变化非常敏感，且对量纲不敏感。而曼哈顿距离对量纲是很敏感的。</p>
<p>因此若$|x_{kj}|$ 非常接近于0的话，兰氏距离会非常大</p>
<h4 id="马氏距离"><a href="#马氏距离" class="headerlink" title="马氏距离"></a>马氏距离</h4><p>马氏距离又称为是广义欧氏距离</p>
<script type="math/tex; mode=display">
\sqrt{(\boldsymbol{x_k}-\boldsymbol{x_l})'\Sigma^{-1}(\boldsymbol{x_k}-\boldsymbol{x_l})}</script><p>其中Σ是多维随机变量的协方差矩阵，当$\Sigma = \boldsymbol I_p$ ，即$\Sigma$​​为单位阵的时候，说明各个维度独立同分布，因此马氏距离等于欧氏距离</p>
<p><strong>马氏距离实际意义</strong></p>
<p>那么马氏距离就能干什么？它比欧氏距离好在哪里？举几个栗子</p>
<p><strong>欧式距离近就一定相似？</strong></p>
<p>先举个比较常用的例子，身高和体重，这两个变量拥有不同的单位标准，也就是有不同的scale。比如身高用毫米计算，而体重用千克计算，显然差10mm的身高与差10kg的体重是完全不同的。但在普通的欧氏距离中，这将会算作相同的差距。</p>
<p><strong>归一化后欧氏距离近就一定相似？</strong></p>
<p>当然我们可以先做归一化来消除这种维度间scale不同的问题，但是样本分布也会影响分类</p>
<p>举个一维的栗子，现在有两个类别，统一单位，第一个类别均值为0，方差为0.1，第二个类别均值为5，方差为5。那么一个值为2的点属于第一类的概率大还是第二类的概率大？距离上说应该是第一类，但是直觉上显然是第二类，因为第一类不太可能到达2这个位置。</p>
<p>所以，在一个方差较小的维度下很小的差别就有可能成为离群点。就像下图一样，A与B相对于原点的距离是相同的。但是由于样本总体沿着横轴分布，所以B点更有可能是这个样本中的点，而A则更有可能是离群点。</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/3.jpg" alt="1"></p>
<p><strong>算上维度的方差就够了？</strong></p>
<p>还有一个问题——如果维度间不独立同分布，样本点一定与欧氏距离近的样本点同类的概率更大吗？</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/4.jpg" alt="1"></p>
<p>可以看到样本基本服从f(x) = x的线性分布，A与B相对于原点的距离依旧相等，显然A更像是一个离群点</p>
<p>即使数据已经经过了标准化，也不会改变AB与原点间距离大小的相互关系。所以要本质上解决这个问题，就要针对主成分分析中的<code>主成分</code>来进行标准化。</p>
<p><strong>马氏距离的几何意义</strong></p>
<p>上面搞懂了，马氏距离就好理解了，只需要将变量<code>按照主成分进行旋转</code>，让维度间相互<strong>独立</strong>，然后进行<code>标准化</code>，让维度<strong>同分布</strong>就OK了</p>
<p>由主成分分析可知，由于主成分就是特征向量方向，每个方向的方差就是对应的特征值，所以只需要按照特征向量的方向旋转，然后缩放特征向量倍就可以了，可以得到以下的结果：</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/5.jpg"></p>
<p>离群点就被成功分离，这时候的欧式距离就是马氏距离。</p>
<p>现在我们可以来推导一下马氏距离：</p>
<p>首先要对数据点进行旋转，旋转至主成分使得维度间线性无关，假设新的坐标为：</p>
<script type="math/tex; mode=display">
F = (F_1,F_2\cdots,F_m) = U^TX\\
\mu_F = (\mu_1,\mu_2,\cdots,\mu_m)\\
(F- \mu_F)= U^T (X-\mu_X)</script><script type="math/tex; mode=display">
(F-\mu_F)(F-\mu_F)^T = \begin{bmatrix}\lambda_1\\&\lambda_2\\&&\cdots\\&&&\lambda_m\end{bmatrix}\\
=U^T(X-\mu_X)(X-\mu_X)^TU\\
=U^T\Sigma_XU</script><p>马氏距离是旋转变换缩放后的欧氏距离没所以马氏距离的计算公式可以写为：</p>
<script type="math/tex; mode=display">
D^2_M = (\frac{f_1-\mu_{F_1}}{\sqrt{\lambda_1}})^2+\cdots+(\frac{f_2-\mu_{F_2}}{\sqrt{\lambda_2}})^2\\=(f_1-\mu_{F_1},f_2-\mu_{F_2},\cdots,f_M-\mu_{F_M})\begin{bmatrix}\frac{1}{\lambda_1}\\&\frac{1}{\lambda_2}\\&&\cdots\\&&&\frac{1}{\lambda_m}\end{bmatrix}\begin{pmatrix}f_1-\mu_{F_1}\\f_2-\mu_{F_2}\\\cdots\\f_m-\mu_{F_m} \end{pmatrix}\\
=(f-\mu_F)^T(U^T\Sigma_X U^{-1})(f-\mu_F)\\
=(x-\mu_X)^TUU^T\Sigma_X^{-1}UU^T(x-\mu_X)\\
=(x-\mu_X)^T\Sigma_X^{-1}(x-\mu_X)</script><h4 id="皮尔逊线性相关系数"><a href="#皮尔逊线性相关系数" class="headerlink" title="皮尔逊线性相关系数"></a>皮尔逊线性相关系数</h4><p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/15.png"></p>
<h4 id="余弦相似度"><a href="#余弦相似度" class="headerlink" title="余弦相似度"></a>余弦相似度</h4><p>余弦相似度的定义为：</p>
<script type="math/tex; mode=display">
\cos\theta = \frac{\sum_{j=1}^p x_{kj}x_{lj}}{\sqrt{\sum_{j=1}^px_{kj}^2\sum_{j=1}^px^2_{lj}}}</script><p>余弦距离是由两个向量之间夹脚的余弦公式得到的</p>
<script type="math/tex; mode=display">
\cos\theta = \frac{\boldsymbol{a'b}}{\boldsymbol{|a||b|}}</script><p>当余弦为1时，说明两个向量完全相似，此时距离为0</p>
<p><strong>余弦相关距离=1-$\cos\theta$</strong></p>
<h4 id="肯德尔秩相关系数"><a href="#肯德尔秩相关系数" class="headerlink" title="肯德尔秩相关系数"></a>肯德尔秩相关系数</h4><p>现在我们来介绍秩相关系数，我们用秩来代替原来的数值，是因为我们要规避一些极端数值带来的影响</p>
<p>肯德尔相关系数，又称肯德尔秩相关系数，它是一种秩相关系数，不过，它的目标对象是<strong>有序的类别变量</strong>，比如名次、年龄段、肥胖等级(重度肥胖，中度肥胖、轻度肥胖、不肥胖)等。它可以度量两个有序变量之间单调关系强弱。肯德尔相关系数使用了“成对“这一概念来决定相关系数的强弱。</p>
<p>成对可以分为协同对(Concordant)和不协同对(Discordant)。协同对是指两个变量取值的相对关系一致，可以理解为$X_2-X_1$与$Y_2-Y_1$有相同的符号；分歧对则是指它们的相对关系不一致，$X_2-X_1$与$Y_2-Y_1$有着相反的符号。</p>
<p>肯德尔秩的相关系数是基于观测值中两个特征同时增加或者同时减少的个数从而计算相关系数</p>
<ul>
<li>协同对:$(x<em>{kj}-x</em>{kj’})(x<em>{lj}-x</em>{lj’})&gt;0$</li>
<li>不协同对：$(x<em>{kj}-x</em>{kj’})(x<em>{lj}-x</em>{lj’})&lt;0$</li>
</ul>
<p>那么，肯德尔相关系数的定义如下：</p>
<script type="math/tex; mode=display">
\text{Kendall}~\tau = \frac{n_c-n_d}{p(p-1)/2}</script><p>其中，$n_c$​表示协同对的个数，而$n_d$​表示不协同对的个数，$p(p-1)/2$ 代表总对数。</p>
<p>如果两个属性排名是相同的，系数为1 ，两个属性正相关。</p>
<p>如果两个属性排名完全相反，系数为-1 ，两个属性负相关。</p>
<p>我们来举一个例子：</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/6.png"></p>
<p><strong>肯德尔相关距离=1-Kendall $\tau $</strong></p>
<h4 id="斯皮尔曼秩相关系数"><a href="#斯皮尔曼秩相关系数" class="headerlink" title="斯皮尔曼秩相关系数"></a>斯皮尔曼秩相关系数</h4><p>斯皮尔曼秩相关系数类似于皮尔逊相关系数，只不过将原始数据$x<em>{kj}$​ 用其秩$r</em>{kj}$​ 来代替</p>
<p>将$\boldsymbol x<em>{k}$ 的各个分量$x</em>{k1}\cdots, x<em>{k2},\cdots,x</em>{kp}$ 按从小到大排序，计算每一个分量所对应的秩，记为$r<em>{k1},r</em>{k2},\cdots,r_{kp}$</p>
<script type="math/tex; mode=display">
\text{Spearman} ~~\rho = \frac{\sum_{j=1}^p(r_{kj}-\overline r_{kj})(r_{lj}-\overline r_{lj})}{\sqrt{\sum_{j=1}^p(r_{kj}-\overline r_{kj})^2\sum_{j=1}^p(r_{lj}-\overline r_{lj}})^2}</script><p>通过化简我们可以得到更精简的式子：</p>
<script type="math/tex; mode=display">
\rho = 1-\frac{6\sum d_i^2}{n(n^2-1)}</script><p>我们用具体的例子来解释计算过程：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>原始位置</th>
<th>原始X</th>
<th>排序后</th>
<th>秩次X’</th>
<th>原始Y</th>
<th>排序后</th>
<th>秩次Y’</th>
<th>秩次差$d_i$的平方</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>11</td>
<td>490</td>
<td>5</td>
<td>2</td>
<td>75</td>
<td>6</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>490</td>
<td>43</td>
<td>1</td>
<td>75</td>
<td>44</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>3</td>
<td>14</td>
<td>30</td>
<td>4</td>
<td>3</td>
<td>42</td>
<td>5</td>
<td>1</td>
</tr>
<tr>
<td>4</td>
<td>43</td>
<td>14</td>
<td>2</td>
<td>44</td>
<td>7</td>
<td>2</td>
<td>0</td>
</tr>
<tr>
<td>5</td>
<td>30</td>
<td>11</td>
<td>3</td>
<td>7</td>
<td>3</td>
<td>4</td>
<td>1</td>
</tr>
<tr>
<td>6</td>
<td>3</td>
<td>3</td>
<td>6</td>
<td>42</td>
<td>2</td>
<td>3</td>
<td>9</td>
</tr>
</tbody>
</table>
</div>
<p>代入公式，得到：$\rho = 1-\frac{6(1+1+1+9)}{6(36-1 )} = 0.657$</p>
<p><strong>斯皮尔曼相关距离 = 1-Spearman $\rho$​​</strong></p>
<h3 id="混合变量的点间距离"><a href="#混合变量的点间距离" class="headerlink" title="混合变量的点间距离"></a>混合变量的点间距离</h3><p>刚才定义的点间距离都是在连续场合定义的，那么如果我们的数据既有连续场合又有离散场合该怎么办？我们需要用统一的量纲去度量才能保证距离是靠谱的。</p>
<p>我们可以令$s<em>j =  s_j(x</em>{kj}-x_{lj})$ 为第k个观测值和第l个观测值在第j个特征或变量之间的相似性。通过$s_j$​ 来定义点与点之间的距离$d_j$, 即 $s_j = 1-d_j$​​​ </p>
<h4 id="定性变量"><a href="#定性变量" class="headerlink" title="定性变量"></a>定性变量</h4><p>比如说，我们考虑性别是否为男性、产品的颜色是否为黑色等，这些特征$x<em>{kj}$和$x</em>{lj}$ 均为<strong>定性变量</strong>。两者相等即为1，否则即为0；那么我们可以定义相似性为：</p>
<script type="math/tex; mode=display">
s_j = s_j(x_{kj},x_{lj}) = \begin{cases}1&x_{kj}=x_{lj}\\0&x_{kj}\neq x_{lj} \end{cases}</script><h4 id="定量变量"><a href="#定量变量" class="headerlink" title="定量变量"></a>定量变量</h4><p>比如考虑年龄、产品的价格等，这些特征$x<em>{kj}$ 和$x</em>{lj}$ 均为<strong>定量变量</strong>，那么我们可以定义相似性为：</p>
<script type="math/tex; mode=display">
s_j = s_j(x_{kj},x_{lj}) = 1-\frac{|x_{kj}-x_{lj}|}{R_j}</script><p>其中，$R<em>j$ 表示第j个特征的极差，即$R_j = \max_ix</em>{ij}-\min<em>ix</em>{ij}$​  ，也就是进行一个归一化后的绝对距离</p>
<h4 id="定序变量"><a href="#定序变量" class="headerlink" title="定序变量"></a>定序变量</h4><p>比如考虑文化程度、空气质量指数级别等，这些特征$x<em>{kj}$和$x</em>{lj}$​ 均为定序变量。那么我们要对$x<em>{1j},x</em>{2j},\cdots,x<em>{nj}$ 从小到大进行排序，并分别计算其秩，记为$r</em>{1j},r<em>{2j},\cdots,r</em>{nj}$​ 。</p>
<p>通常我们定义相似性为：</p>
<script type="math/tex; mode=display">
s_j = s_j(x_{kj},x_{lj}) = 1-\frac{|r_{kj}-r_{lj}|}{\max_k r_{kj}-\min_k r_{kj}}</script><h4 id="相似度"><a href="#相似度" class="headerlink" title="相似度"></a>相似度</h4><p>上面说的都是观测点之间的相似性，现在我们来定义两个观测$\boldsymbol{x}_k,\boldsymbol{x}_l$ 之间的相似度：</p>
<script type="math/tex; mode=display">
s_j(\boldsymbol x_{k}-\boldsymbol x_{l}) = \frac{\sum_{j=1}^ps_j(x_{kj},x_{lj})\delta(x_{kj},x_{lj})w_j}{\sum_{j=1}^p\delta(x_{kj},x_{lj})w_j}</script><p>其中$\delta(x<em>{kj},x</em>{lj})$ 表示观测值中是否存在确是观测，即</p>
<script type="math/tex; mode=display">
\delta(x_{kj},x_{lj}) = \begin{cases}0,&\text{如果}x_{kj}\text{或}x_{lj}\text{存在缺失观测}\\1,&\text{其他} \end{cases}</script><p>$w_j$ 表示权重，一般取值为1，但是如果事先知道第j个特征尤其重要，可以增加相应的权重。</p>
<h3 id="类间距离"><a href="#类间距离" class="headerlink" title="类间距离"></a>类间距离</h3><p>由一个点组成的类是最基本的类，如果每一类都由一个点组成，那么点间距离就是类间距离。</p>
<p>那么，如果某一类上包含不止一个点，就要定义类间距离，也称关联准则。</p>
<h4 id="sinple-linkage"><a href="#sinple-linkage" class="headerlink" title="sinple linkage"></a>sinple linkage</h4><p>将两个类中距离最短的两个点之间的距离定为类间距离，称 <strong>sinple linkage</strong>, 如下图所示：</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/8.png"></p>
<h4 id="complete-linkage"><a href="#complete-linkage" class="headerlink" title="complete linkage"></a>complete linkage</h4><p>将两个类中剧里最长的两个点之间的距离定义为类间距离，称为<strong>complete linkage</strong></p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/9.png"></p>
<h4 id="centroid-linkage"><a href="#centroid-linkage" class="headerlink" title="centroid linkage"></a>centroid linkage</h4><p>将两个类中所有的点的重心距离定义为类间距离，称为 centroid linkage</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/10.png"></p>
<h4 id="关联准则"><a href="#关联准则" class="headerlink" title="关联准则"></a>关联准则</h4><p>为了同一副好，我们将点也看做是类，因此把点(类)k与l之间的距离用$d(k,l)$ 表示。如果点(或者类)k与l聚合成一个类，记为$k\cup l$ ，对于任何其他的一个点(或者类)i，那么类($k \cup l$)与点或类i之间的距离记为$d(k\cup l,i)$ </p>
<p>Lance-Williams 公式为：</p>
<script type="math/tex; mode=display">
d(k\cup l,i) = \alpha_k d(k,i)+\alpha_ld(l,i)+\beta d(k,l)+\gamma ~|d(k,i)-d(l,i)|</script><p>其中，$\alpha_k,\alpha_l,\beta,\gamma$​</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/7.png"></p>
<p>如果我们使用single-linkage,三个参数分别是0.5，0.5，0.5，我们来推一下：</p>
<script type="math/tex; mode=display">
d(k\cup l,i) = \frac{1}{2} d(k,i)+\frac{1}{2}d(l,i)-\frac{1}{2}|d(k,i)-d(l,i)|\\
=\frac{1}{2}|d(k,i)+d(l,i)|-\frac{1}{2}|d(k,i)-d(l,i)|</script><p>如果$d(k,i)&gt;d(l,i)$,原式等于 $d(l,i)$;</p>
<p>如果$d(k,i)&lt;d(l,i)$，原式等于 $d(k,i)$</p>
<p>符合 single-linkage的定义</p>
<h1 id="层次聚类"><a href="#层次聚类" class="headerlink" title="层次聚类"></a>层次聚类</h1><p>在博客：<a href="https://jasonxqh.github.io/2020/10/25/Hierarchical-Clustering/">https://jasonxqh.github.io/2020/10/25/Hierarchical-Clustering/</a> 中介绍过层次聚类，现在我们从数学方面来看一下</p>
<p>层次聚类一般有两种不同的形式，自下而上和自上而下</p>
<p><strong>自下而上</strong>：每个样本各自分到一个类中，之后将类间距离最近的两类关联，并建立一个新的类，反复此过程直到所有的样本聚合至一个类中；<br><strong>自上而下</strong>：将所有样本归到一个类中，之后将在类中相距最远的样本记为两个新的类，基于这两个类，将未进行聚类的点逐一比较其与两个新的类的距离，这样所有样本划分成了两类，在每一个类中重复此过程直到每个样本点各自分到一个类中；</p>
<p>一般来说都会选择自下而上方法，自上而下会比较麻烦。</p>
<p>我们给一个例题：假设有4个点，距离矩阵为：</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/11.png"></p>
<p>找到欧式距离最近的两个类，是A和B，距离为1，把他们聚成一类。然后我们要计算$d(A\cup B,C),d(A\cup B,D)$</p>
<p>我们不妨采用simple linkage，那么$A\cup B$​到C 的距离是A到C的距离与B到C距离的最小值, 即3；那么$A\cup B$​​​到D 的距离是A到D的距离与B到D距离的最小值,即2</p>
<p>因此，重新得到的距离矩阵为：</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/12.png"></p>
<p>接着我们找出距离最近的两个类，A,B和D，把他们聚成一类</p>
<p>$A,B\cup D$ 到C的距离是A,B到C的距离与D到C的距离的最小值，即3，因此我们再次更新距离矩阵如下：</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/13.png"></p>
<p>最后可以将A,B,C,D 聚成一类</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/14.png"></p>
<h1 id="K-Means-聚类"><a href="#K-Means-聚类" class="headerlink" title="K-Means 聚类"></a>K-Means 聚类</h1><p>K-Means我在博客<a href="https://jasonxqh.github.io/2020/10/22/K-Means%E8%81%9A%E7%B1%BB/">https://jasonxqh.github.io/2020/10/22/K-Means%E8%81%9A%E7%B1%BB/</a> 有所介绍。 现在我们从数学角度再来学习一下。</p>
<p>K-Means 是最简单的无监督学习方法之一，且计算速度快，因此也被称为<strong>快速聚类</strong></p>
<p>相较于层次聚类，K均值聚类实现确定聚类数目，这里假定聚类的数目为K(K&lt;n)</p>
<p>那么对于给定n个样本集 $X= (\boldsymbol x_1,\cdots,\boldsymbol x_n)’$ ，K-Means 的目标就是将n个样本划分到K个不同的类中，这K个类$C_1,C_2,\cdots,C_k$ 形成了样本集X的划分，即：</p>
<script type="math/tex; mode=display">
C_k\cap C_l = \empty,\bigcup_{k=1}^K C_k = X</script><p>输出的划分 $\mathcal C = {C_1,C_2,\cdots,C_k}$ 可以对应一个聚类结果</p>
<h3 id="目标与思想"><a href="#目标与思想" class="headerlink" title="目标与思想"></a>目标与思想</h3><p>我们的目标是，希望能找到一个最优化分$\mathcal C^*$​​ ，使得类内距离足够小而类间距离足够大。这和聚类的思想是一致的，即相似的放在同一簇中，不相似的放在不同簇当中。一般来说，我们使用平方欧氏距离来表示点与点之间的距离 ，即：</p>
<script type="math/tex; mode=display">
d_{kl} = d(k,l)=||\boldsymbol x_k-\boldsymbol x_l||_2^2= \sum_{j=1}^p(x_{kj}-x_{lj})^2</script><p>由此，我们可以定义一个合理的损失函数, 如下：</p>
<script type="math/tex; mode=display">
W(\mathcal C) = \sum_{k=1}^{n_k}\sum_{i\in C_k}||\boldsymbol x_i-\boldsymbol m_k||^2</script><ul>
<li>$\boldsymbol m_k$ 表示第k类的均值或中心</li>
<li>这里$n_k$​是第k类中样本的个数</li>
</ul>
<p>而K均值聚类实际上就是解决一个最优化问题：</p>
<script type="math/tex; mode=display">
\mathcal C^* = \arg\min_C W(\mathcal C)</script><p>这是一个NP-Hard问题，可以用迭代法求解。</p>
<h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><p>通常采用迭代法来求解区 均值聚类的问题，每次迭代包括两个步骤：</p>
<ul>
<li>确定K个类的中心$\boldsymbol m_k$，将样本逐一分配到其最近的中心所对应的类中，得到一个聚类结果；</li>
<li>更新每个类的样本均值，作为类的更新后的中心；重复此过程，直到收敛为止。</li>
</ul>
<p>注意事项：</p>
<ul>
<li>收敛条件，通常可以设置为：聚类结果不变；</li>
<li>复杂度是$O(pnK)$，其中p表示特征个数，n 表示样本个数，K是聚类数目；</li>
<li>如果各个类的数据集非凸，基于非凸性质，我们可以构造出一个数据集，存在一个点在两个类之间横跳，导致K均值聚类算法难以收敛；未解决这个问题，我们可以放宽收敛的条件</li>
</ul>
<h1 id="高斯混合模型-GM"><a href="#高斯混合模型-GM" class="headerlink" title="高斯混合模型(GM)"></a>高斯混合模型(GM)</h1><p>参考： <a href="https://blog.csdn.net/lin_limin/article/details/81048411" target="_blank" rel="noopener">https://blog.csdn.net/lin_limin/article/details/81048411</a></p>
<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>高斯混合模型在聚类的分类里面属于<strong>基于模型的聚类方法</strong>。这和Kmeans和Hierarchical 聚类的底层思想是不同的。</p>
<p>高斯混合模型的核心在于：分布的假定。 假定第i个样本$\boldsymbol x<em>i$​ 来自于<strong>第k类</strong>正态分布$N_p(\boldsymbol\mu</em> k,\Sigma_k)$​</p>
<ul>
<li>$\boldsymbol\mu_k$  表示均值向量</li>
<li>$\Sigma_k$ 表示协方差矩阵</li>
</ul>
<p>那么，$\boldsymbol x_i$ 的密度函数为：</p>
<script type="math/tex; mode=display">
f(x_i) = (2\pi)^{-p/2}\abs{\Sigma_k}^{-1/2}\exp\bigg\{-\frac{1}{2}(x_i-\boldsymbol \mu_k)'\Sigma_k^{-1}(x_i-\boldsymbol \mu_k) \bigg\}</script><ul>
<li>K表示聚类数目，可以看做是一个超参数，需要我们事先确定。</li>
<li>n表示样本量</li>
</ul>
<h3 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h3><p>那么，如果我们能够确定第 i 个样本是来自于第k 个高斯分布总体时，我们可以构造变量：</p>
<script type="math/tex; mode=display">
\delta_{ik} = \begin{cases}1,\text{当第i个样本}x_i\text{属于第k个总体}\\0,\text{当第i个样本}x_i\text{不属于第k个总体}\\ \end{cases}</script><p>这就说明，某一个样本是可能属于某一类的，而且仅属于某一类。</p>
<p>因此，$\delta<em>i = (\delta</em>{i1},\delta<em>{i2},\cdots,\delta</em>{iK})’$ 满足 </p>
<ul>
<li><p>独立同分布的<strong>随机向量</strong></p>
</li>
<li><p>这个K维向量某一维是1，其余都是0。这就相当于我掷了一个K面的骰子。某一维朝上的记为1，其余都记为0。因此，这个随机变量是服从多维分布 的。因为我只掷了一次骰子，因此第一个变量是1，由于不同”面”出现的概率是不一定的，我们令其为$\pi_1,\cdots,\pi_K$。最终我们给出随机变量是服从多维分布  $M(1,\pi_1,\pi_2,\cdots,\pi_K)$的。</p>
</li>
<li><p>$\pi<em>k=P(\delta</em>{ik}=1)$且满足：</p>
<script type="math/tex; mode=display">
0<\pi_k<1,\sum_{i=1}^K \pi_k = 1</script></li>
</ul>
<p>根据上面的推论，我们给出 $\delta<em>{i}=(\delta</em>{i1},\delta<em>{i2},\cdots,\delta</em>{iK})’$ 的概率密度函数：</p>
<script type="math/tex; mode=display">
f(\delta_i) = \prod_{k=1}^K (\pi_k)^{\delta_{ik}},i=1,2\cdots,n</script><p>那么，当我们对一个$x_i$ ，给定了$\delta_i$ 的密度函数之后(样本真的落在第k类上的时候)，可以给出$x_i$的密度函数。</p>
<script type="math/tex; mode=display">
f(x_i|\delta_i) = \prod_{k=1}^K\Bigg( (2\pi)^{-p/2}|\Sigma_k|^{-1/2}\exp\bigg\{-\frac{1}{2}(x_i-\boldsymbol \mu_k)'\sum_k^{-1}(x_i-\boldsymbol \mu_k) \bigg\} \Bigg)^{\delta_{ik}}</script><blockquote>
<p>因为$\delta_i$中只有一项是等于1的，其余都等于0。其实就相当于样本属于第k类的密度函数乘以 K-1 个 1。因此这个式子拆开来就是等于上面我给出的$x_i$的密度函数，只不过这个式子更加 General一点</p>
</blockquote>
<p>那么，对于样本 ${x_i,\delta_i},i=1,2\cdots,n$，我们可以给出其联合密度函数</p>
<script type="math/tex; mode=display">
\begin{align}
& \prod_{i=1}^n f(x_i,\delta_i)\\
=&\prod_{i=1}^n f(\delta_i)\cdot f(x_i|\delta_i)\\
=&\prod_{i=1}^n\bigg(\pi_k(2\pi)^{-p/2}|\Sigma_k|^{-1/2}\exp\bigg\{-\frac{1}{2}(x_i-\boldsymbol \mu_k)'\Sigma_k^{-1}(x_i-\boldsymbol \mu_k) \bigg\}\bigg)^{\delta_{ik}}
\end{align}</script><p>对于上面这个密度函数，其实就是未知参数 $\theta = (\pi_1,\cdots,\pi_K,\boldsymbol\mu_1,\cdots,\boldsymbol\mu_K,\Sigma_1,\cdots,\Sigma_K)$ 的似然函数。理论上，基于这个似然函数，我们可以来估计参数$\theta$</p>
<p>但是，在实际中，我们仅能观测到样本${x_i}$ ，$i=1,2\cdots,n$,  是没有办法去观测$\delta_i$ 的。因此，我们没有办法直接估计未知参数$\theta$。因此我们可以对联合密度函数求关于 $x_i$ 的边际分布。</p>
<script type="math/tex; mode=display">
f(x_i) = \sum_{k=1}^K \pi_k(2\pi)^{-p/2}|\Sigma_k|^{-1/2}\exp\bigg\{-\frac{1}{2}(x_i-\boldsymbol \mu_k)'\Sigma_k^{-1}(x_i-\boldsymbol \mu_k) \bigg\}</script><p>这个密度函数是由K 个正态分布的密度函数加权组合而成的，常被称为是高斯混合模型。其中,$p(x|\delta<em>i)$ 就是第k个高斯模型的概率密度函数，可以看成选定第k 个模型后，该模型产生x的概率；$p(\delta_i)=\pi_k$ 是第k个高斯模型的权重，称其为第k个模型的先验概率，满足 $\sum</em>{k=1}^K \pi_k$ = 1</p>
<p>所以，混合高斯模型并不是什么新奇的东西，<strong>它的本质就是融合几个单高斯模型，来使得模型更加复杂，从而产生更复杂的样本。理论上，如果某个混合高斯模型融合的高斯模型个数足够多，它们之间的权重设定得足够合理，这个混合模型可以拟合任意分布的样本。</strong></p>
<h3 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h3><p>由于只观测到$x<em>i$ 去估计参数$\theta$ 是很难估的，因此我们需要用EM算法。将$\delta</em>{ik}$作为潜变量。</p>
<p>对于未知参数$\theta$的对数似然函数为：</p>
<script type="math/tex; mode=display">
\begin{align}
 \ln L(\mu,\Sigma,\pi) &= \sum_{i=1}^n\ln\sum_{k=1}^K\pi_k N(x_i|\mu_k,\Sigma_k)\\
&\propto -\frac{1}{2}\sum_{i=1}^n\sum_{k=1}^K\delta_{ik}\big((x_i-\boldsymbol \mu_k)'\Sigma_k^{-1}(x_i-\boldsymbol \mu_k)+\ln|\Sigma_k|\big)+\sum_{i=1}^n\sum_{k=1}^K\delta_{ik}\ln(\pi_k)\\
&=Q_0(\mu,\Sigma,\pi)
\end{align}</script><h4 id="E-步"><a href="#E-步" class="headerlink" title="E 步"></a>E 步</h4><p>在E步，我们需要将潜变量 $\delta<em>{ik}$的期望$\pi</em>{ik}^*$ 带入$Q_0(\boldsymbol \theta )$, 即</p>
<script type="math/tex; mode=display">
Q(\theta) = -\frac{1}{2}\sum_{i=1}^n\sum_{k=1}^K\pi^*_{ik}\big((x_i-\boldsymbol \mu_k)'\Sigma_k^{-1}(x_i-\boldsymbol \mu_k)+\ln|\Sigma_k|\big)+\sum_{i=1}^n\sum_{k=1}^K\pi^*_{ik}\ln(\pi_k)\\
=Q_1(\theta)+Q_2(\theta)</script><blockquote>
<p>前面这部分只和 $\mu,\Sigma$ 有关，后面只和 $\pi_k$ 有关</p>
</blockquote>
<p>$\delta<em>{ik}$ 的期望$\pi</em>{ik}^*$ 为：</p>
<script type="math/tex; mode=display">
\pi^*_{ik} = E(\delta_{ik}|x_i) = P(\delta_{ik}=1|x_i)=\frac{\pi_k\phi(x_i;\boldsymbol \mu_k,\sum_k)}{\sum_{k=1}^K\pi_k\phi(x_i;\boldsymbol\mu_k,\sum_k)}</script><p>其中：</p>
<script type="math/tex; mode=display">
\phi(\boldsymbol x_i,\boldsymbol \mu_k,\Sigma_k) = (2\pi)^{-p/2}\abs{\Sigma_k}^{-1/2}\exp\bigg\{-\frac{1}{2}(x_i-\boldsymbol \mu_k)'\Sigma_k^{-1}(x_i-\boldsymbol \mu_k) \bigg\}</script><p>① Q函数描述的其实就是在给定$\boldsymbol \theta$ 参数下，先对样本 X 做一个<strong>最有可能的划分</strong>（每个样本来源于各个类的可能性，即对$\delta_i$ 做估计 $E$ ，再描述能够产生这组样本的可能性（Q函数）)</p>
<p>② 有了对于 $\delta_i$ 的估计之后，Q函数只和样本有关（传统意义上的似然函数亦如此，完全数据的似然函数还与 $\gamma$有关），而不再含有隐变量，从而使得最大化Q函数成为可能；</p>
<p>③ 最大化Q函数的过程实则就是使得能够产生这组样本的可能性最大，与最大化似然函数的思路如出一辙。</p>
<h4 id="M步"><a href="#M步" class="headerlink" title="M步"></a>M步</h4><p>求$Q(\theta)$ 的最大值，而确定未知参数的估计</p>
<p>我们发现：</p>
<ul>
<li>$Q<em>1(\theta)$ 仅与未知参数 ${\boldsymbol \mu_k,\Sigma_k}^{K}</em>{k=1}$ 有关</li>
<li>$Q<em>2(\theta)$ 仅与未知参数 ${\pi_k}^K</em>{k=1}$ 有关</li>
</ul>
<p>于是，我们可以分别确定最大值点。</p>
<h5 id="Q1"><a href="#Q1" class="headerlink" title="Q1"></a>Q1</h5><script type="math/tex; mode=display">
Q_1(\boldsymbol\theta) =-\frac{1}{2}\sum_{i=1}^n\sum_{k=1}^K\pi^*_{ik}\big((x_i-\boldsymbol \mu_k)'\Sigma_k^{-1}(x_i-\boldsymbol \mu_k)+\ln|\Sigma_k|\big)</script><p>在这里，由于$\Sigma_k$ 是一个矩阵，我们来复习一下对正定对称矩阵的求导</p>
<ul>
<li><p>假定 $X$ 是一个正定对称矩阵</p>
<ul>
<li><p>非线性的形式：</p>
<script type="math/tex; mode=display">
\frac{\partial \ln \det (X)}{\partial X} = X^{-1}</script></li>
<li><p>关于逆矩阵的求导：</p>
<script type="math/tex; mode=display">
\frac{\partial \tr(AX^{-1}B)}{\partial X} = -(X^{-1}BAX^{-1})'</script></li>
</ul>
</li>
</ul>
<p>现在，对于$Q_1(\theta)$ ,我们要分别对$\boldsymbol \mu_k$ 和 $\Sigma_k$ 求导，并使得导函数为0</p>
<script type="math/tex; mode=display">
\begin{align}
&\frac{\partial Q_1(\boldsymbol \theta)}{\partial\boldsymbol\mu_k}  = -\frac{1}{2}\sum_{i=1}^n\pi_{ik}^*\Sigma_{k}^{-1}(\boldsymbol x_i-\boldsymbol \mu_k)=0 \\~\\ &\frac{\partial Q_1(\boldsymbol \theta)}{\partial \Sigma_k} = \sum_{i=1}^n\pi_{ik}^*(\Sigma_k^{-1}(\boldsymbol x_i-\boldsymbol \mu_k)(\boldsymbol x_i-\boldsymbol \mu_k)'\Sigma_{k}^{-1}+\Sigma_k^{-1})=0 
\end{align}</script><p>由此解得：</p>
<script type="math/tex; mode=display">
\begin{align}
&\boldsymbol \mu_k^{i+1} = \frac{\sum_{i=1}^n\pi_{ik}^*\boldsymbol x_i} {\sum_{i=1}^n\pi_{ik}^*}\\~\\
&\Sigma^{i+1}_k = \frac{\sum_{i=1}^n\pi_{ik}^*(\boldsymbol x_i-\boldsymbol \mu_k)(\boldsymbol x_i-\boldsymbol \mu_k)'}{\sum_{i=1}^n \pi_{ik}^*}
\end{align}</script><p>$\mu_k^{i+1},\sum_k^{i+1}$ 分别表示第$(i+1)$次迭代下第k个类的均值、协方差矩阵</p>
<h5 id="Q2"><a href="#Q2" class="headerlink" title="Q2"></a>Q2</h5><script type="math/tex; mode=display">
Q_2(\boldsymbol \theta) = \sum_{i=1}^n\sum_{k=1}^K\pi^*_{ik}\ln(\pi_k)\\</script><p>对于$Q_1(\theta)$ ,我们要分别对$\pi_k$  求导，并使得导函数为0</p>
<p>注意了，在求$Q_2(\boldsymbol \theta)$ 的最大值时，注意这里是对$\pi_k$ 有限制条件的，即：</p>
<script type="math/tex; mode=display">
\sum_{i=1}^K \pi_k = 1, 0<\pi_k<1,k=1,2\cdots ,K</script><p>因此，我们需要采用拉格朗日乘子法，令</p>
<script type="math/tex; mode=display">
Q_2^*(\boldsymbol \theta) = \sum_{i=1}^n\sum_{k=1}^K\pi_{ik}^*\ln(\pi_k)-\lambda(\sum_{k=1}^K\pi_k-1)</script><p>对 $Q_2^*(\boldsymbol \theta) $ 关于 $\pi_k$ 求导，并使得导函数为0，即：</p>
<script type="math/tex; mode=display">
\frac{\partial Q_2^*(\boldsymbol \theta)}{\partial \pi_{k}} = \sum_{i=1}^n\frac{\pi_{ik}^*}{\pi_k}-\lambda = 0</script><p>由此解得：</p>
<script type="math/tex; mode=display">
\pi_{k} = \frac{1}{n}\sum_{i=1}^n\pi_{ik}^*</script><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>EM算法的核心思想是：通过迭代的过程来找到一组最优的参数 $ (\mu^<em>,\Sigma^</em>,\pi^*)$，使得这组参数表示的模型最有可能产生现有的采样数据。每次迭代的过程就是参数矫正的过程。</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/18.png"></p>
<p>现假设初始化一组参数 $\mu^0,\Sigma^0,\pi^0$。在这组参数下，2类二维高斯分布如图11绿色椭圆所示。现在开始用EM算法</p>
<ul>
<li><p>E-step 开始对样本数据进行划分（对$\delta_i$ 进行估计）。蓝色的样本大多都被划分给第1类模型，橘黄色的样本大多都被划分给第2类模型。但是第1类模型还有优化空间：第1类模型还不能使得蓝色样本出现的联合概率达到最大。第2类模型也是如此。</p>
</li>
<li><p>M-step便优化了2类模型的参数，得到新的参数，$\mu^1,\Sigma^1,\pi^1$ .使得优化后2类高斯分布如图11红色椭圆所示。</p>
<ul>
<li>第1类模型主要优化的是模型均值$\mu$（即椭圆的中心）</li>
<li>第2类模型主要优化的是模型协方差矩阵$\Sigma$（即椭圆的长轴、短轴和长短轴的方向）</li>
</ul>
</li>
<li>然后重复进行E-step和M-step，直到参数$(\mu,\Sigma,\pi)$收敛
　　</li>
</ul>
<p>最后谈谈混合高斯模型的参数$\pi$。</p>
<ul>
<li><p>混合高斯模型的参数$\mu,\Sigma$ 比较好理解，用于描述各个高斯分布的形状，对于它们的调整也比较直观：<strong>使得本高斯分布能够更好地接纳被划分到这类分布的样本</strong>。</p>
</li>
<li><p>而为什么要有参数$\pi$ ? 它描述的是各个高斯分布所占的比重，如果不加“歧视”的话（样本来源于各个高斯分布的可能性一致），则有 $\pi_k=1/K$  。而如果对于某一类高斯分布（即为i）有侧重的话，则相应的 $\pi_i$ 较大，体现在图中就是<strong>被分配给各个类的样本数占样本总数的比例</strong>。如果一轮优化后，某一类高斯分布又接纳了更多样本，则其 $\pi$ 变大，反之变小（所以图11从绿色椭圆调整为红色椭圆实际上两个类所对应的权重也被优化了）。</p>
<ul>
<li>而从本质上来看参数 $\pi$，则是为了混合高斯模型能有更好的曲面拟合能力。当参数 $\pi$ 退化为 某一类高斯分布的权重远远大于其他类高斯分布的时候，混合高斯模型就退化成了单高斯模型！ </li>
</ul>
</li>
</ul>
<p>最后，给出混合高斯分布参数估计的逻辑流程。</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/17.png"></p>
<h1 id="DBScan"><a href="#DBScan" class="headerlink" title="DBScan"></a>DBScan</h1><p>参考博客<a href="https://www.cnblogs.com/pinard/p/6208966.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6208966.html</a></p>
<p>DBSCAN(Density-Based Spatial Clustering of Applications with Noise，具有噪声的基于密度的聚类方法)是一种很典型的密度聚类算法，和K-Means，BIRCH这些一般只适用于凸样本集的聚类相比，DBSCAN既可以适用于凸样本集，也可以适用于非凸样本集。下面我们就对DBSCAN算法的原理做一个总结。</p>
<p>DBSCAN是一种基于密度的聚类算法，这类密度聚类算法一般假定类别可以通过<strong>样本分布的紧密程度</strong>决定。同一类别的样本，他们之间的紧密相连的，也就是说，在该类别任意样本周围不远处一定有同类别的样本存在。</p>
<p>通过将紧密相连的样本划为一类，这样就得到了一个聚类类别。通过将所有各组紧密相连的样本划为各个不同的类别，则我们就得到了最终的所有聚类类别结果。</p>
<h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h3><p>我们给定数据集 $X = (\boldsymbol {x_1,x_2,\cdots,x_n})’$</p>
<p>对于每一个样本 $\boldsymbol x<em>i = (x</em>{i1},x<em>{i2},\cdots,x</em>{ip})’$ 可以看做 $R^p$ 空间中的一个点。</p>
<p>我们假定第k 个点$\boldsymbol x_k$ 和第l个点 $\boldsymbol x_l$ 之间的距离为 $d(k,l)$</p>
<h3 id="基本概念-1"><a href="#基本概念-1" class="headerlink" title="基本概念"></a>基本概念</h3><p>DBSCAN是基于一组邻域来描述样本集的紧密程度的，参数(ϵ, MinPts)用来描述邻域的样本分布紧密程度。其中，ϵ 描述了某一样本的<strong>邻域距离阈 </strong>，MinPts 描述了某一样本的距离为ϵ的邻域中<strong>样本个数的阈值</strong> 。由此，我们给出DBSCAN的一些基本概念如下：</p>
<ol>
<li>$\epsilon$-邻域：对于$x<em>j\in X$ 其 ϵ-邻域包含样本集D中与$x_j$ 的距离不大于 𝜖 的子样本集， 即 $N</em>\epsilon(x<em>j) = {x_i\in D|d(i,j) \leq\epsilon}$ 。称其为点 $x_j$ 的 $\epsilon$-邻域, 并将这个子样本集的个数记为 $|N</em>\epsilon(x_j)|$ </li>
</ol>
<p>2) 核心对象：对于任一样本 $x<em>j\in D$，如果其 $\epsilon$-邻域对应的 $N</em>ϵ(x_j)$ 至少包含MinPts个样本，即如果$|N\in(x_j)|≥MinPts$，则 $x_j$是核心对象。　 </p>
<ol>
<li><p>密度直达：如果 $x_i$ 位于 $x_j$ 的ϵ-邻域中，且$x_j$<strong>是核心对象</strong>，则称$x_i$ 由 $x_j$ 密度直达。注意<strong>反之不一定成立</strong>，即此时不能说$x_j$由$x_i$ 密度直达, 除非 $x_i$ 也是核心对象。</p>
</li>
<li><p>密度可达：对于 $x<em>i$ 和 $x_j$ , 如果存在样本样本序列 $p_1,p_2,\cdots,p_T$ ,满足$p_1 = x_i,p_T=x_j$ , 且$p</em>{t+1}$由$p<em>t$密度直达，则称$x_j$由 $x_i$密度可达。也就是说，<strong>密度可达满足传递性</strong>。此时序列中的传递样本$p_1,p_2,\cdots,p</em>{T−1}$ 均为核心对象，因为只有核心对象才能使其他样本密度直达。注意<strong>密度可达也不满足对称性</strong>，这个可以由密度直达的不对称性得出。</p>
</li>
<li><p>密度相连：对于 $x_i$ 和 $x_j$,如果存在核心对象样本$x_k$，使$x_i$和$x_j$均由$x_k$ 密度可达，则称$x_i$和$x_j$密度相连。注意<strong>密度相连关系是满足对称性的</strong>。</p>
</li>
</ol>
<p>从下图可以很容易看出理解上述定义，图中MinPts=5，红色的点都是核心对象，因为其ϵ-邻域至少有5个样本。黑色的样本是非核心对象。</p>
<ul>
<li>所有核心对象密度直达的样本在以红色核心对象为中心的超球体内，如果不在超球体内，则不能密度直达。</li>
<li>图中用绿色箭头连起来的核心对象组成了密度可达的样本序列。在这些密度可达的样本序列的 ϵ-邻域内</li>
<li>所有的样本相互都是密度相连的。</li>
</ul>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/19.png"></p>
<h3 id="聚类思想"><a href="#聚类思想" class="headerlink" title="聚类思想"></a>聚类思想</h3><p>DBSCAN的聚类定义很简单：由密度可达关系导出的最大密度相连的样本集合，即为我们最终聚类的一个类别，或者说一个簇。</p>
<p>这个DBSCAN的簇里面可以有一个或者多个核心对象。如果只有一个核心对象，则簇里其他的非核心对象样本都在这个核心对象的ϵ-邻域里；如果有多个核心对象，则簇里的任意一个核心对象的ϵ-邻域中一定有一个其他的核心对象，否则这两个核心对象无法密度可达。这些核心对象的ϵ-邻域里所有的样本的集合组成的一个DBSCAN聚类簇。</p>
<p>那么怎么才能找到这样的簇样本集合呢？DBSCAN使用的方法很简单，它任意选择一个没有类别的核心对象作为种子，然后找到所有这个核心对象能够密度可达的样本集合，即为一个聚类簇。接着继续选择另一个没有类别的核心对象去寻找密度可达的样本集合，这样就得到另一个聚类簇。一直运行到所有核心对象都有类别为止。</p>
<p>基本上这就是DBSCAN算法的主要内容了，是不是很简单？但是我们还是有三个问题没有考虑。</p>
<ul>
<li>第一个是一些异常样本点或者说少量游离于簇外的样本点，这些点不在任何一个核心对象在周围，在DBSCAN中，我们一般将这些样本点标记为噪音点。</li>
</ul>
<p>+　第二个是距离的度量问题，即如何计算某样本和核心对象样本的距离。在DBSCAN中，<strong>一般采用最近邻思想</strong>，采用某一种距离度量来衡量样本距离，比如欧式距离。这和KNN分类算法的最近邻思想完全相同。对应少量的样本，寻找最近邻可以直接去计算所有样本的距离，如果样本量较大，则一般采用KD树或者球树来快速的搜索最近邻。如果大家对于最近邻的思想，距离度量，KD树和球树不熟悉，建议参考 <a href="http://www.cnblogs.com/pinard/p/6061661.html" target="_blank" rel="noopener">K近邻法(KNN)原理小结</a>。</p>
<ul>
<li>第三种问题比较特殊，某些样本可能到两个核心对象的距离都小于 ϵ，但是这两个核心对象由于不是密度直达，又不属于同一个聚类簇，那么如果界定这个样本的类别呢？一般来说，此时DBSCAN采用先来后到，先进行聚类的类别簇会标记这个样本为它的类别。也就是说<strong>DBSCAN的算法不是完全稳定的算法</strong>。</li>
</ul>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p>下面我们对DBSCAN聚类算法的流程做一个总结。</p>
<p>输入：样本集 $D= (x_1,x_2,\cdots,x_m)$，邻域参数 (ϵ,MinPts), 样本距离度量方式 </p>
<p>输出：簇划分C.　</p>
<ol>
<li>初始化核心对象集合$\Omega = \emptyset$, 初始化聚类簇数 $k=0$，初始化未访问样本集合 $ \Gamma = D$,  簇划分$C = \emptyset$</li>
<li>对于 $j=1,2\cdots,m$, 按下面步骤，找出所有核心对象<ul>
<li>通过距离度量方式，找到样本 $x<em>j$ 的 ϵ-邻域 样本集 $N</em>\epsilon(x_j)$ </li>
<li>如果子样本集样本个数满足 $|N_\epsilon(x_j)|\geq MinPts$ , 就将样本$x_j$ 加入核心对象的样本集合中去：$\Omega = \Omega \cup{x_j}$ </li>
</ul>
</li>
<li>如果核心对象集合 $\Omega = \emptyset $，则算法结束，否则转入步骤4.</li>
<li>在核心对象集合$\Omega$中，随机选择一个核心对象$o$，初始化当前簇核心对象队列 $\Omega_{cur}={o}$, 初始化类别序号$k=k+1$，初始化当前簇样本集合 $C_k={o}$, 更新未访问样本集合 $\Gamma=\Gamma−{o}$ </li>
<li>如果当前簇核心对象队列 $\Omega_{cur}=\emptyset$，则当前聚类簇 $C_k$ 生成完毕, 更新簇划分$C={C_1,C_2\cdots,C_k}$, 更新核心对象集合$\Omega = \Omega- C_k$， 转入步骤3。否则更新核心对象集合 $\Omega=\Omega−C_k$。</li>
<li>在当前簇核心对象队列 $\Omega<em>{cur}$ 中取出一个核心对象 $o’$,通过邻域距离阈值 ϵ找出所有的 ϵ-邻域子样本集$N</em>\epsilon(o’)$，令$\Delta=N<em>c(o’)\cap \Gamma $, 更新当前簇样本集合$C_k = C_k\cup\Delta$, 更新未访问样本集合$\Gamma=\Gamma−\Delta$, 更新$\Omega</em>{cur} = \Omega_{cur}\cup(\Delta\cap\Omega)-o’$，转入步骤5.</li>
<li>如果 $x_i\in D$ 但是 $x_i\notin C_k,k=1,2\cdots,K$ ，那么我们称点 $x_i$ 为噪声</li>
</ol>
<p>输出结果为： 簇划分 $C = {C_1,C_2\cdots,C_m}$ </p>
<p>我们用一个可视化的例子来展现DBSCAN的聚类效果：这里由于点很多，采取的是在图片上等距离得取几个点，判断这几个点中是否存在核心对象，然后从选中的核心对象开始聚类。我们发现DBSCAN可以对非凸数据集(圆环)进行聚类，而Kmeans是达不到这种效果的。</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/20.gif"></p>
<h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><p>和传统的K-Means算法相比，DBSCAN最大的不同就是<strong>不需要输入类别数k</strong>，当然它最大的优势是可以发现任意形状的聚类簇，而不是像K-Means，一般仅仅使用于凸的样本集聚类。同时它在聚类的同时还可以找出异常点，这点和BIRCH算法类似。</p>
<p>用通俗的话来说：我联系的到的人，都是我的朋友，我联系不到的，反正不属于我这类。如果有人被孤立了，他就是噪声</p>
<p>那么我们什么时候需要用DBSCAN来聚类呢？一般来说，如果数据集是稠密的，并且数据集不是凸的，那么用DBSCAN会比K-Means聚类效果好很多；如果数据集不是稠密的，则不推荐用DBSCAN来聚类，因为如果$\epsilon$设置的不好的话，很可能都是噪声。</p>
<p>下面对DBSCAN算法的优缺点做一个总结。</p>
<p>DBSCAN的主要优点有：</p>
<ol>
<li>可以对任意形状的稠密数据集进行聚类，相对的，K-Means之类的聚类算法一般只适用于凸数据集。</li>
<li>可以在聚类的同时发现异常点，对数据集中的异常点不敏感。</li>
<li>聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。</li>
</ol>
<p>DBSCAN的主要缺点有：</p>
<ol>
<li>如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。</li>
<li>如果样本集较大时，聚类收敛时间较长，此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进。</li>
<li>调参相对于传统的K-Means之类的聚类算法稍复杂，主要需要对距离阈值 ϵ，邻域样本数阈值MinPts联合调参，不同的参数组合对最后的聚类效果有较大影响。</li>
</ol>
<h1 id="聚类方法的评价"><a href="#聚类方法的评价" class="headerlink" title="聚类方法的评价"></a>聚类方法的评价</h1><p>聚类有效性是评价聚类结果的方式，主要分为外部聚类的有效性和内布聚类的有效性。它们的区别就在于：对于这个数据集，我们知不知道它的标签，如果知道，相当于借助了“外部”信息，就是外部聚类的有效性。反之就是内部聚类的有效性</p>
<h2 id="外部聚类的有效性"><a href="#外部聚类的有效性" class="headerlink" title="外部聚类的有效性"></a>外部聚类的有效性</h2><p>对于n个测试样本(数据集) $x_i(i=1,2\cdots,n)$ </p>
<ul>
<li><p>假定分类结果为 $\mathcal C = {C_1,C_2\cdots,C_K}$  并满足：</p>
<script type="math/tex; mode=display">
C_k\cap C_l = \emptyset\\
\bigcup_{k=1}^KC_i = X</script><ul>
<li>K 为聚类的数目</li>
</ul>
</li>
<li><p>假设 “真实的” 标签划分 $\mathcal P = {P<em>1,P_2\cdots,P</em>{K’}}$ 并满足：</p>
<script type="math/tex; mode=display">
P_k\cap P_l = \emptyset\\
\bigcup_{k=1}^KP_i = X</script><ul>
<li>K’ 为真实分类数目</li>
</ul>
</li>
</ul>
<p>因此，我们可以给出可能性矩阵的定义：</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/21.png"></p>
<p>我们可以计算：</p>
<script type="math/tex; mode=display">
p_{ij} = \frac{n_{ij}}{n},p_i = \frac{n_{i\cdot}}{n},p_j = \frac{n_{\cdot j}}{n}</script><h3 id="常用指标"><a href="#常用指标" class="headerlink" title="常用指标"></a>常用指标</h3><p>对 K 均值聚类算法而言，熵和纯度是两种最常用的外部度量</p>
<h4 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h4><script type="math/tex; mode=display">
E = -\sum_i p_i\bigg(\sum_j\frac{p_{ij}}{p_i}\ln\frac{p_{ij}}{p_i} \bigg)</script><h4 id="纯度"><a href="#纯度" class="headerlink" title="纯度"></a>纯度</h4><script type="math/tex; mode=display">
P = \sum_i p_i(\max_j\frac{p_{ij}}{p_i})</script><h2 id="内部聚类的有效性"><a href="#内部聚类的有效性" class="headerlink" title="内部聚类的有效性"></a>内部聚类的有效性</h2><p>聚类一般是没有标签的，所以外部聚类有效性指标有很大的局限性。因此现在来讲讲内部聚类的有效性。</p>
<p>内部聚类有效性要看两个准则：</p>
<h3 id="紧密度-Compactness"><a href="#紧密度-Compactness" class="headerlink" title="紧密度(Compactness)"></a>紧密度(Compactness)</h3><p>紧密度是在同一类内不同个体之间紧密关联的度量</p>
<ul>
<li>方差可以体现数据的紧密度；低方差表明紧密度好。</li>
<li>很多紧密度的定义是依赖于距离的，如：最大或平均两两距离，基于中心的最大或平均距离等。</li>
</ul>
<h3 id="区分度-Seperation"><a href="#区分度-Seperation" class="headerlink" title="区分度(Seperation)"></a>区分度(Seperation)</h3><p>区分度是不同类别区间程度的度量。</p>
<ul>
<li>例如，两个类中心的距离，或者是最短距离，通常作为区分度的度量</li>
<li>密度(density) 也会用于度量区分度。</li>
</ul>
<h3 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h3><h4 id="均方标准差"><a href="#均方标准差" class="headerlink" title="均方标准差"></a>均方标准差</h4><script type="math/tex; mode=display">
RMSSTD = \bigg(\frac{\sum_{k=1}^K \sum_{i\in C_k}||\boldsymbol x_i-\boldsymbol m_k||^2 }{p\sum_{k=1}^K(n_{k}-1)}  \bigg)^{1/2}</script><p>用语言来表述，分子是：每个个体减去每个组的均值；分母中，p代表维度，右边是总的样本的个数。</p>
<p>RMSSTD 越小，就代表紧密度越好</p>
<h4 id="R平方-RS"><a href="#R平方-RS" class="headerlink" title="R平方 (RS)"></a>R平方 (RS)</h4><script type="math/tex; mode=display">
RS = 1-\frac{\sum_{k=1}^K\sum_{i\in C_k}||\boldsymbol x_i-\boldsymbol m_k||^2}{\sum_{i=1}^n||\boldsymbol x_i-\boldsymbol  {\overline x}||^2}</script><p>这个方法是从回归中迁移过来的，分子是 SSE，即组内个体减去组均值的平方和。分母是SST 即所有样本减去整体样本的平方和</p>
<h1 id="选超参"><a href="#选超参" class="headerlink" title="选超参"></a>选超参</h1><p>因为很多聚类算法都需要我们手动选择超参，那么有什么通用的方法吗？</p>
<h2 id="轮廓法"><a href="#轮廓法" class="headerlink" title="轮廓法"></a>轮廓法</h2><p>第一种是轮廓法，是一种直观的、且用于验证聚类结果的方法。</p>
<p>基本思想就是：同类相似，异类不同。</p>
<p>在轮廓法中，需要定义一个重要的概念——轮廓值。 轮廓值是一个-1到1之间的值，需要通过计算获得。</p>
<p>一般认为，轮廓值较高表示该样本被很好得聚到其所属的类，而不和其他类相似。</p>
<p>如果大部分的样本具有较高的轮廓值，那么聚类的结果是恰当的；如果存在许多样本具有较低的轮廓值，甚至是负值，那么聚类的个数可能不合适</p>
<h3 id="具体方法"><a href="#具体方法" class="headerlink" title="具体方法"></a>具体方法</h3><h4 id="计算-a-i"><a href="#计算-a-i" class="headerlink" title="计算 a(i)"></a>计算 a(i)</h4><p>对于某个聚类结果 $\mathcal C = {C_1,C_2\cdots,C_K}$  ：</p>
<p>那么，对于第 i 个样本，假定其属于第k 类，$n_k$ 表示第k 类中的样本量，令$a(i)$ 表示第i个样本与 第k类的 其他样本的 平均距离。即：</p>
<script type="math/tex; mode=display">
a(i) = \frac{1}{n_k-1} \sum_{j\in \mathcal C_k,j\neq i}\text{dist}(i,j)</script><p>我们注意到 $a(i)$ 表示第i 个样本与其同属一类的样本的平均<strong>不相似度</strong>，如果该类只有第i个样本本身，那么$a(i)$ 为0</p>
<h4 id="计算b-i"><a href="#计算b-i" class="headerlink" title="计算b(i)"></a>计算b(i)</h4><p>在计算 b(i) 之前，我们要对第i个样本和另一个样本量为 $n<em>{k’}$ 的类 $\mathcal C</em>{k’}$ ，令$d(i,\mathcal C_{k’})$为第i个样本与第 $k’$ 个类的所有样本的平均<strong>不相似度</strong></p>
<script type="math/tex; mode=display">
d(i,\mathcal C_{k'}) = \frac{1}{n_{k'}}\sum_{j\in \mathcal C_{k'}}\text{dist}(i,j)</script><p>然后我们可以定义 $b(i)$，令</p>
<script type="math/tex; mode=display">
b(i) = \min_{k'\neq k} d(i,\mathcal C_k')</script><p>用来表示<strong>第i 个样本和不属于同一类的最近距离</strong></p>
<h4 id="计算轮廓值-s-i"><a href="#计算轮廓值-s-i" class="headerlink" title="计算轮廓值 s(i)"></a>计算轮廓值 s(i)</h4><p>对于第 i 个样本，我们定义轮廓值 $s(i)$ 为：</p>
<script type="math/tex; mode=display">
s(i) = \frac{b(i)-a(i)}{\max\big(a(i),b(i)\big)}</script><p>对于所有n个样本，可以计算平均轮廓值，用于度量聚类数目 K 是否合适</p>
<p>由此，以<strong>最大的平均轮廓值</strong>对应的K 作为最优聚类数目，这种方法被称为是轮廓法</p>
<h2 id="CH指数"><a href="#CH指数" class="headerlink" title="CH指数"></a>CH指数</h2><p>CH 指数是与方差分析中的 F 检验统计量是相似的。</p>
<p>假定我们对 n 个样本进行聚类，对于某个聚类结果 $\mathcal C = {C_1,C_2\cdots,C_K}$  ：我们分别考虑类间的平方和$B(K)$ 和 类内的平方和 $W(K)$ </p>
<p>那么，CH指数可以定义为：</p>
<script type="math/tex; mode=display">
CH(K) = \frac{B(K)/K-1}{W(K)/(n-K)}</script><p>其中，</p>
<script type="math/tex; mode=display">
\begin{align}
&W(K) = \sum_{k=1}^K \sum_{i\in \mathcal C_k}||\boldsymbol x_i-\boldsymbol{\overline x_k}||^2\\
&\overline x_k = \frac{1}{n_k}\sum_{i\in\mathcal C_k}\boldsymbol x_i\\
\end{align}</script><script type="math/tex; mode=display">
\begin{align}
&B(K) = \sum_{k=1}^K\sum_{i\in\mathcal C_k} ||\overline  {\boldsymbol x_i}-\boldsymbol {\overline{\overline x}}_k ||^2\\
&\overline{\overline {\boldsymbol x}} = \frac{1}{K} \sum_{k=1}^K \overline {\boldsymbol x}_k
\end{align}</script><p>基于方差分解公式，在给定样本时，$B(K)$ 与 $W(K)$ 的和是一个定值</p>
<p>如果 $B(K)$ 越大，则 $W(K)$ 越小，那么满足对聚类的基本思想：类内差异小，类间差异大</p>
<p>我们可以通过最大化 CH指数来得到最优的聚类数目K</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2021/11/01/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90-%E7%9F%A9%E9%98%B5%E7%89%88/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/01/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90-%E7%9F%A9%E9%98%B5%E7%89%88/" class="post-title-link" itemprop="url">线性回归分析-矩阵版</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-01 10:31:48" itemprop="dateCreated datePublished" datetime="2021-11-01T10:31:48+08:00">2021-11-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-12-27 10:59:36" itemprop="dateModified" datetime="2023-12-27T10:59:36+08:00">2023-12-27</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="线性回归分析——PartII"><a href="#线性回归分析——PartII" class="headerlink" title="线性回归分析——PartII"></a>线性回归分析——PartII</h1><p>为什么我们一开始不讲矩阵呢？这是因为如果一上来就研究多元线性回归，就会忽略掉很多细节，而且还听不懂</p>
<h2 id="线性回归的模型与假设"><a href="#线性回归的模型与假设" class="headerlink" title="线性回归的模型与假设"></a>线性回归的模型与假设</h2><p>多元线性回归就是多个x，其线性回归模型为：</p>
<script type="math/tex; mode=display">
y = \beta_0+\beta_1x_1+\beta_2x_2+\cdots+\beta_px_p+\varepsilon</script><p>y为响应变量/因变量，为一个随机变量；</p>
<p>x为协变量/自变量，通常假定是确定性的变量</p>
<p>$\beta_0,\beta_1\cdots,\beta_p $是$p+1$个未知参数：</p>
<p>$\varepsilon$为随机误差，并假定：$\cases{E(\varepsilon) = 0\~\Var(\varepsilon) = \sigma^2}$</p>
<p>那么，对于n组观测数据，$(x<em>{i1},x</em>{i2},\cdots,x_{ip},y_i)$，我们可以写出回归方程模型：</p>
<script type="math/tex; mode=display">
\cases{y_1 = \beta_0+\beta_1x_{11}+\beta_2x_{12}+\cdots+\beta_px_{1p}+\varepsilon_1\\~\\y_2 = \beta_0+\beta_1x_{21}+\beta_2x_{22}+\cdots+\beta_px_{2p}+\varepsilon_2\\ \cdots\\y_n = \beta_0+\beta_1x_{n1}+\beta_2x_{n2}+\cdots+\beta_px_{np}+\varepsilon_n}</script><p>对于这些方程，我们可以将其抽象出来形成矩阵、向量的形式</p>
<script type="math/tex; mode=display">
\boldsymbol  y = \pmatrix{y_1\\y_2\\\vdots \\y_n}~~~~~
\boldsymbol X= \pmatrix{1&x_{11}&x_{12}&\cdots&x_{1p}\\1&x_{21}&x_{22}&\cdots &x_{2p}\\\vdots&\cdots\\1&x_{n1}& x_{n2}&\cdots&x_{np}}_{n\times(p+1)}\\~\\
\boldsymbol {\beta} = (\beta_0,\beta_1,\cdots,\beta_p)'\\
\varepsilon = (\varepsilon_1,\varepsilon_2\cdots,\varepsilon_n)'</script><p>吗线性回归模型的矩阵形式为：</p>
<script type="math/tex; mode=display">
\boldsymbol {y = X\beta+\varepsilon}</script><p>我们看到，用矩阵的方式来表示模型，会比较简洁。</p>
<p>那么，我们要做的就是估计参数向量$\boldsymbol{\beta}$​​ ，带入模型，这样就可以预测出$\hat y$</p>
<h3 id="线性回归的基本假定"><a href="#线性回归的基本假定" class="headerlink" title="线性回归的基本假定"></a>线性回归的基本假定</h3><p>为了便于参数估计，需要对回归方程进行一些假设：</p>
<ol>
<li>关于设计矩阵$\boldsymbol X$ </li>
</ol>
<ul>
<li>它是确定性变量，不是随机变量。在预测的时候，是给定确定的x的条件下去做的预测</li>
<li>要求$rank(\boldsymbol{X}) = p+1&lt;n$​,这表明了这是个列满秩的矩阵，每一维都不能被其他特征线性表出，每一列的自变量之间不相关。样本量应大于自变量的个数，$\boldsymbol X$​是一个满秩矩阵</li>
</ul>
<ol>
<li>关于随机误差是零均值且等方差的</li>
</ol>
<ul>
<li>$E(\varepsilon_i) =0,i=1,2\cdots,n$  表示没有系统误差</li>
<li>$Cov(\varepsilon_i,\varepsilon_j) = \cases{\sigma^2,i=j\0,i\neq j},i,j = 1,2\cdots,n$​ 表明随机在不同的样本点之间是不相关的(在正态假定下即独立的)，不存在序列相关，并且有相同的精度。只有自身和自身存在协方差(等于方差)</li>
</ul>
<p>这个条件常被称为<strong>高斯-马尔可夫条件</strong></p>
<ol>
<li>假定随机误差服从正态分布</li>
</ol>
<p>即：$\cases{\varepsilon_i\sim N(0,\sigma^2),i=1,2\cdots,n\~\\varepsilon_1,\varepsilon_2,\cdots,\varepsilon_n\text{相互独立}}$</p>
<p>在这个假设下，随机误差向量服从：</p>
<script type="math/tex; mode=display">
\boldsymbol\varepsilon\sim N(\boldsymbol{0},\sigma^2\boldsymbol{I_n})</script><p>即每一维的误差都来自一个正态分布。</p>
<p>$\boldsymbol y$ 分为前后两部分，前面是确定性的，而后半部分$\boldsymbol\varepsilon$ 是随机性的，又 $X$也是服从正态分布的，因此等价于假定因变量$\boldsymbol y$ 服从n维正态分布，其期望向量和协方差矩阵分别为：</p>
<script type="math/tex; mode=display">
E(\boldsymbol y) = \boldsymbol {X\beta}\\
Var(\boldsymbol y) = \sigma^2 \boldsymbol{I_n}</script><h2 id="线性回归模型的参数估计"><a href="#线性回归模型的参数估计" class="headerlink" title="线性回归模型的参数估计"></a>线性回归模型的参数估计</h2><h3 id="最小二乘估计"><a href="#最小二乘估计" class="headerlink" title="最小二乘估计"></a>最小二乘估计</h3><p><img src="/2021/11/01/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90-%E7%9F%A9%E9%98%B5%E7%89%88/1.png"></p>
<p>根据这张图我们可以定义离差，也就是<strong>实际观测值和估计值的差</strong>，即 $y_i-\boldsymbol x_i\boldsymbol \beta$</p>
<p>由此我们可以给出最小二乘估计：通过最小化离差平方和而得到的估计方法</p>
<p>对于线性模型，离差平方和可以定义为：</p>
<script type="math/tex; mode=display">
Q(\boldsymbol \beta) = \sum_{i=1}^n(y_i-\boldsymbol x_i' \boldsymbol\beta)^2 = ||\boldsymbol y-\boldsymbol{X\beta}||^2</script><p>最小二乘估计为：</p>
<script type="math/tex; mode=display">
\hat\beta_{LS} = \arg\min_\beta Q(\beta)</script><p>这其实是一个数，即行向量乘以列向量，我们可以这样改写上式：</p>
<script type="math/tex; mode=display">
Q(\boldsymbol \beta)= (\boldsymbol y-\boldsymbol{X\beta})'(\boldsymbol y-\boldsymbol{X\beta})\\
Q(\boldsymbol \beta)= \boldsymbol{y'y}-2\boldsymbol{\beta'X'y}+\boldsymbol{\beta'X'X\beta}</script><p>我们要发现$\boldsymbol {y’X\beta}$和$\boldsymbol{\beta’X’y}$ 都是一个数因此可以合并同类项</p>
<p>这样写好以后，相当于一个$\boldsymbol\beta$ 的二次函数，我们可以通过对其求导来找到$\min \boldsymbol \beta$​ </p>
<h4 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h4><p>对于一个向量，我们怎么求其最小值呢，在这里我们补充一些求导的公式:</p>
<p>对于一个 p 维向量 $\boldsymbol x = (x_1,x_2\cdots,x_p)’$</p>
<ul>
<li>线性函数求导：对于任意常向量 $\boldsymbol a = (a_1,a_2\cdots,a_p)’$, 我们有：</li>
</ul>
<script type="math/tex; mode=display">
\frac{\partial (\boldsymbol{x'a})}{\partial \boldsymbol x} = \frac{\partial (\boldsymbol{a'x})}{\partial \boldsymbol x} = \boldsymbol a\\</script><ul>
<li>二次型求导：对于任意$p\times p$ 常值矩阵 $\boldsymbol B$ 我们有：</li>
</ul>
<script type="math/tex; mode=display">
\frac{\partial {\boldsymbol {x'Bx}}}{\partial {\boldsymbol x}} = (\boldsymbol{B+B'})\boldsymbol x</script><p>​    特别地，若 $\boldsymbol B$ 是一个对称矩阵，那么 </p>
<script type="math/tex; mode=display">
\frac{\partial (\boldsymbol{x'Bx})}{\partial \boldsymbol x} = 2\boldsymbol {Bx}</script><h4 id="具体计算方法"><a href="#具体计算方法" class="headerlink" title="具体计算方法"></a>具体计算方法</h4><p>我们用 $Q(\boldsymbol \beta)$ 关于$\boldsymbol \beta$​ 求导，可得：</p>
<script type="math/tex; mode=display">
\frac{\partial Q(\boldsymbol{\beta})}{\partial \boldsymbol \beta} =  2\boldsymbol{X'X\beta}-2\boldsymbol{X'y}</script><p>令 $\frac{\partial Q(\boldsymbol \beta)}{\partial \boldsymbol \beta}=0$ 可得：</p>
<script type="math/tex; mode=display">
\boldsymbol {X'X\beta} = \boldsymbol{X'y}</script><p>基于假设(1),$\boldsymbol {X’X}$ 是满秩的，因此 $(\boldsymbol{X’X})^{-1}$ 存在，由此，最小二乘估计为：</p>
<script type="math/tex; mode=display">
\boldsymbol{\hat\beta_{LS}} = \boldsymbol{(X'X)}^{-1}\boldsymbol {X'y}</script><p>说明： </p>
<ul>
<li>根据上式可知，在最小二乘估计$ \boldsymbol{\hat \beta}$​ 时，需要$(\boldsymbol{X’X})^{-1}$​ 必须存在。也就是说，$(\boldsymbol{X’X})$是一<strong>非奇异矩阵</strong>，即$\left| {\boldsymbol{X’X}} \right |\neq 0$ </li>
<li>由线性代数可知 ，$rank(\boldsymbol X)\geq rank(\boldsymbol{X’X})$​, 如果$\boldsymbol{X’X}$ 为 p+1阶满秩矩阵，也就是说 $rank(\boldsymbol{X’X})=p+1$,那么$rank(\boldsymbol X)\geq p+1$​ </li>
<li>另一方面，设计矩阵$\boldsymbol X$​ 为 $n\times (p+1)$ 阶矩阵，于是应用 $n\geq (p+1)$ 这表明了采用最小二乘法估计方法求解线性回归的未知参数，<strong>样本量必须不少于模型的参数个数​</strong> </li>
</ul>
<h4 id="拟合值"><a href="#拟合值" class="headerlink" title="拟合值"></a>拟合值</h4><p>求得 $\boldsymbol {\hat\beta}_{LS}$之后，我们可以定义回归值或者拟合值为：</p>
<script type="math/tex; mode=display">
\boldsymbol{\hat y} = \boldsymbol {X\hat\beta}</script><p>其中，$\hat y_i = \boldsymbol x_i’\boldsymbol{\hat\beta} ,~~i=1,2\cdots,n$​​​ </p>
<p>我们将 $\boldsymbol {\hat\beta}$ 用上面求得的最小二乘估计带入，得到：</p>
<script type="math/tex; mode=display">
\boldsymbol{\hat y} = \boldsymbol {X\hat\beta} = \boldsymbol{X(X'X)^{-1}X'y}</script><p>矩阵 $\boldsymbol {X(X’X)^{-1}X’}$​ ： 将观测值$\boldsymbol y$​ 变换为 $\boldsymbol {\hat y}$​ ,从形式上来看，就是给$\boldsymbol y$​ 戴上了一顶帽子  $\hat ~$​ ，因为形象地称矩阵 $\boldsymbol{X(X’X)^{-1}X’}$​ 为帽子矩阵，记为$\boldsymbol H$​ </p>
<p>于是： $\boldsymbol{\hat y} = \boldsymbol {Hy} $​ ​</p>
<h4 id="帽子矩阵的性质"><a href="#帽子矩阵的性质" class="headerlink" title="帽子矩阵的性质"></a>帽子矩阵的性质</h4><p>帽子矩阵 $\boldsymbol H = \boldsymbol {X(X’X)^{-1}X’}$ 具有以下的一些性质：</p>
<ol>
<li><p>$\boldsymbol H$ 是n阶对称矩阵</p>
</li>
<li><p>$\boldsymbol H$ 是幂等矩阵，即$\boldsymbol H =\boldsymbol H^2$</p>
</li>
<li><p>$\boldsymbol H$ 的迹为 $p+1$,即 $tr(\boldsymbol H)=p+1$</p>
</li>
</ol>
<p>证明1：</p>
<script type="math/tex; mode=display">
\boldsymbol H' = (\boldsymbol{X(X'X)^{-1}X'})' = (\boldsymbol X')'((\boldsymbol {X'X})^{-1})'\boldsymbol X'\\
=\boldsymbol {X(X'X)^{-1}X'} = \boldsymbol H</script><p>由于$\boldsymbol H$ 的转置矩阵等于$\boldsymbol H$ ，所以$\boldsymbol H$ 是对称的</p>
<p>证明2：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\boldsymbol H^2 = (\boldsymbol{X(X'X)^{-1}X'})^2\\
&=(\boldsymbol{X(X'X)^{-1}X'})(\boldsymbol{X(X'X)^{-1}X'})\\
&=\boldsymbol {X(X'X)^{-1}(X'X)(X'X)^{-1}X'}\\
&=\boldsymbol {X(X'X)^{-1}X'}=\boldsymbol H
\end{aligned}</script><p>因此$\boldsymbol H$是幂等矩阵</p>
<p>证明3：</p>
<p>易知$\boldsymbol{X’X}$​​是一个 $(p+1)\times(p+1)$ 的满秩矩阵。于是我们计算$\boldsymbol H$ 的迹，即：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&tr(\boldsymbol H) = tr(\boldsymbol {X(X'X)^{-1}X'})\\
&=tr((\boldsymbol{X'X})^{-1}\boldsymbol {X'X})\\
&=tr(\boldsymbol I_{p+1})=p+1
\end{aligned}</script><p>我们要知道矩阵迹的运算性质：多个矩阵相乘得到的方阵的迹，和将这些矩阵中的最后一个挪到最前面之后相乘的迹是相同的，即：</p>
<script type="math/tex; mode=display">
Tr(\boldsymbol{ABC}) = Tr(\boldsymbol{BCA}) = Tr(\boldsymbol{CAB})</script><h4 id="残差"><a href="#残差" class="headerlink" title="残差"></a>残差</h4><p>说完了拟合值，我们聚焦到最后一个部分——残差</p>
<p>我们把残差定义为拟合值和真实值之间的距离</p>
<script type="math/tex; mode=display">
\boldsymbol e = \boldsymbol y -\boldsymbol{\hat y}</script><p>也可以写为 ：</p>
<script type="math/tex; mode=display">
\boldsymbol e = \boldsymbol y - \boldsymbol{Hy} = (\boldsymbol I-\boldsymbol H)\boldsymbol y</script><p>几何上的关系：回归值 $\boldsymbol {\hat y}$​与残差$\boldsymbol e$ 垂直，即：</p>
<script type="math/tex; mode=display">
\boldsymbol {y'e} = (\boldsymbol {Hy})'((\boldsymbol I-\boldsymbol H)\boldsymbol y)= \boldsymbol {y'H'}(\boldsymbol I-\boldsymbol H)\boldsymbol y =0</script><p>然后，我们写出残差的协方差矩阵为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\text{Var}(\boldsymbol e) = \text{Cov}(\boldsymbol e,\boldsymbol e)\\
&=\text{Cov}((\boldsymbol I-\boldsymbol H)\boldsymbol y,(\boldsymbol I-\boldsymbol H)\boldsymbol y)\\
&=(\boldsymbol I-\boldsymbol H)\text{Cov}(\boldsymbol y,\boldsymbol y)(\boldsymbol I-\boldsymbol H)'\\
&=\sigma^2(\boldsymbol I-\boldsymbol H)\boldsymbol I_n(\boldsymbol I-\boldsymbol H)’\\
&=\sigma^2(\boldsymbol I-\boldsymbol H)
\end{aligned}</script><p>由此，我们可以构造误差项方差$\sigma^2$ 的估计，用残差去估算即：</p>
<script type="math/tex; mode=display">
\hat\sigma^2 = \frac{1}{n-p-1}(\boldsymbol {e'e}) = \frac{1}{n-p-1}\sum_{i=1}^n e_i^2</script><h3 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h3><p>现在我们介绍第二种方法——极大似然估计。$\boldsymbol y$ 是服从多元正态分布的，即：</p>
<script type="math/tex; mode=display">
\boldsymbol y\sim N_n(\boldsymbol {X\beta},\sigma^2\boldsymbol I_n)</script><p>因此 $\boldsymbol y$ 的联合密度函数为：</p>
<script type="math/tex; mode=display">
f(\boldsymbol y;\boldsymbol \beta,\sigma^2) = \frac{1}{(2\pi)^{n/2}\left|{\sigma^2 \boldsymbol I_n}^{1/2}\right |}\exp\{-\frac{1}{2}(\boldsymbol y-\boldsymbol{X\beta})'(\sigma^2\boldsymbol (\boldsymbol{y-X\beta}' (\sigma ^2I_n) ^{ -1 }(\boldsymbol{y-X\beta}))</script><p> 参数 $( \beta,\sigma ^2) $​​​的 似然函数为</p>
<script type="math/tex; mode=display">
L (\boldsymbol \beta,\sigma ^2) = (2\pi)^{-n/2}(\sigma ^2) ^{-n/2}\exp\{ (\boldsymbol { y- X\beta} ) ' (\boldsymbol{y-X\beta}) \}</script><p>进而我们给出极大似然估计 ：</p>
<script type="math/tex; mode=display">
(\boldsymbol{\hat\beta}_ {ML},\hat\sigma _{ML} ^2) =\arg\max_{ (\beta,\sigma^ 2) }L(\boldsymbol\beta,\sigma ^2)\\
 =\arg\max_ { (\beta,\sigma ^2)}\ln (L(\boldsymbol\beta,\sigma ^2))</script><h4 id="具体计算方法-1"><a href="#具体计算方法-1" class="headerlink" title="具体计算方法"></a>具体计算方法</h4><script type="math/tex; mode=display">
\ln L(\boldsymbol\beta,\sigma^2) = -\frac{n}{2}\ln(2\pi)-\frac{n}{2}\ln(\sigma^2)-\frac{1}{2\sigma^2}(\boldsymbol{y-X\beta})'(\boldsymbol{y-X\beta})</script><p>对数似然函数分布关于$\boldsymbol \beta$​和和$\sigma^2$​求导，即</p>
<script type="math/tex; mode=display">
\frac{\partial\ln L(\boldsymbol\beta,\sigma^2)}{\partial\beta} = -\frac{1}{\sigma^2}(\boldsymbol{X'X\beta-X'y})=0\\
\frac{\partial\ln L(\boldsymbol\beta,\sigma^2)}{\partial{\sigma^2}} = -\frac{n}{2\sigma^2}+\frac{1}{2\sigma^4}(\boldsymbol {y-X\beta})'(\boldsymbol{y-X\beta})=0</script><p>反解得到：</p>
<script type="math/tex; mode=display">
\boldsymbol{\hat\beta}_{ML} = (\boldsymbol{X'X})^{-1}\boldsymbol{X}'\boldsymbol y\\
\hat\sigma^2_{ML} = \frac{1}{n}(\boldsymbol {y-X'\beta}_{ML})'(\boldsymbol {y-X\hat\beta}_{ML})=\frac{1}{n}\boldsymbol {e'e}</script><blockquote>
<p>说明：</p>
<p>$\boldsymbol{\hat\beta}<em>{ML}=\boldsymbol{\hat\beta}</em>{LS}$ ，因此，我们不用写下标了，一般记为 $\boldsymbol{\hat\beta}  = (\boldsymbol{X’X})^{-1}\boldsymbol{X}’\boldsymbol y$​</p>
<p>$\hat\sigma^2_{ML}$ 不是一个无偏估计，但是一个相合估计</p>
</blockquote>
<h3 id="参数估计性质"><a href="#参数估计性质" class="headerlink" title="参数估计性质"></a>参数估计性质</h3><blockquote>
<p>概率论 关于矩阵的期望方差复习。假设$\boldsymbol {x,y}$是n维随机变量。对任意一个$m\times n$ 维常矩阵 $\boldsymbol A$ 和一个$m’\times n$ 维常矩阵$\boldsymbol B$​,以及一个m维向量$\boldsymbol c$</p>
<ul>
<li>$E(\boldsymbol {Ax+c})=\boldsymbol{A}E(\boldsymbol {x})+\boldsymbol c$</li>
<li>$Var(\boldsymbol{Ax+c})=\boldsymbol A\text{Var(x)}A’$</li>
<li>$Cov(\boldsymbol{Ax,By})=\boldsymbol A Cov(\boldsymbol{x,y})\boldsymbol B’$​ ​​</li>
</ul>
</blockquote>
<h4 id="最小二乘估计的性质"><a href="#最小二乘估计的性质" class="headerlink" title="最小二乘估计的性质"></a>最小二乘估计的性质</h4><p>最小二乘估计 $\boldsymbol{\hat\beta}=(\boldsymbol{X’X})^{-1}\boldsymbol{X’y}$​ 那么$\boldsymbol{\hat\beta}$​ 满足：</p>
<blockquote>
<ul>
<li>$E(\boldsymbol{\hat\beta})=\boldsymbol{\beta}$​, 即$\boldsymbol{\hat\beta}$是$\boldsymbol \beta$ 的无偏估计</li>
<li>$Var(\boldsymbol{\hat\beta})=\sigma^2(\boldsymbol{X’X})^{-1}$</li>
</ul>
</blockquote>
<p>证明1：计算$E(\boldsymbol {\hat\beta})$​ </p>
<script type="math/tex; mode=display">
\begin{aligned}
&E(\boldsymbol {\hat\beta})=E((\boldsymbol{X'X})'\boldsymbol{X'y})=(\boldsymbol{X'X})^{-1}\boldsymbol X'E(\boldsymbol y)\\
&=(\boldsymbol{X'X})^{-1}\boldsymbol X' E(\boldsymbol{X\beta+\epsilon})\\
&=(\boldsymbol{X'X})^{-1}\boldsymbol X'(\boldsymbol{X\beta}+E(\boldsymbol\epsilon))\\
&=(\boldsymbol{X'X})^{-1}\boldsymbol{X'X\beta} = \boldsymbol \beta
\end{aligned}</script><p>证明2：计算$Var(\boldsymbol{\hat\beta})$​​</p>
<script type="math/tex; mode=display">
\begin{aligned}
&Var(\boldsymbol{\hat\beta})=Var((\boldsymbol{X'X})^{-1}\boldsymbol{X'y})\\
&=(\boldsymbol{X'X})^{-1}\boldsymbol {X'}Var(\boldsymbol y)((\boldsymbol{X'X})^{-1}\boldsymbol X')'\\
&=(\boldsymbol{X'X})^{-1}\boldsymbol {X'}Var(\boldsymbol {X\beta}+\boldsymbol\varepsilon)((\boldsymbol{X'X})^{-1}\boldsymbol X')'\\
&=(\boldsymbol{X'X})^{-1}\boldsymbol {X'}(\sigma^2\boldsymbol I_n)((\boldsymbol{X'X})^{-1}\boldsymbol X')'\\
&=\sigma^2(\boldsymbol{X'X})^{-1}\boldsymbol{X'X}(\boldsymbol{X'X})^{-1}\\
&=\sigma^2(\boldsymbol{X'X})^{-1}
\end{aligned}</script><h4 id="最小二乘估计-boldsymbol-hat-beta-与残差-boldsymbol-e-的关系"><a href="#最小二乘估计-boldsymbol-hat-beta-与残差-boldsymbol-e-的关系" class="headerlink" title="最小二乘估计$\boldsymbol {\hat\beta}$与残差$\boldsymbol e$ 的关系"></a>最小二乘估计$\boldsymbol {\hat\beta}$与残差$\boldsymbol e$ 的关系</h4><blockquote>
<p>最小二乘估计$\boldsymbol \beta$与残差$\boldsymbol e$ 线性不相关，即</p>
<script type="math/tex; mode=display">
\text{Cov}(\boldsymbol{\hat\beta,e})=\boldsymbol 0</script></blockquote>
<p>之前我们说到，最小二乘估计$\boldsymbol {\hat\beta}$与残差$\boldsymbol e$​​ 在数学层面是垂直的。那么反映在统计中，就是线性无关的。也就是他们两的协方差等于0</p>
<p>说明：特别地，在正态分布的假定下，最小二乘估计$\boldsymbol{\hat\beta}$​与残差$\boldsymbol e$​ 独立。基于此，最小二乘估计$\boldsymbol {\hat\beta}$​ 与残差平方和 $SS_E=\boldsymbol{e’e}$独立</p>
<p>证明：</p>
<script type="math/tex; mode=display">
\text{Cov}(\boldsymbol{\hat\beta,e}) = \text{Cov}((\boldsymbol{X'X})^{-1}\boldsymbol{X'y}),(\boldsymbol{I-H})\boldsymbol y)\\</script><p>根据上面所说的协方差的性质，又知道帽子矩阵$\boldsymbol H = \boldsymbol {X(X’X)^{-1}X’}$​​ ，以及$\text{Cov}(\boldsymbol{y,y})=\sigma^2$​,原式可化简为：</p>
<script type="math/tex; mode=display">
=\sigma^2((\boldsymbol{X'X})^{-1}\boldsymbol{X}'(\boldsymbol{I}_n-\boldsymbol{X(X'X)^{-1}X'}))\\</script><p>因为 $(\boldsymbol{X’X})^{-1}=\boldsymbol X^{-1}{\boldsymbol X’}^{-1}$, 所以：$\boldsymbol{I}_n-\boldsymbol{X(X’X)^{-1}X’}=0$</p>
<p>因此$\text{Cov}=0$​ ，得证</p>
<h2 id="中心化和标准化"><a href="#中心化和标准化" class="headerlink" title="中心化和标准化"></a>中心化和标准化</h2><h3 id="中心化"><a href="#中心化" class="headerlink" title="中心化"></a>中心化</h3><h4 id="矩阵知识补充"><a href="#矩阵知识补充" class="headerlink" title="矩阵知识补充"></a>矩阵知识补充</h4><p>假定$\boldsymbol A$ 是$m\times m$ 可逆矩阵，$\boldsymbol B$是 $m\times n$矩阵，$\boldsymbol C$ 是$n\times m$ 矩阵，$\boldsymbol D$是$n\times n$ 矩阵。如果$\boldsymbol D-\boldsymbol{CA^{-1}B}$ 是$n\times n$ 可逆矩阵，那么：</p>
<script type="math/tex; mode=display">
\pmatrix{A &B\\C&D}^{-1}=\pmatrix{E_{11}&E_{12}\\E_{21}&E_{22}}</script><p>其中，</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\boldsymbol E_{11}=\boldsymbol A^{-1}+\boldsymbol A^{-1}\boldsymbol B(\boldsymbol D-\boldsymbol {CA}^{-1}\boldsymbol B)^{-1}\\
&\boldsymbol E_{12}=-\boldsymbol A^{-1}\boldsymbol B(\boldsymbol D-\boldsymbol {CA}^{-1}\boldsymbol B)^{-1}\\
&\boldsymbol E_{21} = -(\boldsymbol D-\boldsymbol {CA}^{-1}\boldsymbol B)^{-1}\boldsymbol C\boldsymbol A^{-1}\\
&\boldsymbol E_{22}= (\boldsymbol D-\boldsymbol {CA}^{-1}\boldsymbol B)^{-1}
\end{aligned}</script><h4 id="中心化步骤"><a href="#中心化步骤" class="headerlink" title="中心化步骤"></a>中心化步骤</h4><p>所谓中心化，就是把矩阵的中心拉到0 ，我们用$x_{ij}^<em>,y_i^</em>,\boldsymbol X^*$ 来表示中心化后的数据：</p>
<script type="math/tex; mode=display">
x^*_{ij} = x_{ij}-\overline x_j,~~~\overline x_j = n^{-1}\sum_{i=1}^nx_{ij}\\
y_i^* = y_i-\overline y,~~~~\overline y=n^{-1}\sum_{i=1}^ny_i\\</script><p>令</p>
<script type="math/tex; mode=display">
\cases{\boldsymbol y^* = (y_1^*,\cdots,y_n^*)'\\~\\
\boldsymbol X_c = (\boldsymbol x_1^*,\cdots,\boldsymbol x_p^*)\\~\\
\boldsymbol X^* = (\boldsymbol 1_n,\boldsymbol X_c)}\\</script><p>其中，$\boldsymbol x<em>j^*=(x</em>{1j}^<em>,\cdots,x_{nj}^</em>)’$</p>
<h4 id="中心化前后的关系"><a href="#中心化前后的关系" class="headerlink" title="中心化前后的关系"></a>中心化前后的关系</h4><p>标准化前，原数据集为：</p>
<script type="math/tex; mode=display">
\cases{\boldsymbol y=(y_1,\cdots,y_n)'\\~\\
\boldsymbol X=(\boldsymbol 1_n,\boldsymbol X_o),\boldsymbol X_o=(\boldsymbol x_1,\cdots,\boldsymbol x_p) 
}</script><p>因为根据模型，$\boldsymbol X$​的第一列都是1，所以在上面做一个拼接</p>
<hr>
<p>最小二乘估计为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\boldsymbol {\hat\beta}=(\boldsymbol {X'X})^{-1}\boldsymbol {X'y}\\~\\
=\pmatrix{n&\boldsymbol 1_n'\boldsymbol X_o\\
\boldsymbol X_o'\boldsymbol 1_n&\boldsymbol X_o'\boldsymbol X_o}^{-1}\pmatrix{\boldsymbol 1_n'\\ \boldsymbol X_o'}\boldsymbol y\\~\\
=\pmatrix{n^{-1}+n^{-2}\boldsymbol 1_n'\boldsymbol {X_oA_oX'_o1}_n& -n^{-1}\boldsymbol 1'_n\boldsymbol {X_oA_o}\\
-n^{-1}\boldsymbol {A_oX'_o}\boldsymbol1_n&\boldsymbol A_o }\pmatrix{\boldsymbol 1_n'\\\boldsymbol X_o'}\boldsymbol y\\~\\
=\pmatrix{n^{-1}\boldsymbol 1_n'+n^{-2}\boldsymbol 1_n'\boldsymbol {X_oA_oX_o'1_n1_n'}-n^{-1}\boldsymbol 1_n'\boldsymbol {X_oA_oX_o'}\\-n^{-1}\boldsymbol {A_oX_o'1_n1_n'}+\boldsymbol A_o\boldsymbol X_o'}\boldsymbol y
\end{aligned}</script><p>其中 $\boldsymbol A_o=(\boldsymbol X_o’\boldsymbol X_o-n^{-1}\boldsymbol X_o’\boldsymbol 1_n\boldsymbol 1_n’\boldsymbol X_o)^{-1}$</p>
<hr>
<p>对于中心化的数据，我们有相似的：</p>
<script type="math/tex; mode=display">
=\pmatrix{n^{-1}\boldsymbol 1_n'+n^{-2}\boldsymbol 1_n'\boldsymbol {X_cA_cX_c'1_n1_n'}-n^{-1}\boldsymbol 1_n'\boldsymbol {X_cA_cX_c'}\\-n^{-1}\boldsymbol {A_cX_c'1_n1_n'}+\boldsymbol A_c\boldsymbol X_c'}\boldsymbol y^*</script><p>其中 $\boldsymbol A_c=(\boldsymbol X_c’\boldsymbol X_c-n^{-1}\boldsymbol X_c’\boldsymbol 1_n\boldsymbol 1_n’\boldsymbol X_c)^{-1}$​​</p>
<hr>
<p>中心化的因变量与为中心化的因变量之间的关系：</p>
<script type="math/tex; mode=display">
\boldsymbol y^* = \boldsymbol y-\boldsymbol 1_n(\boldsymbol1_n'\boldsymbol 1_n)^{-1}\boldsymbol 1_n' \boldsymbol y = (\boldsymbol I_n-\boldsymbol H_{1n})\boldsymbol y</script><p>其中 $\boldsymbol H_{1n} = \boldsymbol 1_n(\boldsymbol 1_n’\boldsymbol1_n)^{-1}\boldsymbol1_n’$​是<strong>对称幂等</strong>矩阵, 即一个$\boldsymbol 1_n$​张成的帽子矩阵</p>
<p>中心化的自变量与未中心化的自变量之间的关系：</p>
<script type="math/tex; mode=display">
\boldsymbol X_c = \boldsymbol X_o-\boldsymbol1_n(\boldsymbol1'_n\boldsymbol1_n)^{-1}\boldsymbol 1_n'\boldsymbol X_o\\
=(\boldsymbol I_n-\boldsymbol1_n(\boldsymbol1'_n\boldsymbol1_n)^{-1}\boldsymbol1_n')\boldsymbol X_o\\
=(\boldsymbol I_n-\boldsymbol H_{1n})\boldsymbol X_o</script><p>而且  $\boldsymbol1<em>n$​向量和它张成的空间的补空间$(\boldsymbol I_n-\boldsymbol H</em>{1n})$​​ 是垂直的，可以有如下证明</p>
<script type="math/tex; mode=display">
\boldsymbol1_n'(\boldsymbol I_n-\boldsymbol H_{1n}) = \boldsymbol1_n'-\boldsymbol1_n'\boldsymbol H_{1n}\\
=\boldsymbol1_n'-\boldsymbol1_n'\boldsymbol1_n(\boldsymbol1_n'\boldsymbol1_n)^{-1}\boldsymbol1_n'=0</script><hr>
<p>然后我们要找$\boldsymbol A_c$和$\boldsymbol A_o$的关系</p>
<script type="math/tex; mode=display">
\boldsymbol A_c=(\boldsymbol X_c'\boldsymbol X_c-n^{-1}\boldsymbol X_c'\boldsymbol 1_n\boldsymbol 1_n'\boldsymbol X_c)^{-1}\\
= \boldsymbol X_o'(\boldsymbol I_n-\boldsymbol H_{1n})^2 \boldsymbol X_o-n^{-1} \boldsymbol X_c'(\boldsymbol I_n-\boldsymbol H_{1n})\boldsymbol 1_n\boldsymbol 1_n'(\boldsymbol I_n-\boldsymbol H_{1n})\boldsymbol X_o)^{-1}\\</script><p>因为前面的$(\boldsymbol I<em>n-\boldsymbol H</em>{1n})$是幂等且对称的矩阵 </p>
<p>后面 $(\boldsymbol X<em>o’(\boldsymbol I_n-\boldsymbol H</em>{1n})\boldsymbol X<em>o)^{-1} = \boldsymbol A_0$中有$1_n’(\boldsymbol I_n-\boldsymbol H</em>{1n})$​ 因此直接等于0 </p>
<p>因此：</p>
<script type="math/tex; mode=display">
\boldsymbol A_c =\boldsymbol X_o'(\boldsymbol I_n-\boldsymbol H_{1n})\boldsymbol X_o = \boldsymbol A_o</script><hr>
<p>已知经验回归方程为：</p>
<script type="math/tex; mode=display">
\hat y = \hat \beta_{\text{intercept}}+\boldsymbol x'\boldsymbol {\hat\beta}_{text{slope}}</script><p>我们把中心化的数据用到最小二乘估计模型中，即估算<strong>回归常数</strong>和<strong>回归系数</strong>：</p>
<script type="math/tex; mode=display">
\boldsymbol {\hat\beta_c}=(\boldsymbol {\hat\beta}_{\text{c,intercept}},\boldsymbol {\hat\beta}_{\text{c,slope}})'</script><h5 id="估计-hat-beta-text-c-intercept-​​"><a href="#估计-hat-beta-text-c-intercept-​​" class="headerlink" title="估计 $\hat\beta_{\text{c,intercept}}$​​"></a>估计 $\hat\beta_{\text{c,intercept}}$​​</h5><p>对于回归常数，我们代入数据</p>
<script type="math/tex; mode=display">
\boldsymbol {\hat\beta}_{\text{c,intercept}} = (n^{-1}\boldsymbol 1_n'+n^{-2}\boldsymbol 1_n'\boldsymbol {X_cA_cX_c'1_n1_n'}-n^{-1}\boldsymbol 1_n'\boldsymbol {X_cA_cX_c'})\boldsymbol y^*\\</script><p>我们发现，括号中的后两项都有$\boldsymbol 1<em>n’\boldsymbol X_c’$ ，也就是 $\boldsymbol 1_n’(\boldsymbol I_n-\boldsymbol H</em>{1n})\boldsymbol X_o$ .我们刚刚证明了这是垂直的，乘积为0</p>
<p>因此：</p>
<script type="math/tex; mode=display">
\boldsymbol {\hat\beta}_{\text{c,intercept}} =n^{-1}\boldsymbol 1_n'\boldsymbol y^*\\
=n^{-1}\boldsymbol 1_n' (\boldsymbol I_n-\boldsymbol H_{1n})\boldsymbol y = 0</script><h5 id="估计-hat-beta-text-c-slope"><a href="#估计-hat-beta-text-c-slope" class="headerlink" title="估计$\hat\beta_{\text{c,slope}}$"></a>估计$\hat\beta_{\text{c,slope}}$</h5><script type="math/tex; mode=display">
\begin{aligned}
&\boldsymbol {\hat\beta_{\text{c,slope}}} = (-n^{-1}\boldsymbol A_c\boldsymbol X_c'\boldsymbol 1_n\boldsymbol 1_n'+\boldsymbol A_c\boldsymbol X_c')\boldsymbol y^*\\
&=\boldsymbol A_c\boldsymbol X_c'(\boldsymbol I_n-n^{-1}\boldsymbol 1_n\boldsymbol1_n')\boldsymbol y\\

&=\boldsymbol A_o\boldsymbol X_o'(\boldsymbol I_n-\boldsymbol H_{1n})(\boldsymbol I_n-n^{-1}\boldsymbol 1_n\boldsymbol1_n')\boldsymbol y
\end{aligned}</script><p>因为$\boldsymbol 1<em>n$ 是 竖向量，因此 $\boldsymbol1_n’\boldsymbol1_n$ 是一个常数，为 $\frac{1}{n}$因此，我们有 $\boldsymbol H</em>{1n} = \boldsymbol 1_n(\boldsymbol 1_n’\boldsymbol1_n)^{-1}\boldsymbol1_n’ =\frac{1}{n}\boldsymbol (\boldsymbol 1_n\boldsymbol 1_n’) $​</p>
<p>因此</p>
<script type="math/tex; mode=display">
\begin{aligned}
&=\boldsymbol A_o\boldsymbol X_o'(\boldsymbol I_n-\boldsymbol H_{1n})(\boldsymbol I_n-n^{-1}\boldsymbol 1_n\boldsymbol1_n')\boldsymbol y\\~~
&=\boldsymbol A_o\boldsymbol X_o'(\boldsymbol I_n-\boldsymbol H_{1n})(\boldsymbol I_n-\boldsymbol H_{1n})\boldsymbol y
=\boldsymbol A_o\boldsymbol X_o'(\boldsymbol I_n-\boldsymbol H_{1n})\boldsymbol y
\\
&\boldsymbol {\hat\beta}_{\text{c,slope}} = \boldsymbol {\hat\beta}_{\text{slope}}
\end{aligned}</script><p>采用了中心化的数据得到的经验回归方程为：</p>
<script type="math/tex; mode=display">
\hat y^* =\boldsymbol x'\hat\beta_{\text{slope}}</script><p>也就是说，$\beta_1,\beta_2\cdots,\beta_p$​的估计是不会变的</p>
<h3 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a>标准化</h3><h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4><script type="math/tex; mode=display">
x_{ij}^{**} = \frac{x_{ij}^*}{\sqrt{L_{jj}}} = \frac{x_{ij}-\overline x_j}{\sqrt{L_{jj}}},i=1,2,\cdots,n~~;j=1,2,\cdots,p\\
y_{i}^{**} = \frac{y_i^*}{\sqrt{L_{yy}}},i=1,2\cdots,n</script><p>其中，$L_{jj}$是自变量 $x_j$ 的离差平方和，即：</p>
<script type="math/tex; mode=display">
L_{jj} = \sum_{i=1}^n(x_{ij}-\overline x_j)^2</script><p>而$L_{yy}$是因变量y的离差平方和，即</p>
<script type="math/tex; mode=display">
L_{yy} = \sum_{i=1}^n(y_i-\overline y)^2</script><p>进而我们可以定义向量$\boldsymbol y^{**}$和矩阵$\boldsymbol X_s$</p>
<script type="math/tex; mode=display">
\boldsymbol y^{**} = (\frac{y_1-\overline y}{\sqrt{L_{yy}}},\cdots,\frac{y_n-\overline y}{\sqrt{L_{yy}}})' = \frac{1}{\sqrt{L_{yy}}}\boldsymbol y^*\\~\\
\boldsymbol X_s = (\frac{1}{\sqrt{L_{11}}}\boldsymbol x_1^* ,\cdots,\frac{1}{L_{pp}}\boldsymbol x_p^*)</script><p>我们从$x<em>{ij}^*$的定义可以知道，其实就是中心化的$x</em>{ij}$ ，因此$\boldsymbol X_s$可以被写为：</p>
<script type="math/tex; mode=display">
\boldsymbol X_s = \boldsymbol X_c\boldsymbol L</script><p>其中，$\boldsymbol L=diag{\frac{1}{\sqrt{L<em>{11}}},\cdots,\frac{1}{\sqrt{L</em>{pp}}}}$​</p>
<h4 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h4><p>标准化后的最小二重估计为：</p>
<script type="math/tex; mode=display">
\boldsymbol {\hat\beta}_s = (\boldsymbol{\hat\beta_{\text{s,intercept}}},\boldsymbol{\hat\beta_{\text{s,slope}}})'=(0,\boldsymbol {\hat\beta}_{\text{s,slope}})'</script><p>回归系数为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\boldsymbol{\hat\beta}_{s,slope} = (\boldsymbol {X}_s'\boldsymbol X_s)^{-1} \boldsymbol X_s' \boldsymbol y^{**}\\
&=(\boldsymbol{LX}_c'\boldsymbol X_c\boldsymbol L)^{-1}\boldsymbol L \boldsymbol X_c'\frac{1}{\sqrt{L_{yy}}}\boldsymbol y^*\\
&=\boldsymbol L^{-1}(\boldsymbol X_c'\boldsymbol X_c)^{-1}\boldsymbol L^{-1}\boldsymbol L\boldsymbol X_c'\frac{1}{\sqrt{L_{yy}}} y^*\\
&=\frac{1}{\sqrt{L_{yy}}}\boldsymbol L^{-1}(\boldsymbol X_c'\boldsymbol X_c)^{-1}\boldsymbol X_c' \boldsymbol y^*
\end{aligned}</script><p>那么$\boldsymbol{\hat\beta}<em>{\text{s,slope}} $ 和 $\boldsymbol{\hat\beta}</em>{\text{c,slope}} $ 的关系又如何呢？我们先把$\boldsymbol{\hat\beta}_{\text{c,slope}} $化简成 $\boldsymbol X_c$的表达式 </p>
<blockquote>
<p>注意： $\boldsymbol {\hat\beta_{\text{c,slope}}} $ 还不是最终形态，我们可以这样来化简：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\boldsymbol {\hat\beta_{\text{c,slope}}}  
=\boldsymbol A_c\boldsymbol X_c'(\boldsymbol I_n-\boldsymbol H_{1n})\boldsymbol y^*\\
&=(\boldsymbol X_c\boldsymbol X_c'-\frac{1}{n}\boldsymbol X_c'\boldsymbol 1_n\boldsymbol 1_n'\boldsymbol X_c)^{-1}\boldsymbol X_c'(\boldsymbol I_n-\boldsymbol H_{1n})(\boldsymbol I_n-\boldsymbol H_{1n})\boldsymbol y\\
\end{aligned}</script><p>因为上面证过了，$\boldsymbol A_c$展开后后面一项等于0，因此：</p>
<script type="math/tex; mode=display">
\boldsymbol {\hat\beta_{\text{c,slope}}}  =(\boldsymbol X_c\boldsymbol X_c')^{-1}\boldsymbol X_c'\boldsymbol y^*</script></blockquote>
<p>因此： </p>
<script type="math/tex; mode=display">
\boldsymbol{\hat\beta}_{\text{s,slope}} =\frac{1}{\sqrt{L_{yy}}}\boldsymbol L^{-1}\boldsymbol {\hat\beta}_{\text{c,slope}}</script><p>其中每一个分量为：</p>
<script type="math/tex; mode=display">
\hat\beta_{sj} = \frac{\sqrt{L_{jj}}}{\sqrt{L_{yy}}}\hat\beta_{cj} = \frac{\sqrt{L_{jj}}}{\sqrt{L_{yy}}}\hat\beta_j,j=1,2\cdots,p</script><h2 id="显著性检验"><a href="#显著性检验" class="headerlink" title="显著性检验"></a>显著性检验</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>我们做了这么多参数估计，现在我们进一步判断因变量y和自变量$x_1,x_2\cdots,x_p$ 之间是否存在显著的线性关系。</p>
<p>因此，我们可以采用F检验和t检验这两种统计检验方法：</p>
<ul>
<li>F检验： 用于检验<strong>回归方程</strong>的显著性</li>
<li>t检验：用于检验<strong>回归系数</strong>的显著性</li>
</ul>
<p>我们之前学过，在一元线性回归模型中，F检验(研究方差分析)和t检验(枢轴量法)、相关系数这三类检验是等价的</p>
<p>在多元线性回归模型中，F检验和t检验就不等价了。而且在多元线性回归中，两个变量之间的相关性是很难定义的，因为会受到其他变量的影响</p>
<h3 id="F检验"><a href="#F检验" class="headerlink" title="F检验"></a>F检验</h3><p>F检验是检验回归方程的显著性。对多元线性回归方程的显著性检验是要看自变量 $x_1,x_2\cdots,x_p$ 从整体上对因变量y是否有明显的影响</p>
<p>原假设为</p>
<script type="math/tex; mode=display">
H_0:\beta_1=\beta_2=\cdots=\beta_p = 0</script><p>备择假设为：</p>
<script type="math/tex; mode=display">
H_1:\text{存在}\beta_j\text{不为零},j=1,2\cdots,p</script><p>如果$H_0$ 为真，则表明因变量y与$x_1,x_2\cdots,x_p$ 之间的关系用线性回归模型来刻画是不合适的</p>
<h4 id="检验过程"><a href="#检验过程" class="headerlink" title="检验过程"></a>检验过程</h4><p>离差平方和：</p>
<script type="math/tex; mode=display">
\sum_{i=1}^n(y_i-\overline y)^2 =\sum_{i=1}^n(\hat y_i-\overline y)^2+\sum_{i=1}^n(y_i-\hat y_i)^2</script><p>我们记为 </p>
<script type="math/tex; mode=display">
SS_T = SS_R+SS_E</script><p>拟合值： $\hat y_i = \boldsymbol x_i’\boldsymbol {\hat\beta}$​</p>
<p>偏差： $e_i = y_i-\hat y_i$</p>
<h3 id="t检验"><a href="#t检验" class="headerlink" title="t检验"></a>t检验</h3><h2 id="置信区间与预测"><a href="#置信区间与预测" class="headerlink" title="置信区间与预测"></a>置信区间与预测</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2021/10/31/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch7-%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/10/31/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch7-%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0/" class="post-title-link" itemprop="url">数据科学算法ch7-随机游走</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-10-31 16:54:30" itemprop="dateCreated datePublished" datetime="2021-10-31T16:54:30+08:00">2021-10-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-01-06 20:03:46" itemprop="dateModified" datetime="2022-01-06T20:03:46+08:00">2022-01-06</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="随机游走"><a href="#随机游走" class="headerlink" title="随机游走"></a>随机游走</h1><h2 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h2><p>首先我们来复习一下联合概率分布</p>
<p>给定n个离散随机变量 $X_1,X_2,\cdots,X_n$ 分别取值为$x_1,x_2\cdots,x_n$的联合概率 $f(x_1,x_2\cdots,x_n)$ 表示为：</p>
<script type="math/tex; mode=display">
f(x_1,x_2\cdots,x_n) = P(X_1 = x_1,X_2= x_2,\cdots,X_n=x_n)</script><p>如果n个离散随机变量$X_1,X_2\cdots,X_n$ 是相互独立的，那么它们取值$x_1,x_2\cdots,x_n$ 的联合概率可以很方便得计算为</p>
<script type="math/tex; mode=display">
f(x_1\cdots,x_n) =\prod_{i=1}^nP(X_i=x_i)=\prod_{i=1}^n f_{X_i}(x_i)</script><p>但是，如果$X$之间并不相互独立，那么</p>
<script type="math/tex; mode=display">
f(x_1,\cdots,x_n) = \prod_{i=2}^m P(X_i = x_i|X_{i-1}=x_{i-1},\cdots,X_1 = x_1)\\</script><p>这就是链式法则</p>
<p>但是，对于大规模的数据，会很难处理，因为联合概率的计算复杂度会非常高。此时，需要对数据间的相关关系进行简化。对文本数据或者音频数据建模时，常常假设序列数据点之间存在<strong>一阶相关</strong>关系，即</p>
<script type="math/tex; mode=display">
P(X_i = x_i|X_{i-1}=x_{i-1},\cdots,X_1 = x_1) =P(X_i = x_i |X_{i-1}=x_{i-1})</script><p>因为有些词常常是一前一后出现的。比如在”今天天气炎热，我们去游泳“这一句话中，”天气”与“炎热”这两个词常常一前一后出现，但是“天气”和”游泳”不太可能会出现前后出现。</p>
<p>同样的，对股票价格数据建模时，会假设序列数据点之间存在<strong>t阶相关</strong>关系，即</p>
<script type="math/tex; mode=display">
P(X_i = x_i|X_{i-1}=x_{i-1},\cdots,X_1 = x_1) = P(X_i = x_i|X_{i-1} = x_{i-1}\cdots,X_{i-t}=x_{i-t})</script><h2 id="随机过程"><a href="#随机过程" class="headerlink" title="随机过程"></a>随机过程</h2><p>现在我们来学习随机过程，随机过程${X(t,\omega),t\in T ,\omega\in \Omega}$ 是与时间t有关的一个随机变量序列，现实生活中有很多问题都可以用随机过程来建模，比如说文本数据、语音数据、股票价格和用户上网行为等等。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>例子</th>
<th>时间集合</th>
<th>状态空间</th>
</tr>
</thead>
<tbody>
<tr>
<td>浏览网页</td>
<td>步数</td>
<td>所有网页</td>
</tr>
<tr>
<td>自然语言</td>
<td>单词位置</td>
<td>字典</td>
</tr>
<tr>
<td>股票价格</td>
<td>交易日</td>
<td>价格</td>
</tr>
<tr>
<td>赌博</td>
<td>下注次数</td>
<td>筹码数量</td>
</tr>
<tr>
<td>液体分子</td>
<td>时间</td>
<td>容器空间</td>
</tr>
</tbody>
</table>
</div>
<p>现在我们来具体讲几个随机过程的例子：</p>
<h4 id="抛币游戏"><a href="#抛币游戏" class="headerlink" title="抛币游戏"></a>抛币游戏</h4><p>游戏规则：</p>
<ul>
<li>假设开始有 $10, 参加一场公平无限期抛硬币游戏<ul>
<li>正面朝上，赢得1</li>
<li>反面朝上，输掉1</li>
</ul>
</li>
<li>我们用$X_n$ 表示n次抛币后手上剩余的美元</li>
<li>如果$X_0 = 10$,则 ${X_n:n\in N}$ 构成一个随机序列<ul>
<li>假设$X<em>n = 12$，则$X</em>{n+1}=11$或者13</li>
<li>在第(n+1)次抛币后手上剩余的美元和$X<em>{n-1}$​​​​​值没有关系。是一阶相关关系，即：$P(X</em>{i+1} = x<em>{i+1}|X</em>{i}=x<em>{i},\cdots,X_1 = x_1) = P(X</em>{i+1} = x<em>{i+1}|X</em>{n} = x_{n})$​​​​​</li>
</ul>
</li>
</ul>
<p>在这个抛币游戏中，手上剩余的钱是一个随机过程</p>
<ul>
<li>时间：抛币次数</li>
<li>状态：非负集合</li>
</ul>
<p>在这个游戏中，稳定状态就是输光所有的钱</p>
<h4 id="象棋"><a href="#象棋" class="headerlink" title="象棋"></a>象棋</h4><p>象棋中帅的位置变化</p>
<ul>
<li>按照规则，帅在每个位置都可以向周围移动</li>
<li>$X_n$ 表示n步后帅所在的位置<ul>
<li>如果 $X<em>n = 2$, 则$X</em>{n+1}=1,3,5$</li>
<li>$(n+1)$步帅所在的位置和$X<em>{n-1}$ 的值没有关系，即：$P(X</em>{i+1} = x<em>{i+1}|X</em>{i}=x<em>{i},\cdots,X_1 = x_1) = P(X</em>{i+1} = x<em>{i+1}|X</em>{n} = x_{n})$​</li>
</ul>
</li>
</ul>
<p>因此${X_n:n\in N}$ 构成一个随机过程</p>
<ul>
<li>时间：步数</li>
<li>状态：1到9之间的整数</li>
</ul>
<p><img src="/2021/10/31/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch7-%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0/1.png" alt="1" style="zoom:67%;"></p>
<h4 id="网页浏览"><a href="#网页浏览" class="headerlink" title="网页浏览"></a>网页浏览</h4><p>每个网页都有可能对外的锚连接，</p>
<ul>
<li>$X_n$ 表示n步之后停留的页面<ul>
<li>如果$X=ECNU$， 则$X_{n+1}$ 可以是 MOE、DASE、或者SEI</li>
<li>(n+1)步停留的页面和 $X<em>{n-1}$的所停留的页面无关，即$P(X</em>{i+1} = x<em>{i+1}|X</em>{i}=x<em>{i},\cdots,X_1 = x_1) = P(X</em>{i+1} = x<em>{i+1}|X</em>{n} = x_{n})$</li>
</ul>
</li>
</ul>
<p>因此${X_n:n\in N}$ 构成一个随机过程</p>
<ul>
<li>时间：步数</li>
<li>状态：所有网页组成的集合</li>
</ul>
<h3 id="马尔科夫过程"><a href="#马尔科夫过程" class="headerlink" title="马尔科夫过程"></a>马尔科夫过程</h3><h4 id="马尔科夫性质"><a href="#马尔科夫性质" class="headerlink" title="马尔科夫性质"></a>马尔科夫性质</h4><p>如果随机过程 ${X(t)\in S,t\in I }$​ , 满足</p>
<script type="math/tex; mode=display">
P(X(t_{n+1}) = x_{n+1}|X({t_0})=x_{0},\cdots,X(t_n) = x_n) = P(X(t_{n+1}) = x_{n+1}|X(t_{n}) = x_{n})</script><p>其中，$t<em>0&lt;t_1&lt;\cdots&lt;t_n&lt;t</em>{n+1}\in I,x_i\in S$​,则称该随机过程满足马尔科夫性质</p>
<ul>
<li>如果 $t<em>n$ 表示当前时间，$t</em>{n+1}$ 表示将来，$t<em>1,\cdots,t</em>{n-1}$ 则表示过去</li>
<li>可以理解为：已知现在，将来与过去独立</li>
</ul>
<p>之前我们说的，咖啡溶质的扩散、网络、赌博、象棋，都是满足马尔科夫性质</p>
<h4 id="马尔科夫过程-1"><a href="#马尔科夫过程-1" class="headerlink" title="马尔科夫过程"></a>马尔科夫过程</h4><p>若随机过程满足马尔科夫性质，则称其为<strong>马尔科夫过程</strong></p>
<p>设 ${X(t),t\in T}$ 的空间状态为S，如果$\forall n \geq 0,\forall t<em>0&lt;t_1&lt;\cdots&lt;t_n&lt;t</em>{n+1}\in T$,在条件  $X(t<em>i) = x_i,x_i\in S,i=0,1\cdots,n$ 下，$X(t</em>{n+1})$ 的条件概率刚好等于在条件$X(t_n)=x_n$ 下的条件概率，即：</p>
<script type="math/tex; mode=display">
P(X(t_{n+1}) = x_{n+1}|X({t_0})=x_{0},\cdots,X(t_n) = x_n) = P(X(t_{n+1}) = x_{n+1}|X(t_{n}) = x_{n})</script><p>则称${X(t),t\in T}$ 为马尔科夫过程</p>
<h4 id="马尔科夫链"><a href="#马尔科夫链" class="headerlink" title="马尔科夫链"></a>马尔科夫链</h4><p>马尔科夫链是特殊的马尔科夫过程，它满足：</p>
<ul>
<li>离散的时间集合：步数、下注次数</li>
<li>离散的状态空间：网页、计数 </li>
</ul>
<p>因此，布朗运动是马尔科夫过程，但不是马尔科夫链。</p>
<p>在本章举的例子，马尔科夫过程等于马尔科夫链</p>
<h4 id="转移概率"><a href="#转移概率" class="headerlink" title="转移概率"></a>转移概率</h4><p>假定随机游走的状态空间为 $\Omega =[n]$ 矩阵 $\bold{P}^{(t+1)}\in\mathbb R^{n\times n}$ 被称为该随机游走的<strong>第(t+1)步概率转移矩阵</strong>，如果$p<em>{x,y}^{t+1} = P(X</em>{t+1}=y|X_t=x)$​</p>
<ul>
<li>其中 $p_{x,y}^{(t+1)}$ 表示在$(t+1)$ 步随机游走从状态x转移到y的概率</li>
<li>概率转移矩阵的每行概率和为1，即：</li>
</ul>
<script type="math/tex; mode=display">
\sum_y p_{x,y}^{t+1} = \sum_{y} \frac{P(X_{t+1}=y|X_t= x)}{P(X_t = x)}=1</script><p><img src="/2021/10/31/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch7-%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0/3.png" alt="1" style="zoom:67%;"></p>
<h4 id="齐次马氏链"><a href="#齐次马氏链" class="headerlink" title="齐次马氏链"></a>齐次马氏链</h4><p>如果随机游走满足：</p>
<script type="math/tex; mode=display">
P(X_{t+s}=y|X_t = x)= P(X_s = y|X_0 = x)</script><ul>
<li>即经过t步后，从状态x走到状态y的概率与起始步数s无关，则该随机游走是齐次的</li>
</ul>
<p>也就是说，无论当前是第几步，只要起点相同，一步转移到目标顶点的概率是相同的马氏链被称为齐次马氏链。</p>
<p>例如：在抛币游戏中，无论是开始手上有10元，还是第100次抛币之后手上有10元，下一步手上有11元的概率是相通的，也就是说，抛币游戏中的马氏链是齐次的。也就是说是无记忆性的</p>
<p>例如：</p>
<p><img src="/2021/10/31/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch7-%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0/4.png" style="zoom:100%;"></p>
<p>给定图$G=(V,E)$ 如上图，假设$S=\mathbb Z$ ，无论当前是什么时刻，只要当前在状态a，那么下一步的转移概率为：</p>
<script type="math/tex; mode=display">
p_a(a-1)=\frac{1}{2}\\p_a(a+1)=\frac{1}{2}\\p_{ab} = 0(b\neq a\pm 1)</script><p>因此，这个马尔科夫链也是齐次的。如果$\pi(a)$ 表示该马尔科夫链初始时刻在状态a的概率，那么$\pi(10)=1,\pi(a)=0(a\neq 10)$ 意味着在时刻0马尔科夫链一定是从坐标10出发的</p>
<h3 id="随机游走-1"><a href="#随机游走-1" class="headerlink" title="随机游走"></a>随机游走</h3><p>在学习随机游走之前我们需要了解以下定义：图、度、无向图、路劲、连通分量、邻接矩阵、邻接表</p>
<p>现给出随机游走的定义：随机游走模型是针对图建立的抽象马氏链模型，因此图上的马氏链都可以被称为<strong>随机游走</strong>。图上的随机游走是指给定一个图和一个出发点，随机地选择一个邻居节点，移动到邻居节点上，然后把当前节点作为出发点，重复以上过程。那些被随机选出的节点序列就构成了一个该图上的随机游走。</p>
<p>我们可以将随机游走和之前说的齐次马尔科夫链结合起来理解：现在有状态转移图如下：</p>
<p><img src="/2021/10/31/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch7-%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0/5.png" style="zoom:100%;"></p>
<p>假设$\pi(a) = 1,\pi(i) = 0(i\neq a)$ 意味着在时刻0该随机游走从状态a出发。在第一步，该随机游走可能到达顶点c、e、f、g. 随机游走会随机选择一个顶点，其中：</p>
<script type="math/tex; mode=display">
p_{ac}=p_{ae}=p_{af}=p_{ag}= \frac{1}{4}\\</script><p> 走到第二个点，又有好几种不同的选择。以此类推，可以得到在图G上的一个随机序列。因此，随机游走是一类<strong>特殊的马氏链</strong></p>
<p>无论何时出发，只要当前停留在状态a,那么下一步到达c、e、f、g的概率是相同的。因此，该随机游走对应的马氏链也是齐次的</p>
<h3 id="平稳分布"><a href="#平稳分布" class="headerlink" title="平稳分布"></a>平稳分布</h3><p>之前我们说了概率转移矩阵，基于概率转移矩阵我们提出了平稳分布的概念。</p>
<h4 id="状态分布"><a href="#状态分布" class="headerlink" title="状态分布"></a>状态分布</h4><p>我们令$\pi^{(t)}$为状态空间为S的马氏链<strong>在时刻t</strong>的状态分布，即：</p>
<script type="math/tex; mode=display">
\pi_x^{(t)} = P(X_t = x)</script><p>特别地，将$\pi^{(0)}$ 称为初始分布。</p>
<p>进一步，对于一个概率转移矩阵为P的马氏链，其状态分布$\pi^{(t)}$ 具有如下性质</p>
<ul>
<li>$\pi^{(t)}$​ 的每个分量满足$0\leq\pi_i^{(t)}\leq 1$</li>
<li>$\sum_{j\in S}\pi_j^{(t)} = 1$ 即每行的和为1</li>
<li>$\pi^{(t+1)}= \pi^{(t)}P(t)$</li>
</ul>
<p>比如对于矩阵：</p>
<script type="math/tex; mode=display">
P=\begin{pmatrix}\frac{1}{2}&\frac{1}{2}&0&0\\\frac{1}{3}&\frac{2}{3}&0&0\\0&0&\frac{3}{4}&\frac{1}{4}\\0&0&\frac{1}{4}&\frac{3}{4} \end{pmatrix}</script><p>我们给定第t步的状态分布为$\pi^{(t)}=(0.4,0.6,0,0)$,那么:</p>
<script type="math/tex; mode=display">
\pi^{(t+1)} = \pi^{(t)} P = (0.4,0.6,0,0)\\</script><p>我们给定第t步的状态分布为$\pi^{(t)}=(0,0,0.5,0.5)$​,那么:</p>
<script type="math/tex; mode=display">
\pi^{(t+1)} = \pi^{(t)}P=(0,0,0.5,0.5)\\</script><p>我们给定第t步的状态分布为$\pi^{(t)}=(0.1,0.9,0,0)$​​,那么:</p>
<script type="math/tex; mode=display">
\pi^{(t+1)} = \pi^{(t)}P=(0.35,0.65,0,0)\\</script><h4 id="平稳分布-1"><a href="#平稳分布-1" class="headerlink" title="平稳分布"></a>平稳分布</h4><p>对于一个转移概率矩阵为$\boldsymbol P$ 的有限状态马氏链，若状态分布$\pi$满足：</p>
<script type="math/tex; mode=display">
\pi\boldsymbol P = \pi</script><p>则称$\pi$​ 为该马氏链的平稳分布· </p>
<p>有矩阵为 $P =\begin{pmatrix}\frac{1}{2}&amp;\frac{1}{2} \\frac{1}{4}&amp;\frac{3}{4} \end{pmatrix} $​ 试求其平稳分布：</p>
<p>我们可以令$\pi = (x,y)$​, 那么 $\cases{\frac{1}{2}x+\frac{1}{4}y = x\\frac{1}{2}x+\frac{3}{4}y = y}$ 解得$y=2x$，又要满足平稳分布的行和为1，所以解为：$(\frac{1}{3},\frac{2}{3})$</p>
<h4 id="可约与不可约"><a href="#可约与不可约" class="headerlink" title="可约与不可约"></a>可约与不可约</h4><p>如果从状态x可以通过有限步转移到达状态y,并且从状态y可以通过有限步转移到达x，那么称状态x与y是<strong>连通</strong>的。如果马氏链中任意两个状态都是连通的，那么称该马氏链是<strong>不可约</strong>的</p>
<ul>
<li>马氏链中任意两个状态都是联通的，那么就意味着存在一个n使得矩阵$\boldsymbol P^{(n)}$ 中任意一个元素都大于0</li>
<li>如果一张图是<strong>强连通</strong>的，那么它一定是不可约的，否则这个图所对应的马氏链是<strong>可约的</strong></li>
</ul>
<p>比如对于这张图，马氏链是可月的，因为对于状态2，他没有出边，因此它无法通过有限步到达其他状态。因此状态2与其他状态并不连通，因此该马尔科夫链是可约的</p>
<p><img src="/2021/10/31/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch7-%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0/6.png" style="zoom:100%;"></p>
<h4 id="反周期"><a href="#反周期" class="headerlink" title="反周期"></a>反周期</h4><p>状态x的周期$d<em>x$ 是集合${n|(P^n)</em>{x,x}&gt;0}$的<strong>最大公约数</strong>，特别地，如果$\forall n\geq 1,(\boldsymbol P^{(n)})_{x,x}=0$,那么$d_x=\infty$ 。而如果<strong>一个马氏链所有的状态的周期都为1</strong>，则称这个马氏链是反周期的</p>
<p><img src="/2021/10/31/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch7-%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0/7.jpeg" style="zoom:100%;"></p>
<p>比如说对于这张图，对于每一个状态其集合都为${2,4,6\cdots}$ 其最大公约数为2，因此这个马氏链是周期的。 </p>
<h2 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h2><h3 id="1"><a href="#1" class="headerlink" title="1"></a>1</h3><p>给定一个转移矩阵 $P$ 和状态向量$\pi$ </p>
<script type="math/tex; mode=display">
\boldsymbol P = \begin{pmatrix}0.5&0.5\\0.25&0.75\end{pmatrix}&\pi =  \begin{pmatrix}1\\0\end{pmatrix}</script><ul>
<li>证明 $\pi P^n$ 的结果接近一个常数向量</li>
</ul>
<p>这个常数向量，就是平稳分布。</p>
<p>我们令 $\pi P^n = (x<em>{(n,1)},x</em>{(n,2)})$，那么，对于$n\geq 1$, 我们有：</p>
<script type="math/tex; mode=display">
x_{(n,1)} = \frac{1}{2}x_{(n-1,1)} +\frac{1}{4}x_{(n-1,2)}\\
x_{(n,2)} = \frac{1}{2}x_{(n-1,1)} +\frac{3}{4}x_{(n-1,2)}\\</script><p>由于状态向量$\pi$ 的向量和一定为1，因此 $x<em>{(n-1,2)}=1-x</em>{(n-1,1)}$ ,带入得到：</p>
<script type="math/tex; mode=display">
\begin{align}
x_{(n,1)} &= \frac{1}{2}x_{(n-1,1)}+\frac{1}{4}(1-x_{(n-1,1)})\\~\\
&=\frac{1}{4}x_{(n-1,1)} +\frac{1}{4}
\end{align}</script><p>这是一个很简单的一阶递推</p>
<script type="math/tex; mode=display">
(x_{(n,1)}-\frac{1}{3})=\frac{1}{4}(x_{(n-1,1)}-\frac{1}{3})</script><p>解得：</p>
<script type="math/tex; mode=display">
x_{(n,1)} = \frac{2}{3}\times (\frac{1}{4})^n+\frac{1}{3},n\geq 1</script><p>当$n\rightarrow \infty$ 时，$x<em>{(n,1)}=\frac{1}{3}$ ，进而可以知道 $x</em>{(n,2)}= \frac{2}{3}$ </p>
<h3 id="2"><a href="#2" class="headerlink" title="2"></a>2</h3><p>对于一个转移概率矩阵为 $\boldsymbol P$ 的马氏链</p>
<script type="math/tex; mode=display">
\boldsymbol P = \begin{pmatrix}0.5&0.5\\p&1-p\end{pmatrix}\\</script><p>已知实验多次之后，转移概率矩阵对应的马氏链停在状态1的概率大约为 $20\%$, 停在状态2的概率为$80\%$ ，p为未知参数</p>
<ol>
<li>计算未知参数$p$ 的合理估计值，并给出解释</li>
</ol>
<p>首先，根据题意是我们知道最终的平稳分布 $(0.2,0.8)$ ，又 $\pi \boldsymbol P = \pi $ ，可知 $0.1+0.8p = 0.2~~p=\frac{1}{8}$</p>
<ol>
<li>这个马氏链是否可约，是否反周期？</li>
</ol>
<p>是否可约呢？我们看到这边只有两个状态，两个状态是相互联通的，因此是不可约的。</p>
<p>如图：</p>
<p><img src="/2021/10/31/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch7-%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0/8.jpeg" style="zoom:100%;"></p>
<p>那么是否反周期呢？我们看到，状态1、2的集合为：${1,2\cdots}$ ，其最大公约数都是1，因此这个马氏链是反周期的。</p>
<h3 id="3"><a href="#3" class="headerlink" title="3"></a>3</h3><p><img src="/2021/10/31/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch7-%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0/9.png" style="zoom:100%;"></p>
<ul>
<li>对于7.5<ul>
<li>状态1和状态2都是相互连通的，因此是不可约的</li>
<li>状态1的集合为 ${1,2,\cdots}$ ，状态2的集合为 ${2,3,4\cdots}$ ，两者最大公约数都是1，周期都是1，因此是反周期的</li>
</ul>
</li>
<li>对于7.6<ul>
<li>一共有4个状态，所有状态都是连通的，因此是不可约的</li>
<li>四个状态，每个状态的集合都是 ${2,4,6,\cdots}$ ，最大公约数都是2，因此是非反周期(周期)的</li>
</ul>
</li>
</ul>
<h2 id="Page-Rank"><a href="#Page-Rank" class="headerlink" title="Page Rank"></a>Page Rank</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/" class="post-title-link" itemprop="url">数据科学算法ch5-数据流模型及频繁项挖掘</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-10-29 21:51:03" itemprop="dateCreated datePublished" datetime="2021-10-29T21:51:03+08:00">2021-10-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-01-10 09:55:43" itemprop="dateModified" datetime="2024-01-10T09:55:43+08:00">2024-01-10</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="数据流模型及频繁项挖掘"><a href="#数据流模型及频繁项挖掘" class="headerlink" title="数据流模型及频繁项挖掘"></a>数据流模型及频繁项挖掘</h1><p>在这一讲我们要学习什么是数据流，什么是数据流算法，怎么去评价一个数据流算法的好坏。</p>
<h2 id="数据流和数据流模型"><a href="#数据流和数据流模型" class="headerlink" title="数据流和数据流模型"></a>数据流和数据流模型</h2><p>首先我们要认识数据流，其具有一下几个特征</p>
<ul>
<li>数据总量不受限制，因此我们很难准确估算数据量大小</li>
<li>数据到达速率块，比如大型强子对撞机可以每秒产生40EB的数据</li>
<li>数据到达次序不受约束，我们无法预测数据的到达次序。</li>
<li>除非可以保存，每个数据只能”看“一次。因为很少有系统能把海量的数据都放在内存中，而且数据到达的速度很快，分析的时间很短，否则就会出现数据拥塞</li>
</ul>
<p>我们可以将<strong>数据流</strong>看做是一个无限的元组序列 $\sigma= <a_1,\cdots,a_m,\cdots>$; 在某个固定点一个数据流可以被视作一个长度为d的固定数组 $A[0,\cdots,d-1]$</a_1,\cdots,a_m,\cdots></p>
<p>因此我们对处理数据流的算法也提出了如下要求：</p>
<ul>
<li>实时性：实时、连续地输出查询结果</li>
<li>低空间复杂度：数据流规模理论上是无限的，为了保证算法高效稳定运行，需降低算法的空间开销</li>
<li>结果准确性：数据规模大，速率快，因此对于一些复杂问题，不太可能通过数据的一次遍历就获得准确答案。在实际应用中，往往不要求精准的查询结果</li>
<li>适应性：在很多应用中，涉及多个数据流的处理，需设计具备适应性的算法</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>传统算法</th>
<th>数据流算法</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据类型</td>
<td>有限&amp;静态</td>
<td>无限&amp;动态&amp;高速</td>
</tr>
<tr>
<td>存储</td>
<td>硬盘</td>
<td>内存&amp;空间限制</td>
</tr>
<tr>
<td>效率</td>
<td>非实时</td>
<td>实时&amp; Ad-hoc</td>
</tr>
<tr>
<td>返回值类型</td>
<td>精确/近似</td>
<td>近似</td>
</tr>
</tbody>
</table>
</div>
<p>因为数据流的大小是不确定的，因此我们希望算法的空间消耗和时间消耗是<strong>次线性</strong>的($O(\log n)$​)，或者与<strong>流的大小无关</strong>。</p>
<p><img src="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/1.png" style="zoom:67%;"></p>
<h3 id="数据流子模型"><a href="#数据流子模型" class="headerlink" title="数据流子模型"></a>数据流子模型</h3><h4 id="按照数据流中数据元素对数组A的不同影响方式划分"><a href="#按照数据流中数据元素对数组A的不同影响方式划分" class="headerlink" title="按照数据流中数据元素对数组A的不同影响方式划分"></a>按照数据流中数据元素对数组A的不同影响方式划分</h4><p>数据流模型可以划分为三个子模型：时间序列模型，收音机模型，十字转盘模型</p>
<ol>
<li>时间序列模型。每个数据项 $a_i = A[i]$ 按照i增加的顺序出现，相当于时间序列</li>
<li>收音机模型。每个数据项$a<em>i=(j,I_i),I\geq 0$ 表示对$A[i]$的增量，即$A_i[j] = A</em>{i-1}[j]+I_{i_0}$ 如同多个收音机一样，多个$a_i$ 表示一段时间上$A[j]$的增量。这是一种更为普遍的数据流模型</li>
<li>十字转盘模型。每个数据项$a<em>i= (j,U_i)$ 表示对$A[j]$ 的更新，即 $A_i[j]=A</em>{i-1}[j]+U_i$ ，其中$U_i$可正可负。该模型适合描述数据流动态的出入状态，单位时间内流入和流出的数量不必相同</li>
</ol>
<h4 id="按照数据流上个元素的重要程度划分"><a href="#按照数据流上个元素的重要程度划分" class="headerlink" title="按照数据流上个元素的重要程度划分"></a>按照数据流上个元素的重要程度划分</h4><p>数据流可以分成另外三种子模型：界标模型、滑动窗口模型、衰减窗口模型</p>
<ol>
<li>界标模型。如下图，界标模型有起始点和终止点两个时间点。其中起始点是固定的，终止点随时间推移而不断递增。在界标的任意终点和起点之间，数据的重要性均等。这是最容易理解的模型</li>
</ol>
<p><img src="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/2.png" style="zoom:67%;"></p>
<ol>
<li>滑动窗口模型。令W为窗口大小，在这个模型中，数据流算法仅仅考虑最近的W个元素，即查询范围是$[\max (0,d-W),\cdots,d-1]$ .处在查询范围之外的元素其重要性为0；处在查询范围之内的元素重要性均等。滑动窗口模型也有起始点和终止点，但是会随着时间的推移而同步递增。</li>
</ol>
<p><img src="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/3.png" style="zoom:67%;"></p>
<ol>
<li>在这个模型中，数据流算法的范围是$[0,\cdots,d-1]$ 。处在查询范围中的各个元素的重要程度是不同的。新到达的元素，其重要程度比较高；到达时间较长的元素，其重要程度较低</li>
</ol>
<p><img src="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/4.png" style="zoom:67%;"></p>
<h3 id="近似算法与随机算法"><a href="#近似算法与随机算法" class="headerlink" title="近似算法与随机算法"></a>近似算法与随机算法</h3><p>由于存储空间和时间的限制，我们在数据流上做到精确计算非常困难。</p>
<p><strong>近似算法</strong>：寻找<strong>误差在一定范围内</strong>的近似解。之前我们学的Morris算法、布隆过滤器以及局部敏感哈希算法，都是近似算法</p>
<ul>
<li>比如说，寻找一个误差在10%以内的解</li>
<li>比如说：近似解落在真实值的$(1\pm \epsilon)$ 范围内，其中$\epsilon=0.1$</li>
</ul>
<p><strong>随机算法</strong>：比近似算法更加宽松，是允许小概率失败的(即不在误差范围内)</p>
<ul>
<li>例如，估计值有 1/100 的概率不在误差范围内，即成功地概率为$1-\delta$ ，其中$\delta = 0.01$</li>
</ul>
<p>我们可以这样来理解： <strong>随机算法就等于</strong> $(\epsilon,\delta)$<strong>-近似算法</strong> ,接下来会详细介绍各种类型的近似算法</p>
<h4 id="epsilon-​-近似算法-相对误差"><a href="#epsilon-​-近似算法-相对误差" class="headerlink" title="$\epsilon$​-近似算法(相对误差)"></a>$\epsilon$​-近似算法(相对误差)</h4><p>其实很好理解，我们假设数据流$\sigma$​和其精确输出$\xi(\sigma)$​，其近似算法的输出记为$\mathcal{A}(\sigma)$​ 。那么如果是相对误差版本的话：</p>
<script type="math/tex; mode=display">
|\mathcal A(\sigma)-\phi(\sigma)|<\epsilon\phi(\sigma)</script><h4 id="epsilon-近似算法-绝对误差"><a href="#epsilon-近似算法-绝对误差" class="headerlink" title="$\epsilon$-近似算法(绝对误差)"></a>$\epsilon$-近似算法(绝对误差)</h4><p>绝对误差更好理解，就是要把误差控制在$\epsilon$里面 ，即</p>
<script type="math/tex; mode=display">
|\mathcal A(\sigma)-\phi(\sigma)|<\epsilon</script><h4 id="epsilon-delta-近似算法-相对误差"><a href="#epsilon-delta-近似算法-相对误差" class="headerlink" title="$(\epsilon,\delta)$-近似算法(相对误差)"></a>$(\epsilon,\delta)$-近似算法(相对误差)</h4><p>$(\epsilon,\delta)$ 近似算法往往比$\epsilon$近似算法更加宽松一点，它只要大部分情况下算法给出的<strong>估计值</strong>满足$\epsilon$近似即可。即：</p>
<script type="math/tex; mode=display">
Pr[|\mathcal A-\xi (\sigma)|<\epsilon\xi(\sigma)]>1-\var</script><p>用语言来解释就是说一个$(\epsilon,\delta)$ 近似算法的数据结果可能会出现 $|\mathcal A-\xi (\sigma)|&gt;\epsilon\xi(\sigma)$的情况，但这种情况发生的概率不会超过$\delta$</p>
<h4 id="epsilon-delta-​-近似算法-绝对误差"><a href="#epsilon-delta-​-近似算法-绝对误差" class="headerlink" title="$(\epsilon,\delta)$​-近似算法(绝对误差)"></a>$(\epsilon,\delta)$​-近似算法(绝对误差)</h4><p>当然，我们也可以给出绝对误差版本的定义</p>
<script type="math/tex; mode=display">
Pr[|\mathcal A-\xi (\sigma)|<\epsilon]>1-\var</script><p>这是一个数据流分析挖掘框架：</p>
<p><img src="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/5.png" style="zoom:67%;"></p>
<h2 id="频繁项挖掘"><a href="#频繁项挖掘" class="headerlink" title="频繁项挖掘"></a>频繁项挖掘</h2><p>我们本章的重点就在于找到数据流的频繁项，接下来要介绍的一些算法都是围绕这频繁项展开的。那么首先我们就来介绍频繁项估计</p>
<h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>在数据流$\sigma = <a_1,a_2\cdots,a_m>,a_i\in[n]$中，定义一个频数向量：</a_1,a_2\cdots,a_m></p>
<script type="math/tex; mode=display">
f=(f_1,f_2,\cdots,f_n)</script><p>其中，n为该数据流中不同元素的个数，$f<em>i$ 为元素$a_i$ 的频数，且满足$\sum</em>{i=1}^nf_i = m$</p>
<p>然后，我们可以找出满足需求的元素，比如频繁项。</p>
<ul>
<li>对于大多数问题，如果$\exists a_j : f_j &gt; m/2$, 则输出$a_j$, 否则输出空集$\empty$</li>
<li>频繁项：给定一个参数k,输出频繁元素集合${a_j:f_j\geq m/k}$; 或者给定一个参数$\psi$，输出频繁元素集合 ${a_j:f_j&gt;\psi m}$</li>
</ul>
<p>比如说：对给定的数据流$\sigma = <a,b,a,c,c,a,b,d>$ ，4个不同元素的频数分别为$f_a = 3,f_b=2,f_c=2,f_d=1$</a,b,a,c,c,a,b,d></p>
<ul>
<li>$\sigma$中不存在大多数问题，因为没有大于4的元素</li>
<li>当k等于4的时候，m/k=2,因此频繁项为a,b,c</li>
<li>当$\psi=0.3$ 时候，$m\cdot \psi = 2.4$,因此频繁项为a</li>
</ul>
<h2 id="确定性近似频数算法-Misra-Gries"><a href="#确定性近似频数算法-Misra-Gries" class="headerlink" title="确定性近似频数算法 Misra Gries"></a>确定性近似频数算法 Misra Gries</h2><p>Misra Gries算法是一个<strong>确定性求解频繁元素的近似算法</strong>。其思想和俄罗斯方块非常类似，伪代码如下：</p>
<p><img src="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/6.png" style="zoom:67%;"></p>
<p>算法的输入流$\sigma = <a_1,a_2\cdots,a_m>,a_i\in [n]$​ <strong>使用 k 个计数器</strong>；输出为频繁元素集合<strong>F</strong>. </a_1,a_2\cdots,a_m></p>
<p>该算法的核心思想如下：</p>
<ol>
<li>对到达的元素$a_m$,如果已经为其创建了计数器，便将相应的计数器加1；</li>
<li>如果没有相应的计数器，且计数器的个数少于k(意味着内存中还有足够的空间创建新的计数器)，那么则为该元素分配一个新的计数器，并置为1；</li>
<li>如果当前计数器的个数为k (意味着内存中已经没有足够的空间来创建新的计数器)，则把所有计数器减1，然后删除值为0的计数器</li>
</ol>
<p>给出一个例题：</p>
<p>给定输入流$<b,a,c,a,d,e,a,f,a,d>$ ，计数器个数$k=3$​ ，试逐步写出 Misra Gries 算法执行的结果</b,a,c,a,d,e,a,f,a,d></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>输入</th>
<th>操作</th>
<th>结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>b</td>
<td>插入</td>
<td>$F={(b,1)}$</td>
</tr>
<tr>
<td>a</td>
<td>插入</td>
<td>$F={(b,1),(a,1)}$</td>
</tr>
<tr>
<td>c</td>
<td>插入、删除</td>
<td>$F={(a,1),(b,1),(c,1)}\Rightarrow F={}$</td>
</tr>
<tr>
<td>a</td>
<td>插入</td>
<td>$F={(a,1)}$</td>
</tr>
<tr>
<td>d</td>
<td>插入</td>
<td>$F={(a,1),(d,1)}$</td>
</tr>
<tr>
<td>e</td>
<td>插入、删除</td>
<td>$F={(a,1),(d,1),(e,1)}\Rightarrow F={}$​</td>
</tr>
<tr>
<td>a</td>
<td>插入</td>
<td>$F={(a,1)}$</td>
</tr>
<tr>
<td>f</td>
<td>插入</td>
<td>$F={(a,1),(f,1)}$</td>
</tr>
<tr>
<td>a</td>
<td>更新</td>
<td>$F={(a,2),(f,1)}$</td>
</tr>
<tr>
<td>d</td>
<td>插入、删除</td>
<td>$F={(a,2),(f,1),(d,1)}\Rightarrow {(a,1)}$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h3><p>在 Misra Gries 算法中，算法最后的输出结果可能是不频繁的。试想，如果最后一个元素是数据流中一个新出现的元素，而且最后一个元素到达后，k位置中还有空位，那么这个新元素会出现在最后的结果中。也就是说，Misra Gries算法可能会出现误报。虽然可能出现误报，但是频繁元素一定在最后的输出结果中。</p>
<p>不能否认，Misra Gries算法是一个构思非常巧妙的算法，只需要k个计数器就能找到一个数据流中所有的频繁项元素。</p>
<h2 id="随机近似频数算法-Counting-Sketch"><a href="#随机近似频数算法-Counting-Sketch" class="headerlink" title="随机近似频数算法 Counting Sketch"></a>随机近似频数算法 Counting Sketch</h2><p>对于给定的数据流，Misra Gries 算法输出的结果中，<strong>每个元素的频数是无从知晓的</strong>，而随机近似频数算法Count Sketch 则可以估计频繁元素的频数。</p>
<h3 id="简单抽样算法"><a href="#简单抽样算法" class="headerlink" title="简单抽样算法"></a>简单抽样算法</h3><p>首先我们来介绍一下简单抽样算法。这是一种非常简单的计算元素频数的算法。该算法的核心思想是：对于到达的元素$a_i$, 以概率 $p=M/m$ 对该元素的频数+1.其中，<strong>M表示抽样后的数据流大小，m表示原数据流大小</strong>。伪代码如下：</p>
<p><img src="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/7.png" style="zoom:67%;"></p>
<p>比如说现在给定一个数据流$<2,2,1,2,2,4,5,4,4,2>$ ,$p=\frac{1}{3}$ ，如下图所示，估计元素2的频数为：$f’(2)=\frac{c_2}{p}=6$</2,2,1,2,2,4,5,4,4,2></p>
<p><img src="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/8.png" style="zoom:67%;"></p>
<h4 id="算法分析-1"><a href="#算法分析-1" class="headerlink" title="算法分析"></a>算法分析</h4><p>简单抽样算法称为 $(\epsilon,\delta)-$近似算法的空间需求是： $M = O(\frac{m\log {1}/{\delta}}{\epsilon^2})$ ，其中，m为原数据流的大小，$\epsilon$是误差因子，$\delta$是控制误差在一定范围的尾概率上界。</p>
<p>从空间需求看出，<strong>简单抽样算法所需空间与数据流大小有关</strong>，当数据流规模较大时，该算法所需的空间必将大幅增加。因此我们需要提出更高效的基于概要数据结构的频数估计算法</p>
<h3 id="Basic-Count-Sketch算法"><a href="#Basic-Count-Sketch算法" class="headerlink" title="Basic Count Sketch算法"></a>Basic Count Sketch算法</h3><p>从简单抽样算法的空间需求来看，找到一个更为紧凑的数据结构来维护数据流中高频元素的近似计数时非常必须的。</p>
<p><img src="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/9.png" style="zoom:67%;"></p>
<p>Basic Count Sketch 算法维护一个<strong>计数数组C </strong> 和两个哈希函数$h(\cdot),g(\cdot)$。 哈希函数$h(\cdot)$将n个元素均匀的映射到k个位置(算法第二行)；哈希函数$g(\cdot)$将n个元素映射为$-1$或者$+1$(算法第三行)。</p>
<p>对于一个新到达的元素，将其位置上的计数加1或者减1，对于查询元素a，估计出的频数为 $\hat f_a = g(a)C[h(a)]$</p>
<p>两个哈希函数的操作如图所示。对于元素i，$h(i)$将其映射到第7个位置上，$g(i)$控制该位置计数减1；对于元素了$j$ ,$h(j)$将其映射到第2个位置上，$g(j)$控制该位置计数加 1。</p>
<p><img src="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/10.png" style="zoom:67%;"></p>
<p>比如说现在有一个数据流 $<a,b,c,a,b,a>$​ ，如果a、b、c各有一个计数器(一共三个)，那么可以清晰地计算出每个元素的频数。 当数据流中元素很多，要精确统计的话就需要很多计数器，这是维护不起的</a,b,c,a,b,a></p>
<p>但是<strong>只有两个计数器</strong>(只有两个位置来存放频数)，因此设计一个哈希函数，使得$h(a)=h(b)\neq h(c)$, 我们用这个算法来计算各个元素的频数：</p>
<p>首先，我们还要知道一共有8种($2^3$)可能的哈希函数$g(\cdot)$来使得${a,b,c}\rightarrow {-1,+1}$​,如下图： </p>
<p><img src="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/11.png"></p>
<p>比如说对于第一种哈希函数， $g(a),g(b),g(c)$都是$+1$，因为$h(a)$和$h(b)$在同一个位置上，所以凡是遇到$a$和$b$，就在这个位置上+1，最后得到的频数为5；$h(c)$是另外一个位置，且只出现了一次，因此最后得到的频数为1</p>
<p>那么我们穷举8种 $g(\cdot)$，最终得到了上表，接下来我们要对$a,b,c$三个数的频数进行分别计算。</p>
<p>根据 算法$\hat f_a = g(a)C[h(a)]$，我们对8个哈希函数的结果求均值，如下：</p>
<script type="math/tex; mode=display">
 f_a = \lvert{\frac{(5+5+1+1)-(-5-5-1-1)}{8}}\rvert = 3\\
f_b = \lvert{\frac{(5+5-1-1)-(-5-5+1+1)}{8}}\rvert=2\\
f_c = \lvert{\frac{(1+1+1+1)-(-1-1-1-1)}{8}}\rvert=1</script><p>可以看到，在经过多次实验(取多次不同哈希函数)之后，频数的估计和实际频数相同，这并不是一个巧合</p>
<p>对于每一个 $j\in [n]$ ，根据h函数哈希的结果，定义一个指示变量$Y_j$ 如下：</p>
<script type="math/tex; mode=display">
Y_j = \cases{1,h(j)=h(a)\\~\\0,\text{otherwise}}</script><p>这里的$Y_j$是一个随机变量，表示元素j与元素a是否哈希到了同一个位置。最终，对元素a的频数估计是计数器$h(a)$位置的值。因此所有哈希到h(a)位置上的元素都对元素a的频数估计起到了作用，因此：</p>
<script type="math/tex; mode=display">
\begin{align}
\hat f_a &= g (a)C[h(a)]\\~\\
&=g(a)\sum_j^n f_jg(j)Y_j\\~\\
&=f_a+g(a)\sum_{j\in[n]/ \{a\}}f_j\cdot g(j)\cdot Y_j
\end{align}</script><h4 id="如果只有1个计数器"><a href="#如果只有1个计数器" class="headerlink" title="如果只有1个计数器"></a>如果只有1个计数器</h4><p>同样是上面的数据流 : abcaba, 如果只有一个计数器会怎么样？</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">g(a)</th>
<th style="text-align:center">g(b)</th>
<th style="text-align:center">g(c)</th>
<th style="text-align:center">C[h(a)]=C[h(b)]=C[h(c)]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">+</td>
<td style="text-align:center">+</td>
<td style="text-align:center">+</td>
<td style="text-align:center">3+2+1= 6</td>
</tr>
<tr>
<td style="text-align:center">+</td>
<td style="text-align:center">+</td>
<td style="text-align:center">-</td>
<td style="text-align:center">3+2-1=4</td>
</tr>
<tr>
<td style="text-align:center">+</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">3-2-1= 0</td>
</tr>
<tr>
<td style="text-align:center">+</td>
<td style="text-align:center">-</td>
<td style="text-align:center">+</td>
<td style="text-align:center">3-2+1=2</td>
</tr>
<tr>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">+</td>
<td style="text-align:center">-3-2+1=-4</td>
</tr>
<tr>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-3-2-1=-6</td>
</tr>
<tr>
<td style="text-align:center">-</td>
<td style="text-align:center">+</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-3+2-1=-2</td>
</tr>
<tr>
<td style="text-align:center">-</td>
<td style="text-align:center">+</td>
<td style="text-align:center">+</td>
<td style="text-align:center">-3+2+1 = 0</td>
</tr>
</tbody>
</table>
</div>
<script type="math/tex; mode=display">
f_a = \lvert{\frac{(6+4+0+2)-(-4-6-2-0)}{8}}\rvert=3\\
f_b = \lvert{\frac{(6+4-2-0)-(0+2-4-6)}{8}}\rvert=2\\
f_c = \lvert{\frac{(6+2-4+0)-(4+0-6-2)}{8}}\rvert=1</script><h3 id="Count-Sketch算法"><a href="#Count-Sketch算法" class="headerlink" title="Count Sketch算法"></a>Count Sketch算法</h3><p>Count Sketch 算法基于$t$次频数估计的中位数和真实值很接近的想法，在Basic Count Sketch 算法的基础上，将哈希函数$h(\cdot)$个数增加到$t$个，将每个元素映射到 $t$ 个位置上，同时也有t个函数决定每个位置上的计数加1或者减1。(这里，$t = \log(1/\delta)$​)</p>
<p><img src="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/12.png"></p>
<p>Count Sketch 算法最终是一个空间消耗为$O(\frac{\log ({1}/{\delta})}{\epsilon^2})$​ 的 $(\epsilon,\delta)$​​近似估计</p>
<p>哈希函数个数是 $t = O(\log(1/\delta))$</p>
<h3 id="Count-min-Sketch算法"><a href="#Count-min-Sketch算法" class="headerlink" title="Count-min Sketch算法"></a>Count-min Sketch算法</h3><p><img src="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/13.png"></p>
<p>在CM-sketch算法中，其数据结构是一个宽度为w，深度为d(代表哈希函数的个数)的计数器数组：</p>
<script type="math/tex; mode=display">
C[1,1],C[1,2]\cdots,C[d,w]</script><p>初始化时，每个元素均为0. 另有d个哈希函数使得：</p>
<script type="math/tex; mode=display">
h_i :\{1,2\cdots,n\}\rightarrow \{1,2\cdots,w\},1\leq i\leq d</script><p>一旦w和d确定下来，CM-sketch所需要的空间便确定了，我们发现哈希表相对于Count Sketch的($\frac{3}{\epsilon^2}$)更加小了 ，因此空间性能提升了$1/\epsilon$ 倍左右。 </p>
<h4 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h4><p>给定数据流$<4,1,3,5,1,3,2,6,7,0,9>$, 若哈希函数形如 $h(x)=(ax+b) ~\text{mod~} 8$ ，其中a和b是任意给定的常数。假定给定如下哈希函数：</4,1,3,5,1,3,2,6,7,0,9></p>
<script type="math/tex; mode=display">
h(x) = (3x+2)\mod 8\\
h(x) = (7x+5)\mod 8\\
h(x) = (5x+3)\mod 8</script><p>试解答以下问题：</p>
<ol>
<li>利用 Count-Min Sketch 算法估计频繁项</li>
</ol>
<p>这种情况下，$d=3,w=8$</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>哈希值表</th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>9</th>
</tr>
</thead>
<tbody>
<tr>
<td>$h_1(x)$</td>
<td>2</td>
<td>5</td>
<td>0</td>
<td>3</td>
<td>6</td>
<td>1</td>
<td>4</td>
<td>7</td>
<td>5</td>
</tr>
<tr>
<td>$h_2(x)$</td>
<td>5</td>
<td>4</td>
<td>3</td>
<td>2</td>
<td>1</td>
<td>0</td>
<td>7</td>
<td>6</td>
<td>4</td>
</tr>
<tr>
<td>$h_3(x)$</td>
<td>3</td>
<td>0</td>
<td>5</td>
<td>2</td>
<td>7</td>
<td>4</td>
<td>1</td>
<td>6</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>对于三个哈希函数, 将上述哈希到整个表中的不同位置并计数</p>
<p>比如说，第一个到达的数据是4，那么查询上面的哈希值表，在4这个位置，$h_1(x) = 6,h_2(x)=1,h_3(x)=7$ ，那么我们就在下面这个dw表格中，在$h_1(x) = 6,h_2(x)=1,h_3(x)=7$ 的位置上分别加上1。</p>
<p>这样，把整个数据流过一遍，就得到了下面这张表格</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>dw表格</th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
</tr>
</thead>
<tbody>
<tr>
<td>$h_1(x)$</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>3</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>$h_2(x)$</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>3</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>$h_3(x)$</td>
<td>3</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>然后，对于每一个元素，我们都要取到哈希到的格子中的最小值</p>
<ul>
<li>比如说对于元素0，哈希到$h_1(x)=2,h_2(x)=5,h_3(x)=3$ 这三个格子，格子中的值分别是1、1、1，最小值为1,因此对元素0的频数估计为1.</li>
<li>比如说对于元素1，哈希到$h_1(x)=5,h_2(x)=4,h_3(x)=0$  这三个格子，格子中的值分别是3、3、3,最小值为3，因此元素1的频数估计为3</li>
</ul>
<p>以此类推，得到：</p>
<script type="math/tex; mode=display">
\hat f_0 = 1,\hat f_1 = 3,\hat f_2 = 1,\hat f_3=2,\hat f_4=1,\hat f_5 = 1,\hat f_6 = 1,\hat f_7=1,\hat f_9 = 3</script><p>这里不存在大多数项。因为没有频数大于4的元素</p>
<p>当$k=3$时，大于$8/3=2.6$的元素就可以被认为是频繁项，因此 1和9是频繁项</p>
<p>当$k=4$时，大于$8/4=2$的元素就可以被认为是频繁项，因此 1、3、9是频繁项。</p>
<p>但是我们知道在这个数据流中9只出现了1次，因此这个算法也存在误判的概率，这就是一次collision。Count-min Sketch的确会有这种问题，因为这个模型是从Bloom Filter衍生过来的。所以说Count-min Sketch是一个概率模型，返回的结果是一个上限值（upper-bound）。</p>
<ol>
<li>给定任意元素i，题⽬中的CM Sketch可以达到何种精度</li>
</ol>
<p>因为 哈希函数 $k = \lceil \log(1/\delta) \rceil = 3$ 因此 $ \frac{1}{8}&lt;\delta&lt;\frac{1}{4}$, 也就是说，算法错误的概率在 $12.5\%—25\%$ 之间；</p>
<p>数组宽度 $d = \lceil  2/\epsilon\rceil =8$  ,$\frac{1}{4}&lt;\epsilon &lt;\frac{2}{7}$，也就是说，算法估计精度误差在 $25\%—28.6\% $ 之间</p>
<ol>
<li>如果想找到$(\epsilon,\delta)$ 估计，需要如何修改算法？(从哈希函数的个数入手)</li>
</ol>
<p>可以增加数组宽度，达到更高的算法精度</p>
<p>也可以增加哈希函数的个数，达到更高的准确度</p>
<h4 id="算法解释"><a href="#算法解释" class="headerlink" title="算法解释"></a>算法解释</h4><p> 在算法开始时，构造一个d行w列的空数组，可以认为每一行是独立的，算法在运行时同时记录了d个这样的数组。在每出现一个流数据的时候，对每一个数组进行一次更新，注意元素的第二个下标用的是数据的哈希值。</p>
<p> 算法在运行的过程中可能产生冲突，也就是两个不同的流数据的哈希值可能相同，这个时候就会导致结果偏大，但是因为有相当于t次的重复计算，通过取最小值的方法来进行一些弥补，但是这样的方法也不能完全避免冲突。</p>
<h4 id="算法分析-2"><a href="#算法分析-2" class="headerlink" title="算法分析"></a>算法分析</h4><p>设计一个最优的 Count-min Sketch模型的过程是：</p>
<ol>
<li>估算数据流n 的大小</li>
<li>选择一个合理的 $\epsilon$ 值</li>
<li>选择一个合理的概率值 $1-\delta$</li>
<li>哈希函数的个数  $k = \lceil(\ln\frac{1}{\delta})\rceil$, 数组的宽度 $w = \lceil\frac{2}{\epsilon}\rceil$</li>
</ol>
<p>可以看出，要想错误范围越小，就需要更大的w，也就是表格的宽度</p>
<p>想要更高的概率，就需要更大的k，也就是更多的哈希函数</p>
<p>假设我们现在需要为 $10^6$ 的数据计数，规定精度 $\epsilon = 0.002$ ，由此可知 $d = 1000$ ; 规定准确率为$99\%$ ， 可知$\delta = 0.01$, 因此需要的哈希函数个数为  $k = \lceil(\ln100)\rceil=  5$</p>
<p>假设每个计数单元占内存大小为 4 byte,那么，该模型将占用内存</p>
<script type="math/tex; mode=display">
1000\times 5\times 4\text{bytes} = 20\text{KB}</script><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>几种方法的总结</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>算法</th>
<th>空间需求</th>
<th>参数</th>
</tr>
</thead>
<tbody>
<tr>
<td>简单抽样算法</td>
<td>$M = O(\frac{m\log(1/\delta)}{\epsilon^2})$</td>
<td>m:数据流大小<br>$\delta$: 误差<br>$\epsilon$: 估计精度</td>
</tr>
<tr>
<td>Basic Count Sketch算法</td>
<td>$M = O(\frac{1}{\delta\epsilon^2})$</td>
<td>和数据流大小无关，确定精度和误差即可<br>计数器个数 $k = \frac{3}{\epsilon^2}$</td>
</tr>
<tr>
<td>Count Sketch算法</td>
<td>$M = O(\frac{log(1/\delta)}{\epsilon^2})$</td>
<td>相比于Basic Count Sketch,更低<br>计数器个数  $k=\frac{3}{\epsilon^2}$<br>哈希函数个数 $t = \lceil \ln(1/\delta)\rceil$</td>
</tr>
<tr>
<td>CM Sketch算法</td>
<td>$M = O(\frac{log(1/\delta)}{\epsilon})$</td>
<td>复杂度最低,计数器减少<br>数组的宽度 $w = \lceil\frac{2}{\epsilon}\rceil$<br>哈希函数的个数  $k = \lceil(\ln\frac{1}{\delta})\rceil$</td>
</tr>
</tbody>
</table>
</div>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/" class="post-title-link" itemprop="url">关系数据库及其基本实现原理</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-10-25 23:09:05" itemprop="dateCreated datePublished" datetime="2021-10-25T23:09:05+08:00">2021-10-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-01-16 10:48:20" itemprop="dateModified" datetime="2023-01-16T10:48:20+08:00">2023-01-16</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="关系数据库"><a href="#关系数据库" class="headerlink" title="关系数据库"></a>关系数据库</h1><h3 id="关系数据库简介"><a href="#关系数据库简介" class="headerlink" title="关系数据库简介"></a>关系数据库简介</h3><p>之前我们学了文档数据库，文档数据库更像是面向对象的一种表示模式。但是关系数据库用表格来存储数据，我们将表格的每一列看成是一个属性。</p>
<p>工程师们一开始并没有提出关系模型，一种叫做网状模型，一种叫做层次模型。在六十年代，IBM推出的IMS系统就是使用层次模型，GE推出的IDS使用的是网状模型</p>
<p>后来，Ted Codd提出了关系模型，其思想就是将数据管理系统从软件中解耦合出来形成一套独立的系统</p>
<h3 id="网状模型问题"><a href="#网状模型问题" class="headerlink" title="网状模型问题"></a>网状模型问题</h3><p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/5.jpeg" style="zoom:67%;"></p>
<p>上图是网状数据结构的一个定义，在定义里有三个对象：学生、课程、选课。学生有三个属性，课程和选课各有两个属性。图下方的s数据就是根据上面的结构来存储的。</p>
<p>解决了数据存储的模式，程序员可以在程序中通过一些接口来访问数据库中的对象了。比如说：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">-- 定义变量</span><br><span class="line">student;course;registration;</span><br><span class="line">-- 程序</span><br><span class="line">Course.C_name &#x3D; &quot;math1&quot;;</span><br><span class="line">find course using C_name;</span><br><span class="line">while DB-status &#x3D; 0 do</span><br><span class="line">	get next Registration;</span><br><span class="line">	get student;</span><br><span class="line">	print Student.S_name;</span><br><span class="line">	End;</span><br></pre></td></tr></table></figure>
<ul>
<li>用math1这个名称找到这门课</li>
<li>然后通过while循环，利用 get 来获取到每一个选择这门课的学生</li>
</ul>
<p>我们发现要找到信息，完全是通过程序来实现的。但是Ted Codd认为，这样就让数据管理系统和应用系统之间的耦合度太高了，太复杂了。我们需要经常对数据机构和程序逻辑进行修改。他认为，这些查询的逻辑应该交给数据库自己做。因此他提出了关系型数据库。</p>
<h3 id="关系数据库的构建思想"><a href="#关系数据库的构建思想" class="headerlink" title="关系数据库的构建思想"></a>关系数据库的构建思想</h3><p>上面我们用程序实现了查找所有选择数学课的学生姓名，但这个程序时比较复杂的。能不能用很简单的方式来表达我们的需求呢？——谓词逻辑</p>
<p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/6.jpeg" style="zoom:67%;"></p>
<p>我们的需求可以用这样一段为此逻辑来表示。</p>
<p>存在这样一个选课的对象Y，这个选课对象和class有联系，和student也存在联系。并且，课程的名称为 “Math1”而学生的名字是X。我们要找的就是这个X</p>
<p>这是一种声明式的方式，相比于之前写的程序，这种方式更加简洁简单。对于怎么根据声明表达式来获得所需要的数据，交给数据库去处理。</p>
<p>但是并不是所有的逻辑表达式都可以被计算机以很高效的方式来处理，因此我们必须要考虑逻辑抽象的层次和表达能力的范围。Codd在这种谓词逻辑表达式中找到了一个子集，称为 Relational Calculus(关系演算) ，通过这种方式，计算机可以很快地理解我们的逻辑表达式。进而，这种模式演化成了关系数据库</p>
<h3 id="关系模型"><a href="#关系模型" class="headerlink" title="关系模型"></a>关系模型</h3><p>关系模型是将数据组织成表，这张表在关系模型中被称为<strong>关系</strong>。接下来我们给出一些定义：</p>
<ul>
<li>域(domain)——&gt; 集合(同一数据类型)，比如说<code>{Jason,May,Bob}、{male,female}</code>. 我们可以将其理解为数据库里某一属性的所有取值。有了域的概念，就可以来定义笛卡尔积</li>
<li>笛卡尔积：是数据的组合。从每个域中取出一个值形成一个集合，那么穷尽所有的集合就是这几个域的笛卡尔积。比如说对上面的两个域，其笛卡尔积中一共有 3X2 = 6 个组合。</li>
<li>有了上面的两个概念，我们可以定义关系(relation), 关系就是笛卡尔积的一个子集。子集中的组合可以构成一张表格，即关系</li>
</ul>
<h3 id="关系代数的概念"><a href="#关系代数的概念" class="headerlink" title="关系代数的概念"></a>关系代数的概念</h3><p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/7.png" style="zoom:67%;"></p>
<p>现在我们有三个关系，分别是student,course,sc 每一张表格的表头中存放的信息我们称之为<strong>属性</strong> ，表中的每一行我们称之为<strong>元组</strong>。</p>
<p>现在我们要在这些关系上做计算，计算的方法叫做 <strong>关系代数(Relational Algebra)</strong> 。关系代数需要用到很多算子： </p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Operation</th>
<th>中文</th>
<th>符号</th>
<th>LATEXLATEX</th>
</tr>
</thead>
<tbody>
<tr>
<td>Projection</td>
<td>投影</td>
<td>$\Pi$</td>
<td><code>\Pi</code></td>
</tr>
<tr>
<td>Selection</td>
<td>选择</td>
<td>$\sigma$</td>
<td><code>\sigma</code></td>
</tr>
<tr>
<td>Renaming</td>
<td>重命名</td>
<td>$\rho$</td>
<td><code>\rho</code></td>
</tr>
<tr>
<td>Aggregate Function</td>
<td>聚合函数</td>
<td>$\mathcal{G}$</td>
<td><code>\mathcal{G}</code></td>
</tr>
<tr>
<td>Union</td>
<td>交</td>
<td>$\cap$</td>
<td><code>\cap</code></td>
</tr>
<tr>
<td>Intersection</td>
<td>补</td>
<td>$\cup$</td>
<td><code>\cup</code></td>
</tr>
<tr>
<td>Natural Join</td>
<td>自然连接</td>
<td>$\bowtie$</td>
<td><code>\bowtie</code></td>
</tr>
<tr>
<td>Left Outer Join</td>
<td>左外连接</td>
<td>⟕</td>
<td>… 这几个直接复制吧</td>
</tr>
<tr>
<td>Right Outer Join</td>
<td>右外连接</td>
<td>⟖</td>
<td></td>
</tr>
<tr>
<td>Full Outer Join</td>
<td>全外连接</td>
<td>⟗</td>
<td></td>
</tr>
<tr>
<td>Cartesian product</td>
<td>笛卡尔乘积</td>
<td>$\times$</td>
<td><code>\times</code></td>
</tr>
<tr>
<td>Divide</td>
<td>除</td>
<td>$\div$</td>
<td><code>\div</code></td>
</tr>
<tr>
<td>Assignment</td>
<td>赋值</td>
<td>$\leftarrow$</td>
<td><code>\leftarrow</code></td>
</tr>
<tr>
<td>And</td>
<td>条件并列</td>
<td>$\land$</td>
<td><code>\land</code> or <code>\vee</code></td>
</tr>
<tr>
<td>Negation</td>
<td>非</td>
<td>$\neq$</td>
<td><code>\neg</code></td>
</tr>
<tr>
<td>Exist</td>
<td>存在</td>
<td>$\exists$</td>
<td><code>\exists</code></td>
</tr>
<tr>
<td>For All</td>
<td>对所有</td>
<td>$\forall$</td>
<td><code>\forall</code></td>
</tr>
</tbody>
</table>
</div>
<h3 id="选择操作"><a href="#选择操作" class="headerlink" title="选择操作"></a>选择操作</h3><p>首先我们关注选择算子，用小写的$\sigma$ 来表示。它可以通过计算，将一个关系变为另一个关系。其使用方式如下：</p>
<script type="math/tex; mode=display">
\sigma_A(R)</script><blockquote>
<p>R 代表要选择的关系</p>
<p>A代表选择的条件</p>
</blockquote>
<p>计算的结果就是一个包含了所有在R里面且符合A条件的元组的关系</p>
<p>比如说：$\sigma_{\text{s.name=’Jason’}}(\text{Student})$​​ 最后得到的结果就是 $(S001,Jason,1/1/2001,male)$​。就是在Student关系中选择名字为‘Jason’的元组。</p>
<h3 id="投影操作"><a href="#投影操作" class="headerlink" title="投影操作"></a>投影操作</h3><p>投影我们用$\Pi$​​来表示，起作用是对一个关系进行纵向的切分，将一些属性剔除并返回一个关系。其参数和选择算子是一样的：</p>
<script type="math/tex; mode=display">
\Pi_A(R)</script><blockquote>
<p>A 代表属性的一个集合</p>
<p>R 代表要投影的关系</p>
</blockquote>
<p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/14.png" style="zoom:67%;"></p>
<p>值得注意的是，当切分结束的时候，可能会得到 重复的元组，因此在投影结束之后还需要去重</p>
<h3 id="连接操作"><a href="#连接操作" class="headerlink" title="连接操作"></a>连接操作</h3><p>连接和选择、投影是不一样的，连接是一个二元算子，也就是需要两个关系。用法如下：</p>
<script type="math/tex; mode=display">
S \bowtie_A R</script><blockquote>
<p>S，R代表两个关系</p>
<p>A 代表连接条件</p>
</blockquote>
<p>其操作逻辑就是将S和R先做一个笛卡尔积，再在其基础上做一个条件为A的选择，也就是 $S \bowtie_A R=\sigma_A(S\times R)$</p>
<p>比如说：</p>
<script type="math/tex; mode=display">
\text{Student}\bowtie_{\text{student.sno = sc.sno}}\text{SC}</script><p>这里，$\text{student.sno=sc.sno}$ 称之为等值连接</p>
<h3 id="关系代数表达式"><a href="#关系代数表达式" class="headerlink" title="关系代数表达式"></a>关系代数表达式</h3><p>现在，我们想要所有选了Math1这门课的学生的名字，怎么用关系代数表达式来表述呢？</p>
<p>我们需要将Course和SC进行一个连接，这样就能知道学生的学号了。</p>
<script type="math/tex; mode=display">
\Pi_{\text{student.sname}}(\text{Student}~~~~~\overset\bowtie{\text{student.sno=sc.sno}}~~~~(\text{SC}~~~~ \overset\bowtie{\text{sc.cno=course.cno}}~~~~~~ \sigma_{\text{cname="math1"} }(\text{Course})))\\</script><p>首先，在Course关系里面选择课程名称为math1的课，和SC关系做连接，连接条件为cno相等。接着，连接Student关系，连接条件为sno相等。最后做一个投影，投影条件是学生名字</p>
<p>基于这样的关系表达式，工程师开发了SQL语言。 </p>
<h3 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h3><p>假设学校的教务系统里有三张表：学生表(学号，姓名，生日，性别，学院)、课程表(课程号，课程名称，学分，主讲老师)、选课表(学号，课程号，学期，成绩)。其中，学生表记录了10万个学生的信息，课程表记录了2000门课的信息，选课表中有200万条选课记录。假设我们在学生表的”学号”、课程表的“课程号”、选课表的“学号”和“课程号”上分别创建了四个索引，其余的属性上都没有索引。以下三个关系代数表达式的目的都是查询“所有在张三老师的课上成绩超过90分的学号和姓名”</p>
<script type="math/tex; mode=display">
a.~\Pi_{\text{学号,姓名}}(\text{学生表}\bowtie(\sigma_{\text{成绩}>90}(选课表)\bowtie\sigma_{\text{主讲老师='张三'}}(课程表)))\\
b.~\Pi_{\text{学号,姓名}}(\text{学生表}\bowtie\Pi_{学号}(\sigma_{成绩>90\and\text{主讲老师='张三'}}){(\text{选课表}\bowtie\text{课程表})})\\
c.~\Pi_{\text{学号,姓名}}(\text{学生表}\bowtie(\sigma_{\text{成绩>90}}(\text{选课表}\bowtie\sigma_{\text{主讲老师='张三'}}(课程表))))</script><p>上面三种方法，那种执行方法最好？</p>
<p>答案是：C. </p>
<p>因为选课表有200万张，对于a来说，$\sigma_{\text{成绩&gt;90}}(选课表)$的范围太大了，可能会导致效率很拉跨。<br>对于b，$\text{选课表}\bowtie\text{课程表}$ 需要选课表和课程表做一个笛卡尔积，这边没有任何条件，因此要做一个200万*2000的笛卡尔积，非常慢</p>
<p>对于c，我们现从选课表中找到主讲老师为张三的课程号，这些课程是很少的。而且在课程号上有索引，因此查询会比较快。在做笛卡尔积的时候会减少很多的量，然后再去筛选成绩大于90的同学。</p>
<h1 id="关系数据库的基本实现原理"><a href="#关系数据库的基本实现原理" class="headerlink" title="关系数据库的基本实现原理"></a>关系数据库的基本实现原理</h1><h2 id="关系数据库的基本架构"><a href="#关系数据库的基本架构" class="headerlink" title="关系数据库的基本架构"></a>关系数据库的基本架构</h2><p>我们知道，sql语言是声明式语言。我们告诉数据库系统我们想要什么，数据库会通过内置的算法帮我们找到。这类似于一个黑箱操作，我们学会了sql语言但却并不了解数据库中的原理，那么现在我们就来学习一下其中的操作。</p>
<p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/1.jpeg" style="zoom:67%;"></p>
<p>现在我们通过上面这张图来介绍一下查询的流程。首先，应用将SQL查询提交给DBMS。接着系统要从存储层获取数据到计算层，计算完成后将结果返还给APP。因为SQL语言还是比较直白的，因此在获取了数据之后，计算层需要通过复杂的计算才能获得结果。</p>
<p>首先我们看存储层，关系型数据库是怎么存储的呢？首先，关系是由一张一张表构成的，而我们知道数据库里需要用页来存储才能有效利用空间。于是，我们需要对表进行分页处理，每页里面存放着若干元组，通过若干页面将整张表的信息存储下来，最后通过一个结构(表头)将所有页存储下来(可以是索引、inode):</p>
<p>解决了存放的方式之后，我们可以对数据进行很简单的访问，比如说对数据表进行整体的扫描，对是否称为sql查询的结果进行比对。但是通过扫描或者索引的访问，并不能完成整个sql的查询，因此在计算层需要进行相对复杂的计算</p>
<h2 id="SQL查询的执行过程"><a href="#SQL查询的执行过程" class="headerlink" title="SQL查询的执行过程"></a>SQL查询的执行过程</h2><p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/4.jpeg" style="zoom:67%;"></p>
<h3 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h3><p>首先，要将SQL语句翻译成关系代数表达式。</p>
<p>比如说有这么一段简单的sql语句： </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> sname</span><br><span class="line"><span class="keyword">FROM</span> student,sc</span><br><span class="line"><span class="keyword">WHERE</span> student.sno = sc.sno</span><br><span class="line"><span class="keyword">AND</span> sc.grade &lt; <span class="number">60</span></span><br></pre></td></tr></table></figure>
<p>经过翻译，可以将其变成不同形态的关系代数表达式</p>
<p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/3.jpeg" style="zoom:67%;"></p>
<p>他们的执行步骤可能是不一样的，但是执行的结果确实相同的。比如说第一条关系代数表达式。首先让Student、SC两张表通过SNO进行等值连接，形成一张大表；然后在表里面做选择，选择成绩小于60分的信息；最后在名字这个属性上进行一个投影，最后得到我们想要的结果。</p>
<p>因此，关系表达式确定了一段sql语句在数据库里应该做什么。但是就如上面生成的两条关系代数表达式，如果一段sql非常复杂，他可能生成不计其数的关系代数表达式，那么这时候我们需要做的就是在这里面选择查询效率最高的表达式来执行。</p>
<h3 id="查询优化"><a href="#查询优化" class="headerlink" title="查询优化"></a>查询优化</h3><p>在查询优化这一步，计算机做的更多是一种预测，预计哪一种查询计划最有效。因此在这一步需要一些AI来选择最优解</p>
<h3 id="查询执行"><a href="#查询执行" class="headerlink" title="查询执行"></a>查询执行</h3><p>接下来我们要讲的几节就关注于查询执行。我们要学习每种算子在计算层中是如何实现的。</p>
<h2 id="数据处理的性能问题"><a href="#数据处理的性能问题" class="headerlink" title="数据处理的性能问题"></a>数据处理的性能问题</h2><p>因为数据绝大部分都是存储在DISK上的，而硬盘和内存之间的交互时间非常长，因此在数据密集型计算中，cpu 长时间处于空闲状态，在等待内存和硬盘之间的I/O交互。因此我们用 I/O 次数来衡量数据处理的性能，一个好的算法约等于 I/O次数低的算法</p>
<h2 id="选择算子的实现"><a href="#选择算子的实现" class="headerlink" title="选择算子的实现"></a>选择算子的实现</h2><p>接下来我们来看选择算子的实现。我们知道数据库是将表存放在多个页里面的，然后通过一个结构来把所有页联系起来；此外我们还可以用索引的方式将页给联系起来。</p>
<p>访问一张表，我们可以采用两种方式：</p>
<ul>
<li>扫描/Scan  </li>
<li>索引访问 / Index Access</li>
</ul>
<p>比如说我要进行一个 <code>SELECT * FROM R WHERE A= 100</code></p>
<p>情况如下：采用结构存储的形式，需要在结构中存放100页的地址；采用索引查找的方式有三层索引；</p>
<p>如果使用索引访问，且刚好A是索引，那么我们需要访问4个页面，4次I/O才能访问到单独的页</p>
<p>如果使用扫描的方法，需要100次I/O才能找到。</p>
<p>当满足选择条件的元组特别多的话，那么I/O开销会更加大。</p>
<h2 id="投影算子的实现"><a href="#投影算子的实现" class="headerlink" title="投影算子的实现"></a>投影算子的实现</h2><p>比如说做完 $\Pi_{\text{major}}(R)$​​​​ 投影之后，关系剩下 {(CS),(Math),(CS)} 这三个元组, 我们需要考虑去重。</p>
<p>对于乱序的遗传数据，直接去重是比较复杂的，因此我们可以对其先进行一次排序，然后再去重就比较容易了。因此如何高效率得排序就成了最关键的问题。当然可以使用快排的东西，但是数据量大的话没有办法将其全部存放到内存里面，我们的想法是增强数据访问的局部性来降低 I/O 操作的次数。因此这里介绍<strong>外部排序</strong></p>
<p>对于25212245434215213 这一串数据，假设它无法全部放到内存中，一次只能调入6个，那么首先要对其进行局部排序。</p>
<ul>
<li>从磁盘读取252122 ，进行排序，122225，然后写回到硬盘上</li>
<li>从磁盘中读取454342，进行排序，234445，写回</li>
<li>从磁盘中读取15213，进行排序，11235，写回</li>
</ul>
<p>对每次写回的数据的头部插入一个指针，然后进行归并操作(类似于归并排序)。最后需要写入硬盘中</p>
<p>通过这种方式，可以大大减少I/O的次数，虽然计算的次数比较多，但是在数据密集型计算中，减少I/O次数节省的时间要远远多于增加计算次数所花费的时间</p>
<h2 id="连接算子的实现"><a href="#连接算子的实现" class="headerlink" title="连接算子的实现"></a>连接算子的实现</h2><h3 id="嵌套循环"><a href="#嵌套循环" class="headerlink" title="嵌套循环"></a>嵌套循环</h3><p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/9.jpeg" style="zoom:67%;"></p>
<p>现在我们来介绍一下嵌套循环的实现。假设有两个关系R和S，我们对其进行 sno相等的连接操作，那么很朴素的一种想法是做一个双重循环，R中的每一个Sno和 S中的 Sno一一比对，如果匹配上了，就将其连接起来，放到新关系中去。用简单的逻辑表达式，可以这样写：</p>
<script type="math/tex; mode=display">
\text{For r in R:}\\
~~~~~~~~~~~    \text{For s in S:}\\
~~~~~~~~~~~~~~~~~~~~~~        \text{if(r }\bowtie ~\text{s)}\\
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~            \text{output r}\cdot \text{s}</script><p>我们可以采取另一种策略，就是将R中的n个元组拿出来和S表进行扫描。我们可以把磁盘中的R中的M个元组放到内存里去，然后用S和M个元组一起作比较。在这种情况下需要 $|R|\times B(S)$​  </p>
<p>这时候，逻辑如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">For M in R</span><br><span class="line">	For s in S</span><br><span class="line">		if(M \bowtie s)</span><br><span class="line">			output ...</span><br></pre></td></tr></table></figure>
<p>这时候可以有效减少 I/O 的次数</p>
<p>即： $B(R)/M \cdot B(S)$</p>
<p>适用于查询的选择性强、约束性高并且仅返回小部分记录的结果集。通常要求 <strong>外表的记录</strong>（符合条件的记录，通常通过高效的索引访问） <strong>较少</strong>，且 <strong>内表连接列有唯一索引或者选择性强的非唯一索引</strong>时，嵌套循环连接的效率是比较高的。</p>
<h3 id="MYSQL中的嵌套连接"><a href="#MYSQL中的嵌套连接" class="headerlink" title="MYSQL中的嵌套连接"></a>MYSQL中的嵌套连接</h3><p>在Mysql中使用Join，有很多规矩：比如两表 join 要小表驱动大表，阿里开发者规范禁止三张表以上的 join 操作</p>
<p>Nested Loop Join 是扫描驱动表，每读出一条记录，就根据 join 的关联字段上的索引去被驱动表中查询对应数据。它适用于被连接的数据子集较小的场景，它也是 MySQL join 的唯一算法实现，关于它的细节我们接下来会详细讲解。</p>
<p>MySQL 中有两个 Nested Loop Join 算法的变种，分别是 Index Nested-Loop Join 和 Block Nested-Loop Join(接近于Hash Join)。</p>
<h4 id="Index-Nested-Loop-Join-算法"><a href="#Index-Nested-Loop-Join-算法" class="headerlink" title="Index Nested-Loop Join 算法"></a>Index Nested-Loop Join 算法</h4><p>下面，我们先来初始化一下相关的表结构和数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE &#96;t1&#96; (</span><br><span class="line">  &#96;id&#96; int(11) NOT NULL,</span><br><span class="line">  &#96;a&#96; int(11) DEFAULT NULL,</span><br><span class="line">  &#96;b&#96; int(11) DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (&#96;id&#96;),</span><br><span class="line">  KEY &#96;a&#96; (&#96;a&#96;)</span><br><span class="line">) ENGINE&#x3D;InnoDB;</span><br><span class="line"></span><br><span class="line">delimiter ;;</span><br><span class="line"># 定义存储过程来初始化t1</span><br><span class="line">create procedure init_data()</span><br><span class="line">begin</span><br><span class="line">  declare i int;</span><br><span class="line">  set i&#x3D;1;</span><br><span class="line">  while(i&lt;&#x3D;10000)do</span><br><span class="line">    insert into t1 values(i, i, i);</span><br><span class="line">    set i&#x3D;i+1;</span><br><span class="line">  end while;</span><br><span class="line">end;;</span><br><span class="line">delimiter ;</span><br><span class="line"># 调用存储过来来初始化t1</span><br><span class="line">call init_data();</span><br><span class="line"># 创建并初始化t2</span><br><span class="line">create table t2 like t1;</span><br><span class="line">insert into t2 (select * from t1 where id&lt;&#x3D;500)</span><br></pre></td></tr></table></figure>
<p>有上述命令可知，这两个表都有一个主键索引 id 和一个索引 a，字段 b 上无索引。存储过程 init_data 往表 t1 里插入了 10000 行数据，在表 t2 里插入的是 500 行数据。</p>
<p>为了避免 MySQL 优化器会自行选择表作为驱动表，影响分析 SQL 语句的执行过程，我们直接使用 straight_join 来让 MySQL 使用固定的连接表顺序进行查询，如下语句中，t2是驱动表(较小)，t1是被驱动表(较大)。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from t2 straight_join t1 on (t2.a&#x3D;t1.a);</span><br></pre></td></tr></table></figure>
<p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/2.png" style="zoom:67%;"></p>
<p>我们看到，t1表上的a字段是有一个索引的，Join过程中使用了该索引，因此SQL语句得执行流程如下：</p>
<ul>
<li>从t2表中读取一行数据L1</li>
<li>使用L1的a字段，去t1表中作为条件进行查询，此时会用到索引查询</li>
<li>取出t1中满足条件的行，跟L1组成相应的行，成为结果集的一部分</li>
<li>重复执行，直到扫描完t2表</li>
</ul>
<p>这个流程我们就称之为 Index Nested-Loop Join，简称 NLJ，它对应的流程图如下所示。</p>
<p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/11.jpg" style="zoom: 150%;"></p>
<p>需要注意的是，在第二步中，根据 a 字段去表t1中查询时，使用了索引，所以每次扫描只会扫描一行(从explain结果得出，根据不同的案例场景而变化)。</p>
<p>假设驱动表的行数是N(这里是500)，被驱动表的行数是 M(这里是10000)。因为在这个 join 语句执行过程中，驱动表是走全表扫描(Explain中显示为ALL)，而被驱动表则使用了索引，并且驱动表中的每一行数据都要去被驱动表中进行索引查询，所以整个 join 过程的近似复杂度是 $O(N\log_2M)$。显然，N 对扫描行数的影响更大，因此这种情况下应该让小表来做驱动表(外表)。</p>
<p>当然，这一切的前提是 join 的关联字段是 a，并且 t1 表的 a 字段上有索引。</p>
<p>如果没有索引时，再用上图的执行流程时，每次到 t1 去匹配的时候，就要做一次全表扫描。这也导致整个过程的时间复杂度变成了 $O(N * M)$，这是不可接受的。所以，当没有索引时，MySQL 使用 Block Nested-Loop Join 算法。</p>
<h3 id="Block-Nested-Loop-Join-算法"><a href="#Block-Nested-Loop-Join-算法" class="headerlink" title="Block Nested-Loop Join 算法"></a>Block Nested-Loop Join 算法</h3><p>Block Nested-Loop Join的算法，简称 BNL，它是 MySQL 在被驱动表上无可用索引时使用的 join 算法，其具体流程如下所示：</p>
<ul>
<li>把表 t2 的数据读取当前线程的 join_buffer 中，在本篇文章的示例 SQL 没有在 t2 上做任何条件过滤，所以就是讲 t2 整张表 放入内存中；</li>
<li>扫描表 t1，每取出一行数据，就跟 join_buffer 中的数据进行对比，满足 join 条件的，则放入结果集。</li>
</ul>
<p>比如下面这条 SQL</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from t2 straight_join t1 on (t2.b&#x3D;t1.b);</span><br></pre></td></tr></table></figure>
<p>这条语句的 explain 结果如下所示。可以看出</p>
<p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/3.png" style="zoom:67%;"></p>
<p>可以看出，这次 join 过程对 t1 和 t2 都做了一次全表扫描，并且将表 t2 中的 500 条数据全部放入内存 join_buffer 中，并且对于表 t1 中的每一行数据，都要去 join_buffer 中遍历一遍，都要做 500 次对比，所以一共要进行 500 * 10000 次内存对比操作，具体流程如下图所示。</p>
<p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/12.jpg" style="zoom: 150%;"></p>
<p>主要注意的是，<strong>第一步中，并不是将表 t2 中的所有数据都放入 join_buffer，而是根据具体的 SQL 语句，而放入不同行的数据和不同的字段</strong>。比如下面这条 join 语句则只会将表 t2 中符合 b &gt;= 100 的数据的 b 字段存入 join_buffer。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select t2.b,t1.b from t2 straight_join t1 on (t2.b&#x3D;t1.b) where t2.b &gt;&#x3D; 100;</span><br></pre></td></tr></table></figure>
<p>join_buffer 并不是无限大的，由 join_buffer_size 控制，默认值为 256K。当要存入的数据过大时，就只有分段存储了，整个执行过程就变成了：</p>
<ul>
<li>扫描表 t2，将符合条件的数据行存入 join_buffer，因为其大小有限，存到100行时满了，则执行第二步；</li>
<li>扫描表 t1，每取出一行数据，就跟 join_buffer 中的数据进行对比，满足 join 条件的，则放入结果集；</li>
<li>清空 join_buffer；</li>
<li>再次执行第一步，直到全部数据被扫描完，由于 t2 表中有 500行数据，所以一共重复了 5次</li>
</ul>
<p>这个流程体现了该算法名称中 Block 的由来，分块去执行 join 操作。因为表 t2 的数据被分成了 5 次存入 join_buffer，导致表 t1 要被全表扫描 5次。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>全部存入</th>
<th>分5次存入</th>
</tr>
</thead>
<tbody>
<tr>
<td>内存操作</td>
<td>10000*500</td>
<td>10000*(100+100+100+100+100)</td>
</tr>
<tr>
<td>扫描行数</td>
<td>10000+500</td>
<td>10000*5+500</td>
</tr>
</tbody>
</table>
</div>
<p>如上所示，和表数据可以全部存入 join_buffer 相比，内存判断的次数没有变化，都是两张表行数的乘积，也就是 10000 * 500，但是被驱动表会被多次扫描，每多存入一次，被驱动表就要扫描一遍，影响了最终的执行效率。</p>
<p>基于上述两种算法，我们可以得出下面的结论，这也是网上大多数对 MySQL join 语句的规范。</p>
<ul>
<li><strong>被驱动表上有索引，也就是可以使用Index Nested-Loop Join 算法时，可以使用 join 操作。</strong></li>
<li><strong>无论是Index Nested-Loop Join 算法或者 Block Nested-Loop Join 都要使用小表做驱动表。</strong></li>
</ul>
<p>因为上述两个 join 算法的时间复杂度<strong>至少</strong>也和涉及表的行数成一阶关系，并且要花费大量的内存空间，所以阿里开发者规范所说的严格禁止三张表以上的 join 操作也是可以理解的了。</p>
<p>但是上述这两个算法只是 join 的算法之一，还有<strong>更加高效的 join 算法，比如 Hash Join 和 Sorted Merged join。可惜这两个算法 MySQL 的主流版本中目前都不提供，而 Oracle ，PostgreSQL 和 Spark 则都支持，这也是网上吐槽 MySQL 弱爆了的原因</strong>(MySQL 8.0 版本支持了 Hash join，但是8.0目前还不是主流版本)。</p>
<p>其实阿里开发者规范也是在从 Oracle 迁移到 MySQL 时，因为 MySQL 的 join 操作性能太差而定下的禁止三张表以上的 join 操作规定的 。</p>
<h3 id="散列连接"><a href="#散列连接" class="headerlink" title="散列连接"></a>散列连接</h3><p>散列连接(Hash Join)的实现思路不太一样。其实现逻辑如下：</p>
<p>现在有两张表 R和S，我们对R进行一次扫描，通过哈希函数$H(x)$将里面的元组放到k个桶里。然后对S进行相同的操作，结果如下：</p>
<p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/10.jpeg" style="zoom:67%;"></p>
<p>最后我们得到了两组桶，每个桶对应的sno(连接属性)是不一样的。但是，因为我们使用了同样的哈希函数，对于$ R_1S_1$ ,$R_2S_2$ ,$R_kS_k$​​ 所对应的 sno都是相同的。相当于对R表和S表进行了切分，使得两张表内能够连接的元组放在编号相同的桶中。</p>
<p>之后，我们可依次将$R_iS_i$调入到内存当中进行连接操作，然后输出，直到结束</p>
<p>那么这个方法的IO代价是多少呢？首先要做哈希操作的话，需要将R和S都读入内存一遍，记为$B(R),B(S)$</p>
<p>然后需要写出到桶里，写的代价为$B(R),B(S)$</p>
<p>最后要把桶两两之间做一个连接，需要扫描一遍，代价为$B(R),B(S)$</p>
<p>总的来说整个连接操作的IO代价为 $3\times (B(R)+B(S))$</p>
<p>嵌套循环和散列循环相比，哪个效率更高呢？这需要分情况讨论。</p>
<ul>
<li><p>嵌套循环 ：外表数据量*内表单条（带连接条件）访问性能</p>
</li>
<li><p>散列循环 ：内表全结果集（无连接条件）访问性能</p>
</li>
</ul>
<p>如果是内表没有索引，使用hash join</p>
<p>如果是内表是复杂运算后的结果集，使用hash join 。</p>
<h4 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h4><p><strong>（ 1</strong> <strong>）外表可能记录数相对大</strong></p>
<p><strong>（ 2</strong> <strong>）内表通过连接条件没有合适的索引。</strong></p>
<h4 id="局限"><a href="#局限" class="headerlink" title="局限"></a>局限</h4><p><strong>（ 1</strong> <strong>）必须是等值连接</strong></p>
<p><strong>（ 2</strong> <strong>）要求连接条件有一定的区分度。</strong></p>
<h3 id="MYSQL8-中的散列连接"><a href="#MYSQL8-中的散列连接" class="headerlink" title="MYSQL8 中的散列连接"></a>MYSQL8 中的散列连接</h3><p>Hash Join 是扫描驱动表，利用 join 的关联字段在内存中建立散列表，然后扫描被驱动表，每读出一行数据，并从散列表中找到与之对应数据。它是大数据集连接操时的常用方式，适用于驱动表的数据量较小，可以放入内存的场景，它对于<strong>没有索引的大表</strong>和并行查询的场景下能够提供最好的性能。可惜它只适用于等值连接的场景，比如<code>on a.id = where b.a_id</code></p>
<p>还是上述两张表 join 的语句，其执行过程如下</p>
<p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/13.jpg" style="zoom: 150%;"></p>
<ul>
<li>将驱动表 t2 中符合条件的数据取出，对其每行的 join 字段值进行 hash 操作，然后存入内存中的散列表中；</li>
<li>遍历被驱动表 t1，每取出一行符合条件的数据，也对其 join 字段值进行 hash 操作，拿结果到内存的散列表中查找匹配，如果找到，则成为结果集的一部分。</li>
</ul>
<p>可以看出，<strong>该算法和 Block Nested-Loop Join 有类似之处，只不过是将无序的 Join Buffer 改为了散列表 hash table，从而让数据匹配不再需要将 join buffer 中的数据全部遍历一遍，而是直接通过 hash，以接近 O(1) 的时间复杂度获得匹配的行</strong>，这极大地提高了两张表的 join 速度</p>
<h3 id="Sorted-Merge-Join-算法"><a href="#Sorted-Merge-Join-算法" class="headerlink" title="Sorted Merge Join 算法"></a><strong>Sorted Merge Join 算法</strong></h3><p>Sort Merge Join 则是先根据 join 的关联字段将两张表排序(如果已经排序好了，比如字段上有索引则不需要再排序)，然后在对两张表进行一次归并操作。如果两表已经被排过序，在执行排序合并连接时不需要再排序了，这时Merge Join的性能会优于Hash Join。Merge Join可适于于非等值Join（&gt;，&lt;，&gt;=，&lt;=，但是不包含!=，也即&lt;&gt;）。</p>
<p>需要注意的是，如果连接的字段已经有索引，也就说已经排好序的话，可以直接进行归并操作，但是如果连接的字段没有索引的话，则它的执行过程如下图所示。</p>
<p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/14.jpg" style="zoom: 150%;"></p>
<ul>
<li>遍历表 t2，将符合条件的数据读取出来，按照连接字段 a 的值进行排序；</li>
<li>遍历表 t1，将符合条件的数据读取出来，也按照连接字段 a 的值进行排序；</li>
<li>将两个排序好的数据进行归并操作，得出结果集。</li>
</ul>
<p>Sorted Merge Join 算法的主要时间消耗在于对两个表的排序操作，所以如果两个表已经按照连接字段排序过了，该算法甚至比 Hash Join 算法还要快。在一边情况下，该算法是比 Nested Loop Join 算法要快的。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>Nested Loop Join</th>
<th>Hash Join</th>
<th>Sorted Merge Join</th>
</tr>
</thead>
<tbody>
<tr>
<td>连接条件</td>
<td>适用于任何条件</td>
<td>只适用于等值连接</td>
<td>等值或非等值连接，除了$&lt;&gt;$</td>
</tr>
<tr>
<td>主要消耗资源</td>
<td>CPU、磁盘I/O</td>
<td>内存、临时空间</td>
<td>内存、临时空间</td>
</tr>
<tr>
<td>特点</td>
<td>当有高选择性索引或进行限制性搜索时效率比较高，能够快速返回第一次的搜索结果。此时使用的是Index Nested Loop Join</td>
<td>当缺乏索引或者索引条件比较模糊时，Hash Join比Nested Join 快。在数据仓库环境夏，如果表的记录数多，则其效率高</td>
<td>当缺乏索引或者索引条件模糊时，Sort Merge Join 比 Nested Loop Join有效。当里安桀字段有索引或者提前排好序时，比Hash Join还快，并且支持更多的连接条件</td>
</tr>
<tr>
<td>缺点</td>
<td>无索引或者记录多的时候，效率很低</td>
<td>建立哈希表需要大量内存，第一次的结果返回比较慢</td>
<td>所有的表都需要排序，它为最优化的吞吐量而设计，并且在结果没有全部找到前不返回数据</td>
</tr>
<tr>
<td>需要索引</td>
<td>是(没有索引效率太低)</td>
<td>否</td>
<td>否</td>
</tr>
</tbody>
</table>
</div>
<p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/1.png" style="zoom:67%;"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/8/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><span class="page-number current">9</span><a class="page-number" href="/page/10/">10</a><span class="space">&hellip;</span><a class="page-number" href="/page/44/">44</a><a class="extend next" rel="next" href="/page/10/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Jason</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">439</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">68</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jason</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

    

  

</body>
</html>
