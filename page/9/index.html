<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=">


  <link rel="mask-icon" href="/images/logo.svg?v=" color="#222">














<meta property="og:type" content="website">
<meta property="og:title" content="Jason‘s Blog">
<meta property="og:url" content="https://jasonxqh.github.io/page/9/index.html">
<meta property="og:site_name" content="Jason‘s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Jason">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '',
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    fancybox: false,
    tabs: ,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: 'undefined',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>







  <title>Jason‘s Blog</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', '[object Object]', 'auto');
  ga('send', 'pageview');
</script>





<meta name="generator" content="Hexo 4.2.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">
  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>
<a href="https://github.com/JasonXQH/JasonXQH.github.io" target="_blank" rel="noopener" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jason‘s Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/%20" rel="section">
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/%20" rel="section">
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/%20" rel="section">
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/%20" rel="section">
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/%20" rel="section">
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2021/11/09/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch8-%E7%89%B9%E5%BE%81%E5%80%BC%E8%AE%A1%E7%AE%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/%5Bobject%20Object%5D">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/11/09/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch8-%E7%89%B9%E5%BE%81%E5%80%BC%E8%AE%A1%E7%AE%97/" itemprop="url">数据科学算法ch8-特征值计算</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-11-09T19:03:35+08:00">
                2021-11-09
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2022-01-07T09:28:22+08:00">
                2022-01-07
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="数据科学算法ch8-特征值计算"><a href="#数据科学算法ch8-特征值计算" class="headerlink" title="数据科学算法ch8-特征值计算"></a>数据科学算法ch8-特征值计算</h1><h2 id="幂法"><a href="#幂法" class="headerlink" title="幂法"></a>幂法</h2><p>如果说 低阶方阵我们可以用搞死消元法来计算，但是在高阶方阵中，用这种方法显然效率太低。因此，我们需要找到快速的、高效的特征值和特征向量计算方法</p>
<p>幂法就是这样一种方法，是计算举证<strong>最大的特征值和对应特征向量</strong>的一种<strong>向量迭代法</strong></p>
<p>算法过程：</p>
<ol>
<li>首先，令初始向量 $x_0$ </li>
<li>向前递推，公式：当 $k=1,2\cdots$</li>
</ol>
<script type="math/tex; mode=display">
\begin{cases}
y_k = Ax_{k-1}\\
m_k = \max(y_k)\\
x_{k} = y_k/m_k\\
\end{cases}</script><ol>
<li>收敛到 $x_k^T,m$ 不再变化的时候，停止</li>
</ol>
<h2 id="反幂法"><a href="#反幂法" class="headerlink" title="反幂法"></a>反幂法</h2><p>反幂法是用来求解矩阵A 绝对值最小的非零特征值$\lambda_n$ 的。</p>
<p>算法如下：</p>
<ol>
<li>任取初始向量 $x_0\neq 0$</li>
<li>当 $k=1,2\cdots$ 时</li>
</ol>
<script type="math/tex; mode=display">
\begin{cases}
y_k = A^{-1}x_{k-1}\Rightarrow Ay_k = x_{k-1}\\
m_k = \max(y_k)\\
x_{k} = y_k/m_k\\
\end{cases}</script><p>此外，反幂法的优势就是，可以通过修改幂法算法来求解方程的任意特征值。</p>
<p>算法如下：</p>
<ol>
<li>取 $x_0 = (1,\cdots,1)$， 和一个常数 $\lambda$ </li>
<li>对于 $k=1,2\cdots$</li>
</ol>
<script type="math/tex; mode=display">
\begin{cases}
(A-\lambda I)y_k = x_{k-1}\\
m_k = \max(y_k)\\
x_{k} = y_k/m_k\\
\end{cases}</script><p>最终我们可以得到，$\boldsymbol A$ 最接近于 $\lambda$ 的特征值等于： $1/m_k +\lambda$ ， 然后，标准化 $x_k$ 即为该特征值的对应特征向量</p>
<h2 id="瑞丽商迭代法"><a href="#瑞丽商迭代法" class="headerlink" title="瑞丽商迭代法"></a>瑞丽商迭代法</h2>
          
        
      
    </div>
    
    
    
    <div>
      
    </div>
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/%5Bobject%20Object%5D">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/" itemprop="url">统计方法ch5-聚类1</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-11-02T09:47:18+08:00">
                2021-11-02
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2022-01-03T22:25:42+08:00">
                2022-01-03
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="统计方法ch5-聚类1"><a href="#统计方法ch5-聚类1" class="headerlink" title="统计方法ch5-聚类1"></a>统计方法ch5-聚类1</h1><p>首先我们要来了解一下聚类思想。<strong>以类识物</strong>是人类认识世界的一种重要方式，但是人类自身是没有办法处理眼睛所捕获的大量信息的，因此我们通常会对个体的特征进行归纳，并将相似的个体归并为一类，一次来达到信息的整体性认识。比如说我们看到头上有”王”字的动物会将其归类与老虎，其实捕捉的是老虎脸上的特征。</p>
<p>那么聚类分析有什么作用呢？</p>
<ul>
<li>识别从属特定总体的个体</li>
</ul>
<p>比如说，研究消费者行为从而将市场进行细分，对消费者精准投放广告或者商品推荐</p>
<ul>
<li>识别异常个体</li>
</ul>
<p>比如说，检测用户的上网行为从而判断其行为是否正常，对政府企业等重要数据库进行保护以防止黑客攻击</p>
<p>在这一章中，我们主要来解决一下几个问题：</p>
<ol>
<li>如何定义个体之间的相似性</li>
<li>如何确定类别的数目</li>
<li>如何选取个体的特征</li>
<li>如何评价聚类方法的结果</li>
</ol>
<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><h4 id="基本定义"><a href="#基本定义" class="headerlink" title="基本定义"></a>基本定义</h4><p>我们知道在一些场合下，获取足够多数量的负样本其实是比较困难的。而聚类研究的就是那些无标签的数据集。因此，聚类分析是<strong>无监督学习</strong>中最为常用且重要的方法之一。数据集可以写成矩阵形式如下：</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/1.png" alt="1"></p>
<p>我们可以将每一行看做是一个样本，第 i 个样本$\boldsymbol{x<em>i} = (x</em>{i1},x<em>{i2},\cdots,x</em>{ip})’$ 可以看做是p维空间中的一个点。那么，按行进行聚类的话，我们可以将相似的个体聚成一类，由此在数据集$\boldsymbol X $​中进行<strong>集群发现</strong>。</p>
<p>我们可以将每一列看做是一个特征，第j个特征$x^*<em>{j} = (x</em>{1j},x<em>{2j}\cdots,x</em>{nj})’$ 可以看做是n维空间中的一点​，如果按列进行聚类，可以将相似的变量聚成一类，从而对数据集$X$ 进行降维</p>
<p>在这一章我们主要来了解<strong>集群发现</strong>，也就是按行聚类</p>
<h1 id="距离的意义"><a href="#距离的意义" class="headerlink" title="距离的意义"></a>距离的意义</h1><h2 id="点间距离"><a href="#点间距离" class="headerlink" title="点间距离"></a>点间距离</h2><h3 id="连续变量的点间距离"><a href="#连续变量的点间距离" class="headerlink" title="连续变量的点间距离"></a>连续变量的点间距离</h3><h4 id="欧氏距离"><a href="#欧氏距离" class="headerlink" title="欧氏距离"></a>欧氏距离</h4><p>说起距离，最容易想到的就是欧氏距离了，即对两个样本$\boldsymbol{x<em>k}=(x</em>{k1},\cdots,x<em>{kp})’$ 和 $\boldsymbol{x_l}=(x</em>{l1},\cdots,x_{lp})’$​ 的欧氏距离为：</p>
<script type="math/tex; mode=display">
||x_k-x_l||_2 = \sqrt{\sum_{j=1}^p(x_{kj}-x_{lj})^2}</script><p>我们如果要计算p维空间的两个点之间的距离，相当于求矩阵中第k行和第l行的距离，那么我们要对每个对应的点求平方和在进行开方。</p>
<p>在二维空间中，如下图所示</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/2.png" alt="1"></p>
<p><strong>欧式距离度量的是欧几里得空间中的直线距离</strong></p>
<h4 id="平方欧氏距离"><a href="#平方欧氏距离" class="headerlink" title="平方欧氏距离"></a>平方欧氏距离</h4><script type="math/tex; mode=display">
||x_k-x_l||_2^2  = \sum_{j=1}^p(x_{kj}-x_{lj})^2</script><h4 id="闵式距离"><a href="#闵式距离" class="headerlink" title="闵式距离"></a>闵式距离</h4><p>闵式距离是包含欧氏距离的</p>
<script type="math/tex; mode=display">
(\sum_{j=1}^p(x_{kj}-x_{lj})^q)^{\frac{1}{q}}</script><p>当q=2的时候，闵式距离和欧氏距离是等价的</p>
<p>还有一写特殊的闵式距离——绝对距离和最大距离</p>
<h4 id="曼哈顿距离-绝对距离"><a href="#曼哈顿距离-绝对距离" class="headerlink" title="曼哈顿距离(绝对距离)"></a>曼哈顿距离(绝对距离)</h4><p>当q取1的时候，闵式距离变成了曼哈顿距离又称为绝对距离</p>
<script type="math/tex; mode=display">
||\boldsymbol{x_k}-\boldsymbol{x_l}||_1 = \sum_{i=1}^p|x_{kj}-x_{lj}|</script><h4 id="切比雪夫距离-最大距离"><a href="#切比雪夫距离-最大距离" class="headerlink" title="切比雪夫距离(最大距离)"></a>切比雪夫距离(最大距离)</h4><p>当q取无穷大的时候，闵式距离变成了最大距离</p>
<script type="math/tex; mode=display">
||\boldsymbol{x_k}-\boldsymbol{x_l}||_\infty = \max_{j}|x_{kj}-x_{lj}|</script><p>用来度量所有维度中差异最大的那个维度</p>
<h4 id="兰氏距离"><a href="#兰氏距离" class="headerlink" title="兰氏距离"></a>兰氏距离</h4><script type="math/tex; mode=display">
\sum_{j=1}^p \frac{|x_{kj}-x_{lj}|}{|x_{kj}|+|x_{lj}|}</script><p>兰氏距离可以看做是加权的曼哈顿距离，它对接近于零的值的变化非常敏感，且对量纲不敏感。而曼哈顿距离对量纲是很敏感的。</p>
<p>因此若$|x_{kj}|$ 非常接近于0的话，兰氏距离会非常大</p>
<h4 id="马氏距离"><a href="#马氏距离" class="headerlink" title="马氏距离"></a>马氏距离</h4><p>马氏距离又称为是广义欧氏距离</p>
<script type="math/tex; mode=display">
\sqrt{(\boldsymbol{x_k}-\boldsymbol{x_l})'\Sigma^{-1}(\boldsymbol{x_k}-\boldsymbol{x_l})}</script><p>其中Σ是多维随机变量的协方差矩阵，当$\Sigma = \boldsymbol I_p$ ，即$\Sigma$​​为单位阵的时候，说明各个维度独立同分布，因此马氏距离等于欧氏距离</p>
<p><strong>马氏距离实际意义</strong></p>
<p>那么马氏距离就能干什么？它比欧氏距离好在哪里？举几个栗子</p>
<p><strong>欧式距离近就一定相似？</strong></p>
<p>先举个比较常用的例子，身高和体重，这两个变量拥有不同的单位标准，也就是有不同的scale。比如身高用毫米计算，而体重用千克计算，显然差10mm的身高与差10kg的体重是完全不同的。但在普通的欧氏距离中，这将会算作相同的差距。</p>
<p><strong>归一化后欧氏距离近就一定相似？</strong></p>
<p>当然我们可以先做归一化来消除这种维度间scale不同的问题，但是样本分布也会影响分类</p>
<p>举个一维的栗子，现在有两个类别，统一单位，第一个类别均值为0，方差为0.1，第二个类别均值为5，方差为5。那么一个值为2的点属于第一类的概率大还是第二类的概率大？距离上说应该是第一类，但是直觉上显然是第二类，因为第一类不太可能到达2这个位置。</p>
<p>所以，在一个方差较小的维度下很小的差别就有可能成为离群点。就像下图一样，A与B相对于原点的距离是相同的。但是由于样本总体沿着横轴分布，所以B点更有可能是这个样本中的点，而A则更有可能是离群点。</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/3.jpg" alt="1"></p>
<p><strong>算上维度的方差就够了？</strong></p>
<p>还有一个问题——如果维度间不独立同分布，样本点一定与欧氏距离近的样本点同类的概率更大吗？</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/4.jpg" alt="1"></p>
<p>可以看到样本基本服从f(x) = x的线性分布，A与B相对于原点的距离依旧相等，显然A更像是一个离群点</p>
<p>即使数据已经经过了标准化，也不会改变AB与原点间距离大小的相互关系。所以要本质上解决这个问题，就要针对主成分分析中的<code>主成分</code>来进行标准化。</p>
<p><strong>马氏距离的几何意义</strong></p>
<p>上面搞懂了，马氏距离就好理解了，只需要将变量<code>按照主成分进行旋转</code>，让维度间相互<strong>独立</strong>，然后进行<code>标准化</code>，让维度<strong>同分布</strong>就OK了</p>
<p>由主成分分析可知，由于主成分就是特征向量方向，每个方向的方差就是对应的特征值，所以只需要按照特征向量的方向旋转，然后缩放特征向量倍就可以了，可以得到以下的结果：</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/5.jpg"></p>
<p>离群点就被成功分离，这时候的欧式距离就是马氏距离。</p>
<p>现在我们可以来推导一下马氏距离：</p>
<p>首先要对数据点进行旋转，旋转至主成分使得维度间线性无关，假设新的坐标为：</p>
<script type="math/tex; mode=display">
F = (F_1,F_2\cdots,F_m) = U^TX\\
\mu_F = (\mu_1,\mu_2,\cdots,\mu_m)\\
(F- \mu_F)= U^T (X-\mu_X)</script><script type="math/tex; mode=display">
(F-\mu_F)(F-\mu_F)^T = \begin{bmatrix}\lambda_1\\&\lambda_2\\&&\cdots\\&&&\lambda_m\end{bmatrix}\\
=U^T(X-\mu_X)(X-\mu_X)^TU\\
=U^T\Sigma_XU</script><p>马氏距离是旋转变换缩放后的欧氏距离没所以马氏距离的计算公式可以写为：</p>
<script type="math/tex; mode=display">
D^2_M = (\frac{f_1-\mu_{F_1}}{\sqrt{\lambda_1}})^2+\cdots+(\frac{f_2-\mu_{F_2}}{\sqrt{\lambda_2}})^2\\=(f_1-\mu_{F_1},f_2-\mu_{F_2},\cdots,f_M-\mu_{F_M})\begin{bmatrix}\frac{1}{\lambda_1}\\&\frac{1}{\lambda_2}\\&&\cdots\\&&&\frac{1}{\lambda_m}\end{bmatrix}\begin{pmatrix}f_1-\mu_{F_1}\\f_2-\mu_{F_2}\\\cdots\\f_m-\mu_{F_m} \end{pmatrix}\\
=(f-\mu_F)^T(U^T\Sigma_X U^{-1})(f-\mu_F)\\
=(x-\mu_X)^TUU^T\Sigma_X^{-1}UU^T(x-\mu_X)\\
=(x-\mu_X)^T\Sigma_X^{-1}(x-\mu_X)</script><h4 id="皮尔逊线性相关系数"><a href="#皮尔逊线性相关系数" class="headerlink" title="皮尔逊线性相关系数"></a>皮尔逊线性相关系数</h4><p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/15.png"></p>
<h4 id="余弦相似度"><a href="#余弦相似度" class="headerlink" title="余弦相似度"></a>余弦相似度</h4><p>余弦相似度的定义为：</p>
<script type="math/tex; mode=display">
\cos\theta = \frac{\sum_{j=1}^p x_{kj}x_{lj}}{\sqrt{\sum_{j=1}^px_{kj}^2\sum_{j=1}^px^2_{lj}}}</script><p>余弦距离是由两个向量之间夹脚的余弦公式得到的</p>
<script type="math/tex; mode=display">
\cos\theta = \frac{\boldsymbol{a'b}}{\boldsymbol{|a||b|}}</script><p>当余弦为1时，说明两个向量完全相似，此时距离为0</p>
<p><strong>余弦相关距离=1-$\cos\theta$</strong></p>
<h4 id="肯德尔秩相关系数"><a href="#肯德尔秩相关系数" class="headerlink" title="肯德尔秩相关系数"></a>肯德尔秩相关系数</h4><p>现在我们来介绍秩相关系数，我们用秩来代替原来的数值，是因为我们要规避一些极端数值带来的影响</p>
<p>肯德尔相关系数，又称肯德尔秩相关系数，它是一种秩相关系数，不过，它的目标对象是<strong>有序的类别变量</strong>，比如名次、年龄段、肥胖等级(重度肥胖，中度肥胖、轻度肥胖、不肥胖)等。它可以度量两个有序变量之间单调关系强弱。肯德尔相关系数使用了“成对“这一概念来决定相关系数的强弱。</p>
<p>成对可以分为协同对(Concordant)和不协同对(Discordant)。协同对是指两个变量取值的相对关系一致，可以理解为$X_2-X_1$与$Y_2-Y_1$有相同的符号；分歧对则是指它们的相对关系不一致，$X_2-X_1$与$Y_2-Y_1$有着相反的符号。</p>
<p>肯德尔秩的相关系数是基于观测值中两个特征同时增加或者同时减少的个数从而计算相关系数</p>
<ul>
<li>协同对:$(x<em>{kj}-x</em>{kj’})(x<em>{lj}-x</em>{lj’})&gt;0$</li>
<li>不协同对：$(x<em>{kj}-x</em>{kj’})(x<em>{lj}-x</em>{lj’})&lt;0$</li>
</ul>
<p>那么，肯德尔相关系数的定义如下：</p>
<script type="math/tex; mode=display">
\text{Kendall}~\tau = \frac{n_c-n_d}{p(p-1)/2}</script><p>其中，$n_c$​表示协同对的个数，而$n_d$​表示不协同对的个数，$p(p-1)/2$ 代表总对数。</p>
<p>如果两个属性排名是相同的，系数为1 ，两个属性正相关。</p>
<p>如果两个属性排名完全相反，系数为-1 ，两个属性负相关。</p>
<p>我们来举一个例子：</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/6.png"></p>
<p><strong>肯德尔相关距离=1-Kendall $\tau $</strong></p>
<h4 id="斯皮尔曼秩相关系数"><a href="#斯皮尔曼秩相关系数" class="headerlink" title="斯皮尔曼秩相关系数"></a>斯皮尔曼秩相关系数</h4><p>斯皮尔曼秩相关系数类似于皮尔逊相关系数，只不过将原始数据$x<em>{kj}$​ 用其秩$r</em>{kj}$​ 来代替</p>
<p>将$\boldsymbol x<em>{k}$ 的各个分量$x</em>{k1}\cdots, x<em>{k2},\cdots,x</em>{kp}$ 按从小到大排序，计算每一个分量所对应的秩，记为$r<em>{k1},r</em>{k2},\cdots,r_{kp}$</p>
<script type="math/tex; mode=display">
\text{Spearman} ~~\rho = \frac{\sum_{j=1}^p(r_{kj}-\overline r_{kj})(r_{lj}-\overline r_{lj})}{\sqrt{\sum_{j=1}^p(r_{kj}-\overline r_{kj})^2\sum_{j=1}^p(r_{lj}-\overline r_{lj}})^2}</script><p>通过化简我们可以得到更精简的式子：</p>
<script type="math/tex; mode=display">
\rho = 1-\frac{6\sum d_i^2}{n(n^2-1)}</script><p>我们用具体的例子来解释计算过程：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>原始位置</th>
<th>原始X</th>
<th>排序后</th>
<th>秩次X’</th>
<th>原始Y</th>
<th>排序后</th>
<th>秩次Y’</th>
<th>秩次差$d_i$的平方</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>11</td>
<td>490</td>
<td>5</td>
<td>2</td>
<td>75</td>
<td>6</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>490</td>
<td>43</td>
<td>1</td>
<td>75</td>
<td>44</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>3</td>
<td>14</td>
<td>30</td>
<td>4</td>
<td>3</td>
<td>42</td>
<td>5</td>
<td>1</td>
</tr>
<tr>
<td>4</td>
<td>43</td>
<td>14</td>
<td>2</td>
<td>44</td>
<td>7</td>
<td>2</td>
<td>0</td>
</tr>
<tr>
<td>5</td>
<td>30</td>
<td>11</td>
<td>3</td>
<td>7</td>
<td>3</td>
<td>4</td>
<td>1</td>
</tr>
<tr>
<td>6</td>
<td>3</td>
<td>3</td>
<td>6</td>
<td>42</td>
<td>2</td>
<td>3</td>
<td>9</td>
</tr>
</tbody>
</table>
</div>
<p>代入公式，得到：$\rho = 1-\frac{6(1+1+1+9)}{6(36-1 )} = 0.657$</p>
<p><strong>斯皮尔曼相关距离 = 1-Spearman $\rho$​​</strong></p>
<h3 id="混合变量的点间距离"><a href="#混合变量的点间距离" class="headerlink" title="混合变量的点间距离"></a>混合变量的点间距离</h3><p>刚才定义的点间距离都是在连续场合定义的，那么如果我们的数据既有连续场合又有离散场合该怎么办？我们需要用统一的量纲去度量才能保证距离是靠谱的。</p>
<p>我们可以令$s<em>j =  s_j(x</em>{kj}-x_{lj})$ 为第k个观测值和第l个观测值在第j个特征或变量之间的相似性。通过$s_j$​ 来定义点与点之间的距离$d_j$, 即 $s_j = 1-d_j$​​​ </p>
<h4 id="定性变量"><a href="#定性变量" class="headerlink" title="定性变量"></a>定性变量</h4><p>比如说，我们考虑性别是否为男性、产品的颜色是否为黑色等，这些特征$x<em>{kj}$和$x</em>{lj}$ 均为<strong>定性变量</strong>。两者相等即为1，否则即为0；那么我们可以定义相似性为：</p>
<script type="math/tex; mode=display">
s_j = s_j(x_{kj},x_{lj}) = \begin{cases}1&x_{kj}=x_{lj}\\0&x_{kj}\neq x_{lj} \end{cases}</script><h4 id="定量变量"><a href="#定量变量" class="headerlink" title="定量变量"></a>定量变量</h4><p>比如考虑年龄、产品的价格等，这些特征$x<em>{kj}$ 和$x</em>{lj}$ 均为<strong>定量变量</strong>，那么我们可以定义相似性为：</p>
<script type="math/tex; mode=display">
s_j = s_j(x_{kj},x_{lj}) = 1-\frac{|x_{kj}-x_{lj}|}{R_j}</script><p>其中，$R<em>j$ 表示第j个特征的极差，即$R_j = \max_ix</em>{ij}-\min<em>ix</em>{ij}$​  ，也就是进行一个归一化后的绝对距离</p>
<h4 id="定序变量"><a href="#定序变量" class="headerlink" title="定序变量"></a>定序变量</h4><p>比如考虑文化程度、空气质量指数级别等，这些特征$x<em>{kj}$和$x</em>{lj}$​ 均为定序变量。那么我们要对$x<em>{1j},x</em>{2j},\cdots,x<em>{nj}$ 从小到大进行排序，并分别计算其秩，记为$r</em>{1j},r<em>{2j},\cdots,r</em>{nj}$​ 。</p>
<p>通常我们定义相似性为：</p>
<script type="math/tex; mode=display">
s_j = s_j(x_{kj},x_{lj}) = 1-\frac{|r_{kj}-r_{lj}|}{\max_k r_{kj}-\min_k r_{kj}}</script><h4 id="相似度"><a href="#相似度" class="headerlink" title="相似度"></a>相似度</h4><p>上面说的都是观测点之间的相似性，现在我们来定义两个观测$\boldsymbol{x}_k,\boldsymbol{x}_l$ 之间的相似度：</p>
<script type="math/tex; mode=display">
s_j(\boldsymbol x_{k}-\boldsymbol x_{l}) = \frac{\sum_{j=1}^ps_j(x_{kj},x_{lj})\delta(x_{kj},x_{lj})w_j}{\sum_{j=1}^p\delta(x_{kj},x_{lj})w_j}</script><p>其中$\delta(x<em>{kj},x</em>{lj})$ 表示观测值中是否存在确是观测，即</p>
<script type="math/tex; mode=display">
\delta(x_{kj},x_{lj}) = \begin{cases}0,&\text{如果}x_{kj}\text{或}x_{lj}\text{存在缺失观测}\\1,&\text{其他} \end{cases}</script><p>$w_j$ 表示权重，一般取值为1，但是如果事先知道第j个特征尤其重要，可以增加相应的权重。</p>
<h3 id="类间距离"><a href="#类间距离" class="headerlink" title="类间距离"></a>类间距离</h3><p>由一个点组成的类是最基本的类，如果每一类都由一个点组成，那么点间距离就是类间距离。</p>
<p>那么，如果某一类上包含不止一个点，就要定义类间距离，也称关联准则。</p>
<h4 id="sinple-linkage"><a href="#sinple-linkage" class="headerlink" title="sinple linkage"></a>sinple linkage</h4><p>将两个类中距离最短的两个点之间的距离定为类间距离，称 <strong>sinple linkage</strong>, 如下图所示：</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/8.png"></p>
<h4 id="complete-linkage"><a href="#complete-linkage" class="headerlink" title="complete linkage"></a>complete linkage</h4><p>将两个类中剧里最长的两个点之间的距离定义为类间距离，称为<strong>complete linkage</strong></p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/9.png"></p>
<h4 id="centroid-linkage"><a href="#centroid-linkage" class="headerlink" title="centroid linkage"></a>centroid linkage</h4><p>将两个类中所有的点的重心距离定义为类间距离，称为 centroid linkage</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/10.png"></p>
<h4 id="关联准则"><a href="#关联准则" class="headerlink" title="关联准则"></a>关联准则</h4><p>为了同一副好，我们将点也看做是类，因此把点(类)k与l之间的距离用$d(k,l)$ 表示。如果点(或者类)k与l聚合成一个类，记为$k\cup l$ ，对于任何其他的一个点(或者类)i，那么类($k \cup l$)与点或类i之间的距离记为$d(k\cup l,i)$ </p>
<p>Lance-Williams 公式为：</p>
<script type="math/tex; mode=display">
d(k\cup l,i) = \alpha_k d(k,i)+\alpha_ld(l,i)+\beta d(k,l)+\gamma ~|d(k,i)-d(l,i)|</script><p>其中，$\alpha_k,\alpha_l,\beta,\gamma$​</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/7.png"></p>
<p>如果我们使用single-linkage,三个参数分别是0.5，0.5，0.5，我们来推一下：</p>
<script type="math/tex; mode=display">
d(k\cup l,i) = \frac{1}{2} d(k,i)+\frac{1}{2}d(l,i)-\frac{1}{2}|d(k,i)-d(l,i)|\\
=\frac{1}{2}|d(k,i)+d(l,i)|-\frac{1}{2}|d(k,i)-d(l,i)|</script><p>如果$d(k,i)&gt;d(l,i)$,原式等于 $d(l,i)$;</p>
<p>如果$d(k,i)&lt;d(l,i)$，原式等于 $d(k,i)$</p>
<p>符合 single-linkage的定义</p>
<h1 id="层次聚类"><a href="#层次聚类" class="headerlink" title="层次聚类"></a>层次聚类</h1><p>在博客：<a href="https://jasonxqh.github.io/2020/10/25/Hierarchical-Clustering/">https://jasonxqh.github.io/2020/10/25/Hierarchical-Clustering/</a> 中介绍过层次聚类，现在我们从数学方面来看一下</p>
<p>层次聚类一般有两种不同的形式，自下而上和自上而下</p>
<p><strong>自下而上</strong>：每个样本各自分到一个类中，之后将类间距离最近的两类关联，并建立一个新的类，反复此过程直到所有的样本聚合至一个类中；<br><strong>自上而下</strong>：将所有样本归到一个类中，之后将在类中相距最远的样本记为两个新的类，基于这两个类，将未进行聚类的点逐一比较其与两个新的类的距离，这样所有样本划分成了两类，在每一个类中重复此过程直到每个样本点各自分到一个类中；</p>
<p>一般来说都会选择自下而上方法，自上而下会比较麻烦。</p>
<p>我们给一个例题：假设有4个点，距离矩阵为：</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/11.png"></p>
<p>找到欧式距离最近的两个类，是A和B，距离为1，把他们聚成一类。然后我们要计算$d(A\cup B,C),d(A\cup B,D)$</p>
<p>我们不妨采用simple linkage，那么$A\cup B$​到C 的距离是A到C的距离与B到C距离的最小值, 即3；那么$A\cup B$​​​到D 的距离是A到D的距离与B到D距离的最小值,即2</p>
<p>因此，重新得到的距离矩阵为：</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/12.png"></p>
<p>接着我们找出距离最近的两个类，A,B和D，把他们聚成一类</p>
<p>$A,B\cup D$ 到C的距离是A,B到C的距离与D到C的距离的最小值，即3，因此我们再次更新距离矩阵如下：</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/13.png"></p>
<p>最后可以将A,B,C,D 聚成一类</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/14.png"></p>
<h1 id="K-Means-聚类"><a href="#K-Means-聚类" class="headerlink" title="K-Means 聚类"></a>K-Means 聚类</h1><p>K-Means我在博客<a href="https://jasonxqh.github.io/2020/10/22/K-Means%E8%81%9A%E7%B1%BB/">https://jasonxqh.github.io/2020/10/22/K-Means%E8%81%9A%E7%B1%BB/</a> 有所介绍。 现在我们从数学角度再来学习一下。</p>
<p>K-Means 是最简单的无监督学习方法之一，且计算速度快，因此也被称为<strong>快速聚类</strong></p>
<p>相较于层次聚类，K均值聚类实现确定聚类数目，这里假定聚类的数目为K(K&lt;n)</p>
<p>那么对于给定n个样本集 $X= (\boldsymbol x_1,\cdots,\boldsymbol x_n)’$ ，K-Means 的目标就是将n个样本划分到K个不同的类中，这K个类$C_1,C_2,\cdots,C_k$ 形成了样本集X的划分，即：</p>
<script type="math/tex; mode=display">
C_k\cap C_l = \empty,\bigcup_{k=1}^K C_k = X</script><p>输出的划分 $\mathcal C = {C_1,C_2,\cdots,C_k}$ 可以对应一个聚类结果</p>
<h3 id="目标与思想"><a href="#目标与思想" class="headerlink" title="目标与思想"></a>目标与思想</h3><p>我们的目标是，希望能找到一个最优化分$\mathcal C^*$​​ ，使得类内距离足够小而类间距离足够大。这和聚类的思想是一致的，即相似的放在同一簇中，不相似的放在不同簇当中。一般来说，我们使用平方欧氏距离来表示点与点之间的距离 ，即：</p>
<script type="math/tex; mode=display">
d_{kl} = d(k,l)=||\boldsymbol x_k-\boldsymbol x_l||_2^2= \sum_{j=1}^p(x_{kj}-x_{lj})^2</script><p>由此，我们可以定义一个合理的损失函数, 如下：</p>
<script type="math/tex; mode=display">
W(\mathcal C) = \sum_{k=1}^{n_k}\sum_{i\in C_k}||\boldsymbol x_i-\boldsymbol m_k||^2</script><ul>
<li>$\boldsymbol m_k$ 表示第k类的均值或中心</li>
<li>这里$n_k$​是第k类中样本的个数</li>
</ul>
<p>而K均值聚类实际上就是解决一个最优化问题：</p>
<script type="math/tex; mode=display">
\mathcal C^* = \arg\min_C W(\mathcal C)</script><p>这是一个NP-Hard问题，可以用迭代法求解。</p>
<h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><p>通常采用迭代法来求解区 均值聚类的问题，每次迭代包括两个步骤：</p>
<ul>
<li>确定K个类的中心$\boldsymbol m_k$，将样本逐一分配到其最近的中心所对应的类中，得到一个聚类结果；</li>
<li>更新每个类的样本均值，作为类的更新后的中心；重复此过程，直到收敛为止。</li>
</ul>
<p>注意事项：</p>
<ul>
<li>收敛条件，通常可以设置为：聚类结果不变；</li>
<li>复杂度是$O(pnK)$，其中p表示特征个数，n 表示样本个数，K是聚类数目；</li>
<li>如果各个类的数据集非凸，基于非凸性质，我们可以构造出一个数据集，存在一个点在两个类之间横跳，导致K均值聚类算法难以收敛；未解决这个问题，我们可以放宽收敛的条件</li>
</ul>
<h1 id="高斯混合模型-GM"><a href="#高斯混合模型-GM" class="headerlink" title="高斯混合模型(GM)"></a>高斯混合模型(GM)</h1><p>参考： <a href="https://blog.csdn.net/lin_limin/article/details/81048411" target="_blank" rel="noopener">https://blog.csdn.net/lin_limin/article/details/81048411</a></p>
<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>高斯混合模型在聚类的分类里面属于<strong>基于模型的聚类方法</strong>。这和Kmeans和Hierarchical 聚类的底层思想是不同的。</p>
<p>高斯混合模型的核心在于：分布的假定。 假定第i个样本$\boldsymbol x<em>i$​ 来自于<strong>第k类</strong>正态分布$N_p(\boldsymbol\mu</em> k,\Sigma_k)$​</p>
<ul>
<li>$\boldsymbol\mu_k$  表示均值向量</li>
<li>$\Sigma_k$ 表示协方差矩阵</li>
</ul>
<p>那么，$\boldsymbol x_i$ 的密度函数为：</p>
<script type="math/tex; mode=display">
f(x_i) = (2\pi)^{-p/2}\abs{\Sigma_k}^{-1/2}\exp\bigg\{-\frac{1}{2}(x_i-\boldsymbol \mu_k)'\Sigma_k^{-1}(x_i-\boldsymbol \mu_k) \bigg\}</script><ul>
<li>K表示聚类数目，可以看做是一个超参数，需要我们事先确定。</li>
<li>n表示样本量</li>
</ul>
<h3 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h3><p>那么，如果我们能够确定第 i 个样本是来自于第k 个高斯分布总体时，我们可以构造变量：</p>
<script type="math/tex; mode=display">
\delta_{ik} = \begin{cases}1,\text{当第i个样本}x_i\text{属于第k个总体}\\0,\text{当第i个样本}x_i\text{不属于第k个总体}\\ \end{cases}</script><p>这就说明，某一个样本是可能属于某一类的，而且仅属于某一类。</p>
<p>因此，$\delta<em>i = (\delta</em>{i1},\delta<em>{i2},\cdots,\delta</em>{iK})’$ 满足 </p>
<ul>
<li><p>独立同分布的<strong>随机向量</strong></p>
</li>
<li><p>这个K维向量某一维是1，其余都是0。这就相当于我掷了一个K面的骰子。某一维朝上的记为1，其余都记为0。因此，这个随机变量是服从多维分布 的。因为我只掷了一次骰子，因此第一个变量是1，由于不同”面”出现的概率是不一定的，我们令其为$\pi_1,\cdots,\pi_K$。最终我们给出随机变量是服从多维分布  $M(1,\pi_1,\pi_2,\cdots,\pi_K)$的。</p>
</li>
<li><p>$\pi<em>k=P(\delta</em>{ik}=1)$且满足：</p>
<script type="math/tex; mode=display">
0<\pi_k<1,\sum_{i=1}^K \pi_k = 1</script></li>
</ul>
<p>根据上面的推论，我们给出 $\delta<em>{i}=(\delta</em>{i1},\delta<em>{i2},\cdots,\delta</em>{iK})’$ 的概率密度函数：</p>
<script type="math/tex; mode=display">
f(\delta_i) = \prod_{k=1}^K (\pi_k)^{\delta_{ik}},i=1,2\cdots,n</script><p>那么，当我们对一个$x_i$ ，给定了$\delta_i$ 的密度函数之后(样本真的落在第k类上的时候)，可以给出$x_i$的密度函数。</p>
<script type="math/tex; mode=display">
f(x_i|\delta_i) = \prod_{k=1}^K\Bigg( (2\pi)^{-p/2}|\Sigma_k|^{-1/2}\exp\bigg\{-\frac{1}{2}(x_i-\boldsymbol \mu_k)'\sum_k^{-1}(x_i-\boldsymbol \mu_k) \bigg\} \Bigg)^{\delta_{ik}}</script><blockquote>
<p>因为$\delta_i$中只有一项是等于1的，其余都等于0。其实就相当于样本属于第k类的密度函数乘以 K-1 个 1。因此这个式子拆开来就是等于上面我给出的$x_i$的密度函数，只不过这个式子更加 General一点</p>
</blockquote>
<p>那么，对于样本 ${x_i,\delta_i},i=1,2\cdots,n$，我们可以给出其联合密度函数</p>
<script type="math/tex; mode=display">
\begin{align}
& \prod_{i=1}^n f(x_i,\delta_i)\\
=&\prod_{i=1}^n f(\delta_i)\cdot f(x_i|\delta_i)\\
=&\prod_{i=1}^n\bigg(\pi_k(2\pi)^{-p/2}|\Sigma_k|^{-1/2}\exp\bigg\{-\frac{1}{2}(x_i-\boldsymbol \mu_k)'\Sigma_k^{-1}(x_i-\boldsymbol \mu_k) \bigg\}\bigg)^{\delta_{ik}}
\end{align}</script><p>对于上面这个密度函数，其实就是未知参数 $\theta = (\pi_1,\cdots,\pi_K,\boldsymbol\mu_1,\cdots,\boldsymbol\mu_K,\Sigma_1,\cdots,\Sigma_K)$ 的似然函数。理论上，基于这个似然函数，我们可以来估计参数$\theta$</p>
<p>但是，在实际中，我们仅能观测到样本${x_i}$ ，$i=1,2\cdots,n$,  是没有办法去观测$\delta_i$ 的。因此，我们没有办法直接估计未知参数$\theta$。因此我们可以对联合密度函数求关于 $x_i$ 的边际分布。</p>
<script type="math/tex; mode=display">
f(x_i) = \sum_{k=1}^K \pi_k(2\pi)^{-p/2}|\Sigma_k|^{-1/2}\exp\bigg\{-\frac{1}{2}(x_i-\boldsymbol \mu_k)'\Sigma_k^{-1}(x_i-\boldsymbol \mu_k) \bigg\}</script><p>这个密度函数是由K 个正态分布的密度函数加权组合而成的，常被称为是高斯混合模型。其中,$p(x|\delta<em>i)$ 就是第k个高斯模型的概率密度函数，可以看成选定第k 个模型后，该模型产生x的概率；$p(\delta_i)=\pi_k$ 是第k个高斯模型的权重，称其为第k个模型的先验概率，满足 $\sum</em>{k=1}^K \pi_k$ = 1</p>
<p>所以，混合高斯模型并不是什么新奇的东西，<strong>它的本质就是融合几个单高斯模型，来使得模型更加复杂，从而产生更复杂的样本。理论上，如果某个混合高斯模型融合的高斯模型个数足够多，它们之间的权重设定得足够合理，这个混合模型可以拟合任意分布的样本。</strong></p>
<h3 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h3><p>由于只观测到$x<em>i$ 去估计参数$\theta$ 是很难估的，因此我们需要用EM算法。将$\delta</em>{ik}$作为潜变量。</p>
<p>对于未知参数$\theta$的对数似然函数为：</p>
<script type="math/tex; mode=display">
\begin{align}
 \ln L(\mu,\Sigma,\pi) &= \sum_{i=1}^n\ln\sum_{k=1}^K\pi_k N(x_i|\mu_k,\Sigma_k)\\
&\propto -\frac{1}{2}\sum_{i=1}^n\sum_{k=1}^K\delta_{ik}\big((x_i-\boldsymbol \mu_k)'\Sigma_k^{-1}(x_i-\boldsymbol \mu_k)+\ln|\Sigma_k|\big)+\sum_{i=1}^n\sum_{k=1}^K\delta_{ik}\ln(\pi_k)\\
&=Q_0(\mu,\Sigma,\pi)
\end{align}</script><h4 id="E-步"><a href="#E-步" class="headerlink" title="E 步"></a>E 步</h4><p>在E步，我们需要将潜变量 $\delta<em>{ik}$的期望$\pi</em>{ik}^*$ 带入$Q_0(\boldsymbol \theta )$, 即</p>
<script type="math/tex; mode=display">
Q(\theta) = -\frac{1}{2}\sum_{i=1}^n\sum_{k=1}^K\pi^*_{ik}\big((x_i-\boldsymbol \mu_k)'\Sigma_k^{-1}(x_i-\boldsymbol \mu_k)+\ln|\Sigma_k|\big)+\sum_{i=1}^n\sum_{k=1}^K\pi^*_{ik}\ln(\pi_k)\\
=Q_1(\theta)+Q_2(\theta)</script><blockquote>
<p>前面这部分只和 $\mu,\Sigma$ 有关，后面只和 $\pi_k$ 有关</p>
</blockquote>
<p>$\delta<em>{ik}$ 的期望$\pi</em>{ik}^*$ 为：</p>
<script type="math/tex; mode=display">
\pi^*_{ik} = E(\delta_{ik}|x_i) = P(\delta_{ik}=1|x_i)=\frac{\pi_k\phi(x_i;\boldsymbol \mu_k,\sum_k)}{\sum_{k=1}^K\pi_k\phi(x_i;\boldsymbol\mu_k,\sum_k)}</script><p>其中：</p>
<script type="math/tex; mode=display">
\phi(\boldsymbol x_i,\boldsymbol \mu_k,\Sigma_k) = (2\pi)^{-p/2}\abs{\Sigma_k}^{-1/2}\exp\bigg\{-\frac{1}{2}(x_i-\boldsymbol \mu_k)'\Sigma_k^{-1}(x_i-\boldsymbol \mu_k) \bigg\}</script><p>① Q函数描述的其实就是在给定$\boldsymbol \theta$ 参数下，先对样本 X 做一个<strong>最有可能的划分</strong>（每个样本来源于各个类的可能性，即对$\delta_i$ 做估计 $E$ ，再描述能够产生这组样本的可能性（Q函数）)</p>
<p>② 有了对于 $\delta_i$ 的估计之后，Q函数只和样本有关（传统意义上的似然函数亦如此，完全数据的似然函数还与 $\gamma$有关），而不再含有隐变量，从而使得最大化Q函数成为可能；</p>
<p>③ 最大化Q函数的过程实则就是使得能够产生这组样本的可能性最大，与最大化似然函数的思路如出一辙。</p>
<h4 id="M步"><a href="#M步" class="headerlink" title="M步"></a>M步</h4><p>求$Q(\theta)$ 的最大值，而确定未知参数的估计</p>
<p>我们发现：</p>
<ul>
<li>$Q<em>1(\theta)$ 仅与未知参数 ${\boldsymbol \mu_k,\Sigma_k}^{K}</em>{k=1}$ 有关</li>
<li>$Q<em>2(\theta)$ 仅与未知参数 ${\pi_k}^K</em>{k=1}$ 有关</li>
</ul>
<p>于是，我们可以分别确定最大值点。</p>
<h5 id="Q1"><a href="#Q1" class="headerlink" title="Q1"></a>Q1</h5><script type="math/tex; mode=display">
Q_1(\boldsymbol\theta) =-\frac{1}{2}\sum_{i=1}^n\sum_{k=1}^K\pi^*_{ik}\big((x_i-\boldsymbol \mu_k)'\Sigma_k^{-1}(x_i-\boldsymbol \mu_k)+\ln|\Sigma_k|\big)</script><p>在这里，由于$\Sigma_k$ 是一个矩阵，我们来复习一下对正定对称矩阵的求导</p>
<ul>
<li><p>假定 $X$ 是一个正定对称矩阵</p>
<ul>
<li><p>非线性的形式：</p>
<script type="math/tex; mode=display">
\frac{\partial \ln \det (X)}{\partial X} = X^{-1}</script></li>
<li><p>关于逆矩阵的求导：</p>
<script type="math/tex; mode=display">
\frac{\partial \tr(AX^{-1}B)}{\partial X} = -(X^{-1}BAX^{-1})'</script></li>
</ul>
</li>
</ul>
<p>现在，对于$Q_1(\theta)$ ,我们要分别对$\boldsymbol \mu_k$ 和 $\Sigma_k$ 求导，并使得导函数为0</p>
<script type="math/tex; mode=display">
\begin{align}
&\frac{\partial Q_1(\boldsymbol \theta)}{\partial\boldsymbol\mu_k}  = -\frac{1}{2}\sum_{i=1}^n\pi_{ik}^*\Sigma_{k}^{-1}(\boldsymbol x_i-\boldsymbol \mu_k)=0 \\~\\ &\frac{\partial Q_1(\boldsymbol \theta)}{\partial \Sigma_k} = \sum_{i=1}^n\pi_{ik}^*(\Sigma_k^{-1}(\boldsymbol x_i-\boldsymbol \mu_k)(\boldsymbol x_i-\boldsymbol \mu_k)'\Sigma_{k}^{-1}+\Sigma_k^{-1})=0 
\end{align}</script><p>由此解得：</p>
<script type="math/tex; mode=display">
\begin{align}
&\boldsymbol \mu_k^{i+1} = \frac{\sum_{i=1}^n\pi_{ik}^*\boldsymbol x_i} {\sum_{i=1}^n\pi_{ik}^*}\\~\\
&\Sigma^{i+1}_k = \frac{\sum_{i=1}^n\pi_{ik}^*(\boldsymbol x_i-\boldsymbol \mu_k)(\boldsymbol x_i-\boldsymbol \mu_k)'}{\sum_{i=1}^n \pi_{ik}^*}
\end{align}</script><p>$\mu_k^{i+1},\sum_k^{i+1}$ 分别表示第$(i+1)$次迭代下第k个类的均值、协方差矩阵</p>
<h5 id="Q2"><a href="#Q2" class="headerlink" title="Q2"></a>Q2</h5><script type="math/tex; mode=display">
Q_2(\boldsymbol \theta) = \sum_{i=1}^n\sum_{k=1}^K\pi^*_{ik}\ln(\pi_k)\\</script><p>对于$Q_1(\theta)$ ,我们要分别对$\pi_k$  求导，并使得导函数为0</p>
<p>注意了，在求$Q_2(\boldsymbol \theta)$ 的最大值时，注意这里是对$\pi_k$ 有限制条件的，即：</p>
<script type="math/tex; mode=display">
\sum_{i=1}^K \pi_k = 1, 0<\pi_k<1,k=1,2\cdots ,K</script><p>因此，我们需要采用拉格朗日乘子法，令</p>
<script type="math/tex; mode=display">
Q_2^*(\boldsymbol \theta) = \sum_{i=1}^n\sum_{k=1}^K\pi_{ik}^*\ln(\pi_k)-\lambda(\sum_{k=1}^K\pi_k-1)</script><p>对 $Q_2^*(\boldsymbol \theta) $ 关于 $\pi_k$ 求导，并使得导函数为0，即：</p>
<script type="math/tex; mode=display">
\frac{\partial Q_2^*(\boldsymbol \theta)}{\partial \pi_{k}} = \sum_{i=1}^n\frac{\pi_{ik}^*}{\pi_k}-\lambda = 0</script><p>由此解得：</p>
<script type="math/tex; mode=display">
\pi_{k} = \frac{1}{n}\sum_{i=1}^n\pi_{ik}^*</script><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>EM算法的核心思想是：通过迭代的过程来找到一组最优的参数 $ (\mu^<em>,\Sigma^</em>,\pi^*)$，使得这组参数表示的模型最有可能产生现有的采样数据。每次迭代的过程就是参数矫正的过程。</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/18.png"></p>
<p>现假设初始化一组参数 $\mu^0,\Sigma^0,\pi^0$。在这组参数下，2类二维高斯分布如图11绿色椭圆所示。现在开始用EM算法</p>
<ul>
<li><p>E-step 开始对样本数据进行划分（对$\delta_i$ 进行估计）。蓝色的样本大多都被划分给第1类模型，橘黄色的样本大多都被划分给第2类模型。但是第1类模型还有优化空间：第1类模型还不能使得蓝色样本出现的联合概率达到最大。第2类模型也是如此。</p>
</li>
<li><p>M-step便优化了2类模型的参数，得到新的参数，$\mu^1,\Sigma^1,\pi^1$ .使得优化后2类高斯分布如图11红色椭圆所示。</p>
<ul>
<li>第1类模型主要优化的是模型均值$\mu$（即椭圆的中心）</li>
<li>第2类模型主要优化的是模型协方差矩阵$\Sigma$（即椭圆的长轴、短轴和长短轴的方向）</li>
</ul>
</li>
<li>然后重复进行E-step和M-step，直到参数$(\mu,\Sigma,\pi)$收敛
　　</li>
</ul>
<p>最后谈谈混合高斯模型的参数$\pi$。</p>
<ul>
<li><p>混合高斯模型的参数$\mu,\Sigma$ 比较好理解，用于描述各个高斯分布的形状，对于它们的调整也比较直观：<strong>使得本高斯分布能够更好地接纳被划分到这类分布的样本</strong>。</p>
</li>
<li><p>而为什么要有参数$\pi$ ? 它描述的是各个高斯分布所占的比重，如果不加“歧视”的话（样本来源于各个高斯分布的可能性一致），则有 $\pi_k=1/K$  。而如果对于某一类高斯分布（即为i）有侧重的话，则相应的 $\pi_i$ 较大，体现在图中就是<strong>被分配给各个类的样本数占样本总数的比例</strong>。如果一轮优化后，某一类高斯分布又接纳了更多样本，则其 $\pi$ 变大，反之变小（所以图11从绿色椭圆调整为红色椭圆实际上两个类所对应的权重也被优化了）。</p>
<ul>
<li>而从本质上来看参数 $\pi$，则是为了混合高斯模型能有更好的曲面拟合能力。当参数 $\pi$ 退化为 某一类高斯分布的权重远远大于其他类高斯分布的时候，混合高斯模型就退化成了单高斯模型！ </li>
</ul>
</li>
</ul>
<p>最后，给出混合高斯分布参数估计的逻辑流程。</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/17.png"></p>
<h1 id="DBScan"><a href="#DBScan" class="headerlink" title="DBScan"></a>DBScan</h1><p>参考博客<a href="https://www.cnblogs.com/pinard/p/6208966.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6208966.html</a></p>
<p>DBSCAN(Density-Based Spatial Clustering of Applications with Noise，具有噪声的基于密度的聚类方法)是一种很典型的密度聚类算法，和K-Means，BIRCH这些一般只适用于凸样本集的聚类相比，DBSCAN既可以适用于凸样本集，也可以适用于非凸样本集。下面我们就对DBSCAN算法的原理做一个总结。</p>
<p>DBSCAN是一种基于密度的聚类算法，这类密度聚类算法一般假定类别可以通过<strong>样本分布的紧密程度</strong>决定。同一类别的样本，他们之间的紧密相连的，也就是说，在该类别任意样本周围不远处一定有同类别的样本存在。</p>
<p>通过将紧密相连的样本划为一类，这样就得到了一个聚类类别。通过将所有各组紧密相连的样本划为各个不同的类别，则我们就得到了最终的所有聚类类别结果。</p>
<h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h3><p>我们给定数据集 $X = (\boldsymbol {x_1,x_2,\cdots,x_n})’$</p>
<p>对于每一个样本 $\boldsymbol x<em>i = (x</em>{i1},x<em>{i2},\cdots,x</em>{ip})’$ 可以看做 $R^p$ 空间中的一个点。</p>
<p>我们假定第k 个点$\boldsymbol x_k$ 和第l个点 $\boldsymbol x_l$ 之间的距离为 $d(k,l)$</p>
<h3 id="基本概念-1"><a href="#基本概念-1" class="headerlink" title="基本概念"></a>基本概念</h3><p>DBSCAN是基于一组邻域来描述样本集的紧密程度的，参数(ϵ, MinPts)用来描述邻域的样本分布紧密程度。其中，ϵ 描述了某一样本的<strong>邻域距离阈 </strong>，MinPts 描述了某一样本的距离为ϵ的邻域中<strong>样本个数的阈值</strong> 。由此，我们给出DBSCAN的一些基本概念如下：</p>
<ol>
<li>$\epsilon$-邻域：对于$x<em>j\in X$ 其 ϵ-邻域包含样本集D中与$x_j$ 的距离不大于 𝜖 的子样本集， 即 $N</em>\epsilon(x<em>j) = {x_i\in D|d(i,j) \leq\epsilon}$ 。称其为点 $x_j$ 的 $\epsilon$-邻域, 并将这个子样本集的个数记为 $|N</em>\epsilon(x_j)|$ </li>
</ol>
<p>2) 核心对象：对于任一样本 $x<em>j\in D$，如果其 $\epsilon$-邻域对应的 $N</em>ϵ(x_j)$ 至少包含MinPts个样本，即如果$|N\in(x_j)|≥MinPts$，则 $x_j$是核心对象。　 </p>
<ol>
<li><p>密度直达：如果 $x_i$ 位于 $x_j$ 的ϵ-邻域中，且$x_j$<strong>是核心对象</strong>，则称$x_i$ 由 $x_j$ 密度直达。注意<strong>反之不一定成立</strong>，即此时不能说$x_j$由$x_i$ 密度直达, 除非 $x_i$ 也是核心对象。</p>
</li>
<li><p>密度可达：对于 $x<em>i$ 和 $x_j$ , 如果存在样本样本序列 $p_1,p_2,\cdots,p_T$ ,满足$p_1 = x_i,p_T=x_j$ , 且$p</em>{t+1}$由$p<em>t$密度直达，则称$x_j$由 $x_i$密度可达。也就是说，<strong>密度可达满足传递性</strong>。此时序列中的传递样本$p_1,p_2,\cdots,p</em>{T−1}$ 均为核心对象，因为只有核心对象才能使其他样本密度直达。注意<strong>密度可达也不满足对称性</strong>，这个可以由密度直达的不对称性得出。</p>
</li>
<li><p>密度相连：对于 $x_i$ 和 $x_j$,如果存在核心对象样本$x_k$，使$x_i$和$x_j$均由$x_k$ 密度可达，则称$x_i$和$x_j$密度相连。注意<strong>密度相连关系是满足对称性的</strong>。</p>
</li>
</ol>
<p>从下图可以很容易看出理解上述定义，图中MinPts=5，红色的点都是核心对象，因为其ϵ-邻域至少有5个样本。黑色的样本是非核心对象。</p>
<ul>
<li>所有核心对象密度直达的样本在以红色核心对象为中心的超球体内，如果不在超球体内，则不能密度直达。</li>
<li>图中用绿色箭头连起来的核心对象组成了密度可达的样本序列。在这些密度可达的样本序列的 ϵ-邻域内</li>
<li>所有的样本相互都是密度相连的。</li>
</ul>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/19.png"></p>
<h3 id="聚类思想"><a href="#聚类思想" class="headerlink" title="聚类思想"></a>聚类思想</h3><p>DBSCAN的聚类定义很简单：由密度可达关系导出的最大密度相连的样本集合，即为我们最终聚类的一个类别，或者说一个簇。</p>
<p>这个DBSCAN的簇里面可以有一个或者多个核心对象。如果只有一个核心对象，则簇里其他的非核心对象样本都在这个核心对象的ϵ-邻域里；如果有多个核心对象，则簇里的任意一个核心对象的ϵ-邻域中一定有一个其他的核心对象，否则这两个核心对象无法密度可达。这些核心对象的ϵ-邻域里所有的样本的集合组成的一个DBSCAN聚类簇。</p>
<p>那么怎么才能找到这样的簇样本集合呢？DBSCAN使用的方法很简单，它任意选择一个没有类别的核心对象作为种子，然后找到所有这个核心对象能够密度可达的样本集合，即为一个聚类簇。接着继续选择另一个没有类别的核心对象去寻找密度可达的样本集合，这样就得到另一个聚类簇。一直运行到所有核心对象都有类别为止。</p>
<p>基本上这就是DBSCAN算法的主要内容了，是不是很简单？但是我们还是有三个问题没有考虑。</p>
<ul>
<li>第一个是一些异常样本点或者说少量游离于簇外的样本点，这些点不在任何一个核心对象在周围，在DBSCAN中，我们一般将这些样本点标记为噪音点。</li>
</ul>
<p>+　第二个是距离的度量问题，即如何计算某样本和核心对象样本的距离。在DBSCAN中，<strong>一般采用最近邻思想</strong>，采用某一种距离度量来衡量样本距离，比如欧式距离。这和KNN分类算法的最近邻思想完全相同。对应少量的样本，寻找最近邻可以直接去计算所有样本的距离，如果样本量较大，则一般采用KD树或者球树来快速的搜索最近邻。如果大家对于最近邻的思想，距离度量，KD树和球树不熟悉，建议参考 <a href="http://www.cnblogs.com/pinard/p/6061661.html" target="_blank" rel="noopener">K近邻法(KNN)原理小结</a>。</p>
<ul>
<li>第三种问题比较特殊，某些样本可能到两个核心对象的距离都小于 ϵ，但是这两个核心对象由于不是密度直达，又不属于同一个聚类簇，那么如果界定这个样本的类别呢？一般来说，此时DBSCAN采用先来后到，先进行聚类的类别簇会标记这个样本为它的类别。也就是说<strong>DBSCAN的算法不是完全稳定的算法</strong>。</li>
</ul>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p>下面我们对DBSCAN聚类算法的流程做一个总结。</p>
<p>输入：样本集 $D= (x_1,x_2,\cdots,x_m)$，邻域参数 (ϵ,MinPts), 样本距离度量方式 </p>
<p>输出：簇划分C.　</p>
<ol>
<li>初始化核心对象集合$\Omega = \emptyset$, 初始化聚类簇数 $k=0$，初始化未访问样本集合 $ \Gamma = D$,  簇划分$C = \emptyset$</li>
<li>对于 $j=1,2\cdots,m$, 按下面步骤，找出所有核心对象<ul>
<li>通过距离度量方式，找到样本 $x<em>j$ 的 ϵ-邻域 样本集 $N</em>\epsilon(x_j)$ </li>
<li>如果子样本集样本个数满足 $|N_\epsilon(x_j)|\geq MinPts$ , 就将样本$x_j$ 加入核心对象的样本集合中去：$\Omega = \Omega \cup{x_j}$ </li>
</ul>
</li>
<li>如果核心对象集合 $\Omega = \emptyset $，则算法结束，否则转入步骤4.</li>
<li>在核心对象集合$\Omega$中，随机选择一个核心对象$o$，初始化当前簇核心对象队列 $\Omega_{cur}={o}$, 初始化类别序号$k=k+1$，初始化当前簇样本集合 $C_k={o}$, 更新未访问样本集合 $\Gamma=\Gamma−{o}$ </li>
<li>如果当前簇核心对象队列 $\Omega_{cur}=\emptyset$，则当前聚类簇 $C_k$ 生成完毕, 更新簇划分$C={C_1,C_2\cdots,C_k}$, 更新核心对象集合$\Omega = \Omega- C_k$， 转入步骤3。否则更新核心对象集合 $\Omega=\Omega−C_k$。</li>
<li>在当前簇核心对象队列 $\Omega<em>{cur}$ 中取出一个核心对象 $o’$,通过邻域距离阈值 ϵ找出所有的 ϵ-邻域子样本集$N</em>\epsilon(o’)$，令$\Delta=N<em>c(o’)\cap \Gamma $, 更新当前簇样本集合$C_k = C_k\cup\Delta$, 更新未访问样本集合$\Gamma=\Gamma−\Delta$, 更新$\Omega</em>{cur} = \Omega_{cur}\cup(\Delta\cap\Omega)-o’$，转入步骤5.</li>
<li>如果 $x_i\in D$ 但是 $x_i\notin C_k,k=1,2\cdots,K$ ，那么我们称点 $x_i$ 为噪声</li>
</ol>
<p>输出结果为： 簇划分 $C = {C_1,C_2\cdots,C_m}$ </p>
<p>我们用一个可视化的例子来展现DBSCAN的聚类效果：这里由于点很多，采取的是在图片上等距离得取几个点，判断这几个点中是否存在核心对象，然后从选中的核心对象开始聚类。我们发现DBSCAN可以对非凸数据集(圆环)进行聚类，而Kmeans是达不到这种效果的。</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/20.gif"></p>
<h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><p>和传统的K-Means算法相比，DBSCAN最大的不同就是<strong>不需要输入类别数k</strong>，当然它最大的优势是可以发现任意形状的聚类簇，而不是像K-Means，一般仅仅使用于凸的样本集聚类。同时它在聚类的同时还可以找出异常点，这点和BIRCH算法类似。</p>
<p>用通俗的话来说：我联系的到的人，都是我的朋友，我联系不到的，反正不属于我这类。如果有人被孤立了，他就是噪声</p>
<p>那么我们什么时候需要用DBSCAN来聚类呢？一般来说，如果数据集是稠密的，并且数据集不是凸的，那么用DBSCAN会比K-Means聚类效果好很多；如果数据集不是稠密的，则不推荐用DBSCAN来聚类，因为如果$\epsilon$设置的不好的话，很可能都是噪声。</p>
<p>下面对DBSCAN算法的优缺点做一个总结。</p>
<p>DBSCAN的主要优点有：</p>
<ol>
<li>可以对任意形状的稠密数据集进行聚类，相对的，K-Means之类的聚类算法一般只适用于凸数据集。</li>
<li>可以在聚类的同时发现异常点，对数据集中的异常点不敏感。</li>
<li>聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。</li>
</ol>
<p>DBSCAN的主要缺点有：</p>
<ol>
<li>如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。</li>
<li>如果样本集较大时，聚类收敛时间较长，此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进。</li>
<li>调参相对于传统的K-Means之类的聚类算法稍复杂，主要需要对距离阈值 ϵ，邻域样本数阈值MinPts联合调参，不同的参数组合对最后的聚类效果有较大影响。</li>
</ol>
<h1 id="聚类方法的评价"><a href="#聚类方法的评价" class="headerlink" title="聚类方法的评价"></a>聚类方法的评价</h1><p>聚类有效性是评价聚类结果的方式，主要分为外部聚类的有效性和内布聚类的有效性。它们的区别就在于：对于这个数据集，我们知不知道它的标签，如果知道，相当于借助了“外部”信息，就是外部聚类的有效性。反之就是内部聚类的有效性</p>
<h2 id="外部聚类的有效性"><a href="#外部聚类的有效性" class="headerlink" title="外部聚类的有效性"></a>外部聚类的有效性</h2><p>对于n个测试样本(数据集) $x_i(i=1,2\cdots,n)$ </p>
<ul>
<li><p>假定分类结果为 $\mathcal C = {C_1,C_2\cdots,C_K}$  并满足：</p>
<script type="math/tex; mode=display">
C_k\cap C_l = \emptyset\\
\bigcup_{k=1}^KC_i = X</script><ul>
<li>K 为聚类的数目</li>
</ul>
</li>
<li><p>假设 “真实的” 标签划分 $\mathcal P = {P<em>1,P_2\cdots,P</em>{K’}}$ 并满足：</p>
<script type="math/tex; mode=display">
P_k\cap P_l = \emptyset\\
\bigcup_{k=1}^KP_i = X</script><ul>
<li>K’ 为真实分类数目</li>
</ul>
</li>
</ul>
<p>因此，我们可以给出可能性矩阵的定义：</p>
<p><img src="/2021/11/02/%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95ch5-%E8%81%9A%E7%B1%BB1/21.png"></p>
<p>我们可以计算：</p>
<script type="math/tex; mode=display">
p_{ij} = \frac{n_{ij}}{n},p_i = \frac{n_{i\cdot}}{n},p_j = \frac{n_{\cdot j}}{n}</script><h3 id="常用指标"><a href="#常用指标" class="headerlink" title="常用指标"></a>常用指标</h3><p>对 K 均值聚类算法而言，熵和纯度是两种最常用的外部度量</p>
<h4 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h4><script type="math/tex; mode=display">
E = -\sum_i p_i\bigg(\sum_j\frac{p_{ij}}{p_i}\ln\frac{p_{ij}}{p_i} \bigg)</script><h4 id="纯度"><a href="#纯度" class="headerlink" title="纯度"></a>纯度</h4><script type="math/tex; mode=display">
P = \sum_i p_i(\max_j\frac{p_{ij}}{p_i})</script><h2 id="内部聚类的有效性"><a href="#内部聚类的有效性" class="headerlink" title="内部聚类的有效性"></a>内部聚类的有效性</h2><p>聚类一般是没有标签的，所以外部聚类有效性指标有很大的局限性。因此现在来讲讲内部聚类的有效性。</p>
<p>内部聚类有效性要看两个准则：</p>
<h3 id="紧密度-Compactness"><a href="#紧密度-Compactness" class="headerlink" title="紧密度(Compactness)"></a>紧密度(Compactness)</h3><p>紧密度是在同一类内不同个体之间紧密关联的度量</p>
<ul>
<li>方差可以体现数据的紧密度；低方差表明紧密度好。</li>
<li>很多紧密度的定义是依赖于距离的，如：最大或平均两两距离，基于中心的最大或平均距离等。</li>
</ul>
<h3 id="区分度-Seperation"><a href="#区分度-Seperation" class="headerlink" title="区分度(Seperation)"></a>区分度(Seperation)</h3><p>区分度是不同类别区间程度的度量。</p>
<ul>
<li>例如，两个类中心的距离，或者是最短距离，通常作为区分度的度量</li>
<li>密度(density) 也会用于度量区分度。</li>
</ul>
<h3 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h3><h4 id="均方标准差"><a href="#均方标准差" class="headerlink" title="均方标准差"></a>均方标准差</h4><script type="math/tex; mode=display">
RMSSTD = \bigg(\frac{\sum_{k=1}^K \sum_{i\in C_k}||\boldsymbol x_i-\boldsymbol m_k||^2 }{p\sum_{k=1}^K(n_{k}-1)}  \bigg)^{1/2}</script><p>用语言来表述，分子是：每个个体减去每个组的均值；分母中，p代表维度，右边是总的样本的个数。</p>
<p>RMSSTD 越小，就代表紧密度越好</p>
<h4 id="R平方-RS"><a href="#R平方-RS" class="headerlink" title="R平方 (RS)"></a>R平方 (RS)</h4><script type="math/tex; mode=display">
RS = 1-\frac{\sum_{k=1}^K\sum_{i\in C_k}||\boldsymbol x_i-\boldsymbol m_k||^2}{\sum_{i=1}^n||\boldsymbol x_i-\boldsymbol  {\overline x}||^2}</script><p>这个方法是从回归中迁移过来的，分子是 SSE，即组内个体减去组均值的平方和。分母是SST 即所有样本减去整体样本的平方和</p>
<h1 id="选超参"><a href="#选超参" class="headerlink" title="选超参"></a>选超参</h1><p>因为很多聚类算法都需要我们手动选择超参，那么有什么通用的方法吗？</p>
<h2 id="轮廓法"><a href="#轮廓法" class="headerlink" title="轮廓法"></a>轮廓法</h2><p>第一种是轮廓法，是一种直观的、且用于验证聚类结果的方法。</p>
<p>基本思想就是：同类相似，异类不同。</p>
<p>在轮廓法中，需要定义一个重要的概念——轮廓值。 轮廓值是一个-1到1之间的值，需要通过计算获得。</p>
<p>一般认为，轮廓值较高表示该样本被很好得聚到其所属的类，而不和其他类相似。</p>
<p>如果大部分的样本具有较高的轮廓值，那么聚类的结果是恰当的；如果存在许多样本具有较低的轮廓值，甚至是负值，那么聚类的个数可能不合适</p>
<h3 id="具体方法"><a href="#具体方法" class="headerlink" title="具体方法"></a>具体方法</h3><h4 id="计算-a-i"><a href="#计算-a-i" class="headerlink" title="计算 a(i)"></a>计算 a(i)</h4><p>对于某个聚类结果 $\mathcal C = {C_1,C_2\cdots,C_K}$  ：</p>
<p>那么，对于第 i 个样本，假定其属于第k 类，$n_k$ 表示第k 类中的样本量，令$a(i)$ 表示第i个样本与 第k类的 其他样本的 平均距离。即：</p>
<script type="math/tex; mode=display">
a(i) = \frac{1}{n_k-1} \sum_{j\in \mathcal C_k,j\neq i}\text{dist}(i,j)</script><p>我们注意到 $a(i)$ 表示第i 个样本与其同属一类的样本的平均<strong>不相似度</strong>，如果该类只有第i个样本本身，那么$a(i)$ 为0</p>
<h4 id="计算b-i"><a href="#计算b-i" class="headerlink" title="计算b(i)"></a>计算b(i)</h4><p>在计算 b(i) 之前，我们要对第i个样本和另一个样本量为 $n<em>{k’}$ 的类 $\mathcal C</em>{k’}$ ，令$d(i,\mathcal C_{k’})$为第i个样本与第 $k’$ 个类的所有样本的平均<strong>不相似度</strong></p>
<script type="math/tex; mode=display">
d(i,\mathcal C_{k'}) = \frac{1}{n_{k'}}\sum_{j\in \mathcal C_{k'}}\text{dist}(i,j)</script><p>然后我们可以定义 $b(i)$，令</p>
<script type="math/tex; mode=display">
b(i) = \min_{k'\neq k} d(i,\mathcal C_k')</script><p>用来表示<strong>第i 个样本和不属于同一类的最近距离</strong></p>
<h4 id="计算轮廓值-s-i"><a href="#计算轮廓值-s-i" class="headerlink" title="计算轮廓值 s(i)"></a>计算轮廓值 s(i)</h4><p>对于第 i 个样本，我们定义轮廓值 $s(i)$ 为：</p>
<script type="math/tex; mode=display">
s(i) = \frac{b(i)-a(i)}{\max\big(a(i),b(i)\big)}</script><p>对于所有n个样本，可以计算平均轮廓值，用于度量聚类数目 K 是否合适</p>
<p>由此，以<strong>最大的平均轮廓值</strong>对应的K 作为最优聚类数目，这种方法被称为是轮廓法</p>
<h2 id="CH指数"><a href="#CH指数" class="headerlink" title="CH指数"></a>CH指数</h2><p>CH 指数是与方差分析中的 F 检验统计量是相似的。</p>
<p>假定我们对 n 个样本进行聚类，对于某个聚类结果 $\mathcal C = {C_1,C_2\cdots,C_K}$  ：我们分别考虑类间的平方和$B(K)$ 和 类内的平方和 $W(K)$ </p>
<p>那么，CH指数可以定义为：</p>
<script type="math/tex; mode=display">
CH(K) = \frac{B(K)/K-1}{W(K)/(n-K)}</script><p>其中，</p>
<script type="math/tex; mode=display">
\begin{align}
&W(K) = \sum_{k=1}^K \sum_{i\in \mathcal C_k}||\boldsymbol x_i-\boldsymbol{\overline x_k}||^2\\
&\overline x_k = \frac{1}{n_k}\sum_{i\in\mathcal C_k}\boldsymbol x_i\\
\end{align}</script><script type="math/tex; mode=display">
\begin{align}
&B(K) = \sum_{k=1}^K\sum_{i\in\mathcal C_k} ||\overline  {\boldsymbol x_i}-\boldsymbol {\overline{\overline x}}_k ||^2\\
&\overline{\overline {\boldsymbol x}} = \frac{1}{K} \sum_{k=1}^K \overline {\boldsymbol x}_k
\end{align}</script><p>基于方差分解公式，在给定样本时，$B(K)$ 与 $W(K)$ 的和是一个定值</p>
<p>如果 $B(K)$ 越大，则 $W(K)$ 越小，那么满足对聚类的基本思想：类内差异小，类间差异大</p>
<p>我们可以通过最大化 CH指数来得到最优的聚类数目K</p>

          
        
      
    </div>
    
    
    
    <div>
      
    </div>
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2021/11/01/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90-%E7%9F%A9%E9%98%B5%E7%89%88/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/%5Bobject%20Object%5D">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/11/01/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90-%E7%9F%A9%E9%98%B5%E7%89%88/" itemprop="url">线性回归分析-矩阵版</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-11-01T10:31:48+08:00">
                2021-11-01
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2023-12-27T10:59:36+08:00">
                2023-12-27
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="线性回归分析——PartII"><a href="#线性回归分析——PartII" class="headerlink" title="线性回归分析——PartII"></a>线性回归分析——PartII</h1><p>为什么我们一开始不讲矩阵呢？这是因为如果一上来就研究多元线性回归，就会忽略掉很多细节，而且还听不懂</p>
<h2 id="线性回归的模型与假设"><a href="#线性回归的模型与假设" class="headerlink" title="线性回归的模型与假设"></a>线性回归的模型与假设</h2><p>多元线性回归就是多个x，其线性回归模型为：</p>
<script type="math/tex; mode=display">
y = \beta_0+\beta_1x_1+\beta_2x_2+\cdots+\beta_px_p+\varepsilon</script><p>y为响应变量/因变量，为一个随机变量；</p>
<p>x为协变量/自变量，通常假定是确定性的变量</p>
<p>$\beta_0,\beta_1\cdots,\beta_p $是$p+1$个未知参数：</p>
<p>$\varepsilon$为随机误差，并假定：$\cases{E(\varepsilon) = 0\~\Var(\varepsilon) = \sigma^2}$</p>
<p>那么，对于n组观测数据，$(x<em>{i1},x</em>{i2},\cdots,x_{ip},y_i)$，我们可以写出回归方程模型：</p>
<script type="math/tex; mode=display">
\cases{y_1 = \beta_0+\beta_1x_{11}+\beta_2x_{12}+\cdots+\beta_px_{1p}+\varepsilon_1\\~\\y_2 = \beta_0+\beta_1x_{21}+\beta_2x_{22}+\cdots+\beta_px_{2p}+\varepsilon_2\\ \cdots\\y_n = \beta_0+\beta_1x_{n1}+\beta_2x_{n2}+\cdots+\beta_px_{np}+\varepsilon_n}</script><p>对于这些方程，我们可以将其抽象出来形成矩阵、向量的形式</p>
<script type="math/tex; mode=display">
\boldsymbol  y = \pmatrix{y_1\\y_2\\\vdots \\y_n}~~~~~
\boldsymbol X= \pmatrix{1&x_{11}&x_{12}&\cdots&x_{1p}\\1&x_{21}&x_{22}&\cdots &x_{2p}\\\vdots&\cdots\\1&x_{n1}& x_{n2}&\cdots&x_{np}}_{n\times(p+1)}\\~\\
\boldsymbol {\beta} = (\beta_0,\beta_1,\cdots,\beta_p)'\\
\varepsilon = (\varepsilon_1,\varepsilon_2\cdots,\varepsilon_n)'</script><p>吗线性回归模型的矩阵形式为：</p>
<script type="math/tex; mode=display">
\boldsymbol {y = X\beta+\varepsilon}</script><p>我们看到，用矩阵的方式来表示模型，会比较简洁。</p>
<p>那么，我们要做的就是估计参数向量$\boldsymbol{\beta}$​​ ，带入模型，这样就可以预测出$\hat y$</p>
<h3 id="线性回归的基本假定"><a href="#线性回归的基本假定" class="headerlink" title="线性回归的基本假定"></a>线性回归的基本假定</h3><p>为了便于参数估计，需要对回归方程进行一些假设：</p>
<ol>
<li>关于设计矩阵$\boldsymbol X$ </li>
</ol>
<ul>
<li>它是确定性变量，不是随机变量。在预测的时候，是给定确定的x的条件下去做的预测</li>
<li>要求$rank(\boldsymbol{X}) = p+1&lt;n$​,这表明了这是个列满秩的矩阵，每一维都不能被其他特征线性表出，每一列的自变量之间不相关。样本量应大于自变量的个数，$\boldsymbol X$​是一个满秩矩阵</li>
</ul>
<ol>
<li>关于随机误差是零均值且等方差的</li>
</ol>
<ul>
<li>$E(\varepsilon_i) =0,i=1,2\cdots,n$  表示没有系统误差</li>
<li>$Cov(\varepsilon_i,\varepsilon_j) = \cases{\sigma^2,i=j\0,i\neq j},i,j = 1,2\cdots,n$​ 表明随机在不同的样本点之间是不相关的(在正态假定下即独立的)，不存在序列相关，并且有相同的精度。只有自身和自身存在协方差(等于方差)</li>
</ul>
<p>这个条件常被称为<strong>高斯-马尔可夫条件</strong></p>
<ol>
<li>假定随机误差服从正态分布</li>
</ol>
<p>即：$\cases{\varepsilon_i\sim N(0,\sigma^2),i=1,2\cdots,n\~\\varepsilon_1,\varepsilon_2,\cdots,\varepsilon_n\text{相互独立}}$</p>
<p>在这个假设下，随机误差向量服从：</p>
<script type="math/tex; mode=display">
\boldsymbol\varepsilon\sim N(\boldsymbol{0},\sigma^2\boldsymbol{I_n})</script><p>即每一维的误差都来自一个正态分布。</p>
<p>$\boldsymbol y$ 分为前后两部分，前面是确定性的，而后半部分$\boldsymbol\varepsilon$ 是随机性的，又 $X$也是服从正态分布的，因此等价于假定因变量$\boldsymbol y$ 服从n维正态分布，其期望向量和协方差矩阵分别为：</p>
<script type="math/tex; mode=display">
E(\boldsymbol y) = \boldsymbol {X\beta}\\
Var(\boldsymbol y) = \sigma^2 \boldsymbol{I_n}</script><h2 id="线性回归模型的参数估计"><a href="#线性回归模型的参数估计" class="headerlink" title="线性回归模型的参数估计"></a>线性回归模型的参数估计</h2><h3 id="最小二乘估计"><a href="#最小二乘估计" class="headerlink" title="最小二乘估计"></a>最小二乘估计</h3><p><img src="/2021/11/01/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90-%E7%9F%A9%E9%98%B5%E7%89%88/1.png"></p>
<p>根据这张图我们可以定义离差，也就是<strong>实际观测值和估计值的差</strong>，即 $y_i-\boldsymbol x_i\boldsymbol \beta$</p>
<p>由此我们可以给出最小二乘估计：通过最小化离差平方和而得到的估计方法</p>
<p>对于线性模型，离差平方和可以定义为：</p>
<script type="math/tex; mode=display">
Q(\boldsymbol \beta) = \sum_{i=1}^n(y_i-\boldsymbol x_i' \boldsymbol\beta)^2 = ||\boldsymbol y-\boldsymbol{X\beta}||^2</script><p>最小二乘估计为：</p>
<script type="math/tex; mode=display">
\hat\beta_{LS} = \arg\min_\beta Q(\beta)</script><p>这其实是一个数，即行向量乘以列向量，我们可以这样改写上式：</p>
<script type="math/tex; mode=display">
Q(\boldsymbol \beta)= (\boldsymbol y-\boldsymbol{X\beta})'(\boldsymbol y-\boldsymbol{X\beta})\\
Q(\boldsymbol \beta)= \boldsymbol{y'y}-2\boldsymbol{\beta'X'y}+\boldsymbol{\beta'X'X\beta}</script><p>我们要发现$\boldsymbol {y’X\beta}$和$\boldsymbol{\beta’X’y}$ 都是一个数因此可以合并同类项</p>
<p>这样写好以后，相当于一个$\boldsymbol\beta$ 的二次函数，我们可以通过对其求导来找到$\min \boldsymbol \beta$​ </p>
<h4 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h4><p>对于一个向量，我们怎么求其最小值呢，在这里我们补充一些求导的公式:</p>
<p>对于一个 p 维向量 $\boldsymbol x = (x_1,x_2\cdots,x_p)’$</p>
<ul>
<li>线性函数求导：对于任意常向量 $\boldsymbol a = (a_1,a_2\cdots,a_p)’$, 我们有：</li>
</ul>
<script type="math/tex; mode=display">
\frac{\partial (\boldsymbol{x'a})}{\partial \boldsymbol x} = \frac{\partial (\boldsymbol{a'x})}{\partial \boldsymbol x} = \boldsymbol a\\</script><ul>
<li>二次型求导：对于任意$p\times p$ 常值矩阵 $\boldsymbol B$ 我们有：</li>
</ul>
<script type="math/tex; mode=display">
\frac{\partial {\boldsymbol {x'Bx}}}{\partial {\boldsymbol x}} = (\boldsymbol{B+B'})\boldsymbol x</script><p>​    特别地，若 $\boldsymbol B$ 是一个对称矩阵，那么 </p>
<script type="math/tex; mode=display">
\frac{\partial (\boldsymbol{x'Bx})}{\partial \boldsymbol x} = 2\boldsymbol {Bx}</script><h4 id="具体计算方法"><a href="#具体计算方法" class="headerlink" title="具体计算方法"></a>具体计算方法</h4><p>我们用 $Q(\boldsymbol \beta)$ 关于$\boldsymbol \beta$​ 求导，可得：</p>
<script type="math/tex; mode=display">
\frac{\partial Q(\boldsymbol{\beta})}{\partial \boldsymbol \beta} =  2\boldsymbol{X'X\beta}-2\boldsymbol{X'y}</script><p>令 $\frac{\partial Q(\boldsymbol \beta)}{\partial \boldsymbol \beta}=0$ 可得：</p>
<script type="math/tex; mode=display">
\boldsymbol {X'X\beta} = \boldsymbol{X'y}</script><p>基于假设(1),$\boldsymbol {X’X}$ 是满秩的，因此 $(\boldsymbol{X’X})^{-1}$ 存在，由此，最小二乘估计为：</p>
<script type="math/tex; mode=display">
\boldsymbol{\hat\beta_{LS}} = \boldsymbol{(X'X)}^{-1}\boldsymbol {X'y}</script><p>说明： </p>
<ul>
<li>根据上式可知，在最小二乘估计$ \boldsymbol{\hat \beta}$​ 时，需要$(\boldsymbol{X’X})^{-1}$​ 必须存在。也就是说，$(\boldsymbol{X’X})$是一<strong>非奇异矩阵</strong>，即$\left| {\boldsymbol{X’X}} \right |\neq 0$ </li>
<li>由线性代数可知 ，$rank(\boldsymbol X)\geq rank(\boldsymbol{X’X})$​, 如果$\boldsymbol{X’X}$ 为 p+1阶满秩矩阵，也就是说 $rank(\boldsymbol{X’X})=p+1$,那么$rank(\boldsymbol X)\geq p+1$​ </li>
<li>另一方面，设计矩阵$\boldsymbol X$​ 为 $n\times (p+1)$ 阶矩阵，于是应用 $n\geq (p+1)$ 这表明了采用最小二乘法估计方法求解线性回归的未知参数，<strong>样本量必须不少于模型的参数个数​</strong> </li>
</ul>
<h4 id="拟合值"><a href="#拟合值" class="headerlink" title="拟合值"></a>拟合值</h4><p>求得 $\boldsymbol {\hat\beta}_{LS}$之后，我们可以定义回归值或者拟合值为：</p>
<script type="math/tex; mode=display">
\boldsymbol{\hat y} = \boldsymbol {X\hat\beta}</script><p>其中，$\hat y_i = \boldsymbol x_i’\boldsymbol{\hat\beta} ,~~i=1,2\cdots,n$​​​ </p>
<p>我们将 $\boldsymbol {\hat\beta}$ 用上面求得的最小二乘估计带入，得到：</p>
<script type="math/tex; mode=display">
\boldsymbol{\hat y} = \boldsymbol {X\hat\beta} = \boldsymbol{X(X'X)^{-1}X'y}</script><p>矩阵 $\boldsymbol {X(X’X)^{-1}X’}$​ ： 将观测值$\boldsymbol y$​ 变换为 $\boldsymbol {\hat y}$​ ,从形式上来看，就是给$\boldsymbol y$​ 戴上了一顶帽子  $\hat ~$​ ，因为形象地称矩阵 $\boldsymbol{X(X’X)^{-1}X’}$​ 为帽子矩阵，记为$\boldsymbol H$​ </p>
<p>于是： $\boldsymbol{\hat y} = \boldsymbol {Hy} $​ ​</p>
<h4 id="帽子矩阵的性质"><a href="#帽子矩阵的性质" class="headerlink" title="帽子矩阵的性质"></a>帽子矩阵的性质</h4><p>帽子矩阵 $\boldsymbol H = \boldsymbol {X(X’X)^{-1}X’}$ 具有以下的一些性质：</p>
<ol>
<li><p>$\boldsymbol H$ 是n阶对称矩阵</p>
</li>
<li><p>$\boldsymbol H$ 是幂等矩阵，即$\boldsymbol H =\boldsymbol H^2$</p>
</li>
<li><p>$\boldsymbol H$ 的迹为 $p+1$,即 $tr(\boldsymbol H)=p+1$</p>
</li>
</ol>
<p>证明1：</p>
<script type="math/tex; mode=display">
\boldsymbol H' = (\boldsymbol{X(X'X)^{-1}X'})' = (\boldsymbol X')'((\boldsymbol {X'X})^{-1})'\boldsymbol X'\\
=\boldsymbol {X(X'X)^{-1}X'} = \boldsymbol H</script><p>由于$\boldsymbol H$ 的转置矩阵等于$\boldsymbol H$ ，所以$\boldsymbol H$ 是对称的</p>
<p>证明2：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\boldsymbol H^2 = (\boldsymbol{X(X'X)^{-1}X'})^2\\
&=(\boldsymbol{X(X'X)^{-1}X'})(\boldsymbol{X(X'X)^{-1}X'})\\
&=\boldsymbol {X(X'X)^{-1}(X'X)(X'X)^{-1}X'}\\
&=\boldsymbol {X(X'X)^{-1}X'}=\boldsymbol H
\end{aligned}</script><p>因此$\boldsymbol H$是幂等矩阵</p>
<p>证明3：</p>
<p>易知$\boldsymbol{X’X}$​​是一个 $(p+1)\times(p+1)$ 的满秩矩阵。于是我们计算$\boldsymbol H$ 的迹，即：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&tr(\boldsymbol H) = tr(\boldsymbol {X(X'X)^{-1}X'})\\
&=tr((\boldsymbol{X'X})^{-1}\boldsymbol {X'X})\\
&=tr(\boldsymbol I_{p+1})=p+1
\end{aligned}</script><p>我们要知道矩阵迹的运算性质：多个矩阵相乘得到的方阵的迹，和将这些矩阵中的最后一个挪到最前面之后相乘的迹是相同的，即：</p>
<script type="math/tex; mode=display">
Tr(\boldsymbol{ABC}) = Tr(\boldsymbol{BCA}) = Tr(\boldsymbol{CAB})</script><h4 id="残差"><a href="#残差" class="headerlink" title="残差"></a>残差</h4><p>说完了拟合值，我们聚焦到最后一个部分——残差</p>
<p>我们把残差定义为拟合值和真实值之间的距离</p>
<script type="math/tex; mode=display">
\boldsymbol e = \boldsymbol y -\boldsymbol{\hat y}</script><p>也可以写为 ：</p>
<script type="math/tex; mode=display">
\boldsymbol e = \boldsymbol y - \boldsymbol{Hy} = (\boldsymbol I-\boldsymbol H)\boldsymbol y</script><p>几何上的关系：回归值 $\boldsymbol {\hat y}$​与残差$\boldsymbol e$ 垂直，即：</p>
<script type="math/tex; mode=display">
\boldsymbol {y'e} = (\boldsymbol {Hy})'((\boldsymbol I-\boldsymbol H)\boldsymbol y)= \boldsymbol {y'H'}(\boldsymbol I-\boldsymbol H)\boldsymbol y =0</script><p>然后，我们写出残差的协方差矩阵为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\text{Var}(\boldsymbol e) = \text{Cov}(\boldsymbol e,\boldsymbol e)\\
&=\text{Cov}((\boldsymbol I-\boldsymbol H)\boldsymbol y,(\boldsymbol I-\boldsymbol H)\boldsymbol y)\\
&=(\boldsymbol I-\boldsymbol H)\text{Cov}(\boldsymbol y,\boldsymbol y)(\boldsymbol I-\boldsymbol H)'\\
&=\sigma^2(\boldsymbol I-\boldsymbol H)\boldsymbol I_n(\boldsymbol I-\boldsymbol H)’\\
&=\sigma^2(\boldsymbol I-\boldsymbol H)
\end{aligned}</script><p>由此，我们可以构造误差项方差$\sigma^2$ 的估计，用残差去估算即：</p>
<script type="math/tex; mode=display">
\hat\sigma^2 = \frac{1}{n-p-1}(\boldsymbol {e'e}) = \frac{1}{n-p-1}\sum_{i=1}^n e_i^2</script><h3 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h3><p>现在我们介绍第二种方法——极大似然估计。$\boldsymbol y$ 是服从多元正态分布的，即：</p>
<script type="math/tex; mode=display">
\boldsymbol y\sim N_n(\boldsymbol {X\beta},\sigma^2\boldsymbol I_n)</script><p>因此 $\boldsymbol y$ 的联合密度函数为：</p>
<script type="math/tex; mode=display">
f(\boldsymbol y;\boldsymbol \beta,\sigma^2) = \frac{1}{(2\pi)^{n/2}\left|{\sigma^2 \boldsymbol I_n}^{1/2}\right |}\exp\{-\frac{1}{2}(\boldsymbol y-\boldsymbol{X\beta})'(\sigma^2\boldsymbol (\boldsymbol{y-X\beta}' (\sigma ^2I_n) ^{ -1 }(\boldsymbol{y-X\beta}))</script><p> 参数 $( \beta,\sigma ^2) $​​​的 似然函数为</p>
<script type="math/tex; mode=display">
L (\boldsymbol \beta,\sigma ^2) = (2\pi)^{-n/2}(\sigma ^2) ^{-n/2}\exp\{ (\boldsymbol { y- X\beta} ) ' (\boldsymbol{y-X\beta}) \}</script><p>进而我们给出极大似然估计 ：</p>
<script type="math/tex; mode=display">
(\boldsymbol{\hat\beta}_ {ML},\hat\sigma _{ML} ^2) =\arg\max_{ (\beta,\sigma^ 2) }L(\boldsymbol\beta,\sigma ^2)\\
 =\arg\max_ { (\beta,\sigma ^2)}\ln (L(\boldsymbol\beta,\sigma ^2))</script><h4 id="具体计算方法-1"><a href="#具体计算方法-1" class="headerlink" title="具体计算方法"></a>具体计算方法</h4><script type="math/tex; mode=display">
\ln L(\boldsymbol\beta,\sigma^2) = -\frac{n}{2}\ln(2\pi)-\frac{n}{2}\ln(\sigma^2)-\frac{1}{2\sigma^2}(\boldsymbol{y-X\beta})'(\boldsymbol{y-X\beta})</script><p>对数似然函数分布关于$\boldsymbol \beta$​和和$\sigma^2$​求导，即</p>
<script type="math/tex; mode=display">
\frac{\partial\ln L(\boldsymbol\beta,\sigma^2)}{\partial\beta} = -\frac{1}{\sigma^2}(\boldsymbol{X'X\beta-X'y})=0\\
\frac{\partial\ln L(\boldsymbol\beta,\sigma^2)}{\partial{\sigma^2}} = -\frac{n}{2\sigma^2}+\frac{1}{2\sigma^4}(\boldsymbol {y-X\beta})'(\boldsymbol{y-X\beta})=0</script><p>反解得到：</p>
<script type="math/tex; mode=display">
\boldsymbol{\hat\beta}_{ML} = (\boldsymbol{X'X})^{-1}\boldsymbol{X}'\boldsymbol y\\
\hat\sigma^2_{ML} = \frac{1}{n}(\boldsymbol {y-X'\beta}_{ML})'(\boldsymbol {y-X\hat\beta}_{ML})=\frac{1}{n}\boldsymbol {e'e}</script><blockquote>
<p>说明：</p>
<p>$\boldsymbol{\hat\beta}<em>{ML}=\boldsymbol{\hat\beta}</em>{LS}$ ，因此，我们不用写下标了，一般记为 $\boldsymbol{\hat\beta}  = (\boldsymbol{X’X})^{-1}\boldsymbol{X}’\boldsymbol y$​</p>
<p>$\hat\sigma^2_{ML}$ 不是一个无偏估计，但是一个相合估计</p>
</blockquote>
<h3 id="参数估计性质"><a href="#参数估计性质" class="headerlink" title="参数估计性质"></a>参数估计性质</h3><blockquote>
<p>概率论 关于矩阵的期望方差复习。假设$\boldsymbol {x,y}$是n维随机变量。对任意一个$m\times n$ 维常矩阵 $\boldsymbol A$ 和一个$m’\times n$ 维常矩阵$\boldsymbol B$​,以及一个m维向量$\boldsymbol c$</p>
<ul>
<li>$E(\boldsymbol {Ax+c})=\boldsymbol{A}E(\boldsymbol {x})+\boldsymbol c$</li>
<li>$Var(\boldsymbol{Ax+c})=\boldsymbol A\text{Var(x)}A’$</li>
<li>$Cov(\boldsymbol{Ax,By})=\boldsymbol A Cov(\boldsymbol{x,y})\boldsymbol B’$​ ​​</li>
</ul>
</blockquote>
<h4 id="最小二乘估计的性质"><a href="#最小二乘估计的性质" class="headerlink" title="最小二乘估计的性质"></a>最小二乘估计的性质</h4><p>最小二乘估计 $\boldsymbol{\hat\beta}=(\boldsymbol{X’X})^{-1}\boldsymbol{X’y}$​ 那么$\boldsymbol{\hat\beta}$​ 满足：</p>
<blockquote>
<ul>
<li>$E(\boldsymbol{\hat\beta})=\boldsymbol{\beta}$​, 即$\boldsymbol{\hat\beta}$是$\boldsymbol \beta$ 的无偏估计</li>
<li>$Var(\boldsymbol{\hat\beta})=\sigma^2(\boldsymbol{X’X})^{-1}$</li>
</ul>
</blockquote>
<p>证明1：计算$E(\boldsymbol {\hat\beta})$​ </p>
<script type="math/tex; mode=display">
\begin{aligned}
&E(\boldsymbol {\hat\beta})=E((\boldsymbol{X'X})'\boldsymbol{X'y})=(\boldsymbol{X'X})^{-1}\boldsymbol X'E(\boldsymbol y)\\
&=(\boldsymbol{X'X})^{-1}\boldsymbol X' E(\boldsymbol{X\beta+\epsilon})\\
&=(\boldsymbol{X'X})^{-1}\boldsymbol X'(\boldsymbol{X\beta}+E(\boldsymbol\epsilon))\\
&=(\boldsymbol{X'X})^{-1}\boldsymbol{X'X\beta} = \boldsymbol \beta
\end{aligned}</script><p>证明2：计算$Var(\boldsymbol{\hat\beta})$​​</p>
<script type="math/tex; mode=display">
\begin{aligned}
&Var(\boldsymbol{\hat\beta})=Var((\boldsymbol{X'X})^{-1}\boldsymbol{X'y})\\
&=(\boldsymbol{X'X})^{-1}\boldsymbol {X'}Var(\boldsymbol y)((\boldsymbol{X'X})^{-1}\boldsymbol X')'\\
&=(\boldsymbol{X'X})^{-1}\boldsymbol {X'}Var(\boldsymbol {X\beta}+\boldsymbol\varepsilon)((\boldsymbol{X'X})^{-1}\boldsymbol X')'\\
&=(\boldsymbol{X'X})^{-1}\boldsymbol {X'}(\sigma^2\boldsymbol I_n)((\boldsymbol{X'X})^{-1}\boldsymbol X')'\\
&=\sigma^2(\boldsymbol{X'X})^{-1}\boldsymbol{X'X}(\boldsymbol{X'X})^{-1}\\
&=\sigma^2(\boldsymbol{X'X})^{-1}
\end{aligned}</script><h4 id="最小二乘估计-boldsymbol-hat-beta-与残差-boldsymbol-e-的关系"><a href="#最小二乘估计-boldsymbol-hat-beta-与残差-boldsymbol-e-的关系" class="headerlink" title="最小二乘估计$\boldsymbol {\hat\beta}$与残差$\boldsymbol e$ 的关系"></a>最小二乘估计$\boldsymbol {\hat\beta}$与残差$\boldsymbol e$ 的关系</h4><blockquote>
<p>最小二乘估计$\boldsymbol \beta$与残差$\boldsymbol e$ 线性不相关，即</p>
<script type="math/tex; mode=display">
\text{Cov}(\boldsymbol{\hat\beta,e})=\boldsymbol 0</script></blockquote>
<p>之前我们说到，最小二乘估计$\boldsymbol {\hat\beta}$与残差$\boldsymbol e$​​ 在数学层面是垂直的。那么反映在统计中，就是线性无关的。也就是他们两的协方差等于0</p>
<p>说明：特别地，在正态分布的假定下，最小二乘估计$\boldsymbol{\hat\beta}$​与残差$\boldsymbol e$​ 独立。基于此，最小二乘估计$\boldsymbol {\hat\beta}$​ 与残差平方和 $SS_E=\boldsymbol{e’e}$独立</p>
<p>证明：</p>
<script type="math/tex; mode=display">
\text{Cov}(\boldsymbol{\hat\beta,e}) = \text{Cov}((\boldsymbol{X'X})^{-1}\boldsymbol{X'y}),(\boldsymbol{I-H})\boldsymbol y)\\</script><p>根据上面所说的协方差的性质，又知道帽子矩阵$\boldsymbol H = \boldsymbol {X(X’X)^{-1}X’}$​​ ，以及$\text{Cov}(\boldsymbol{y,y})=\sigma^2$​,原式可化简为：</p>
<script type="math/tex; mode=display">
=\sigma^2((\boldsymbol{X'X})^{-1}\boldsymbol{X}'(\boldsymbol{I}_n-\boldsymbol{X(X'X)^{-1}X'}))\\</script><p>因为 $(\boldsymbol{X’X})^{-1}=\boldsymbol X^{-1}{\boldsymbol X’}^{-1}$, 所以：$\boldsymbol{I}_n-\boldsymbol{X(X’X)^{-1}X’}=0$</p>
<p>因此$\text{Cov}=0$​ ，得证</p>
<h2 id="中心化和标准化"><a href="#中心化和标准化" class="headerlink" title="中心化和标准化"></a>中心化和标准化</h2><h3 id="中心化"><a href="#中心化" class="headerlink" title="中心化"></a>中心化</h3><h4 id="矩阵知识补充"><a href="#矩阵知识补充" class="headerlink" title="矩阵知识补充"></a>矩阵知识补充</h4><p>假定$\boldsymbol A$ 是$m\times m$ 可逆矩阵，$\boldsymbol B$是 $m\times n$矩阵，$\boldsymbol C$ 是$n\times m$ 矩阵，$\boldsymbol D$是$n\times n$ 矩阵。如果$\boldsymbol D-\boldsymbol{CA^{-1}B}$ 是$n\times n$ 可逆矩阵，那么：</p>
<script type="math/tex; mode=display">
\pmatrix{A &B\\C&D}^{-1}=\pmatrix{E_{11}&E_{12}\\E_{21}&E_{22}}</script><p>其中，</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\boldsymbol E_{11}=\boldsymbol A^{-1}+\boldsymbol A^{-1}\boldsymbol B(\boldsymbol D-\boldsymbol {CA}^{-1}\boldsymbol B)^{-1}\\
&\boldsymbol E_{12}=-\boldsymbol A^{-1}\boldsymbol B(\boldsymbol D-\boldsymbol {CA}^{-1}\boldsymbol B)^{-1}\\
&\boldsymbol E_{21} = -(\boldsymbol D-\boldsymbol {CA}^{-1}\boldsymbol B)^{-1}\boldsymbol C\boldsymbol A^{-1}\\
&\boldsymbol E_{22}= (\boldsymbol D-\boldsymbol {CA}^{-1}\boldsymbol B)^{-1}
\end{aligned}</script><h4 id="中心化步骤"><a href="#中心化步骤" class="headerlink" title="中心化步骤"></a>中心化步骤</h4><p>所谓中心化，就是把矩阵的中心拉到0 ，我们用$x_{ij}^<em>,y_i^</em>,\boldsymbol X^*$ 来表示中心化后的数据：</p>
<script type="math/tex; mode=display">
x^*_{ij} = x_{ij}-\overline x_j,~~~\overline x_j = n^{-1}\sum_{i=1}^nx_{ij}\\
y_i^* = y_i-\overline y,~~~~\overline y=n^{-1}\sum_{i=1}^ny_i\\</script><p>令</p>
<script type="math/tex; mode=display">
\cases{\boldsymbol y^* = (y_1^*,\cdots,y_n^*)'\\~\\
\boldsymbol X_c = (\boldsymbol x_1^*,\cdots,\boldsymbol x_p^*)\\~\\
\boldsymbol X^* = (\boldsymbol 1_n,\boldsymbol X_c)}\\</script><p>其中，$\boldsymbol x<em>j^*=(x</em>{1j}^<em>,\cdots,x_{nj}^</em>)’$</p>
<h4 id="中心化前后的关系"><a href="#中心化前后的关系" class="headerlink" title="中心化前后的关系"></a>中心化前后的关系</h4><p>标准化前，原数据集为：</p>
<script type="math/tex; mode=display">
\cases{\boldsymbol y=(y_1,\cdots,y_n)'\\~\\
\boldsymbol X=(\boldsymbol 1_n,\boldsymbol X_o),\boldsymbol X_o=(\boldsymbol x_1,\cdots,\boldsymbol x_p) 
}</script><p>因为根据模型，$\boldsymbol X$​的第一列都是1，所以在上面做一个拼接</p>
<hr>
<p>最小二乘估计为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\boldsymbol {\hat\beta}=(\boldsymbol {X'X})^{-1}\boldsymbol {X'y}\\~\\
=\pmatrix{n&\boldsymbol 1_n'\boldsymbol X_o\\
\boldsymbol X_o'\boldsymbol 1_n&\boldsymbol X_o'\boldsymbol X_o}^{-1}\pmatrix{\boldsymbol 1_n'\\ \boldsymbol X_o'}\boldsymbol y\\~\\
=\pmatrix{n^{-1}+n^{-2}\boldsymbol 1_n'\boldsymbol {X_oA_oX'_o1}_n& -n^{-1}\boldsymbol 1'_n\boldsymbol {X_oA_o}\\
-n^{-1}\boldsymbol {A_oX'_o}\boldsymbol1_n&\boldsymbol A_o }\pmatrix{\boldsymbol 1_n'\\\boldsymbol X_o'}\boldsymbol y\\~\\
=\pmatrix{n^{-1}\boldsymbol 1_n'+n^{-2}\boldsymbol 1_n'\boldsymbol {X_oA_oX_o'1_n1_n'}-n^{-1}\boldsymbol 1_n'\boldsymbol {X_oA_oX_o'}\\-n^{-1}\boldsymbol {A_oX_o'1_n1_n'}+\boldsymbol A_o\boldsymbol X_o'}\boldsymbol y
\end{aligned}</script><p>其中 $\boldsymbol A_o=(\boldsymbol X_o’\boldsymbol X_o-n^{-1}\boldsymbol X_o’\boldsymbol 1_n\boldsymbol 1_n’\boldsymbol X_o)^{-1}$</p>
<hr>
<p>对于中心化的数据，我们有相似的：</p>
<script type="math/tex; mode=display">
=\pmatrix{n^{-1}\boldsymbol 1_n'+n^{-2}\boldsymbol 1_n'\boldsymbol {X_cA_cX_c'1_n1_n'}-n^{-1}\boldsymbol 1_n'\boldsymbol {X_cA_cX_c'}\\-n^{-1}\boldsymbol {A_cX_c'1_n1_n'}+\boldsymbol A_c\boldsymbol X_c'}\boldsymbol y^*</script><p>其中 $\boldsymbol A_c=(\boldsymbol X_c’\boldsymbol X_c-n^{-1}\boldsymbol X_c’\boldsymbol 1_n\boldsymbol 1_n’\boldsymbol X_c)^{-1}$​​</p>
<hr>
<p>中心化的因变量与为中心化的因变量之间的关系：</p>
<script type="math/tex; mode=display">
\boldsymbol y^* = \boldsymbol y-\boldsymbol 1_n(\boldsymbol1_n'\boldsymbol 1_n)^{-1}\boldsymbol 1_n' \boldsymbol y = (\boldsymbol I_n-\boldsymbol H_{1n})\boldsymbol y</script><p>其中 $\boldsymbol H_{1n} = \boldsymbol 1_n(\boldsymbol 1_n’\boldsymbol1_n)^{-1}\boldsymbol1_n’$​是<strong>对称幂等</strong>矩阵, 即一个$\boldsymbol 1_n$​张成的帽子矩阵</p>
<p>中心化的自变量与未中心化的自变量之间的关系：</p>
<script type="math/tex; mode=display">
\boldsymbol X_c = \boldsymbol X_o-\boldsymbol1_n(\boldsymbol1'_n\boldsymbol1_n)^{-1}\boldsymbol 1_n'\boldsymbol X_o\\
=(\boldsymbol I_n-\boldsymbol1_n(\boldsymbol1'_n\boldsymbol1_n)^{-1}\boldsymbol1_n')\boldsymbol X_o\\
=(\boldsymbol I_n-\boldsymbol H_{1n})\boldsymbol X_o</script><p>而且  $\boldsymbol1<em>n$​向量和它张成的空间的补空间$(\boldsymbol I_n-\boldsymbol H</em>{1n})$​​ 是垂直的，可以有如下证明</p>
<script type="math/tex; mode=display">
\boldsymbol1_n'(\boldsymbol I_n-\boldsymbol H_{1n}) = \boldsymbol1_n'-\boldsymbol1_n'\boldsymbol H_{1n}\\
=\boldsymbol1_n'-\boldsymbol1_n'\boldsymbol1_n(\boldsymbol1_n'\boldsymbol1_n)^{-1}\boldsymbol1_n'=0</script><hr>
<p>然后我们要找$\boldsymbol A_c$和$\boldsymbol A_o$的关系</p>
<script type="math/tex; mode=display">
\boldsymbol A_c=(\boldsymbol X_c'\boldsymbol X_c-n^{-1}\boldsymbol X_c'\boldsymbol 1_n\boldsymbol 1_n'\boldsymbol X_c)^{-1}\\
= \boldsymbol X_o'(\boldsymbol I_n-\boldsymbol H_{1n})^2 \boldsymbol X_o-n^{-1} \boldsymbol X_c'(\boldsymbol I_n-\boldsymbol H_{1n})\boldsymbol 1_n\boldsymbol 1_n'(\boldsymbol I_n-\boldsymbol H_{1n})\boldsymbol X_o)^{-1}\\</script><p>因为前面的$(\boldsymbol I<em>n-\boldsymbol H</em>{1n})$是幂等且对称的矩阵 </p>
<p>后面 $(\boldsymbol X<em>o’(\boldsymbol I_n-\boldsymbol H</em>{1n})\boldsymbol X<em>o)^{-1} = \boldsymbol A_0$中有$1_n’(\boldsymbol I_n-\boldsymbol H</em>{1n})$​ 因此直接等于0 </p>
<p>因此：</p>
<script type="math/tex; mode=display">
\boldsymbol A_c =\boldsymbol X_o'(\boldsymbol I_n-\boldsymbol H_{1n})\boldsymbol X_o = \boldsymbol A_o</script><hr>
<p>已知经验回归方程为：</p>
<script type="math/tex; mode=display">
\hat y = \hat \beta_{\text{intercept}}+\boldsymbol x'\boldsymbol {\hat\beta}_{text{slope}}</script><p>我们把中心化的数据用到最小二乘估计模型中，即估算<strong>回归常数</strong>和<strong>回归系数</strong>：</p>
<script type="math/tex; mode=display">
\boldsymbol {\hat\beta_c}=(\boldsymbol {\hat\beta}_{\text{c,intercept}},\boldsymbol {\hat\beta}_{\text{c,slope}})'</script><h5 id="估计-hat-beta-text-c-intercept-​​"><a href="#估计-hat-beta-text-c-intercept-​​" class="headerlink" title="估计 $\hat\beta_{\text{c,intercept}}$​​"></a>估计 $\hat\beta_{\text{c,intercept}}$​​</h5><p>对于回归常数，我们代入数据</p>
<script type="math/tex; mode=display">
\boldsymbol {\hat\beta}_{\text{c,intercept}} = (n^{-1}\boldsymbol 1_n'+n^{-2}\boldsymbol 1_n'\boldsymbol {X_cA_cX_c'1_n1_n'}-n^{-1}\boldsymbol 1_n'\boldsymbol {X_cA_cX_c'})\boldsymbol y^*\\</script><p>我们发现，括号中的后两项都有$\boldsymbol 1<em>n’\boldsymbol X_c’$ ，也就是 $\boldsymbol 1_n’(\boldsymbol I_n-\boldsymbol H</em>{1n})\boldsymbol X_o$ .我们刚刚证明了这是垂直的，乘积为0</p>
<p>因此：</p>
<script type="math/tex; mode=display">
\boldsymbol {\hat\beta}_{\text{c,intercept}} =n^{-1}\boldsymbol 1_n'\boldsymbol y^*\\
=n^{-1}\boldsymbol 1_n' (\boldsymbol I_n-\boldsymbol H_{1n})\boldsymbol y = 0</script><h5 id="估计-hat-beta-text-c-slope"><a href="#估计-hat-beta-text-c-slope" class="headerlink" title="估计$\hat\beta_{\text{c,slope}}$"></a>估计$\hat\beta_{\text{c,slope}}$</h5><script type="math/tex; mode=display">
\begin{aligned}
&\boldsymbol {\hat\beta_{\text{c,slope}}} = (-n^{-1}\boldsymbol A_c\boldsymbol X_c'\boldsymbol 1_n\boldsymbol 1_n'+\boldsymbol A_c\boldsymbol X_c')\boldsymbol y^*\\
&=\boldsymbol A_c\boldsymbol X_c'(\boldsymbol I_n-n^{-1}\boldsymbol 1_n\boldsymbol1_n')\boldsymbol y\\

&=\boldsymbol A_o\boldsymbol X_o'(\boldsymbol I_n-\boldsymbol H_{1n})(\boldsymbol I_n-n^{-1}\boldsymbol 1_n\boldsymbol1_n')\boldsymbol y
\end{aligned}</script><p>因为$\boldsymbol 1<em>n$ 是 竖向量，因此 $\boldsymbol1_n’\boldsymbol1_n$ 是一个常数，为 $\frac{1}{n}$因此，我们有 $\boldsymbol H</em>{1n} = \boldsymbol 1_n(\boldsymbol 1_n’\boldsymbol1_n)^{-1}\boldsymbol1_n’ =\frac{1}{n}\boldsymbol (\boldsymbol 1_n\boldsymbol 1_n’) $​</p>
<p>因此</p>
<script type="math/tex; mode=display">
\begin{aligned}
&=\boldsymbol A_o\boldsymbol X_o'(\boldsymbol I_n-\boldsymbol H_{1n})(\boldsymbol I_n-n^{-1}\boldsymbol 1_n\boldsymbol1_n')\boldsymbol y\\~~
&=\boldsymbol A_o\boldsymbol X_o'(\boldsymbol I_n-\boldsymbol H_{1n})(\boldsymbol I_n-\boldsymbol H_{1n})\boldsymbol y
=\boldsymbol A_o\boldsymbol X_o'(\boldsymbol I_n-\boldsymbol H_{1n})\boldsymbol y
\\
&\boldsymbol {\hat\beta}_{\text{c,slope}} = \boldsymbol {\hat\beta}_{\text{slope}}
\end{aligned}</script><p>采用了中心化的数据得到的经验回归方程为：</p>
<script type="math/tex; mode=display">
\hat y^* =\boldsymbol x'\hat\beta_{\text{slope}}</script><p>也就是说，$\beta_1,\beta_2\cdots,\beta_p$​的估计是不会变的</p>
<h3 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a>标准化</h3><h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4><script type="math/tex; mode=display">
x_{ij}^{**} = \frac{x_{ij}^*}{\sqrt{L_{jj}}} = \frac{x_{ij}-\overline x_j}{\sqrt{L_{jj}}},i=1,2,\cdots,n~~;j=1,2,\cdots,p\\
y_{i}^{**} = \frac{y_i^*}{\sqrt{L_{yy}}},i=1,2\cdots,n</script><p>其中，$L_{jj}$是自变量 $x_j$ 的离差平方和，即：</p>
<script type="math/tex; mode=display">
L_{jj} = \sum_{i=1}^n(x_{ij}-\overline x_j)^2</script><p>而$L_{yy}$是因变量y的离差平方和，即</p>
<script type="math/tex; mode=display">
L_{yy} = \sum_{i=1}^n(y_i-\overline y)^2</script><p>进而我们可以定义向量$\boldsymbol y^{**}$和矩阵$\boldsymbol X_s$</p>
<script type="math/tex; mode=display">
\boldsymbol y^{**} = (\frac{y_1-\overline y}{\sqrt{L_{yy}}},\cdots,\frac{y_n-\overline y}{\sqrt{L_{yy}}})' = \frac{1}{\sqrt{L_{yy}}}\boldsymbol y^*\\~\\
\boldsymbol X_s = (\frac{1}{\sqrt{L_{11}}}\boldsymbol x_1^* ,\cdots,\frac{1}{L_{pp}}\boldsymbol x_p^*)</script><p>我们从$x<em>{ij}^*$的定义可以知道，其实就是中心化的$x</em>{ij}$ ，因此$\boldsymbol X_s$可以被写为：</p>
<script type="math/tex; mode=display">
\boldsymbol X_s = \boldsymbol X_c\boldsymbol L</script><p>其中，$\boldsymbol L=diag{\frac{1}{\sqrt{L<em>{11}}},\cdots,\frac{1}{\sqrt{L</em>{pp}}}}$​</p>
<h4 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h4><p>标准化后的最小二重估计为：</p>
<script type="math/tex; mode=display">
\boldsymbol {\hat\beta}_s = (\boldsymbol{\hat\beta_{\text{s,intercept}}},\boldsymbol{\hat\beta_{\text{s,slope}}})'=(0,\boldsymbol {\hat\beta}_{\text{s,slope}})'</script><p>回归系数为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\boldsymbol{\hat\beta}_{s,slope} = (\boldsymbol {X}_s'\boldsymbol X_s)^{-1} \boldsymbol X_s' \boldsymbol y^{**}\\
&=(\boldsymbol{LX}_c'\boldsymbol X_c\boldsymbol L)^{-1}\boldsymbol L \boldsymbol X_c'\frac{1}{\sqrt{L_{yy}}}\boldsymbol y^*\\
&=\boldsymbol L^{-1}(\boldsymbol X_c'\boldsymbol X_c)^{-1}\boldsymbol L^{-1}\boldsymbol L\boldsymbol X_c'\frac{1}{\sqrt{L_{yy}}} y^*\\
&=\frac{1}{\sqrt{L_{yy}}}\boldsymbol L^{-1}(\boldsymbol X_c'\boldsymbol X_c)^{-1}\boldsymbol X_c' \boldsymbol y^*
\end{aligned}</script><p>那么$\boldsymbol{\hat\beta}<em>{\text{s,slope}} $ 和 $\boldsymbol{\hat\beta}</em>{\text{c,slope}} $ 的关系又如何呢？我们先把$\boldsymbol{\hat\beta}_{\text{c,slope}} $化简成 $\boldsymbol X_c$的表达式 </p>
<blockquote>
<p>注意： $\boldsymbol {\hat\beta_{\text{c,slope}}} $ 还不是最终形态，我们可以这样来化简：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\boldsymbol {\hat\beta_{\text{c,slope}}}  
=\boldsymbol A_c\boldsymbol X_c'(\boldsymbol I_n-\boldsymbol H_{1n})\boldsymbol y^*\\
&=(\boldsymbol X_c\boldsymbol X_c'-\frac{1}{n}\boldsymbol X_c'\boldsymbol 1_n\boldsymbol 1_n'\boldsymbol X_c)^{-1}\boldsymbol X_c'(\boldsymbol I_n-\boldsymbol H_{1n})(\boldsymbol I_n-\boldsymbol H_{1n})\boldsymbol y\\
\end{aligned}</script><p>因为上面证过了，$\boldsymbol A_c$展开后后面一项等于0，因此：</p>
<script type="math/tex; mode=display">
\boldsymbol {\hat\beta_{\text{c,slope}}}  =(\boldsymbol X_c\boldsymbol X_c')^{-1}\boldsymbol X_c'\boldsymbol y^*</script></blockquote>
<p>因此： </p>
<script type="math/tex; mode=display">
\boldsymbol{\hat\beta}_{\text{s,slope}} =\frac{1}{\sqrt{L_{yy}}}\boldsymbol L^{-1}\boldsymbol {\hat\beta}_{\text{c,slope}}</script><p>其中每一个分量为：</p>
<script type="math/tex; mode=display">
\hat\beta_{sj} = \frac{\sqrt{L_{jj}}}{\sqrt{L_{yy}}}\hat\beta_{cj} = \frac{\sqrt{L_{jj}}}{\sqrt{L_{yy}}}\hat\beta_j,j=1,2\cdots,p</script><h2 id="显著性检验"><a href="#显著性检验" class="headerlink" title="显著性检验"></a>显著性检验</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>我们做了这么多参数估计，现在我们进一步判断因变量y和自变量$x_1,x_2\cdots,x_p$ 之间是否存在显著的线性关系。</p>
<p>因此，我们可以采用F检验和t检验这两种统计检验方法：</p>
<ul>
<li>F检验： 用于检验<strong>回归方程</strong>的显著性</li>
<li>t检验：用于检验<strong>回归系数</strong>的显著性</li>
</ul>
<p>我们之前学过，在一元线性回归模型中，F检验(研究方差分析)和t检验(枢轴量法)、相关系数这三类检验是等价的</p>
<p>在多元线性回归模型中，F检验和t检验就不等价了。而且在多元线性回归中，两个变量之间的相关性是很难定义的，因为会受到其他变量的影响</p>
<h3 id="F检验"><a href="#F检验" class="headerlink" title="F检验"></a>F检验</h3><p>F检验是检验回归方程的显著性。对多元线性回归方程的显著性检验是要看自变量 $x_1,x_2\cdots,x_p$ 从整体上对因变量y是否有明显的影响</p>
<p>原假设为</p>
<script type="math/tex; mode=display">
H_0:\beta_1=\beta_2=\cdots=\beta_p = 0</script><p>备择假设为：</p>
<script type="math/tex; mode=display">
H_1:\text{存在}\beta_j\text{不为零},j=1,2\cdots,p</script><p>如果$H_0$ 为真，则表明因变量y与$x_1,x_2\cdots,x_p$ 之间的关系用线性回归模型来刻画是不合适的</p>
<h4 id="检验过程"><a href="#检验过程" class="headerlink" title="检验过程"></a>检验过程</h4><p>离差平方和：</p>
<script type="math/tex; mode=display">
\sum_{i=1}^n(y_i-\overline y)^2 =\sum_{i=1}^n(\hat y_i-\overline y)^2+\sum_{i=1}^n(y_i-\hat y_i)^2</script><p>我们记为 </p>
<script type="math/tex; mode=display">
SS_T = SS_R+SS_E</script><p>拟合值： $\hat y_i = \boldsymbol x_i’\boldsymbol {\hat\beta}$​</p>
<p>偏差： $e_i = y_i-\hat y_i$</p>
<h3 id="t检验"><a href="#t检验" class="headerlink" title="t检验"></a>t检验</h3><h2 id="置信区间与预测"><a href="#置信区间与预测" class="headerlink" title="置信区间与预测"></a>置信区间与预测</h2>
          
        
      
    </div>
    
    
    
    <div>
      
    </div>
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2021/10/31/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch7-%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/%5Bobject%20Object%5D">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/10/31/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch7-%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0/" itemprop="url">数据科学算法ch7-随机游走</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-10-31T16:54:30+08:00">
                2021-10-31
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2022-01-06T20:03:46+08:00">
                2022-01-06
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="随机游走"><a href="#随机游走" class="headerlink" title="随机游走"></a>随机游走</h1><h2 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h2><p>首先我们来复习一下联合概率分布</p>
<p>给定n个离散随机变量 $X_1,X_2,\cdots,X_n$ 分别取值为$x_1,x_2\cdots,x_n$的联合概率 $f(x_1,x_2\cdots,x_n)$ 表示为：</p>
<script type="math/tex; mode=display">
f(x_1,x_2\cdots,x_n) = P(X_1 = x_1,X_2= x_2,\cdots,X_n=x_n)</script><p>如果n个离散随机变量$X_1,X_2\cdots,X_n$ 是相互独立的，那么它们取值$x_1,x_2\cdots,x_n$ 的联合概率可以很方便得计算为</p>
<script type="math/tex; mode=display">
f(x_1\cdots,x_n) =\prod_{i=1}^nP(X_i=x_i)=\prod_{i=1}^n f_{X_i}(x_i)</script><p>但是，如果$X$之间并不相互独立，那么</p>
<script type="math/tex; mode=display">
f(x_1,\cdots,x_n) = \prod_{i=2}^m P(X_i = x_i|X_{i-1}=x_{i-1},\cdots,X_1 = x_1)\\</script><p>这就是链式法则</p>
<p>但是，对于大规模的数据，会很难处理，因为联合概率的计算复杂度会非常高。此时，需要对数据间的相关关系进行简化。对文本数据或者音频数据建模时，常常假设序列数据点之间存在<strong>一阶相关</strong>关系，即</p>
<script type="math/tex; mode=display">
P(X_i = x_i|X_{i-1}=x_{i-1},\cdots,X_1 = x_1) =P(X_i = x_i |X_{i-1}=x_{i-1})</script><p>因为有些词常常是一前一后出现的。比如在”今天天气炎热，我们去游泳“这一句话中，”天气”与“炎热”这两个词常常一前一后出现，但是“天气”和”游泳”不太可能会出现前后出现。</p>
<p>同样的，对股票价格数据建模时，会假设序列数据点之间存在<strong>t阶相关</strong>关系，即</p>
<script type="math/tex; mode=display">
P(X_i = x_i|X_{i-1}=x_{i-1},\cdots,X_1 = x_1) = P(X_i = x_i|X_{i-1} = x_{i-1}\cdots,X_{i-t}=x_{i-t})</script><h2 id="随机过程"><a href="#随机过程" class="headerlink" title="随机过程"></a>随机过程</h2><p>现在我们来学习随机过程，随机过程${X(t,\omega),t\in T ,\omega\in \Omega}$ 是与时间t有关的一个随机变量序列，现实生活中有很多问题都可以用随机过程来建模，比如说文本数据、语音数据、股票价格和用户上网行为等等。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>例子</th>
<th>时间集合</th>
<th>状态空间</th>
</tr>
</thead>
<tbody>
<tr>
<td>浏览网页</td>
<td>步数</td>
<td>所有网页</td>
</tr>
<tr>
<td>自然语言</td>
<td>单词位置</td>
<td>字典</td>
</tr>
<tr>
<td>股票价格</td>
<td>交易日</td>
<td>价格</td>
</tr>
<tr>
<td>赌博</td>
<td>下注次数</td>
<td>筹码数量</td>
</tr>
<tr>
<td>液体分子</td>
<td>时间</td>
<td>容器空间</td>
</tr>
</tbody>
</table>
</div>
<p>现在我们来具体讲几个随机过程的例子：</p>
<h4 id="抛币游戏"><a href="#抛币游戏" class="headerlink" title="抛币游戏"></a>抛币游戏</h4><p>游戏规则：</p>
<ul>
<li>假设开始有 $10, 参加一场公平无限期抛硬币游戏<ul>
<li>正面朝上，赢得1</li>
<li>反面朝上，输掉1</li>
</ul>
</li>
<li>我们用$X_n$ 表示n次抛币后手上剩余的美元</li>
<li>如果$X_0 = 10$,则 ${X_n:n\in N}$ 构成一个随机序列<ul>
<li>假设$X<em>n = 12$，则$X</em>{n+1}=11$或者13</li>
<li>在第(n+1)次抛币后手上剩余的美元和$X<em>{n-1}$​​​​​值没有关系。是一阶相关关系，即：$P(X</em>{i+1} = x<em>{i+1}|X</em>{i}=x<em>{i},\cdots,X_1 = x_1) = P(X</em>{i+1} = x<em>{i+1}|X</em>{n} = x_{n})$​​​​​</li>
</ul>
</li>
</ul>
<p>在这个抛币游戏中，手上剩余的钱是一个随机过程</p>
<ul>
<li>时间：抛币次数</li>
<li>状态：非负集合</li>
</ul>
<p>在这个游戏中，稳定状态就是输光所有的钱</p>
<h4 id="象棋"><a href="#象棋" class="headerlink" title="象棋"></a>象棋</h4><p>象棋中帅的位置变化</p>
<ul>
<li>按照规则，帅在每个位置都可以向周围移动</li>
<li>$X_n$ 表示n步后帅所在的位置<ul>
<li>如果 $X<em>n = 2$, 则$X</em>{n+1}=1,3,5$</li>
<li>$(n+1)$步帅所在的位置和$X<em>{n-1}$ 的值没有关系，即：$P(X</em>{i+1} = x<em>{i+1}|X</em>{i}=x<em>{i},\cdots,X_1 = x_1) = P(X</em>{i+1} = x<em>{i+1}|X</em>{n} = x_{n})$​</li>
</ul>
</li>
</ul>
<p>因此${X_n:n\in N}$ 构成一个随机过程</p>
<ul>
<li>时间：步数</li>
<li>状态：1到9之间的整数</li>
</ul>
<p><img src="/2021/10/31/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch7-%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0/1.png" alt="1" style="zoom:67%;"></p>
<h4 id="网页浏览"><a href="#网页浏览" class="headerlink" title="网页浏览"></a>网页浏览</h4><p>每个网页都有可能对外的锚连接，</p>
<ul>
<li>$X_n$ 表示n步之后停留的页面<ul>
<li>如果$X=ECNU$， 则$X_{n+1}$ 可以是 MOE、DASE、或者SEI</li>
<li>(n+1)步停留的页面和 $X<em>{n-1}$的所停留的页面无关，即$P(X</em>{i+1} = x<em>{i+1}|X</em>{i}=x<em>{i},\cdots,X_1 = x_1) = P(X</em>{i+1} = x<em>{i+1}|X</em>{n} = x_{n})$</li>
</ul>
</li>
</ul>
<p>因此${X_n:n\in N}$ 构成一个随机过程</p>
<ul>
<li>时间：步数</li>
<li>状态：所有网页组成的集合</li>
</ul>
<h3 id="马尔科夫过程"><a href="#马尔科夫过程" class="headerlink" title="马尔科夫过程"></a>马尔科夫过程</h3><h4 id="马尔科夫性质"><a href="#马尔科夫性质" class="headerlink" title="马尔科夫性质"></a>马尔科夫性质</h4><p>如果随机过程 ${X(t)\in S,t\in I }$​ , 满足</p>
<script type="math/tex; mode=display">
P(X(t_{n+1}) = x_{n+1}|X({t_0})=x_{0},\cdots,X(t_n) = x_n) = P(X(t_{n+1}) = x_{n+1}|X(t_{n}) = x_{n})</script><p>其中，$t<em>0&lt;t_1&lt;\cdots&lt;t_n&lt;t</em>{n+1}\in I,x_i\in S$​,则称该随机过程满足马尔科夫性质</p>
<ul>
<li>如果 $t<em>n$ 表示当前时间，$t</em>{n+1}$ 表示将来，$t<em>1,\cdots,t</em>{n-1}$ 则表示过去</li>
<li>可以理解为：已知现在，将来与过去独立</li>
</ul>
<p>之前我们说的，咖啡溶质的扩散、网络、赌博、象棋，都是满足马尔科夫性质</p>
<h4 id="马尔科夫过程-1"><a href="#马尔科夫过程-1" class="headerlink" title="马尔科夫过程"></a>马尔科夫过程</h4><p>若随机过程满足马尔科夫性质，则称其为<strong>马尔科夫过程</strong></p>
<p>设 ${X(t),t\in T}$ 的空间状态为S，如果$\forall n \geq 0,\forall t<em>0&lt;t_1&lt;\cdots&lt;t_n&lt;t</em>{n+1}\in T$,在条件  $X(t<em>i) = x_i,x_i\in S,i=0,1\cdots,n$ 下，$X(t</em>{n+1})$ 的条件概率刚好等于在条件$X(t_n)=x_n$ 下的条件概率，即：</p>
<script type="math/tex; mode=display">
P(X(t_{n+1}) = x_{n+1}|X({t_0})=x_{0},\cdots,X(t_n) = x_n) = P(X(t_{n+1}) = x_{n+1}|X(t_{n}) = x_{n})</script><p>则称${X(t),t\in T}$ 为马尔科夫过程</p>
<h4 id="马尔科夫链"><a href="#马尔科夫链" class="headerlink" title="马尔科夫链"></a>马尔科夫链</h4><p>马尔科夫链是特殊的马尔科夫过程，它满足：</p>
<ul>
<li>离散的时间集合：步数、下注次数</li>
<li>离散的状态空间：网页、计数 </li>
</ul>
<p>因此，布朗运动是马尔科夫过程，但不是马尔科夫链。</p>
<p>在本章举的例子，马尔科夫过程等于马尔科夫链</p>
<h4 id="转移概率"><a href="#转移概率" class="headerlink" title="转移概率"></a>转移概率</h4><p>假定随机游走的状态空间为 $\Omega =[n]$ 矩阵 $\bold{P}^{(t+1)}\in\mathbb R^{n\times n}$ 被称为该随机游走的<strong>第(t+1)步概率转移矩阵</strong>，如果$p<em>{x,y}^{t+1} = P(X</em>{t+1}=y|X_t=x)$​</p>
<ul>
<li>其中 $p_{x,y}^{(t+1)}$ 表示在$(t+1)$ 步随机游走从状态x转移到y的概率</li>
<li>概率转移矩阵的每行概率和为1，即：</li>
</ul>
<script type="math/tex; mode=display">
\sum_y p_{x,y}^{t+1} = \sum_{y} \frac{P(X_{t+1}=y|X_t= x)}{P(X_t = x)}=1</script><p><img src="/2021/10/31/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch7-%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0/3.png" alt="1" style="zoom:67%;"></p>
<h4 id="齐次马氏链"><a href="#齐次马氏链" class="headerlink" title="齐次马氏链"></a>齐次马氏链</h4><p>如果随机游走满足：</p>
<script type="math/tex; mode=display">
P(X_{t+s}=y|X_t = x)= P(X_s = y|X_0 = x)</script><ul>
<li>即经过t步后，从状态x走到状态y的概率与起始步数s无关，则该随机游走是齐次的</li>
</ul>
<p>也就是说，无论当前是第几步，只要起点相同，一步转移到目标顶点的概率是相同的马氏链被称为齐次马氏链。</p>
<p>例如：在抛币游戏中，无论是开始手上有10元，还是第100次抛币之后手上有10元，下一步手上有11元的概率是相通的，也就是说，抛币游戏中的马氏链是齐次的。也就是说是无记忆性的</p>
<p>例如：</p>
<p><img src="/2021/10/31/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch7-%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0/4.png" style="zoom:100%;"></p>
<p>给定图$G=(V,E)$ 如上图，假设$S=\mathbb Z$ ，无论当前是什么时刻，只要当前在状态a，那么下一步的转移概率为：</p>
<script type="math/tex; mode=display">
p_a(a-1)=\frac{1}{2}\\p_a(a+1)=\frac{1}{2}\\p_{ab} = 0(b\neq a\pm 1)</script><p>因此，这个马尔科夫链也是齐次的。如果$\pi(a)$ 表示该马尔科夫链初始时刻在状态a的概率，那么$\pi(10)=1,\pi(a)=0(a\neq 10)$ 意味着在时刻0马尔科夫链一定是从坐标10出发的</p>
<h3 id="随机游走-1"><a href="#随机游走-1" class="headerlink" title="随机游走"></a>随机游走</h3><p>在学习随机游走之前我们需要了解以下定义：图、度、无向图、路劲、连通分量、邻接矩阵、邻接表</p>
<p>现给出随机游走的定义：随机游走模型是针对图建立的抽象马氏链模型，因此图上的马氏链都可以被称为<strong>随机游走</strong>。图上的随机游走是指给定一个图和一个出发点，随机地选择一个邻居节点，移动到邻居节点上，然后把当前节点作为出发点，重复以上过程。那些被随机选出的节点序列就构成了一个该图上的随机游走。</p>
<p>我们可以将随机游走和之前说的齐次马尔科夫链结合起来理解：现在有状态转移图如下：</p>
<p><img src="/2021/10/31/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch7-%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0/5.png" style="zoom:100%;"></p>
<p>假设$\pi(a) = 1,\pi(i) = 0(i\neq a)$ 意味着在时刻0该随机游走从状态a出发。在第一步，该随机游走可能到达顶点c、e、f、g. 随机游走会随机选择一个顶点，其中：</p>
<script type="math/tex; mode=display">
p_{ac}=p_{ae}=p_{af}=p_{ag}= \frac{1}{4}\\</script><p> 走到第二个点，又有好几种不同的选择。以此类推，可以得到在图G上的一个随机序列。因此，随机游走是一类<strong>特殊的马氏链</strong></p>
<p>无论何时出发，只要当前停留在状态a,那么下一步到达c、e、f、g的概率是相同的。因此，该随机游走对应的马氏链也是齐次的</p>
<h3 id="平稳分布"><a href="#平稳分布" class="headerlink" title="平稳分布"></a>平稳分布</h3><p>之前我们说了概率转移矩阵，基于概率转移矩阵我们提出了平稳分布的概念。</p>
<h4 id="状态分布"><a href="#状态分布" class="headerlink" title="状态分布"></a>状态分布</h4><p>我们令$\pi^{(t)}$为状态空间为S的马氏链<strong>在时刻t</strong>的状态分布，即：</p>
<script type="math/tex; mode=display">
\pi_x^{(t)} = P(X_t = x)</script><p>特别地，将$\pi^{(0)}$ 称为初始分布。</p>
<p>进一步，对于一个概率转移矩阵为P的马氏链，其状态分布$\pi^{(t)}$ 具有如下性质</p>
<ul>
<li>$\pi^{(t)}$​ 的每个分量满足$0\leq\pi_i^{(t)}\leq 1$</li>
<li>$\sum_{j\in S}\pi_j^{(t)} = 1$ 即每行的和为1</li>
<li>$\pi^{(t+1)}= \pi^{(t)}P(t)$</li>
</ul>
<p>比如对于矩阵：</p>
<script type="math/tex; mode=display">
P=\begin{pmatrix}\frac{1}{2}&\frac{1}{2}&0&0\\\frac{1}{3}&\frac{2}{3}&0&0\\0&0&\frac{3}{4}&\frac{1}{4}\\0&0&\frac{1}{4}&\frac{3}{4} \end{pmatrix}</script><p>我们给定第t步的状态分布为$\pi^{(t)}=(0.4,0.6,0,0)$,那么:</p>
<script type="math/tex; mode=display">
\pi^{(t+1)} = \pi^{(t)} P = (0.4,0.6,0,0)\\</script><p>我们给定第t步的状态分布为$\pi^{(t)}=(0,0,0.5,0.5)$​,那么:</p>
<script type="math/tex; mode=display">
\pi^{(t+1)} = \pi^{(t)}P=(0,0,0.5,0.5)\\</script><p>我们给定第t步的状态分布为$\pi^{(t)}=(0.1,0.9,0,0)$​​,那么:</p>
<script type="math/tex; mode=display">
\pi^{(t+1)} = \pi^{(t)}P=(0.35,0.65,0,0)\\</script><h4 id="平稳分布-1"><a href="#平稳分布-1" class="headerlink" title="平稳分布"></a>平稳分布</h4><p>对于一个转移概率矩阵为$\boldsymbol P$ 的有限状态马氏链，若状态分布$\pi$满足：</p>
<script type="math/tex; mode=display">
\pi\boldsymbol P = \pi</script><p>则称$\pi$​ 为该马氏链的平稳分布· </p>
<p>有矩阵为 $P =\begin{pmatrix}\frac{1}{2}&amp;\frac{1}{2} \\frac{1}{4}&amp;\frac{3}{4} \end{pmatrix} $​ 试求其平稳分布：</p>
<p>我们可以令$\pi = (x,y)$​, 那么 $\cases{\frac{1}{2}x+\frac{1}{4}y = x\\frac{1}{2}x+\frac{3}{4}y = y}$ 解得$y=2x$，又要满足平稳分布的行和为1，所以解为：$(\frac{1}{3},\frac{2}{3})$</p>
<h4 id="可约与不可约"><a href="#可约与不可约" class="headerlink" title="可约与不可约"></a>可约与不可约</h4><p>如果从状态x可以通过有限步转移到达状态y,并且从状态y可以通过有限步转移到达x，那么称状态x与y是<strong>连通</strong>的。如果马氏链中任意两个状态都是连通的，那么称该马氏链是<strong>不可约</strong>的</p>
<ul>
<li>马氏链中任意两个状态都是联通的，那么就意味着存在一个n使得矩阵$\boldsymbol P^{(n)}$ 中任意一个元素都大于0</li>
<li>如果一张图是<strong>强连通</strong>的，那么它一定是不可约的，否则这个图所对应的马氏链是<strong>可约的</strong></li>
</ul>
<p>比如对于这张图，马氏链是可月的，因为对于状态2，他没有出边，因此它无法通过有限步到达其他状态。因此状态2与其他状态并不连通，因此该马尔科夫链是可约的</p>
<p><img src="/2021/10/31/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch7-%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0/6.png" style="zoom:100%;"></p>
<h4 id="反周期"><a href="#反周期" class="headerlink" title="反周期"></a>反周期</h4><p>状态x的周期$d<em>x$ 是集合${n|(P^n)</em>{x,x}&gt;0}$的<strong>最大公约数</strong>，特别地，如果$\forall n\geq 1,(\boldsymbol P^{(n)})_{x,x}=0$,那么$d_x=\infty$ 。而如果<strong>一个马氏链所有的状态的周期都为1</strong>，则称这个马氏链是反周期的</p>
<p><img src="/2021/10/31/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch7-%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0/7.jpeg" style="zoom:100%;"></p>
<p>比如说对于这张图，对于每一个状态其集合都为${2,4,6\cdots}$ 其最大公约数为2，因此这个马氏链是周期的。 </p>
<h2 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h2><h3 id="1"><a href="#1" class="headerlink" title="1"></a>1</h3><p>给定一个转移矩阵 $P$ 和状态向量$\pi$ </p>
<script type="math/tex; mode=display">
\boldsymbol P = \begin{pmatrix}0.5&0.5\\0.25&0.75\end{pmatrix}&\pi =  \begin{pmatrix}1\\0\end{pmatrix}</script><ul>
<li>证明 $\pi P^n$ 的结果接近一个常数向量</li>
</ul>
<p>这个常数向量，就是平稳分布。</p>
<p>我们令 $\pi P^n = (x<em>{(n,1)},x</em>{(n,2)})$，那么，对于$n\geq 1$, 我们有：</p>
<script type="math/tex; mode=display">
x_{(n,1)} = \frac{1}{2}x_{(n-1,1)} +\frac{1}{4}x_{(n-1,2)}\\
x_{(n,2)} = \frac{1}{2}x_{(n-1,1)} +\frac{3}{4}x_{(n-1,2)}\\</script><p>由于状态向量$\pi$ 的向量和一定为1，因此 $x<em>{(n-1,2)}=1-x</em>{(n-1,1)}$ ,带入得到：</p>
<script type="math/tex; mode=display">
\begin{align}
x_{(n,1)} &= \frac{1}{2}x_{(n-1,1)}+\frac{1}{4}(1-x_{(n-1,1)})\\~\\
&=\frac{1}{4}x_{(n-1,1)} +\frac{1}{4}
\end{align}</script><p>这是一个很简单的一阶递推</p>
<script type="math/tex; mode=display">
(x_{(n,1)}-\frac{1}{3})=\frac{1}{4}(x_{(n-1,1)}-\frac{1}{3})</script><p>解得：</p>
<script type="math/tex; mode=display">
x_{(n,1)} = \frac{2}{3}\times (\frac{1}{4})^n+\frac{1}{3},n\geq 1</script><p>当$n\rightarrow \infty$ 时，$x<em>{(n,1)}=\frac{1}{3}$ ，进而可以知道 $x</em>{(n,2)}= \frac{2}{3}$ </p>
<h3 id="2"><a href="#2" class="headerlink" title="2"></a>2</h3><p>对于一个转移概率矩阵为 $\boldsymbol P$ 的马氏链</p>
<script type="math/tex; mode=display">
\boldsymbol P = \begin{pmatrix}0.5&0.5\\p&1-p\end{pmatrix}\\</script><p>已知实验多次之后，转移概率矩阵对应的马氏链停在状态1的概率大约为 $20\%$, 停在状态2的概率为$80\%$ ，p为未知参数</p>
<ol>
<li>计算未知参数$p$ 的合理估计值，并给出解释</li>
</ol>
<p>首先，根据题意是我们知道最终的平稳分布 $(0.2,0.8)$ ，又 $\pi \boldsymbol P = \pi $ ，可知 $0.1+0.8p = 0.2~~p=\frac{1}{8}$</p>
<ol>
<li>这个马氏链是否可约，是否反周期？</li>
</ol>
<p>是否可约呢？我们看到这边只有两个状态，两个状态是相互联通的，因此是不可约的。</p>
<p>如图：</p>
<p><img src="/2021/10/31/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch7-%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0/8.jpeg" style="zoom:100%;"></p>
<p>那么是否反周期呢？我们看到，状态1、2的集合为：${1,2\cdots}$ ，其最大公约数都是1，因此这个马氏链是反周期的。</p>
<h3 id="3"><a href="#3" class="headerlink" title="3"></a>3</h3><p><img src="/2021/10/31/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch7-%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0/9.png" style="zoom:100%;"></p>
<ul>
<li>对于7.5<ul>
<li>状态1和状态2都是相互连通的，因此是不可约的</li>
<li>状态1的集合为 ${1,2,\cdots}$ ，状态2的集合为 ${2,3,4\cdots}$ ，两者最大公约数都是1，周期都是1，因此是反周期的</li>
</ul>
</li>
<li>对于7.6<ul>
<li>一共有4个状态，所有状态都是连通的，因此是不可约的</li>
<li>四个状态，每个状态的集合都是 ${2,4,6,\cdots}$ ，最大公约数都是2，因此是非反周期(周期)的</li>
</ul>
</li>
</ul>
<h2 id="Page-Rank"><a href="#Page-Rank" class="headerlink" title="Page Rank"></a>Page Rank</h2>
          
        
      
    </div>
    
    
    
    <div>
      
    </div>
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/%5Bobject%20Object%5D">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/" itemprop="url">数据科学算法ch5-数据流模型及频繁项挖掘</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-10-29T21:51:03+08:00">
                2021-10-29
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2024-01-10T09:55:43+08:00">
                2024-01-10
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="数据流模型及频繁项挖掘"><a href="#数据流模型及频繁项挖掘" class="headerlink" title="数据流模型及频繁项挖掘"></a>数据流模型及频繁项挖掘</h1><p>在这一讲我们要学习什么是数据流，什么是数据流算法，怎么去评价一个数据流算法的好坏。</p>
<h2 id="数据流和数据流模型"><a href="#数据流和数据流模型" class="headerlink" title="数据流和数据流模型"></a>数据流和数据流模型</h2><p>首先我们要认识数据流，其具有一下几个特征</p>
<ul>
<li>数据总量不受限制，因此我们很难准确估算数据量大小</li>
<li>数据到达速率块，比如大型强子对撞机可以每秒产生40EB的数据</li>
<li>数据到达次序不受约束，我们无法预测数据的到达次序。</li>
<li>除非可以保存，每个数据只能”看“一次。因为很少有系统能把海量的数据都放在内存中，而且数据到达的速度很快，分析的时间很短，否则就会出现数据拥塞</li>
</ul>
<p>我们可以将<strong>数据流</strong>看做是一个无限的元组序列 $\sigma= <a_1,\cdots,a_m,\cdots>$; 在某个固定点一个数据流可以被视作一个长度为d的固定数组 $A[0,\cdots,d-1]$</a_1,\cdots,a_m,\cdots></p>
<p>因此我们对处理数据流的算法也提出了如下要求：</p>
<ul>
<li>实时性：实时、连续地输出查询结果</li>
<li>低空间复杂度：数据流规模理论上是无限的，为了保证算法高效稳定运行，需降低算法的空间开销</li>
<li>结果准确性：数据规模大，速率快，因此对于一些复杂问题，不太可能通过数据的一次遍历就获得准确答案。在实际应用中，往往不要求精准的查询结果</li>
<li>适应性：在很多应用中，涉及多个数据流的处理，需设计具备适应性的算法</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>传统算法</th>
<th>数据流算法</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据类型</td>
<td>有限&amp;静态</td>
<td>无限&amp;动态&amp;高速</td>
</tr>
<tr>
<td>存储</td>
<td>硬盘</td>
<td>内存&amp;空间限制</td>
</tr>
<tr>
<td>效率</td>
<td>非实时</td>
<td>实时&amp; Ad-hoc</td>
</tr>
<tr>
<td>返回值类型</td>
<td>精确/近似</td>
<td>近似</td>
</tr>
</tbody>
</table>
</div>
<p>因为数据流的大小是不确定的，因此我们希望算法的空间消耗和时间消耗是<strong>次线性</strong>的($O(\log n)$​)，或者与<strong>流的大小无关</strong>。</p>
<p><img src="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/1.png" style="zoom:67%;"></p>
<h3 id="数据流子模型"><a href="#数据流子模型" class="headerlink" title="数据流子模型"></a>数据流子模型</h3><h4 id="按照数据流中数据元素对数组A的不同影响方式划分"><a href="#按照数据流中数据元素对数组A的不同影响方式划分" class="headerlink" title="按照数据流中数据元素对数组A的不同影响方式划分"></a>按照数据流中数据元素对数组A的不同影响方式划分</h4><p>数据流模型可以划分为三个子模型：时间序列模型，收音机模型，十字转盘模型</p>
<ol>
<li>时间序列模型。每个数据项 $a_i = A[i]$ 按照i增加的顺序出现，相当于时间序列</li>
<li>收音机模型。每个数据项$a<em>i=(j,I_i),I\geq 0$ 表示对$A[i]$的增量，即$A_i[j] = A</em>{i-1}[j]+I_{i_0}$ 如同多个收音机一样，多个$a_i$ 表示一段时间上$A[j]$的增量。这是一种更为普遍的数据流模型</li>
<li>十字转盘模型。每个数据项$a<em>i= (j,U_i)$ 表示对$A[j]$ 的更新，即 $A_i[j]=A</em>{i-1}[j]+U_i$ ，其中$U_i$可正可负。该模型适合描述数据流动态的出入状态，单位时间内流入和流出的数量不必相同</li>
</ol>
<h4 id="按照数据流上个元素的重要程度划分"><a href="#按照数据流上个元素的重要程度划分" class="headerlink" title="按照数据流上个元素的重要程度划分"></a>按照数据流上个元素的重要程度划分</h4><p>数据流可以分成另外三种子模型：界标模型、滑动窗口模型、衰减窗口模型</p>
<ol>
<li>界标模型。如下图，界标模型有起始点和终止点两个时间点。其中起始点是固定的，终止点随时间推移而不断递增。在界标的任意终点和起点之间，数据的重要性均等。这是最容易理解的模型</li>
</ol>
<p><img src="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/2.png" style="zoom:67%;"></p>
<ol>
<li>滑动窗口模型。令W为窗口大小，在这个模型中，数据流算法仅仅考虑最近的W个元素，即查询范围是$[\max (0,d-W),\cdots,d-1]$ .处在查询范围之外的元素其重要性为0；处在查询范围之内的元素重要性均等。滑动窗口模型也有起始点和终止点，但是会随着时间的推移而同步递增。</li>
</ol>
<p><img src="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/3.png" style="zoom:67%;"></p>
<ol>
<li>在这个模型中，数据流算法的范围是$[0,\cdots,d-1]$ 。处在查询范围中的各个元素的重要程度是不同的。新到达的元素，其重要程度比较高；到达时间较长的元素，其重要程度较低</li>
</ol>
<p><img src="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/4.png" style="zoom:67%;"></p>
<h3 id="近似算法与随机算法"><a href="#近似算法与随机算法" class="headerlink" title="近似算法与随机算法"></a>近似算法与随机算法</h3><p>由于存储空间和时间的限制，我们在数据流上做到精确计算非常困难。</p>
<p><strong>近似算法</strong>：寻找<strong>误差在一定范围内</strong>的近似解。之前我们学的Morris算法、布隆过滤器以及局部敏感哈希算法，都是近似算法</p>
<ul>
<li>比如说，寻找一个误差在10%以内的解</li>
<li>比如说：近似解落在真实值的$(1\pm \epsilon)$ 范围内，其中$\epsilon=0.1$</li>
</ul>
<p><strong>随机算法</strong>：比近似算法更加宽松，是允许小概率失败的(即不在误差范围内)</p>
<ul>
<li>例如，估计值有 1/100 的概率不在误差范围内，即成功地概率为$1-\delta$ ，其中$\delta = 0.01$</li>
</ul>
<p>我们可以这样来理解： <strong>随机算法就等于</strong> $(\epsilon,\delta)$<strong>-近似算法</strong> ,接下来会详细介绍各种类型的近似算法</p>
<h4 id="epsilon-​-近似算法-相对误差"><a href="#epsilon-​-近似算法-相对误差" class="headerlink" title="$\epsilon$​-近似算法(相对误差)"></a>$\epsilon$​-近似算法(相对误差)</h4><p>其实很好理解，我们假设数据流$\sigma$​和其精确输出$\xi(\sigma)$​，其近似算法的输出记为$\mathcal{A}(\sigma)$​ 。那么如果是相对误差版本的话：</p>
<script type="math/tex; mode=display">
|\mathcal A(\sigma)-\phi(\sigma)|<\epsilon\phi(\sigma)</script><h4 id="epsilon-近似算法-绝对误差"><a href="#epsilon-近似算法-绝对误差" class="headerlink" title="$\epsilon$-近似算法(绝对误差)"></a>$\epsilon$-近似算法(绝对误差)</h4><p>绝对误差更好理解，就是要把误差控制在$\epsilon$里面 ，即</p>
<script type="math/tex; mode=display">
|\mathcal A(\sigma)-\phi(\sigma)|<\epsilon</script><h4 id="epsilon-delta-近似算法-相对误差"><a href="#epsilon-delta-近似算法-相对误差" class="headerlink" title="$(\epsilon,\delta)$-近似算法(相对误差)"></a>$(\epsilon,\delta)$-近似算法(相对误差)</h4><p>$(\epsilon,\delta)$ 近似算法往往比$\epsilon$近似算法更加宽松一点，它只要大部分情况下算法给出的<strong>估计值</strong>满足$\epsilon$近似即可。即：</p>
<script type="math/tex; mode=display">
Pr[|\mathcal A-\xi (\sigma)|<\epsilon\xi(\sigma)]>1-\var</script><p>用语言来解释就是说一个$(\epsilon,\delta)$ 近似算法的数据结果可能会出现 $|\mathcal A-\xi (\sigma)|&gt;\epsilon\xi(\sigma)$的情况，但这种情况发生的概率不会超过$\delta$</p>
<h4 id="epsilon-delta-​-近似算法-绝对误差"><a href="#epsilon-delta-​-近似算法-绝对误差" class="headerlink" title="$(\epsilon,\delta)$​-近似算法(绝对误差)"></a>$(\epsilon,\delta)$​-近似算法(绝对误差)</h4><p>当然，我们也可以给出绝对误差版本的定义</p>
<script type="math/tex; mode=display">
Pr[|\mathcal A-\xi (\sigma)|<\epsilon]>1-\var</script><p>这是一个数据流分析挖掘框架：</p>
<p><img src="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/5.png" style="zoom:67%;"></p>
<h2 id="频繁项挖掘"><a href="#频繁项挖掘" class="headerlink" title="频繁项挖掘"></a>频繁项挖掘</h2><p>我们本章的重点就在于找到数据流的频繁项，接下来要介绍的一些算法都是围绕这频繁项展开的。那么首先我们就来介绍频繁项估计</p>
<h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>在数据流$\sigma = <a_1,a_2\cdots,a_m>,a_i\in[n]$中，定义一个频数向量：</a_1,a_2\cdots,a_m></p>
<script type="math/tex; mode=display">
f=(f_1,f_2,\cdots,f_n)</script><p>其中，n为该数据流中不同元素的个数，$f<em>i$ 为元素$a_i$ 的频数，且满足$\sum</em>{i=1}^nf_i = m$</p>
<p>然后，我们可以找出满足需求的元素，比如频繁项。</p>
<ul>
<li>对于大多数问题，如果$\exists a_j : f_j &gt; m/2$, 则输出$a_j$, 否则输出空集$\empty$</li>
<li>频繁项：给定一个参数k,输出频繁元素集合${a_j:f_j\geq m/k}$; 或者给定一个参数$\psi$，输出频繁元素集合 ${a_j:f_j&gt;\psi m}$</li>
</ul>
<p>比如说：对给定的数据流$\sigma = <a,b,a,c,c,a,b,d>$ ，4个不同元素的频数分别为$f_a = 3,f_b=2,f_c=2,f_d=1$</a,b,a,c,c,a,b,d></p>
<ul>
<li>$\sigma$中不存在大多数问题，因为没有大于4的元素</li>
<li>当k等于4的时候，m/k=2,因此频繁项为a,b,c</li>
<li>当$\psi=0.3$ 时候，$m\cdot \psi = 2.4$,因此频繁项为a</li>
</ul>
<h2 id="确定性近似频数算法-Misra-Gries"><a href="#确定性近似频数算法-Misra-Gries" class="headerlink" title="确定性近似频数算法 Misra Gries"></a>确定性近似频数算法 Misra Gries</h2><p>Misra Gries算法是一个<strong>确定性求解频繁元素的近似算法</strong>。其思想和俄罗斯方块非常类似，伪代码如下：</p>
<p><img src="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/6.png" style="zoom:67%;"></p>
<p>算法的输入流$\sigma = <a_1,a_2\cdots,a_m>,a_i\in [n]$​ <strong>使用 k 个计数器</strong>；输出为频繁元素集合<strong>F</strong>. </a_1,a_2\cdots,a_m></p>
<p>该算法的核心思想如下：</p>
<ol>
<li>对到达的元素$a_m$,如果已经为其创建了计数器，便将相应的计数器加1；</li>
<li>如果没有相应的计数器，且计数器的个数少于k(意味着内存中还有足够的空间创建新的计数器)，那么则为该元素分配一个新的计数器，并置为1；</li>
<li>如果当前计数器的个数为k (意味着内存中已经没有足够的空间来创建新的计数器)，则把所有计数器减1，然后删除值为0的计数器</li>
</ol>
<p>给出一个例题：</p>
<p>给定输入流$<b,a,c,a,d,e,a,f,a,d>$ ，计数器个数$k=3$​ ，试逐步写出 Misra Gries 算法执行的结果</b,a,c,a,d,e,a,f,a,d></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>输入</th>
<th>操作</th>
<th>结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>b</td>
<td>插入</td>
<td>$F={(b,1)}$</td>
</tr>
<tr>
<td>a</td>
<td>插入</td>
<td>$F={(b,1),(a,1)}$</td>
</tr>
<tr>
<td>c</td>
<td>插入、删除</td>
<td>$F={(a,1),(b,1),(c,1)}\Rightarrow F={}$</td>
</tr>
<tr>
<td>a</td>
<td>插入</td>
<td>$F={(a,1)}$</td>
</tr>
<tr>
<td>d</td>
<td>插入</td>
<td>$F={(a,1),(d,1)}$</td>
</tr>
<tr>
<td>e</td>
<td>插入、删除</td>
<td>$F={(a,1),(d,1),(e,1)}\Rightarrow F={}$​</td>
</tr>
<tr>
<td>a</td>
<td>插入</td>
<td>$F={(a,1)}$</td>
</tr>
<tr>
<td>f</td>
<td>插入</td>
<td>$F={(a,1),(f,1)}$</td>
</tr>
<tr>
<td>a</td>
<td>更新</td>
<td>$F={(a,2),(f,1)}$</td>
</tr>
<tr>
<td>d</td>
<td>插入、删除</td>
<td>$F={(a,2),(f,1),(d,1)}\Rightarrow {(a,1)}$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h3><p>在 Misra Gries 算法中，算法最后的输出结果可能是不频繁的。试想，如果最后一个元素是数据流中一个新出现的元素，而且最后一个元素到达后，k位置中还有空位，那么这个新元素会出现在最后的结果中。也就是说，Misra Gries算法可能会出现误报。虽然可能出现误报，但是频繁元素一定在最后的输出结果中。</p>
<p>不能否认，Misra Gries算法是一个构思非常巧妙的算法，只需要k个计数器就能找到一个数据流中所有的频繁项元素。</p>
<h2 id="随机近似频数算法-Counting-Sketch"><a href="#随机近似频数算法-Counting-Sketch" class="headerlink" title="随机近似频数算法 Counting Sketch"></a>随机近似频数算法 Counting Sketch</h2><p>对于给定的数据流，Misra Gries 算法输出的结果中，<strong>每个元素的频数是无从知晓的</strong>，而随机近似频数算法Count Sketch 则可以估计频繁元素的频数。</p>
<h3 id="简单抽样算法"><a href="#简单抽样算法" class="headerlink" title="简单抽样算法"></a>简单抽样算法</h3><p>首先我们来介绍一下简单抽样算法。这是一种非常简单的计算元素频数的算法。该算法的核心思想是：对于到达的元素$a_i$, 以概率 $p=M/m$ 对该元素的频数+1.其中，<strong>M表示抽样后的数据流大小，m表示原数据流大小</strong>。伪代码如下：</p>
<p><img src="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/7.png" style="zoom:67%;"></p>
<p>比如说现在给定一个数据流$<2,2,1,2,2,4,5,4,4,2>$ ,$p=\frac{1}{3}$ ，如下图所示，估计元素2的频数为：$f’(2)=\frac{c_2}{p}=6$</2,2,1,2,2,4,5,4,4,2></p>
<p><img src="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/8.png" style="zoom:67%;"></p>
<h4 id="算法分析-1"><a href="#算法分析-1" class="headerlink" title="算法分析"></a>算法分析</h4><p>简单抽样算法称为 $(\epsilon,\delta)-$近似算法的空间需求是： $M = O(\frac{m\log {1}/{\delta}}{\epsilon^2})$ ，其中，m为原数据流的大小，$\epsilon$是误差因子，$\delta$是控制误差在一定范围的尾概率上界。</p>
<p>从空间需求看出，<strong>简单抽样算法所需空间与数据流大小有关</strong>，当数据流规模较大时，该算法所需的空间必将大幅增加。因此我们需要提出更高效的基于概要数据结构的频数估计算法</p>
<h3 id="Basic-Count-Sketch算法"><a href="#Basic-Count-Sketch算法" class="headerlink" title="Basic Count Sketch算法"></a>Basic Count Sketch算法</h3><p>从简单抽样算法的空间需求来看，找到一个更为紧凑的数据结构来维护数据流中高频元素的近似计数时非常必须的。</p>
<p><img src="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/9.png" style="zoom:67%;"></p>
<p>Basic Count Sketch 算法维护一个<strong>计数数组C </strong> 和两个哈希函数$h(\cdot),g(\cdot)$。 哈希函数$h(\cdot)$将n个元素均匀的映射到k个位置(算法第二行)；哈希函数$g(\cdot)$将n个元素映射为$-1$或者$+1$(算法第三行)。</p>
<p>对于一个新到达的元素，将其位置上的计数加1或者减1，对于查询元素a，估计出的频数为 $\hat f_a = g(a)C[h(a)]$</p>
<p>两个哈希函数的操作如图所示。对于元素i，$h(i)$将其映射到第7个位置上，$g(i)$控制该位置计数减1；对于元素了$j$ ,$h(j)$将其映射到第2个位置上，$g(j)$控制该位置计数加 1。</p>
<p><img src="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/10.png" style="zoom:67%;"></p>
<p>比如说现在有一个数据流 $<a,b,c,a,b,a>$​ ，如果a、b、c各有一个计数器(一共三个)，那么可以清晰地计算出每个元素的频数。 当数据流中元素很多，要精确统计的话就需要很多计数器，这是维护不起的</a,b,c,a,b,a></p>
<p>但是<strong>只有两个计数器</strong>(只有两个位置来存放频数)，因此设计一个哈希函数，使得$h(a)=h(b)\neq h(c)$, 我们用这个算法来计算各个元素的频数：</p>
<p>首先，我们还要知道一共有8种($2^3$)可能的哈希函数$g(\cdot)$来使得${a,b,c}\rightarrow {-1,+1}$​,如下图： </p>
<p><img src="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/11.png"></p>
<p>比如说对于第一种哈希函数， $g(a),g(b),g(c)$都是$+1$，因为$h(a)$和$h(b)$在同一个位置上，所以凡是遇到$a$和$b$，就在这个位置上+1，最后得到的频数为5；$h(c)$是另外一个位置，且只出现了一次，因此最后得到的频数为1</p>
<p>那么我们穷举8种 $g(\cdot)$，最终得到了上表，接下来我们要对$a,b,c$三个数的频数进行分别计算。</p>
<p>根据 算法$\hat f_a = g(a)C[h(a)]$，我们对8个哈希函数的结果求均值，如下：</p>
<script type="math/tex; mode=display">
 f_a = \lvert{\frac{(5+5+1+1)-(-5-5-1-1)}{8}}\rvert = 3\\
f_b = \lvert{\frac{(5+5-1-1)-(-5-5+1+1)}{8}}\rvert=2\\
f_c = \lvert{\frac{(1+1+1+1)-(-1-1-1-1)}{8}}\rvert=1</script><p>可以看到，在经过多次实验(取多次不同哈希函数)之后，频数的估计和实际频数相同，这并不是一个巧合</p>
<p>对于每一个 $j\in [n]$ ，根据h函数哈希的结果，定义一个指示变量$Y_j$ 如下：</p>
<script type="math/tex; mode=display">
Y_j = \cases{1,h(j)=h(a)\\~\\0,\text{otherwise}}</script><p>这里的$Y_j$是一个随机变量，表示元素j与元素a是否哈希到了同一个位置。最终，对元素a的频数估计是计数器$h(a)$位置的值。因此所有哈希到h(a)位置上的元素都对元素a的频数估计起到了作用，因此：</p>
<script type="math/tex; mode=display">
\begin{align}
\hat f_a &= g (a)C[h(a)]\\~\\
&=g(a)\sum_j^n f_jg(j)Y_j\\~\\
&=f_a+g(a)\sum_{j\in[n]/ \{a\}}f_j\cdot g(j)\cdot Y_j
\end{align}</script><h4 id="如果只有1个计数器"><a href="#如果只有1个计数器" class="headerlink" title="如果只有1个计数器"></a>如果只有1个计数器</h4><p>同样是上面的数据流 : abcaba, 如果只有一个计数器会怎么样？</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">g(a)</th>
<th style="text-align:center">g(b)</th>
<th style="text-align:center">g(c)</th>
<th style="text-align:center">C[h(a)]=C[h(b)]=C[h(c)]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">+</td>
<td style="text-align:center">+</td>
<td style="text-align:center">+</td>
<td style="text-align:center">3+2+1= 6</td>
</tr>
<tr>
<td style="text-align:center">+</td>
<td style="text-align:center">+</td>
<td style="text-align:center">-</td>
<td style="text-align:center">3+2-1=4</td>
</tr>
<tr>
<td style="text-align:center">+</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">3-2-1= 0</td>
</tr>
<tr>
<td style="text-align:center">+</td>
<td style="text-align:center">-</td>
<td style="text-align:center">+</td>
<td style="text-align:center">3-2+1=2</td>
</tr>
<tr>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">+</td>
<td style="text-align:center">-3-2+1=-4</td>
</tr>
<tr>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-3-2-1=-6</td>
</tr>
<tr>
<td style="text-align:center">-</td>
<td style="text-align:center">+</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-3+2-1=-2</td>
</tr>
<tr>
<td style="text-align:center">-</td>
<td style="text-align:center">+</td>
<td style="text-align:center">+</td>
<td style="text-align:center">-3+2+1 = 0</td>
</tr>
</tbody>
</table>
</div>
<script type="math/tex; mode=display">
f_a = \lvert{\frac{(6+4+0+2)-(-4-6-2-0)}{8}}\rvert=3\\
f_b = \lvert{\frac{(6+4-2-0)-(0+2-4-6)}{8}}\rvert=2\\
f_c = \lvert{\frac{(6+2-4+0)-(4+0-6-2)}{8}}\rvert=1</script><h3 id="Count-Sketch算法"><a href="#Count-Sketch算法" class="headerlink" title="Count Sketch算法"></a>Count Sketch算法</h3><p>Count Sketch 算法基于$t$次频数估计的中位数和真实值很接近的想法，在Basic Count Sketch 算法的基础上，将哈希函数$h(\cdot)$个数增加到$t$个，将每个元素映射到 $t$ 个位置上，同时也有t个函数决定每个位置上的计数加1或者减1。(这里，$t = \log(1/\delta)$​)</p>
<p><img src="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/12.png"></p>
<p>Count Sketch 算法最终是一个空间消耗为$O(\frac{\log ({1}/{\delta})}{\epsilon^2})$​ 的 $(\epsilon,\delta)$​​近似估计</p>
<p>哈希函数个数是 $t = O(\log(1/\delta))$</p>
<h3 id="Count-min-Sketch算法"><a href="#Count-min-Sketch算法" class="headerlink" title="Count-min Sketch算法"></a>Count-min Sketch算法</h3><p><img src="/2021/10/29/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E7%AE%97%E6%B3%95ch5-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B%E5%8F%8A%E9%A2%91%E7%B9%81%E9%A1%B9%E6%8C%96%E6%8E%98/13.png"></p>
<p>在CM-sketch算法中，其数据结构是一个宽度为w，深度为d(代表哈希函数的个数)的计数器数组：</p>
<script type="math/tex; mode=display">
C[1,1],C[1,2]\cdots,C[d,w]</script><p>初始化时，每个元素均为0. 另有d个哈希函数使得：</p>
<script type="math/tex; mode=display">
h_i :\{1,2\cdots,n\}\rightarrow \{1,2\cdots,w\},1\leq i\leq d</script><p>一旦w和d确定下来，CM-sketch所需要的空间便确定了，我们发现哈希表相对于Count Sketch的($\frac{3}{\epsilon^2}$)更加小了 ，因此空间性能提升了$1/\epsilon$ 倍左右。 </p>
<h4 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h4><p>给定数据流$<4,1,3,5,1,3,2,6,7,0,9>$, 若哈希函数形如 $h(x)=(ax+b) ~\text{mod~} 8$ ，其中a和b是任意给定的常数。假定给定如下哈希函数：</4,1,3,5,1,3,2,6,7,0,9></p>
<script type="math/tex; mode=display">
h(x) = (3x+2)\mod 8\\
h(x) = (7x+5)\mod 8\\
h(x) = (5x+3)\mod 8</script><p>试解答以下问题：</p>
<ol>
<li>利用 Count-Min Sketch 算法估计频繁项</li>
</ol>
<p>这种情况下，$d=3,w=8$</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>哈希值表</th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>9</th>
</tr>
</thead>
<tbody>
<tr>
<td>$h_1(x)$</td>
<td>2</td>
<td>5</td>
<td>0</td>
<td>3</td>
<td>6</td>
<td>1</td>
<td>4</td>
<td>7</td>
<td>5</td>
</tr>
<tr>
<td>$h_2(x)$</td>
<td>5</td>
<td>4</td>
<td>3</td>
<td>2</td>
<td>1</td>
<td>0</td>
<td>7</td>
<td>6</td>
<td>4</td>
</tr>
<tr>
<td>$h_3(x)$</td>
<td>3</td>
<td>0</td>
<td>5</td>
<td>2</td>
<td>7</td>
<td>4</td>
<td>1</td>
<td>6</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>对于三个哈希函数, 将上述哈希到整个表中的不同位置并计数</p>
<p>比如说，第一个到达的数据是4，那么查询上面的哈希值表，在4这个位置，$h_1(x) = 6,h_2(x)=1,h_3(x)=7$ ，那么我们就在下面这个dw表格中，在$h_1(x) = 6,h_2(x)=1,h_3(x)=7$ 的位置上分别加上1。</p>
<p>这样，把整个数据流过一遍，就得到了下面这张表格</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>dw表格</th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
</tr>
</thead>
<tbody>
<tr>
<td>$h_1(x)$</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>3</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>$h_2(x)$</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>3</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>$h_3(x)$</td>
<td>3</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>然后，对于每一个元素，我们都要取到哈希到的格子中的最小值</p>
<ul>
<li>比如说对于元素0，哈希到$h_1(x)=2,h_2(x)=5,h_3(x)=3$ 这三个格子，格子中的值分别是1、1、1，最小值为1,因此对元素0的频数估计为1.</li>
<li>比如说对于元素1，哈希到$h_1(x)=5,h_2(x)=4,h_3(x)=0$  这三个格子，格子中的值分别是3、3、3,最小值为3，因此元素1的频数估计为3</li>
</ul>
<p>以此类推，得到：</p>
<script type="math/tex; mode=display">
\hat f_0 = 1,\hat f_1 = 3,\hat f_2 = 1,\hat f_3=2,\hat f_4=1,\hat f_5 = 1,\hat f_6 = 1,\hat f_7=1,\hat f_9 = 3</script><p>这里不存在大多数项。因为没有频数大于4的元素</p>
<p>当$k=3$时，大于$8/3=2.6$的元素就可以被认为是频繁项，因此 1和9是频繁项</p>
<p>当$k=4$时，大于$8/4=2$的元素就可以被认为是频繁项，因此 1、3、9是频繁项。</p>
<p>但是我们知道在这个数据流中9只出现了1次，因此这个算法也存在误判的概率，这就是一次collision。Count-min Sketch的确会有这种问题，因为这个模型是从Bloom Filter衍生过来的。所以说Count-min Sketch是一个概率模型，返回的结果是一个上限值（upper-bound）。</p>
<ol>
<li>给定任意元素i，题⽬中的CM Sketch可以达到何种精度</li>
</ol>
<p>因为 哈希函数 $k = \lceil \log(1/\delta) \rceil = 3$ 因此 $ \frac{1}{8}&lt;\delta&lt;\frac{1}{4}$, 也就是说，算法错误的概率在 $12.5\%—25\%$ 之间；</p>
<p>数组宽度 $d = \lceil  2/\epsilon\rceil =8$  ,$\frac{1}{4}&lt;\epsilon &lt;\frac{2}{7}$，也就是说，算法估计精度误差在 $25\%—28.6\% $ 之间</p>
<ol>
<li>如果想找到$(\epsilon,\delta)$ 估计，需要如何修改算法？(从哈希函数的个数入手)</li>
</ol>
<p>可以增加数组宽度，达到更高的算法精度</p>
<p>也可以增加哈希函数的个数，达到更高的准确度</p>
<h4 id="算法解释"><a href="#算法解释" class="headerlink" title="算法解释"></a>算法解释</h4><p> 在算法开始时，构造一个d行w列的空数组，可以认为每一行是独立的，算法在运行时同时记录了d个这样的数组。在每出现一个流数据的时候，对每一个数组进行一次更新，注意元素的第二个下标用的是数据的哈希值。</p>
<p> 算法在运行的过程中可能产生冲突，也就是两个不同的流数据的哈希值可能相同，这个时候就会导致结果偏大，但是因为有相当于t次的重复计算，通过取最小值的方法来进行一些弥补，但是这样的方法也不能完全避免冲突。</p>
<h4 id="算法分析-2"><a href="#算法分析-2" class="headerlink" title="算法分析"></a>算法分析</h4><p>设计一个最优的 Count-min Sketch模型的过程是：</p>
<ol>
<li>估算数据流n 的大小</li>
<li>选择一个合理的 $\epsilon$ 值</li>
<li>选择一个合理的概率值 $1-\delta$</li>
<li>哈希函数的个数  $k = \lceil(\ln\frac{1}{\delta})\rceil$, 数组的宽度 $w = \lceil\frac{2}{\epsilon}\rceil$</li>
</ol>
<p>可以看出，要想错误范围越小，就需要更大的w，也就是表格的宽度</p>
<p>想要更高的概率，就需要更大的k，也就是更多的哈希函数</p>
<p>假设我们现在需要为 $10^6$ 的数据计数，规定精度 $\epsilon = 0.002$ ，由此可知 $d = 1000$ ; 规定准确率为$99\%$ ， 可知$\delta = 0.01$, 因此需要的哈希函数个数为  $k = \lceil(\ln100)\rceil=  5$</p>
<p>假设每个计数单元占内存大小为 4 byte,那么，该模型将占用内存</p>
<script type="math/tex; mode=display">
1000\times 5\times 4\text{bytes} = 20\text{KB}</script><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>几种方法的总结</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>算法</th>
<th>空间需求</th>
<th>参数</th>
</tr>
</thead>
<tbody>
<tr>
<td>简单抽样算法</td>
<td>$M = O(\frac{m\log(1/\delta)}{\epsilon^2})$</td>
<td>m:数据流大小<br>$\delta$: 误差<br>$\epsilon$: 估计精度</td>
</tr>
<tr>
<td>Basic Count Sketch算法</td>
<td>$M = O(\frac{1}{\delta\epsilon^2})$</td>
<td>和数据流大小无关，确定精度和误差即可<br>计数器个数 $k = \frac{3}{\epsilon^2}$</td>
</tr>
<tr>
<td>Count Sketch算法</td>
<td>$M = O(\frac{log(1/\delta)}{\epsilon^2})$</td>
<td>相比于Basic Count Sketch,更低<br>计数器个数  $k=\frac{3}{\epsilon^2}$<br>哈希函数个数 $t = \lceil \ln(1/\delta)\rceil$</td>
</tr>
<tr>
<td>CM Sketch算法</td>
<td>$M = O(\frac{log(1/\delta)}{\epsilon})$</td>
<td>复杂度最低,计数器减少<br>数组的宽度 $w = \lceil\frac{2}{\epsilon}\rceil$<br>哈希函数的个数  $k = \lceil(\ln\frac{1}{\delta})\rceil$</td>
</tr>
</tbody>
</table>
</div>

          
        
      
    </div>
    
    
    
    <div>
      
    </div>
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/%5Bobject%20Object%5D">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/" itemprop="url">关系数据库及其基本实现原理</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-10-25T23:09:05+08:00">
                2021-10-25
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2023-01-16T10:48:20+08:00">
                2023-01-16
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="关系数据库"><a href="#关系数据库" class="headerlink" title="关系数据库"></a>关系数据库</h1><h3 id="关系数据库简介"><a href="#关系数据库简介" class="headerlink" title="关系数据库简介"></a>关系数据库简介</h3><p>之前我们学了文档数据库，文档数据库更像是面向对象的一种表示模式。但是关系数据库用表格来存储数据，我们将表格的每一列看成是一个属性。</p>
<p>工程师们一开始并没有提出关系模型，一种叫做网状模型，一种叫做层次模型。在六十年代，IBM推出的IMS系统就是使用层次模型，GE推出的IDS使用的是网状模型</p>
<p>后来，Ted Codd提出了关系模型，其思想就是将数据管理系统从软件中解耦合出来形成一套独立的系统</p>
<h3 id="网状模型问题"><a href="#网状模型问题" class="headerlink" title="网状模型问题"></a>网状模型问题</h3><p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/5.jpeg" style="zoom:67%;"></p>
<p>上图是网状数据结构的一个定义，在定义里有三个对象：学生、课程、选课。学生有三个属性，课程和选课各有两个属性。图下方的s数据就是根据上面的结构来存储的。</p>
<p>解决了数据存储的模式，程序员可以在程序中通过一些接口来访问数据库中的对象了。比如说：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">-- 定义变量</span><br><span class="line">student;course;registration;</span><br><span class="line">-- 程序</span><br><span class="line">Course.C_name &#x3D; &quot;math1&quot;;</span><br><span class="line">find course using C_name;</span><br><span class="line">while DB-status &#x3D; 0 do</span><br><span class="line">	get next Registration;</span><br><span class="line">	get student;</span><br><span class="line">	print Student.S_name;</span><br><span class="line">	End;</span><br></pre></td></tr></table></figure>
<ul>
<li>用math1这个名称找到这门课</li>
<li>然后通过while循环，利用 get 来获取到每一个选择这门课的学生</li>
</ul>
<p>我们发现要找到信息，完全是通过程序来实现的。但是Ted Codd认为，这样就让数据管理系统和应用系统之间的耦合度太高了，太复杂了。我们需要经常对数据机构和程序逻辑进行修改。他认为，这些查询的逻辑应该交给数据库自己做。因此他提出了关系型数据库。</p>
<h3 id="关系数据库的构建思想"><a href="#关系数据库的构建思想" class="headerlink" title="关系数据库的构建思想"></a>关系数据库的构建思想</h3><p>上面我们用程序实现了查找所有选择数学课的学生姓名，但这个程序时比较复杂的。能不能用很简单的方式来表达我们的需求呢？——谓词逻辑</p>
<p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/6.jpeg" style="zoom:67%;"></p>
<p>我们的需求可以用这样一段为此逻辑来表示。</p>
<p>存在这样一个选课的对象Y，这个选课对象和class有联系，和student也存在联系。并且，课程的名称为 “Math1”而学生的名字是X。我们要找的就是这个X</p>
<p>这是一种声明式的方式，相比于之前写的程序，这种方式更加简洁简单。对于怎么根据声明表达式来获得所需要的数据，交给数据库去处理。</p>
<p>但是并不是所有的逻辑表达式都可以被计算机以很高效的方式来处理，因此我们必须要考虑逻辑抽象的层次和表达能力的范围。Codd在这种谓词逻辑表达式中找到了一个子集，称为 Relational Calculus(关系演算) ，通过这种方式，计算机可以很快地理解我们的逻辑表达式。进而，这种模式演化成了关系数据库</p>
<h3 id="关系模型"><a href="#关系模型" class="headerlink" title="关系模型"></a>关系模型</h3><p>关系模型是将数据组织成表，这张表在关系模型中被称为<strong>关系</strong>。接下来我们给出一些定义：</p>
<ul>
<li>域(domain)——&gt; 集合(同一数据类型)，比如说<code>{Jason,May,Bob}、{male,female}</code>. 我们可以将其理解为数据库里某一属性的所有取值。有了域的概念，就可以来定义笛卡尔积</li>
<li>笛卡尔积：是数据的组合。从每个域中取出一个值形成一个集合，那么穷尽所有的集合就是这几个域的笛卡尔积。比如说对上面的两个域，其笛卡尔积中一共有 3X2 = 6 个组合。</li>
<li>有了上面的两个概念，我们可以定义关系(relation), 关系就是笛卡尔积的一个子集。子集中的组合可以构成一张表格，即关系</li>
</ul>
<h3 id="关系代数的概念"><a href="#关系代数的概念" class="headerlink" title="关系代数的概念"></a>关系代数的概念</h3><p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/7.png" style="zoom:67%;"></p>
<p>现在我们有三个关系，分别是student,course,sc 每一张表格的表头中存放的信息我们称之为<strong>属性</strong> ，表中的每一行我们称之为<strong>元组</strong>。</p>
<p>现在我们要在这些关系上做计算，计算的方法叫做 <strong>关系代数(Relational Algebra)</strong> 。关系代数需要用到很多算子： </p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Operation</th>
<th>中文</th>
<th>符号</th>
<th>LATEXLATEX</th>
</tr>
</thead>
<tbody>
<tr>
<td>Projection</td>
<td>投影</td>
<td>$\Pi$</td>
<td><code>\Pi</code></td>
</tr>
<tr>
<td>Selection</td>
<td>选择</td>
<td>$\sigma$</td>
<td><code>\sigma</code></td>
</tr>
<tr>
<td>Renaming</td>
<td>重命名</td>
<td>$\rho$</td>
<td><code>\rho</code></td>
</tr>
<tr>
<td>Aggregate Function</td>
<td>聚合函数</td>
<td>$\mathcal{G}$</td>
<td><code>\mathcal{G}</code></td>
</tr>
<tr>
<td>Union</td>
<td>交</td>
<td>$\cap$</td>
<td><code>\cap</code></td>
</tr>
<tr>
<td>Intersection</td>
<td>补</td>
<td>$\cup$</td>
<td><code>\cup</code></td>
</tr>
<tr>
<td>Natural Join</td>
<td>自然连接</td>
<td>$\bowtie$</td>
<td><code>\bowtie</code></td>
</tr>
<tr>
<td>Left Outer Join</td>
<td>左外连接</td>
<td>⟕</td>
<td>… 这几个直接复制吧</td>
</tr>
<tr>
<td>Right Outer Join</td>
<td>右外连接</td>
<td>⟖</td>
<td></td>
</tr>
<tr>
<td>Full Outer Join</td>
<td>全外连接</td>
<td>⟗</td>
<td></td>
</tr>
<tr>
<td>Cartesian product</td>
<td>笛卡尔乘积</td>
<td>$\times$</td>
<td><code>\times</code></td>
</tr>
<tr>
<td>Divide</td>
<td>除</td>
<td>$\div$</td>
<td><code>\div</code></td>
</tr>
<tr>
<td>Assignment</td>
<td>赋值</td>
<td>$\leftarrow$</td>
<td><code>\leftarrow</code></td>
</tr>
<tr>
<td>And</td>
<td>条件并列</td>
<td>$\land$</td>
<td><code>\land</code> or <code>\vee</code></td>
</tr>
<tr>
<td>Negation</td>
<td>非</td>
<td>$\neq$</td>
<td><code>\neg</code></td>
</tr>
<tr>
<td>Exist</td>
<td>存在</td>
<td>$\exists$</td>
<td><code>\exists</code></td>
</tr>
<tr>
<td>For All</td>
<td>对所有</td>
<td>$\forall$</td>
<td><code>\forall</code></td>
</tr>
</tbody>
</table>
</div>
<h3 id="选择操作"><a href="#选择操作" class="headerlink" title="选择操作"></a>选择操作</h3><p>首先我们关注选择算子，用小写的$\sigma$ 来表示。它可以通过计算，将一个关系变为另一个关系。其使用方式如下：</p>
<script type="math/tex; mode=display">
\sigma_A(R)</script><blockquote>
<p>R 代表要选择的关系</p>
<p>A代表选择的条件</p>
</blockquote>
<p>计算的结果就是一个包含了所有在R里面且符合A条件的元组的关系</p>
<p>比如说：$\sigma_{\text{s.name=’Jason’}}(\text{Student})$​​ 最后得到的结果就是 $(S001,Jason,1/1/2001,male)$​。就是在Student关系中选择名字为‘Jason’的元组。</p>
<h3 id="投影操作"><a href="#投影操作" class="headerlink" title="投影操作"></a>投影操作</h3><p>投影我们用$\Pi$​​来表示，起作用是对一个关系进行纵向的切分，将一些属性剔除并返回一个关系。其参数和选择算子是一样的：</p>
<script type="math/tex; mode=display">
\Pi_A(R)</script><blockquote>
<p>A 代表属性的一个集合</p>
<p>R 代表要投影的关系</p>
</blockquote>
<p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/14.png" style="zoom:67%;"></p>
<p>值得注意的是，当切分结束的时候，可能会得到 重复的元组，因此在投影结束之后还需要去重</p>
<h3 id="连接操作"><a href="#连接操作" class="headerlink" title="连接操作"></a>连接操作</h3><p>连接和选择、投影是不一样的，连接是一个二元算子，也就是需要两个关系。用法如下：</p>
<script type="math/tex; mode=display">
S \bowtie_A R</script><blockquote>
<p>S，R代表两个关系</p>
<p>A 代表连接条件</p>
</blockquote>
<p>其操作逻辑就是将S和R先做一个笛卡尔积，再在其基础上做一个条件为A的选择，也就是 $S \bowtie_A R=\sigma_A(S\times R)$</p>
<p>比如说：</p>
<script type="math/tex; mode=display">
\text{Student}\bowtie_{\text{student.sno = sc.sno}}\text{SC}</script><p>这里，$\text{student.sno=sc.sno}$ 称之为等值连接</p>
<h3 id="关系代数表达式"><a href="#关系代数表达式" class="headerlink" title="关系代数表达式"></a>关系代数表达式</h3><p>现在，我们想要所有选了Math1这门课的学生的名字，怎么用关系代数表达式来表述呢？</p>
<p>我们需要将Course和SC进行一个连接，这样就能知道学生的学号了。</p>
<script type="math/tex; mode=display">
\Pi_{\text{student.sname}}(\text{Student}~~~~~\overset\bowtie{\text{student.sno=sc.sno}}~~~~(\text{SC}~~~~ \overset\bowtie{\text{sc.cno=course.cno}}~~~~~~ \sigma_{\text{cname="math1"} }(\text{Course})))\\</script><p>首先，在Course关系里面选择课程名称为math1的课，和SC关系做连接，连接条件为cno相等。接着，连接Student关系，连接条件为sno相等。最后做一个投影，投影条件是学生名字</p>
<p>基于这样的关系表达式，工程师开发了SQL语言。 </p>
<h3 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h3><p>假设学校的教务系统里有三张表：学生表(学号，姓名，生日，性别，学院)、课程表(课程号，课程名称，学分，主讲老师)、选课表(学号，课程号，学期，成绩)。其中，学生表记录了10万个学生的信息，课程表记录了2000门课的信息，选课表中有200万条选课记录。假设我们在学生表的”学号”、课程表的“课程号”、选课表的“学号”和“课程号”上分别创建了四个索引，其余的属性上都没有索引。以下三个关系代数表达式的目的都是查询“所有在张三老师的课上成绩超过90分的学号和姓名”</p>
<script type="math/tex; mode=display">
a.~\Pi_{\text{学号,姓名}}(\text{学生表}\bowtie(\sigma_{\text{成绩}>90}(选课表)\bowtie\sigma_{\text{主讲老师='张三'}}(课程表)))\\
b.~\Pi_{\text{学号,姓名}}(\text{学生表}\bowtie\Pi_{学号}(\sigma_{成绩>90\and\text{主讲老师='张三'}}){(\text{选课表}\bowtie\text{课程表})})\\
c.~\Pi_{\text{学号,姓名}}(\text{学生表}\bowtie(\sigma_{\text{成绩>90}}(\text{选课表}\bowtie\sigma_{\text{主讲老师='张三'}}(课程表))))</script><p>上面三种方法，那种执行方法最好？</p>
<p>答案是：C. </p>
<p>因为选课表有200万张，对于a来说，$\sigma_{\text{成绩&gt;90}}(选课表)$的范围太大了，可能会导致效率很拉跨。<br>对于b，$\text{选课表}\bowtie\text{课程表}$ 需要选课表和课程表做一个笛卡尔积，这边没有任何条件，因此要做一个200万*2000的笛卡尔积，非常慢</p>
<p>对于c，我们现从选课表中找到主讲老师为张三的课程号，这些课程是很少的。而且在课程号上有索引，因此查询会比较快。在做笛卡尔积的时候会减少很多的量，然后再去筛选成绩大于90的同学。</p>
<h1 id="关系数据库的基本实现原理"><a href="#关系数据库的基本实现原理" class="headerlink" title="关系数据库的基本实现原理"></a>关系数据库的基本实现原理</h1><h2 id="关系数据库的基本架构"><a href="#关系数据库的基本架构" class="headerlink" title="关系数据库的基本架构"></a>关系数据库的基本架构</h2><p>我们知道，sql语言是声明式语言。我们告诉数据库系统我们想要什么，数据库会通过内置的算法帮我们找到。这类似于一个黑箱操作，我们学会了sql语言但却并不了解数据库中的原理，那么现在我们就来学习一下其中的操作。</p>
<p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/1.jpeg" style="zoom:67%;"></p>
<p>现在我们通过上面这张图来介绍一下查询的流程。首先，应用将SQL查询提交给DBMS。接着系统要从存储层获取数据到计算层，计算完成后将结果返还给APP。因为SQL语言还是比较直白的，因此在获取了数据之后，计算层需要通过复杂的计算才能获得结果。</p>
<p>首先我们看存储层，关系型数据库是怎么存储的呢？首先，关系是由一张一张表构成的，而我们知道数据库里需要用页来存储才能有效利用空间。于是，我们需要对表进行分页处理，每页里面存放着若干元组，通过若干页面将整张表的信息存储下来，最后通过一个结构(表头)将所有页存储下来(可以是索引、inode):</p>
<p>解决了存放的方式之后，我们可以对数据进行很简单的访问，比如说对数据表进行整体的扫描，对是否称为sql查询的结果进行比对。但是通过扫描或者索引的访问，并不能完成整个sql的查询，因此在计算层需要进行相对复杂的计算</p>
<h2 id="SQL查询的执行过程"><a href="#SQL查询的执行过程" class="headerlink" title="SQL查询的执行过程"></a>SQL查询的执行过程</h2><p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/4.jpeg" style="zoom:67%;"></p>
<h3 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h3><p>首先，要将SQL语句翻译成关系代数表达式。</p>
<p>比如说有这么一段简单的sql语句： </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> sname</span><br><span class="line"><span class="keyword">FROM</span> student,sc</span><br><span class="line"><span class="keyword">WHERE</span> student.sno = sc.sno</span><br><span class="line"><span class="keyword">AND</span> sc.grade &lt; <span class="number">60</span></span><br></pre></td></tr></table></figure>
<p>经过翻译，可以将其变成不同形态的关系代数表达式</p>
<p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/3.jpeg" style="zoom:67%;"></p>
<p>他们的执行步骤可能是不一样的，但是执行的结果确实相同的。比如说第一条关系代数表达式。首先让Student、SC两张表通过SNO进行等值连接，形成一张大表；然后在表里面做选择，选择成绩小于60分的信息；最后在名字这个属性上进行一个投影，最后得到我们想要的结果。</p>
<p>因此，关系表达式确定了一段sql语句在数据库里应该做什么。但是就如上面生成的两条关系代数表达式，如果一段sql非常复杂，他可能生成不计其数的关系代数表达式，那么这时候我们需要做的就是在这里面选择查询效率最高的表达式来执行。</p>
<h3 id="查询优化"><a href="#查询优化" class="headerlink" title="查询优化"></a>查询优化</h3><p>在查询优化这一步，计算机做的更多是一种预测，预计哪一种查询计划最有效。因此在这一步需要一些AI来选择最优解</p>
<h3 id="查询执行"><a href="#查询执行" class="headerlink" title="查询执行"></a>查询执行</h3><p>接下来我们要讲的几节就关注于查询执行。我们要学习每种算子在计算层中是如何实现的。</p>
<h2 id="数据处理的性能问题"><a href="#数据处理的性能问题" class="headerlink" title="数据处理的性能问题"></a>数据处理的性能问题</h2><p>因为数据绝大部分都是存储在DISK上的，而硬盘和内存之间的交互时间非常长，因此在数据密集型计算中，cpu 长时间处于空闲状态，在等待内存和硬盘之间的I/O交互。因此我们用 I/O 次数来衡量数据处理的性能，一个好的算法约等于 I/O次数低的算法</p>
<h2 id="选择算子的实现"><a href="#选择算子的实现" class="headerlink" title="选择算子的实现"></a>选择算子的实现</h2><p>接下来我们来看选择算子的实现。我们知道数据库是将表存放在多个页里面的，然后通过一个结构来把所有页联系起来；此外我们还可以用索引的方式将页给联系起来。</p>
<p>访问一张表，我们可以采用两种方式：</p>
<ul>
<li>扫描/Scan  </li>
<li>索引访问 / Index Access</li>
</ul>
<p>比如说我要进行一个 <code>SELECT * FROM R WHERE A= 100</code></p>
<p>情况如下：采用结构存储的形式，需要在结构中存放100页的地址；采用索引查找的方式有三层索引；</p>
<p>如果使用索引访问，且刚好A是索引，那么我们需要访问4个页面，4次I/O才能访问到单独的页</p>
<p>如果使用扫描的方法，需要100次I/O才能找到。</p>
<p>当满足选择条件的元组特别多的话，那么I/O开销会更加大。</p>
<h2 id="投影算子的实现"><a href="#投影算子的实现" class="headerlink" title="投影算子的实现"></a>投影算子的实现</h2><p>比如说做完 $\Pi_{\text{major}}(R)$​​​​ 投影之后，关系剩下 {(CS),(Math),(CS)} 这三个元组, 我们需要考虑去重。</p>
<p>对于乱序的遗传数据，直接去重是比较复杂的，因此我们可以对其先进行一次排序，然后再去重就比较容易了。因此如何高效率得排序就成了最关键的问题。当然可以使用快排的东西，但是数据量大的话没有办法将其全部存放到内存里面，我们的想法是增强数据访问的局部性来降低 I/O 操作的次数。因此这里介绍<strong>外部排序</strong></p>
<p>对于25212245434215213 这一串数据，假设它无法全部放到内存中，一次只能调入6个，那么首先要对其进行局部排序。</p>
<ul>
<li>从磁盘读取252122 ，进行排序，122225，然后写回到硬盘上</li>
<li>从磁盘中读取454342，进行排序，234445，写回</li>
<li>从磁盘中读取15213，进行排序，11235，写回</li>
</ul>
<p>对每次写回的数据的头部插入一个指针，然后进行归并操作(类似于归并排序)。最后需要写入硬盘中</p>
<p>通过这种方式，可以大大减少I/O的次数，虽然计算的次数比较多，但是在数据密集型计算中，减少I/O次数节省的时间要远远多于增加计算次数所花费的时间</p>
<h2 id="连接算子的实现"><a href="#连接算子的实现" class="headerlink" title="连接算子的实现"></a>连接算子的实现</h2><h3 id="嵌套循环"><a href="#嵌套循环" class="headerlink" title="嵌套循环"></a>嵌套循环</h3><p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/9.jpeg" style="zoom:67%;"></p>
<p>现在我们来介绍一下嵌套循环的实现。假设有两个关系R和S，我们对其进行 sno相等的连接操作，那么很朴素的一种想法是做一个双重循环，R中的每一个Sno和 S中的 Sno一一比对，如果匹配上了，就将其连接起来，放到新关系中去。用简单的逻辑表达式，可以这样写：</p>
<script type="math/tex; mode=display">
\text{For r in R:}\\
~~~~~~~~~~~    \text{For s in S:}\\
~~~~~~~~~~~~~~~~~~~~~~        \text{if(r }\bowtie ~\text{s)}\\
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~            \text{output r}\cdot \text{s}</script><p>我们可以采取另一种策略，就是将R中的n个元组拿出来和S表进行扫描。我们可以把磁盘中的R中的M个元组放到内存里去，然后用S和M个元组一起作比较。在这种情况下需要 $|R|\times B(S)$​  </p>
<p>这时候，逻辑如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">For M in R</span><br><span class="line">	For s in S</span><br><span class="line">		if(M \bowtie s)</span><br><span class="line">			output ...</span><br></pre></td></tr></table></figure>
<p>这时候可以有效减少 I/O 的次数</p>
<p>即： $B(R)/M \cdot B(S)$</p>
<p>适用于查询的选择性强、约束性高并且仅返回小部分记录的结果集。通常要求 <strong>外表的记录</strong>（符合条件的记录，通常通过高效的索引访问） <strong>较少</strong>，且 <strong>内表连接列有唯一索引或者选择性强的非唯一索引</strong>时，嵌套循环连接的效率是比较高的。</p>
<h3 id="MYSQL中的嵌套连接"><a href="#MYSQL中的嵌套连接" class="headerlink" title="MYSQL中的嵌套连接"></a>MYSQL中的嵌套连接</h3><p>在Mysql中使用Join，有很多规矩：比如两表 join 要小表驱动大表，阿里开发者规范禁止三张表以上的 join 操作</p>
<p>Nested Loop Join 是扫描驱动表，每读出一条记录，就根据 join 的关联字段上的索引去被驱动表中查询对应数据。它适用于被连接的数据子集较小的场景，它也是 MySQL join 的唯一算法实现，关于它的细节我们接下来会详细讲解。</p>
<p>MySQL 中有两个 Nested Loop Join 算法的变种，分别是 Index Nested-Loop Join 和 Block Nested-Loop Join(接近于Hash Join)。</p>
<h4 id="Index-Nested-Loop-Join-算法"><a href="#Index-Nested-Loop-Join-算法" class="headerlink" title="Index Nested-Loop Join 算法"></a>Index Nested-Loop Join 算法</h4><p>下面，我们先来初始化一下相关的表结构和数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE &#96;t1&#96; (</span><br><span class="line">  &#96;id&#96; int(11) NOT NULL,</span><br><span class="line">  &#96;a&#96; int(11) DEFAULT NULL,</span><br><span class="line">  &#96;b&#96; int(11) DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (&#96;id&#96;),</span><br><span class="line">  KEY &#96;a&#96; (&#96;a&#96;)</span><br><span class="line">) ENGINE&#x3D;InnoDB;</span><br><span class="line"></span><br><span class="line">delimiter ;;</span><br><span class="line"># 定义存储过程来初始化t1</span><br><span class="line">create procedure init_data()</span><br><span class="line">begin</span><br><span class="line">  declare i int;</span><br><span class="line">  set i&#x3D;1;</span><br><span class="line">  while(i&lt;&#x3D;10000)do</span><br><span class="line">    insert into t1 values(i, i, i);</span><br><span class="line">    set i&#x3D;i+1;</span><br><span class="line">  end while;</span><br><span class="line">end;;</span><br><span class="line">delimiter ;</span><br><span class="line"># 调用存储过来来初始化t1</span><br><span class="line">call init_data();</span><br><span class="line"># 创建并初始化t2</span><br><span class="line">create table t2 like t1;</span><br><span class="line">insert into t2 (select * from t1 where id&lt;&#x3D;500)</span><br></pre></td></tr></table></figure>
<p>有上述命令可知，这两个表都有一个主键索引 id 和一个索引 a，字段 b 上无索引。存储过程 init_data 往表 t1 里插入了 10000 行数据，在表 t2 里插入的是 500 行数据。</p>
<p>为了避免 MySQL 优化器会自行选择表作为驱动表，影响分析 SQL 语句的执行过程，我们直接使用 straight_join 来让 MySQL 使用固定的连接表顺序进行查询，如下语句中，t2是驱动表(较小)，t1是被驱动表(较大)。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from t2 straight_join t1 on (t2.a&#x3D;t1.a);</span><br></pre></td></tr></table></figure>
<p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/2.png" style="zoom:67%;"></p>
<p>我们看到，t1表上的a字段是有一个索引的，Join过程中使用了该索引，因此SQL语句得执行流程如下：</p>
<ul>
<li>从t2表中读取一行数据L1</li>
<li>使用L1的a字段，去t1表中作为条件进行查询，此时会用到索引查询</li>
<li>取出t1中满足条件的行，跟L1组成相应的行，成为结果集的一部分</li>
<li>重复执行，直到扫描完t2表</li>
</ul>
<p>这个流程我们就称之为 Index Nested-Loop Join，简称 NLJ，它对应的流程图如下所示。</p>
<p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/11.jpg" style="zoom: 150%;"></p>
<p>需要注意的是，在第二步中，根据 a 字段去表t1中查询时，使用了索引，所以每次扫描只会扫描一行(从explain结果得出，根据不同的案例场景而变化)。</p>
<p>假设驱动表的行数是N(这里是500)，被驱动表的行数是 M(这里是10000)。因为在这个 join 语句执行过程中，驱动表是走全表扫描(Explain中显示为ALL)，而被驱动表则使用了索引，并且驱动表中的每一行数据都要去被驱动表中进行索引查询，所以整个 join 过程的近似复杂度是 $O(N\log_2M)$。显然，N 对扫描行数的影响更大，因此这种情况下应该让小表来做驱动表(外表)。</p>
<p>当然，这一切的前提是 join 的关联字段是 a，并且 t1 表的 a 字段上有索引。</p>
<p>如果没有索引时，再用上图的执行流程时，每次到 t1 去匹配的时候，就要做一次全表扫描。这也导致整个过程的时间复杂度变成了 $O(N * M)$，这是不可接受的。所以，当没有索引时，MySQL 使用 Block Nested-Loop Join 算法。</p>
<h3 id="Block-Nested-Loop-Join-算法"><a href="#Block-Nested-Loop-Join-算法" class="headerlink" title="Block Nested-Loop Join 算法"></a>Block Nested-Loop Join 算法</h3><p>Block Nested-Loop Join的算法，简称 BNL，它是 MySQL 在被驱动表上无可用索引时使用的 join 算法，其具体流程如下所示：</p>
<ul>
<li>把表 t2 的数据读取当前线程的 join_buffer 中，在本篇文章的示例 SQL 没有在 t2 上做任何条件过滤，所以就是讲 t2 整张表 放入内存中；</li>
<li>扫描表 t1，每取出一行数据，就跟 join_buffer 中的数据进行对比，满足 join 条件的，则放入结果集。</li>
</ul>
<p>比如下面这条 SQL</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from t2 straight_join t1 on (t2.b&#x3D;t1.b);</span><br></pre></td></tr></table></figure>
<p>这条语句的 explain 结果如下所示。可以看出</p>
<p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/3.png" style="zoom:67%;"></p>
<p>可以看出，这次 join 过程对 t1 和 t2 都做了一次全表扫描，并且将表 t2 中的 500 条数据全部放入内存 join_buffer 中，并且对于表 t1 中的每一行数据，都要去 join_buffer 中遍历一遍，都要做 500 次对比，所以一共要进行 500 * 10000 次内存对比操作，具体流程如下图所示。</p>
<p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/12.jpg" style="zoom: 150%;"></p>
<p>主要注意的是，<strong>第一步中，并不是将表 t2 中的所有数据都放入 join_buffer，而是根据具体的 SQL 语句，而放入不同行的数据和不同的字段</strong>。比如下面这条 join 语句则只会将表 t2 中符合 b &gt;= 100 的数据的 b 字段存入 join_buffer。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select t2.b,t1.b from t2 straight_join t1 on (t2.b&#x3D;t1.b) where t2.b &gt;&#x3D; 100;</span><br></pre></td></tr></table></figure>
<p>join_buffer 并不是无限大的，由 join_buffer_size 控制，默认值为 256K。当要存入的数据过大时，就只有分段存储了，整个执行过程就变成了：</p>
<ul>
<li>扫描表 t2，将符合条件的数据行存入 join_buffer，因为其大小有限，存到100行时满了，则执行第二步；</li>
<li>扫描表 t1，每取出一行数据，就跟 join_buffer 中的数据进行对比，满足 join 条件的，则放入结果集；</li>
<li>清空 join_buffer；</li>
<li>再次执行第一步，直到全部数据被扫描完，由于 t2 表中有 500行数据，所以一共重复了 5次</li>
</ul>
<p>这个流程体现了该算法名称中 Block 的由来，分块去执行 join 操作。因为表 t2 的数据被分成了 5 次存入 join_buffer，导致表 t1 要被全表扫描 5次。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>全部存入</th>
<th>分5次存入</th>
</tr>
</thead>
<tbody>
<tr>
<td>内存操作</td>
<td>10000*500</td>
<td>10000*(100+100+100+100+100)</td>
</tr>
<tr>
<td>扫描行数</td>
<td>10000+500</td>
<td>10000*5+500</td>
</tr>
</tbody>
</table>
</div>
<p>如上所示，和表数据可以全部存入 join_buffer 相比，内存判断的次数没有变化，都是两张表行数的乘积，也就是 10000 * 500，但是被驱动表会被多次扫描，每多存入一次，被驱动表就要扫描一遍，影响了最终的执行效率。</p>
<p>基于上述两种算法，我们可以得出下面的结论，这也是网上大多数对 MySQL join 语句的规范。</p>
<ul>
<li><strong>被驱动表上有索引，也就是可以使用Index Nested-Loop Join 算法时，可以使用 join 操作。</strong></li>
<li><strong>无论是Index Nested-Loop Join 算法或者 Block Nested-Loop Join 都要使用小表做驱动表。</strong></li>
</ul>
<p>因为上述两个 join 算法的时间复杂度<strong>至少</strong>也和涉及表的行数成一阶关系，并且要花费大量的内存空间，所以阿里开发者规范所说的严格禁止三张表以上的 join 操作也是可以理解的了。</p>
<p>但是上述这两个算法只是 join 的算法之一，还有<strong>更加高效的 join 算法，比如 Hash Join 和 Sorted Merged join。可惜这两个算法 MySQL 的主流版本中目前都不提供，而 Oracle ，PostgreSQL 和 Spark 则都支持，这也是网上吐槽 MySQL 弱爆了的原因</strong>(MySQL 8.0 版本支持了 Hash join，但是8.0目前还不是主流版本)。</p>
<p>其实阿里开发者规范也是在从 Oracle 迁移到 MySQL 时，因为 MySQL 的 join 操作性能太差而定下的禁止三张表以上的 join 操作规定的 。</p>
<h3 id="散列连接"><a href="#散列连接" class="headerlink" title="散列连接"></a>散列连接</h3><p>散列连接(Hash Join)的实现思路不太一样。其实现逻辑如下：</p>
<p>现在有两张表 R和S，我们对R进行一次扫描，通过哈希函数$H(x)$将里面的元组放到k个桶里。然后对S进行相同的操作，结果如下：</p>
<p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/10.jpeg" style="zoom:67%;"></p>
<p>最后我们得到了两组桶，每个桶对应的sno(连接属性)是不一样的。但是，因为我们使用了同样的哈希函数，对于$ R_1S_1$ ,$R_2S_2$ ,$R_kS_k$​​ 所对应的 sno都是相同的。相当于对R表和S表进行了切分，使得两张表内能够连接的元组放在编号相同的桶中。</p>
<p>之后，我们可依次将$R_iS_i$调入到内存当中进行连接操作，然后输出，直到结束</p>
<p>那么这个方法的IO代价是多少呢？首先要做哈希操作的话，需要将R和S都读入内存一遍，记为$B(R),B(S)$</p>
<p>然后需要写出到桶里，写的代价为$B(R),B(S)$</p>
<p>最后要把桶两两之间做一个连接，需要扫描一遍，代价为$B(R),B(S)$</p>
<p>总的来说整个连接操作的IO代价为 $3\times (B(R)+B(S))$</p>
<p>嵌套循环和散列循环相比，哪个效率更高呢？这需要分情况讨论。</p>
<ul>
<li><p>嵌套循环 ：外表数据量*内表单条（带连接条件）访问性能</p>
</li>
<li><p>散列循环 ：内表全结果集（无连接条件）访问性能</p>
</li>
</ul>
<p>如果是内表没有索引，使用hash join</p>
<p>如果是内表是复杂运算后的结果集，使用hash join 。</p>
<h4 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h4><p><strong>（ 1</strong> <strong>）外表可能记录数相对大</strong></p>
<p><strong>（ 2</strong> <strong>）内表通过连接条件没有合适的索引。</strong></p>
<h4 id="局限"><a href="#局限" class="headerlink" title="局限"></a>局限</h4><p><strong>（ 1</strong> <strong>）必须是等值连接</strong></p>
<p><strong>（ 2</strong> <strong>）要求连接条件有一定的区分度。</strong></p>
<h3 id="MYSQL8-中的散列连接"><a href="#MYSQL8-中的散列连接" class="headerlink" title="MYSQL8 中的散列连接"></a>MYSQL8 中的散列连接</h3><p>Hash Join 是扫描驱动表，利用 join 的关联字段在内存中建立散列表，然后扫描被驱动表，每读出一行数据，并从散列表中找到与之对应数据。它是大数据集连接操时的常用方式，适用于驱动表的数据量较小，可以放入内存的场景，它对于<strong>没有索引的大表</strong>和并行查询的场景下能够提供最好的性能。可惜它只适用于等值连接的场景，比如<code>on a.id = where b.a_id</code></p>
<p>还是上述两张表 join 的语句，其执行过程如下</p>
<p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/13.jpg" style="zoom: 150%;"></p>
<ul>
<li>将驱动表 t2 中符合条件的数据取出，对其每行的 join 字段值进行 hash 操作，然后存入内存中的散列表中；</li>
<li>遍历被驱动表 t1，每取出一行符合条件的数据，也对其 join 字段值进行 hash 操作，拿结果到内存的散列表中查找匹配，如果找到，则成为结果集的一部分。</li>
</ul>
<p>可以看出，<strong>该算法和 Block Nested-Loop Join 有类似之处，只不过是将无序的 Join Buffer 改为了散列表 hash table，从而让数据匹配不再需要将 join buffer 中的数据全部遍历一遍，而是直接通过 hash，以接近 O(1) 的时间复杂度获得匹配的行</strong>，这极大地提高了两张表的 join 速度</p>
<h3 id="Sorted-Merge-Join-算法"><a href="#Sorted-Merge-Join-算法" class="headerlink" title="Sorted Merge Join 算法"></a><strong>Sorted Merge Join 算法</strong></h3><p>Sort Merge Join 则是先根据 join 的关联字段将两张表排序(如果已经排序好了，比如字段上有索引则不需要再排序)，然后在对两张表进行一次归并操作。如果两表已经被排过序，在执行排序合并连接时不需要再排序了，这时Merge Join的性能会优于Hash Join。Merge Join可适于于非等值Join（&gt;，&lt;，&gt;=，&lt;=，但是不包含!=，也即&lt;&gt;）。</p>
<p>需要注意的是，如果连接的字段已经有索引，也就说已经排好序的话，可以直接进行归并操作，但是如果连接的字段没有索引的话，则它的执行过程如下图所示。</p>
<p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/14.jpg" style="zoom: 150%;"></p>
<ul>
<li>遍历表 t2，将符合条件的数据读取出来，按照连接字段 a 的值进行排序；</li>
<li>遍历表 t1，将符合条件的数据读取出来，也按照连接字段 a 的值进行排序；</li>
<li>将两个排序好的数据进行归并操作，得出结果集。</li>
</ul>
<p>Sorted Merge Join 算法的主要时间消耗在于对两个表的排序操作，所以如果两个表已经按照连接字段排序过了，该算法甚至比 Hash Join 算法还要快。在一边情况下，该算法是比 Nested Loop Join 算法要快的。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>Nested Loop Join</th>
<th>Hash Join</th>
<th>Sorted Merge Join</th>
</tr>
</thead>
<tbody>
<tr>
<td>连接条件</td>
<td>适用于任何条件</td>
<td>只适用于等值连接</td>
<td>等值或非等值连接，除了$&lt;&gt;$</td>
</tr>
<tr>
<td>主要消耗资源</td>
<td>CPU、磁盘I/O</td>
<td>内存、临时空间</td>
<td>内存、临时空间</td>
</tr>
<tr>
<td>特点</td>
<td>当有高选择性索引或进行限制性搜索时效率比较高，能够快速返回第一次的搜索结果。此时使用的是Index Nested Loop Join</td>
<td>当缺乏索引或者索引条件比较模糊时，Hash Join比Nested Join 快。在数据仓库环境夏，如果表的记录数多，则其效率高</td>
<td>当缺乏索引或者索引条件模糊时，Sort Merge Join 比 Nested Loop Join有效。当里安桀字段有索引或者提前排好序时，比Hash Join还快，并且支持更多的连接条件</td>
</tr>
<tr>
<td>缺点</td>
<td>无索引或者记录多的时候，效率很低</td>
<td>建立哈希表需要大量内存，第一次的结果返回比较慢</td>
<td>所有的表都需要排序，它为最优化的吞吐量而设计，并且在结果没有全部找到前不返回数据</td>
</tr>
<tr>
<td>需要索引</td>
<td>是(没有索引效率太低)</td>
<td>否</td>
<td>否</td>
</tr>
</tbody>
</table>
</div>
<p><img src="/2021/10/25/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/1.png" style="zoom:67%;"></p>

          
        
      
    </div>
    
    
    
    <div>
      
    </div>
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2021/10/16/Nodejs%E5%9F%BA%E7%A1%805%E6%B5%8B%E8%AF%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/%5Bobject%20Object%5D">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/10/16/Nodejs%E5%9F%BA%E7%A1%805%E6%B5%8B%E8%AF%95/" itemprop="url">Nodejs基础5测试</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-10-16T15:13:18+08:00">
                2021-10-16
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2021-10-16T15:31:04+08:00">
                2021-10-16
              </time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/node/" itemprop="url" rel="index">
                    <span itemprop="name">node</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Nodejs基础5测试"><a href="#Nodejs基础5测试" class="headerlink" title="Nodejs基础5测试"></a>Nodejs基础5测试</h1><h2 id="Integration-Testing"><a href="#Integration-Testing" class="headerlink" title="Integration Testing"></a>Integration Testing</h2><p>和单元测试不一样，集成测试可以引入外部的一些信息(数据库)对我们设计的api进行调试。</p>
<h3 id="Setting-Up-the-Test-Db"><a href="#Setting-Up-the-Test-Db" class="headerlink" title="Setting Up the Test Db"></a>Setting Up the Test Db</h3><p>我们需要真实的数据库来运行集成测试，但是我们又不想用已经创建好了的项目数据库来直接测试，因为在集成测试中，我们会插入或者删除一些数据，为了避免造成混乱，我们需要新建一个测试数据库。</p>
<h3 id="Your-First-Integration-Test"><a href="#Your-First-Integration-Test" class="headerlink" title="Your First Integration Test"></a>Your First Integration Test</h3><h3 id="Populating-the-Test-Db"><a href="#Populating-the-Test-Db" class="headerlink" title="Populating the Test Db"></a>Populating the Test Db</h3><h3 id="Testing-Routes-with-Parameters"><a href="#Testing-Routes-with-Parameters" class="headerlink" title="Testing Routes with Parameters"></a>Testing Routes with Parameters</h3><h3 id="Validating-ObjectIDs"><a href="#Validating-ObjectIDs" class="headerlink" title="Validating ObjectIDs"></a>Validating ObjectIDs</h3><h3 id="Refactoring-with-Confidence"><a href="#Refactoring-with-Confidence" class="headerlink" title="Refactoring with Confidence"></a>Refactoring with Confidence</h3><h3 id="Testing-the-Authorization"><a href="#Testing-the-Authorization" class="headerlink" title="Testing the Authorization"></a>Testing the Authorization</h3><h3 id="Testing-the-Invalid-Inputs"><a href="#Testing-the-Invalid-Inputs" class="headerlink" title="Testing the Invalid Inputs"></a>Testing the Invalid Inputs</h3><h3 id="Testing-the-Happy-Path"><a href="#Testing-the-Happy-Path" class="headerlink" title="Testing the Happy Path"></a>Testing the Happy Path</h3><h3 id="Writing-Clean-Tests"><a href="#Writing-Clean-Tests" class="headerlink" title="Writing Clean Tests"></a>Writing Clean Tests</h3><h3 id="Testing-the-Auth-Middleware"><a href="#Testing-the-Auth-Middleware" class="headerlink" title="Testing the Auth Middleware"></a>Testing the Auth Middleware</h3><h3 id="Unit-Testing-the-Auth-Middleware"><a href="#Unit-Testing-the-Auth-Middleware" class="headerlink" title="Unit Testing the Auth Middleware"></a>Unit Testing the Auth Middleware</h3><h3 id="Code-Coverage-7-09"><a href="#Code-Coverage-7-09" class="headerlink" title="Code Coverage (7:09)"></a>Code Coverage (7:09)</h3>
          
        
      
    </div>
    
    
    
    <div>
      
    </div>
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2021/10/11/%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/%5Bobject%20Object%5D">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/10/11/%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" itemprop="url">一元线性回归</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-10-11T21:24:56+08:00">
                2021-10-11
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2022-01-05T14:33:18+08:00">
                2022-01-05
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="线性回归的背景"><a href="#线性回归的背景" class="headerlink" title="线性回归的背景"></a>线性回归的背景</h1><p>在实际问题中，感兴趣的变量y与易于获得的变量x之间存在紧密关联，但又不由变量x而唯一确定的，这种关系通常称为<strong>统计关系</strong></p>
<p>若变量y与x间有统计关系，那么通常称y为因变量或者响应变量，x为自变量或者解释变量，这里x在机器学习方法中也会被称为特征。</p>
<p>在给定x的取值之后，y的取值是无法唯一确定的。于是，我们可以将y认为是一个随机变量，并需要通过概率分布来对其进行描述，而我们常常关心的是这个概率分布的数字特征，如<strong>期望和方差</strong>。</p>
<p>在给定x的时候，称y的条件数学期望为 y 关于 x 的(均值)回归函数，即：</p>
<script type="math/tex; mode=display">
f(x) = E(y|x)</script><p>注意到，$f(x)$​​ 不仅是 x 的一个确定性的函数，并且从平均意义上刻画了变量y与x间统计关系的规律</p>
<p>而如何确定这个<strong>确定性</strong>的函数f是回归问题中最为核心的问题。</p>
<ul>
<li>线性回归模型可看作将这个函数f 取为x的一个线性函数模式, 如 $f(x)=\beta_0+\beta_1 x$</li>
<li>神经网络模型可看作将这个函数f 取为x的一个非线性函数的形式，如 $f(x)=\max(0,\beta_0+\beta_1x)$​ (Relu函数)</li>
<li>深度学习模型可理解为这个函数f 取为x的多个非线性函数的复合形式。</li>
</ul>
<h1 id="一元线性回归"><a href="#一元线性回归" class="headerlink" title="一元线性回归"></a>一元线性回归</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="模型版"><a href="#模型版" class="headerlink" title="模型版"></a>模型版</h3><p>一元回归模型为：</p>
<script type="math/tex; mode=display">
y = \beta_0+\beta_1x+\epsilon</script><p>其中，$\beta_0,\beta_1$ 为两个未知参数，常称为回归系数，而$\varepsilon$​ 是随机误差。</p>
<p>一元回归模型与数据模型$y=\beta_0+\beta_1x$ 在理解上是不同的，主要的差异在于是否引入了随机误差项$\varepsilon$ :</p>
<ul>
<li>在数学模型中，两个变量之间的关系是确定性的</li>
<li>在统计模型中，两个变量之间的关系是不确定的</li>
</ul>
<p>那么，在一元线性回归模型中，确定性的部分是$\beta_0+\beta_1x$, 随机性的部分为$\varepsilon$ </p>
<p>随机误差项用来概括由于人们认识以及其他客观原因的局限而没有考虑的种种偶然因素，一般，随机误差$\epsilon$​ 没有办法被观测，但<strong>通常假定</strong>$\varepsilon$​ 满足：</p>
<script type="math/tex; mode=display">
\cases{E(\varepsilon) = 0\\Var(\varepsilon) = \sigma^2<\infty}</script><p>其中，$E(\varepsilon)$​​ 表示$\varepsilon$​​ 的数学期望设其为0是为了避免参数不可识别，$Var(\varepsilon)$​​表示$\varepsilon$​​​​的方差</p>
<p>那么因为$\varepsilon$是一个随机变量，所以$y=\beta_0+\beta_1 x+\varepsilon$ 也是随机变量，那么关于x求条件期望，即：</p>
<script type="math/tex; mode=display">
E(y|x) = \beta_0+\beta_1x</script><p>注意到$E(y|x)$​是关于x的一个函数，表示用x的信息刻画因变量y，作为y的”预测“，我们称$E(y|x)=\beta_0+\beta_1x$为回归方程。</p>
<h3 id="数据版"><a href="#数据版" class="headerlink" title="数据版"></a>数据版</h3><p>一般假定我们观测到的数据${(x<em>i,y_i)}</em>{i=1}^n$ 符合线性回归模型及其假设，即：</p>
<script type="math/tex; mode=display">
\cases{y_i= \beta_0+\beta_1x_i+\varepsilon_i,i=1,2\cdots,n\\~\\
E(\varepsilon)=0 ~~~ Var(\varepsilon_i)=\sigma^2}</script><p>同时假定n组数据是独立观测的，即$\varepsilon_1,\varepsilon_2,\cdots,\varepsilon_n$​​是独立同分布的随机变量. </p>
<p>所以，$y_1,y_2\cdots,y_n$ 的期望与方差分别为：</p>
<script type="math/tex; mode=display">
\begin{align}
&E(y_i)= \beta_0+\beta_1x_i\\
&Var(y_i) = \sigma^2, i = 1,2\cdots,n
\end{align}</script><p>这表明，随机变量$y_1,y_2\cdots,y_n$​ 服从不同的分布，方差相等，但是期望不等，且相互独立</p>
<p>模型版和数据版的差别就是，前者强调的是模型，后者侧重的是数据</p>
<h2 id="任务：预测和参数估计"><a href="#任务：预测和参数估计" class="headerlink" title="任务：预测和参数估计"></a>任务：预测和参数估计</h2><p>首先我们来说说回归的最常见的任务之一就是通过n组样本观测值$(x_i,y_i),i=1,2\cdots,n$ 对一个新的个体进行预测。具体来说，如果$x_0$ 已知，那么 $\beta_0+\beta_1x_0$ 是$y_0$的一个合理的预测值 </p>
<p>现在问题来了，这个方程里面的两个参数$\beta_0$​和$\beta_1$​怎么估呢？我们需要通过观测到的数据$(x_i,y_i),i=1,\cdots,n$​​进行估计。一般我们会用$\hat\beta_0$​ 和 $\hat\beta_1$​ 分别表示$\beta_0$​和$\beta_1$​ 的估计值。那么，y关于x的一元线性<strong>经验</strong>回归方程为:</p>
<script type="math/tex; mode=display">
\hat y = \hat\beta_0+\hat\beta_1 x</script><p>称其图形为<strong>经验回归直线</strong>，其中，$\hat\beta_0$ 表示经验回归直线的截距，$\hat\beta_1$表示经验回归直线的斜率</p>
<p>给定$x=x_0$后，称：</p>
<script type="math/tex; mode=display">
\hat y_0 = \hat \beta_0+\hat\beta_1x_0</script><p>为回归值。又是也称为拟合值或者预测值。</p>
<h3 id="最小二乘估计"><a href="#最小二乘估计" class="headerlink" title="最小二乘估计"></a>最小二乘估计</h3><p>在概率论中我们讲了估计的三种思想：似然、替换和拟合</p>
<p>拟合的方法就是配直线，也就是我们现在要讲的<strong>最小二乘</strong></p>
<p>对于每一个样本观测值$(x_i,y_i)$ ，定义偏差为观测值$y_i$ 与其回归至$E(y_i|x_i)$的差异为：</p>
<script type="math/tex; mode=display">
y_i - E(y_i|x_i) = y_i - \beta_0-\beta_1x_i</script><p>偏差平方和为：</p>
<script type="math/tex; mode=display">
Q(\beta_0,\beta_1) = \sum_{i=1}^n(y_i-E(y_i))^2\\
=\sum_{i=1}^n (y_i-\beta_0-\beta_1x_i)^2</script><p>用图形来表示，就如下图：</p>
<p><img src="/2021/10/11/%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.jpeg" alt="1" style="zoom:67%;"></p>
<p>我们称通过最小化偏差平方和$Q(\beta_0,\beta_1)$​​而得到的参数估计方法，为<strong>最小二乘估计</strong> ,下面给出其数学定义：</p>
<script type="math/tex; mode=display">
\begin{align}
(\hat\beta_0,\hat\beta_1)& = \arg\min_{\beta_0,\beta_1} Q(\beta_0,\beta_1)\\
&=\arg\min_{\beta_0,\beta_1}\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)^2
\end{align}</script><p>我们发现这其实就是一个求极值的问题，因为$Q$ 是关于$\hat\beta_0,\hat\beta_1$的非负二次函数，所以其最小值总是存在的。</p>
<p>那么要求极值，我们首先要求一阶导数，即：</p>
<script type="math/tex; mode=display">
\cases{\frac{\partial Q}{\partial \beta_0} = -2\sum_{i=1}^n(y_i-\beta_0-\beta_1x_i)=0\\~\\
\frac{\partial Q}{\partial\beta_1} = -2\sum_{i=1}^n(y_i-\beta_0-\beta_1x_i)x_i = 0
}</script><p>整理得到<strong>正规方程</strong>：</p>
<script type="math/tex; mode=display">
\cases{n\beta_0+\overline x\beta_1 = n\overline y\\~\\
n\overline x\beta_0+\sum_{i=1}^nx_i^2\beta_1=\sum_{i=1}^n x_iy_i}</script><p>于是，$\beta_0,\beta_1$的最小二乘估计为：</p>
<script type="math/tex; mode=display">
\begin{align}
&\hat\beta_0 = \overline y -\hat\beta_1\overline x\\~\\
&\hat\beta_1 = \frac{\sum_{i=1}^n(x_i-\overline x)(y_i-\overline y)}{\sum_{i=1}^n(x_i-\overline x)^2}
\end{align}</script><p>其中，</p>
<script type="math/tex; mode=display">
\overline x = \frac{1}{n} \sum_{i=1}^n x_i,\overline y = \frac{1}{n}\sum_{i=1}^n y_i</script><p>分别为$x_1,x_2\cdots,x_n$ 和 $y_1,y_2\cdots,y_n$ 的样本均值</p>
<p>我们可以用跟简单的记号来表示$\hat\beta_0$和$\hat\beta_1$ </p>
<script type="math/tex; mode=display">
\begin{align}
&l_{xx} = \sum_{i=1}^n (x_i-\overline x)^2 = \sum_{i=1}^n x_i^2-n(\overline x)^2\\~\\
&l_{xy} = \sum_{i=1}^n (x_i-\overline x)(y_i-\overline y) = \sum_{i=1}^n x_iy_i -n\overline x\cdot\overline y
\end{align}</script><p>于是，最小二乘估计简写为：</p>
<script type="math/tex; mode=display">
\cases{
\hat\beta_0 = \overline y -\hat\beta_1\overline x ,\\~\\
\hat\beta_1 = l_{xx}^{-1}l_{xy}
}</script><p>但是，我们是根据一阶导数求出的$\hat\beta$​ ，这实际上是 $Q(\beta_0,\beta_1)$​的稳定点, 但是是否为最小值点，仍需要根据其二阶导在$(\hat\beta_0,\hat\beta_1)$​上表现来判断是否为最小值点：</p>
<script type="math/tex; mode=display">
\cases{\frac{\partial Q}{\partial \beta_0} = -2\sum_{i=1}^n(y_i-\beta_0-\beta_1x_i)=0\\~\\
\frac{\partial Q}{\partial\beta_1} = -2\sum_{i=1}^n(y_i-\beta_0-\beta_1x_i)x_i = 0
}</script><p>从上式可以推得：$Q(\beta_0,\beta_1)$的二阶偏导为：</p>
<script type="math/tex; mode=display">
\begin{vmatrix} \begin{pmatrix} 2n & 2n\overline x \\ 2n\overline x & 2\sum_{i=1}^nx_i^2 \end{pmatrix} \end{vmatrix} = 4n\sum_{i=1}^nx_i^2-4n^2(\overline x)^2</script><p>我们提取出4n,发现剩下的式子就是$l_{xx}$,因此：</p>
<script type="math/tex; mode=display">
= 4n(\sum_{i=1}^nx_i^2-n(\overline x)^2) = \sum_{i-1}^n(x_i-\overline x)^2</script><p>因此，二阶导大于0，说明Q在$\hat\beta_0,\hat\beta_1$​处取最小值</p>
<h3 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h3><p>我们首先来回顾一下最大似然估计：</p>
<p>最大似然估计是依赖于总体的概率函数$f(x;\theta)$以及样本所提供的信息来求未知参数的估计。当总体X为连续随机变量时，其密度函数为：</p>
<script type="math/tex; mode=display">
\{f(x;\theta),\theta\in \Theta\}</script><p>假定总体X的一个独立同分布的样本为$x_1,x_2\cdots,x_n$ 参数的似然函数为：</p>
<script type="math/tex; mode=display">
L(\theta;x_1,x_2,\cdots,x_n) = \prod_{i=1}^nf(x_i;\theta)</script><p>最大似然估计指的是在参数空间 $\Theta$​ 中选取随机样本$(X_1,X_2,\cdots,X_n)$落在点$(x_1,x_2\cdots,x_n)$附近最大概率的$\hat\theta$ 为未知参数$\theta$的估计值，即$\hat\theta$ 需要满足：</p>
<script type="math/tex; mode=display">
\hat\theta = \arg \max_{\theta} L(\theta;x_1,x_2\cdots,x_n)</script><h4 id="分布假定"><a href="#分布假定" class="headerlink" title="分布假定"></a>分布假定</h4><p>那么要做极大似然估计的话，我们首先要假定分布，有了密度函数才能得到似然函数。</p>
<p>在一元线性回归模型中，最常见的假定为$\varepsilon$ 服从正态分布，即</p>
<script type="math/tex; mode=display">
\varepsilon\sim N(0,\sigma^2)</script><p>从数据角度看，由于 $\varepsilon_1,\varepsilon_2\cdots,\varepsilon_n$ 都是与$\varepsilon$独立同分布的随机变量，因而有：</p>
<script type="math/tex; mode=display">
\varepsilon_i\sim N(0,\sigma^2),i=1,2,\cdots,n</script><p>在 $\varepsilon_i$ 服从正态分布的假定下，$y_i$ 也服从正态分布 ，即</p>
<script type="math/tex; mode=display">
y_i\sim N(\beta_0+\beta_1x_i,\sigma^2),i = 1,2\cdots,n</script><p>因此，我们可以得到$y_i$的密度函数：</p>
<script type="math/tex; mode=display">
f_i(y_i) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\{-\frac{1}{2\sigma^2}[y_i-(\beta_0+\beta_1x_i)]^2 \}</script><p>因为 $y_1,y_2\cdots,y_n$​的密度函数是和i相关的，各不相同，我们用 $f_i$ 代替 $f(y_i)$ 更为合适</p>
<p>那么，似然函数为：</p>
<script type="math/tex; mode=display">
\begin{align}
L(\beta_0,\beta_1,\sigma^2) &= \prod_{i=1}^n f_i(y_i)\\
&=(2\pi\sigma^2)^{-\frac{n}{2}}\exp\{-\frac{1}{2\sigma^2}\sum_{i=1}^n[y_i-(\beta_0+\beta_1x_i)]^2 \}
\end{align}</script><p>于是，对数似然函数为：</p>
<script type="math/tex; mode=display">
\ln L (\beta_0,\beta_1,\sigma^2) = -\frac{n}{2}\ln(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-(\beta_0+\beta_1x_i))^2</script><p>易知 L 的最大值点与 $\ln L$ 的最大值点是相同的。我们发现：</p>
<script type="math/tex; mode=display">
\arg\max_{\beta_0,\beta_1}\ln L(\beta_0,\beta_1,\sigma^2) \Leftrightarrow \min_{\beta_0,\beta_1}Q(\beta_0,\beta_1)</script><p>即：回归系数$\beta_0,\beta_1$的最大似然估计和最小二乘估计的形式是一致的。</p>
<h4 id="求解-sigma-2"><a href="#求解-sigma-2" class="headerlink" title="求解$\sigma^2$"></a>求解$\sigma^2$</h4><p>我们可以对$\ln L(\beta_0,\beta_1,\sigma^2)$关于$\sigma^2$求导，并令一阶导数为0，得到：</p>
<script type="math/tex; mode=display">
\frac{\partial \ln L}{\partial \sigma^2} =-\frac{n}{2\sigma^2}+\frac{1}{4\sigma^2}\sum_{i=1}^n(y_i-(\beta_0+\beta_1x_i))^2 = 0</script><p>那么，$\sigma^2$ 的最大似然估计为：</p>
<script type="math/tex; mode=display">
\hat\sigma^2_{ML} = \frac{1}{n}\sum_{i=1}^n(y_i-\hat y_i)^2\\
=\frac{1}{n}\sum_{i=1}^n(y_i-(\hat\beta_0+\hat\beta_1x_i))^2</script><p>$\hat\sigma_{ML}^2$是$\sigma^2$​ 的有偏估计。因为极大似然估计就是有偏的。这里我们有两个参数，因此要除以 n-2。</p>
<p>在实际应用中，更为常用的是$\sigma^2$ 的无偏估计，即：</p>
<script type="math/tex; mode=display">
\hat\sigma^2 = \frac{1}{n-2}\sum_{i=1}^n(y_i-(\hat\beta_0+\hat\beta_1x_i))^2</script><p>最大似然估计是在 $\varepsilon_i\sim N(0,\sigma^2)$ 的正态分布假设下求得的，而最小二乘估计则估计则对分布假设没有要求。所以说，极大似然估计也可以看成是最小二乘估计，只是多了一个条件。因此极大似然估计相对于最小二乘估计更为稳健</p>
<p>$y_1,y_2,\cdots,y_n$ 是独立的正态分布样本，而不是同分布的，但这并不妨碍最大似然方法去求解。因此极大似然估计只要定义出似然函数即可，不要求样本同分布。</p>
<h4 id="定理1"><a href="#定理1" class="headerlink" title="定理1"></a>定理1</h4><p>如果$y_1,y_2\cdots,y_n$ 是相互独立的且$y_i$是正态分布随机变量，即</p>
<script type="math/tex; mode=display">
y_i\sim N(\beta_0+\beta_1x_i,\sigma^2)</script><p>那么</p>
<ul>
<li><p>$\beta_0,\beta_1$ 均服从正态分布</p>
<script type="math/tex; mode=display">
\begin{align}
&\hat\beta_0\sim N(\beta_0,(\frac{1}{n}+\frac{\overline x^2}{l_{xx}})\sigma^2)\\
&\hat\beta_1\sim N(\beta_1,\frac{\sigma^2}{l_{xx}})
\end{align}</script></li>
<li><p>$\beta_0,\beta_1$两者的协方差如下：</p>
<script type="math/tex; mode=display">
Cov(\hat\beta_0,\hat\beta_1) = -\frac{\overline x}{l_{xx}}\sigma^2</script><p>从这个公式我们可以得出：只有当$\overline x=0$ 的时候，两者才不相关，要做到这一点需要对样本数据进行<strong>中心化</strong>，否则我们得到的$\beta_0$ 和 $\beta_1$ 都是相关的。</p>
</li>
</ul>
<h4 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h4><h5 id="步骤1：用-yi-线性表出-beta-0-beta-1"><a href="#步骤1：用-yi-线性表出-beta-0-beta-1" class="headerlink" title="步骤1：用$yi$线性表出$\beta{0},\beta_1$"></a>步骤1：用$y<em>i$线性表出$\beta</em>{0},\beta_1$</h5><ul>
<li>首先我们可以将 $\hat\beta_1$写成 $y_i$ 随机变量的一个加权求和的形式。(因为x是确定的)</li>
</ul>
<script type="math/tex; mode=display">
\begin{align}
\hat\beta_1 = l_{xx}^{-1}l_{yy} &= l_{xx}^{-1}\sum_{i=1}^n(x_i-\overline x)(y_i-\overline y)\\
&=l_{xx}^{-1}(\sum_{i=1}^m(x_i-\overline x)y_i-\sum_{i=1}^n(x_i-\overline x)\overline y)\\
&=l_{xx}^{-1}(\sum_{i=1}^n (x_i-\overline x)y_i)\\
&=\sum_{i=1}^n\frac{(x_i-\overline x)}{l_{xx}}y_i
\end{align}</script><p>第二个等式中的后者为0，因为 $\sum<em>{i=1}^n(x_i-\overline x)=\sum</em>{i=1}^n x_i-n\overline x=0$​</p>
<ul>
<li>我们可以将$\hat\beta_0$写为以下的形式：</li>
</ul>
<script type="math/tex; mode=display">
\begin{align}
\hat\beta_0& = \overline y -\hat\beta_1 \overline x\\
&=\frac{1}{n}\sum_{i=1}^n y_i -\sum_{i=1}^n\frac{(x_i-\overline x)}{l_{xx}}y_i\overline x\\
&=\sum_{i=1}^n(\frac{1}{n}-\frac{\overline x(x_i-\overline x)}{l_{xx}})y_i
\end{align}</script><p>因此$\hat\beta_0$和$\hat\beta_1$ 均可以看做是 $y_1,y_2\cdots,y_n$的线性组合。已知$y_1,y_2\cdots,y_n$是相互独立的正态随机变量，那么 $\hat\beta_0$和$\hat\beta_1$​均服从正态分布</p>
<h5 id="步骤2：考虑期望和方差"><a href="#步骤2：考虑期望和方差" class="headerlink" title="步骤2：考虑期望和方差"></a>步骤2：考虑期望和方差</h5><p>接下来，我们需要考虑这两个估计的均值与方差，从而进一步确定分布</p>
<ul>
<li>一方面，考虑 $\hat\beta_1$ 的期望与方差，即：</li>
</ul>
<script type="math/tex; mode=display">
\begin{align}
E(\hat\beta_1) &=\sum_{i=1}^n\frac{(x_i-\overline x)}{l_{xx}}E(y_i)
\\& = \sum_{i=1}^n (\frac{x_i-\overline x}{l_{xx}})(\beta_0+\beta_1x_i)\\
&=\sum_{i=1}^n \frac{(x_i-\overline x)}{l_{xx}}\beta_0+\sum_{i=1}^n\frac{x_i(x_i-\overline x)}{l_{xx}}\beta_1
\end{align}</script><p>我们可以让其减去$\sum<em>{i=1}^n\frac{\overline x(x_i-\overline x)}{l</em>{xx}}\beta_1$ 因为这个式子为0</p>
<script type="math/tex; mode=display">
=\sum_{i=1}^n \frac{(x_i-\overline x)}{l_{xx}}\beta_0+\sum_{i=1}^n\frac{x_i(x_i-\overline x)}{l_{xx}}\beta_1-\sum_{i=1}^n\frac{\overline x(x_i-\overline x)}{l_{xx}}\beta_1\\</script><p>将后两项展开，可以得到一个平方项</p>
<script type="math/tex; mode=display">
\sum_{i=1}^n\frac{(x_i-\overline x)}{l_{xx}}\beta_0+\sum_{i=1}^n\frac{(x_i-\overline x)^2}{l_{xx}}\beta_1</script><p>前面一项，因为$\sum<em>{i=1}^n(x_i-\overline x)=\sum</em>{i=1}^n x_i-n\overline x=0$​ ,因此</p>
<script type="math/tex; mode=display">
E(\hat\beta_1) = \beta_1</script><p>对于方差，就更为简单了：</p>
<script type="math/tex; mode=display">
Var(\hat\beta_1) = \sum_{i=1}^n(\frac{x_i-\overline x}{l_{xx}})^2Var(y_i)\\
= \sum_{i=1}^n \frac{(x_i-\overline x)^2}{(l_{xx})^2}\sigma^2 = \frac{\sigma^2}{l_{xx}}</script><ul>
<li>另一方面，考虑 $\hat\beta_0$ 的期望与方差，即</li>
</ul>
<script type="math/tex; mode=display">
E(\hat\beta_0) = E(\overline y)-E(\hat\beta_1)\overline x\\
~~~~~~~~~~~~~~~~~~~~~~~~=\frac{1}{n}\sum_{i=1}^n(\beta_0+\beta_1x)-\beta_1\overline x\\
~~~~~~~~~~~~~~~~~~~=\beta_0+\beta_1\overline x-\beta_1\overline x=\beta_0</script><p>方差：</p>
<script type="math/tex; mode=display">
Var(\hat\beta_0) = \sum_{i=1}^n(\frac{1}{n}-\frac{(x_i-\overline x)\overline x}{l_{xx}})^2\text{Var}(y_i)\\
=\sigma^2\sum_{i=1}^n(\frac{1}{n^2}-\frac{2(x_i-\overline x)\overline x}{nl_{xx}}+\frac{(x_i-\overline x)^2\overline x^2}{l^2_{xx}})\\
=\sigma^2(\frac{1}{n}+\frac{\overline x^2}{l_{xx}})</script><h5 id="第三步：求协方差"><a href="#第三步：求协方差" class="headerlink" title="第三步：求协方差"></a>第三步：求协方差</h5><p>因为 $y_1,y_2\cdots,y_n$相互独立，所以我们把第一步求出的$\beta_0,\beta_1$的式子带入，得到： </p>
<script type="math/tex; mode=display">
\text{Cov}(\hat\beta_0,\hat\beta_1) = \text{Cov}(\sum_{i=1}^n(\frac{1}{n}-\frac{(x_i-\overline x)\overline x}{l_{xx}})y_i~,\sum_{i=1}^n\frac{(x_i-\overline x)}{l_{xx}}y_i)</script><p>我们可以将它们看做是 $y_i$的线性组合之间的协方差。因为当$y_i$ 之间不相等，就说明他们是独立的，两个独立的协方差之间是0，因此我们只要看做是 $y_i,y_i$​即可.</p>
<p>那么，$y_i$​和$y_i$​的协方差就是$y_i$​​的方差，其方差为$\sigma^2$ ,前面$\sum$可以都看成是系数，提取出来即可，最后化简：</p>
<script type="math/tex; mode=display">
=\sum_{i=1}^n(\frac{1}{n}-\frac{(x_i-\overline x)\overline x}{l_{xx}})\sum_{i=1}^n\frac{(x_i-\overline x)}{l_{xx}}\sigma^2\\
=(\sum_{i=1}^n\frac{x_i-\overline x}{l_{xx}}-\sum_{i=1}^n\frac{(x_i-\overline x)^2\overline x}{l_{xx}} )\sigma^2\\
=-\frac{\overline x}{l_{xx}}\sigma^2</script><h5 id="第四步：给出推论"><a href="#第四步：给出推论" class="headerlink" title="第四步：给出推论"></a>第四步：给出推论</h5><ul>
<li>$\hat\beta_0,\hat\beta_1$​分别是$\beta_0,\beta_1$​​ 的无偏估计，而且这个无偏估计并不需要服从正态分布。 </li>
<li>除 $\overline x=0$ 外，$\hat\beta_0$与$\hat\beta_1$是相关的，只有当数据中心化以后才是不相关的</li>
<li><p>为了提高$\hat\beta<em>0,\hat\beta_1$的估计精度(即降低它们的方差)就要求样本量n增加，这样会使得$l</em>{xx}$增大，即要求 $x_1,\cdots,x_n$比较分散。这样会使得方差变小—数据波动更小</p>
<h4 id="tips"><a href="#tips" class="headerlink" title="tips"></a>tips</h4></li>
</ul>
<blockquote>
<ul>
<li>我提出了一个假设之后，在建模中就要以这个假设为默认条件</li>
<li>在建模的过程中，提出了很多假设，我们要用数据去验证是否符合这些假设</li>
</ul>
</blockquote>
<p>那么在这个线性回归模型中，我们提出了什么假设呢？</p>
<ul>
<li>最核心的假定为：线性性。即x是会影响y的。否则，如果x不在式子里，我们能用样本均值来预测y了。那么加了x之后，预测效果会不会提升呢？肯定是会的。因此，我们首先要检验出：x是否会对y产生影响。如果没有，我们干嘛用x去预测y呢？</li>
</ul>
<h2 id="回归方程的显著性检验"><a href="#回归方程的显著性检验" class="headerlink" title="回归方程的显著性检验"></a>回归方程的显著性检验</h2><p>首先我们要来考虑$\beta_1$是否为0，若真的为0，那么其实$y$和 $x$之间是没有必然联系的，但是，我们估出来的$\hat\beta_1$可能不是0，那么这时候怎么进行判断呢？那么在这种情况下，对一个参数的估计有点类似于假设检验的问题。 检验问题为：</p>
<script type="math/tex; mode=display">
H_0:\beta_1 =0 ~~\text{vs} ~~H_1 : \beta_1 \neq 0</script><p>如果我们的得到的结论是拒绝$H_0$ ,那么我们认为回归方程是显著的。</p>
<p>接下来我们会介绍三种估计方法</p>
<h3 id="F检验"><a href="#F检验" class="headerlink" title="F检验"></a>F检验</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>我们用<strong>方差分析</strong>的思想：</p>
<ul>
<li>令回归值为 $\hat y_i = \hat\beta_0+\hat\beta_1 x_i$   残差：$e_i = y_i-\hat y_i$ </li>
<li><strong>偏差平方和</strong>为 $SS<em>T = \sum</em>{i=1}^n(y<em>i-\overline y)^2 = l</em>{yy}$​ ,其中 $\overline y = \frac{1}{n}\sum_{i=1}^n y_i$​ </li>
<li>引起$y<em>i$​ 不同的原因主要是因为$H_0$​ 可能不真，即$\beta_1\neq 0$​ ，即在每一个x的观测处的回归值都不相同。因此我们可以定义<strong>回归平方和</strong>为：$SS_R = \sum</em>{i=1}^n (\hat y_i-\overline y)^2$​​​ . 也就是落在直线上的点与均值的平方和</li>
<li>引起$y<em>i$​​​ 不同还可能因为误差，因此在得到回归值之后，y的观测值与回归值之间还有差异。因此可以定义<strong>残差平方和</strong>为：$SS_E = \sum</em>{i=1}^n (y_i-\hat y_i)^2$​ </li>
</ul>
<p>那么，我们可以在一元线性回归场合下的平方和分解式：</p>
<script type="math/tex; mode=display">
\sum_{i=1}^n(y_i-\overline y)^2 = \sum_{i=1}^n(y_i-\hat y_i+\hat y_i-\overline y)^2\\
=\sum_{i=1}^n(y_i-\hat y_i)^2+\sum_{i=1}^n(\hat y_i-\overline y)^2\\</script><p>我们发现，其交叉项是0</p>
<p>最后这个式子就代表： $SS_T = SS_R+SS_E$ </p>
<h4 id="定理2"><a href="#定理2" class="headerlink" title="定理2"></a>定理2</h4><p>设 $y_i = \beta_0+\beta_1x_i+\varepsilon_i$</p>
<p>其中， $\varepsilon_i,\cdots,\varepsilon_n$ 相互独立，且：</p>
<script type="math/tex; mode=display">
E(\varepsilon_i)= 0,\text{Var} (\varepsilon_i) = \sigma^2 ,i=1,2\cdots,n</script><p>我们有：</p>
<script type="math/tex; mode=display">
E(SS_R) = \sigma^2+\beta_1^2l_{xx}\\
E(SS_E) = (n-2)\sigma^2</script><h5 id="证明-1"><a href="#证明-1" class="headerlink" title="证明"></a>证明</h5><ul>
<li>我们首先来关注$SS_R$​</li>
</ul>
<p>它可以看做是有线性回归模型下的估计($\hat y$​) 与无线性回归模型下的估计($\overline y$​)的差的平方和</p>
<p>在最小二乘估计中我们得到了 $\hat\beta_0 = \overline y -\hat\beta_1\overline x $​,从中可以获得$\beta$关于 $\overline y$ 的式子</p>
<script type="math/tex; mode=display">
SS_R = \sum_{i=1}^n(\hat y_i-\overline y) = \sum_{i=1}^n((\hat\beta_0+\hat\beta_1 x_i)-(\hat\beta_0+\hat\beta_1\overline x))^2=\hat\beta_1^2l_{xx}</script><p>然后我们对 $SS_R$​求期望，这里面$\hat \beta_1$​是随机变量，这里假定x是固定的变量。运用 $Var(x)=(E(X))^2-E(X^2)$​ 可得</p>
<script type="math/tex; mode=display">
E(SS_R) =E(\hat\beta^2_1)l_{xx} =(Var(\hat\beta_1)+(E(\hat\beta_1))^2)l_{xx}\\</script><p>又因为在定理1中证明了$E(\hat\beta_1)$​是无偏的，$E(\hat\beta_1)=\beta_1$​</p>
<p>因此</p>
<script type="math/tex; mode=display">
=(\frac{\sigma^2}{l_{xx}}+\beta_1^2)l_{xx}\\
=\sigma^2+\beta_1^2l_{xx}</script><ul>
<li>然后来关注$SS_E$​</li>
</ul>
<script type="math/tex; mode=display">
SS_E = \sum_{i=1}^n(y_i-\hat y_i)^2 = \sum_{i=1}^n(\beta_0+\beta_1 x_i+\epsilon_i-\hat\beta_0-\hat\beta_1 x_i)^2\\
=\sum_{i=1}^n((\beta_0-\hat\beta_0)+(\beta_1-\hat\beta_1)x_i+\varepsilon_i)^2\\
=\sum_{i=1}^n((\beta_0-\hat\beta_0)^2+(\beta_1-\hat\beta_1)^2x_i^2+\varepsilon_i^2+2(\beta_0-\hat\beta_0)(\beta_1-\hat\beta_1)+2(\beta_0-\hat\beta_0)\varepsilon_i+2(\beta_1-\hat\beta_1)x_i\varepsilon_i)</script><blockquote>
<p> 我们可以把 $\sum_{i=1}^nE(\beta_0-\hat\beta_0)^2$​ 看做是n倍的$Var(\hat\beta_0)$​​  因为 $\hat\beta_0$是$\beta$的无偏估计，因此$\beta$可以看做是$E(\beta_0)$​</p>
<p>同理，$\sum<em>{i=1}^nE(\beta_1-\hat\beta_1)^2x_i^2=Var(\hat\beta_1)\sum</em>{i=1}^n x_i^2$​ </p>
<p>$2E(\sum_{i=1}^n(\beta_0-\hat\beta_0)(\beta_1-\hat\beta_1))=2nE(\beta_0-\hat\beta_0)(\beta_1-\hat\beta_1) = 2nCov(\hat\beta_1,\hat\beta_0)$​</p>
<p>又因为$\beta_0,\varepsilon_i$是独立的，因此：$E(\beta_0\varepsilon_i)=E(\beta_0)E(\varepsilon_i) = 0$​</p>
</blockquote>
<p>因此，</p>
<script type="math/tex; mode=display">
\begin{align}
E(SS_E) = &nVar(\hat\beta_0)+Var(\hat\beta_1)\sum_{i=1}^nx_i^2+nVar(\varepsilon_i)+2n\text{Cov}(\hat\beta_1,\hat\beta_0) \\
&-2\sum_{i=1}^nE(\hat\beta_0\varepsilon_i)-2\sum_{i=1}^nx_iE(\hat\beta_1\varepsilon_i)
\end{align}</script><p>在这个式子中，我们已知的是$nVar(\hat\beta_0),Var(\hat\beta_1),Var(\varepsilon_i),2nCov(\hat\beta_1,\hat\beta_0)$ 因此我们现在要算 $E(\hat\beta_0\varepsilon_i),E(\hat\beta_1\varepsilon_i)$​</p>
<p>首先我们要把$\hat\beta_0,\hat\beta_1$​​ 写成是$y_1,\cdots,y_n$的线性组合</p>
<script type="math/tex; mode=display">
\hat\beta_0 = \sum_i (\frac{1}{n}-\frac{(x_i-\overline x)\overline x}{l_{xx}})y_i\\
\hat\beta_1 = \sum_i \frac{(x_i-\overline x)}{l_{xx}}y_i</script><ul>
<li>对于 $E(\hat\beta_0\varepsilon_i)$,我们知道$y_i$之间是独立的 </li>
</ul>
<script type="math/tex; mode=display">
E(\hat\beta_0\varepsilon_i) =E(\varepsilon_i\sum_{j}(\frac{1}{n}-\frac{(x_j-\overline x)\overline x}{l_{xx}})y_i)\\

=E(\varepsilon_i\sum_{j}(\frac{1}{n}-\frac{(x_j-\overline x)\overline x}{l_{xx}})(\beta_0+\beta_1x_j+\varepsilon_j))\\</script><p>因此当 $j\neq i$ 的时候，期望可以写成 $E(\varepsilon_i)\cdot E(\cdots) = 0$,因此，只有当 $i=j$ 的时候才需要被留下：</p>
<script type="math/tex; mode=display">
\begin{align}
&E(\varepsilon_i\sum_{j}(\frac{1}{n}-\frac{(x_j-\overline x)\overline x}{l_{xx}})(\beta_0+\beta_1x_i+\varepsilon_i))\\
&= E(\varepsilon^2_i(\frac{1}{n}-\frac{(x_i-\overline x)\overline x}{l_{xx}}))\\
&=(\frac{1}{n}-\frac{(x_i-\overline x)\overline x}{l_{xx}})\sigma^2
\end{align}</script><ul>
<li>对于$E(\hat\beta_1\varepsilon_i)$,同样用类似的思路证明：</li>
</ul>
<script type="math/tex; mode=display">
\begin{align}
E(\hat\beta_1\varepsilon_i) &= E(\varepsilon_i\sum_{j}\frac{x_j-\overline x}{l_{xx}}y_j)\\
&=E(\varepsilon_i\sum_{j}\frac{(x_j-\overline x)}{l_{xx}}(\beta_0+\beta_1x_j+\varepsilon_i))\\
&=\frac{(x_i-\overline x)}{l_{xx}}\sigma^2
\end{align}</script><p>因此：</p>
<script type="math/tex; mode=display">
\sum_{i=1}^nE(\hat\beta_0\varepsilon_i) = \sum_{i=1}^n(\frac{1}{n}-\frac{(x_i-\overline x)\overline x}{l_{xx}})\sigma^2= \sigma^2\\
\sum_{i=1}^n x_iE(\hat\beta_1\varepsilon_i) = \sum_{i=1}^n x_i\frac{(x_i-\overline x)}{l_{xx}}\sigma^2</script><p>已知 $l<em>{xx}=\sum</em>{i=1}^n(x<em>i-\overline x)^2 = \sum</em>{i=1}^n x<em>i^2-2\sum</em>{i=1}^nx<em>i\overline x+n(\overline x)^2=\sum</em>{i=1}^n x_i^2-n\overline x$​.  因此原式 $ = \sigma^2$</p>
<p>综上，</p>
<script type="math/tex; mode=display">
\begin{align}
E(SS_E ) &= n(\frac{1}{n}+\frac{\overline x^2}{l_{xx}})\sigma^2+\frac{\sigma^2}{l_{xx}}\sum_ix_i^2+n\sigma^2-2n\frac{\overline x^2}{l_{xx}}\sigma^2-2\sigma^2-2\sigma^2\\
&=\sigma^2(1+\frac{\sum x_i^2}{l_{xx}}-\frac{n\overline x^2}{l_{xx}}+n-4)\\
&=(n-2)\sigma^2
\end{align}</script><h4 id="定理3"><a href="#定理3" class="headerlink" title="定理3"></a>定理3</h4><p>设 $y_1,\cdots,y_n$ 相互独立，且：</p>
<script type="math/tex; mode=display">
y_i\sim N(\beta_0+\beta_1x_i,\sigma^2),i = 1,2\cdots,n\\</script><p>则有：</p>
<ul>
<li>$SS_E/\sigma^2\sim\mathcal X^2(n-2)$</li>
<li>若$H_0$ 成立，则有 $SS_R/\sigma^2\sim \mathcal X^2(1)$</li>
<li>$SS_R$​​与 $SS_E$​​，$\overline y$​​ 独立 </li>
</ul>
<p>这三点在回归里面是非常核心的，其重要性在于其构造出了F检验。</p>
<h5 id="证明-1"><a href="#证明-1" class="headerlink" title="证明-1"></a>证明-1</h5><p>首先，我们需要构造一个<strong>正交矩阵A</strong>，形如：</p>
<p><img src="/2021/10/11/%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/2.png" alt="1" style="zoom:67%;"></p>
<p>根据正交矩阵的性质$AA’=I$​, 则A需要满足：</p>
<script type="math/tex; mode=display">
\sum_k  a_{i,k}a_{j,k} =0,1\leq i<j\leq n-2\\
\frac{1}{\sqrt n} \sum_j a_{i,j} = 0\\
\sum_j a_{i,j}\frac{x_j-\overline x}{\sqrt{l_{xx}}}\\
\sum_{j}(a_{i,j})^2 = 1</script><blockquote>
<p>第一个条件是说，如果i和j不相等的话，那么第i行和第j行对应元素的乘积的和是要等于0的</p>
<p>第二个条件是：前面n-2行与倒数第二行的对应元素的乘积的和要等于0，因为不再对角线上</p>
<p>第三个条件是：前面n-2行与最后一行的对应元素的乘积的和要等于0，因为也不再对角线上</p>
<p>第四个条件是: 对角线上要满足都是1，因此每一行各个元素的平方和为1</p>
</blockquote>
<p>在这个矩阵中，前面 n-2行是不确定的，因此有 n(n-2)的未知数。然后第一个条件有$\pmatrix{n-2\2}$​个方程，后面三个条件每一个都有$n-2$​个方程，因此一共有 $\frac{(n-2)(n-3)}{2}$个方程。​</p>
<p>因此，只要 $n\geq 3$​ ，未知参数个数不少于方程个数，因此，正交矩阵A一定是存在的</p>
<p>接下来，我们令 $z=Ay$, 其中 $z=(z_1,z_2\cdots,z_n)’$ 满足</p>
<script type="math/tex; mode=display">
z_i = \sum_ja_{ij}y_j, ~~i = 1,\cdots,n-2\\
z_{n-1} = \frac{\sum_{j}(x_j-\overline x)y_j}{\sqrt{l_{xx}}} = \frac{\sum_{j} (x_j-\overline x)(y_j-\overline y)}{\sqrt{l_{xx}}} = \frac{l_{xy}}{\sqrt{l_{xx}}} = \sqrt{l_{xx}} \cdot  \frac{l_{xy}}{l_{xx }} = \sqrt {l_{xx }}\hat\beta_1\\
z_n = \frac{1}{\sqrt n}\sum_j y_j = \sqrt n \overline y</script><blockquote>
<p>第二个式子是因为：$l<em>{xy} = \sum</em>{i=1}^n x_iy_i-n\overline x\cdot \overline y$</p>
<p>又： $\sum<em>{i=1}^n (x_i-\overline x)y_i = \sum</em>{i=1}^nx<em>iy_i-\sum</em>{i=1}^n y<em>i\overline x= \sum</em>{i=1}^n x_iy_i-n\overline y\cdot\overline x$ </p>
<p>两者是等价的</p>
</blockquote>
<p>我们看到z也是正态随机变量的线性组合，因此可以很方便的记录其均值和方差：</p>
<script type="math/tex; mode=display">
E(z_i) = E(\sum_j a_{ij}y_j) = \sum_j a_{ij}(\beta_0+\beta_1x_j)\\
=\beta_0\sum_j a_{ij} +\beta_1\sum_ja_{ij}x_j = 0,i=1,\cdots,n\\
E(z_{n-1}) = \sqrt{l_{xx}}\beta_1\\
E(z_n) = \sqrt n (\beta_0+\beta_1\hat x)\\
Var(z) = Var(Ay) = \bold{A}Var(y)\bold{A}' = \bold{A}\sigma^2\bold{I_n}\bold{A'} = \sigma^2\bold{I_n}</script><blockquote>
<p>$\sum<em>ja</em>{ij}=0$​,因此 第一个式子等于0</p>
</blockquote>
<p>由上面的结论，可以得出以下结论：</p>
<ul>
<li>我们知道Z是服从正态分布的，又z的方差为对角矩阵，是不存在协方差的。因此，$z_1,z_2\cdots,z_n$是相互独立的。</li>
<li>前n-2个分量$z_1,z_2\cdots,z_n$是独立同分布的，且分布为$N(0,\sigma^2)$</li>
<li>$z<em>{n-1}$的分布为$N(\sqrt{l</em>{xx}}\beta_1,\sigma^2 )$</li>
<li>$z_n$ 的分布为$N(\sqrt n(\beta_0+\beta_1\overline x),\sigma^2)$</li>
</ul>
<p>最后，我们要用$z$去表示 $SS_T,SS_R,SS_E$</p>
<script type="math/tex; mode=display">
\sum_{i=1}^n z_i^2 = z'z = \bold{y'A'Ay} = \bold{y'y} = \sum_{i}y_i^2 =l_{yy}+n\overline y^2= SS_T +n\overline y^2\\
z_{n-1} = \sqrt{l_{xx}}\hat\beta_1 = \sqrt {SS_R}\\
z_n = \sqrt n\overline y</script><p>然后我们可以整理为：</p>
<script type="math/tex; mode=display">
SS_T+n\overline y^2 =\sum_{i=1}^n z_i^2 =  \sum_{i=1}^{n-2} z_i^2+SS_R+n\overline y^2\\</script><p>即 $SS<em>T = \sum</em>{i=1}^{n-2}z_i^2+SS_R$</p>
<p>因为 $SS_T = SS_R+SS_E$,所以 $SS_E$的分布就得到了：是一些列正态随机变量的平方和，是一个卡方分布</p>
<script type="math/tex; mode=display">
SS_E = \sum_{i=1}^{n-2}z_i^2\sim \mathcal X^2(n-2)</script><h5 id="证明-2"><a href="#证明-2" class="headerlink" title="证明-2"></a>证明-2</h5><p>若$H_0$成立,则$\beta_1= 0$, 可以得到：</p>
<script type="math/tex; mode=display">
\sqrt {SS_R} = z_{n-1}\sim N(0,\sigma^2)</script><p>所以，变换得到：</p>
<script type="math/tex; mode=display">
\frac{SS_R}{\sigma^2} = (\frac{z_{n-1}}{\sigma})^2\sim \mathcal X^2(1)</script><h5 id="证明-3"><a href="#证明-3" class="headerlink" title="证明-3"></a>证明-3</h5><p>因为$SS<em>E$与前 $n-2$个$z_i$有关，$SS_R$仅与 $z</em>{n-1}$有关，$\overline y$仅与$z_n$有关，因此$SS_R$和 $SS_E,\overline y$相互独立</p>
<p>因为$\hat\beta_1$ 仅与$SS_R$有关，所以$\hat\beta_1$与$SS_E,\overline y$​相互独立</p>
<h4 id="检验统计量"><a href="#检验统计量" class="headerlink" title="检验统计量"></a>检验统计量</h4><p>现在我们来进行假设检验：</p>
<p>构造形如</p>
<script type="math/tex; mode=display">
F_0 = \frac{SS_R}{SS_E/(n-2)}</script><p>的检验统计量来检验。那么，在$\beta_1 = 0$时，$F\sim F(1,n-2)$，对于给定的显著性水平$\alpha$,其拒绝域为：</p>
<script type="math/tex; mode=display">
F_0\geq F_{1-\alpha}(1,n-2)</script><p>即大于临界值，SSR就越大，说明$\beta_1$是起作用的，我们就拒绝原假设</p>
<p>这就是F检验</p>
<p><img src="/2021/10/11/%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/3.png" style="zoom:67%;"></p>
<h3 id="t检验"><a href="#t检验" class="headerlink" title="t检验"></a>t检验</h3><p>如果说F检验的方法的内核是方差分析，那么t检验的方法的内核就是参数估计了——我通过去找到点估计，找到枢轴量，构造检验统计量然后去做检验。</p>
<h4 id="检验统计量-1"><a href="#检验统计量-1" class="headerlink" title="检验统计量"></a>检验统计量</h4><p><img src="/2021/10/11/%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/6.png" style="zoom:67%;"></p>
<h3 id="相关系数的检验"><a href="#相关系数的检验" class="headerlink" title="相关系数的检验"></a>相关系数的检验</h3><h4 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h4><p>现在我们来聊第三个问题，一个模型中$\beta_1$ 如果不起作用的话，就说明x，y之间是没有线性关系的。</p>
<p>那么，由于一元线性回归方差可以反映出两个随机变量x与y间的相关关系时，它的显著性检验还可以通过对二维总体相关系数$\rho$​的检验进行。</p>
<p>假设为：</p>
<script type="math/tex; mode=display">
H_0:\rho = 0~~~vs~~~H_1:\rho\neq 0</script><p>如果样本相关系数很小，很有理由认为总体均值为0，接受原假设，反之，我们拒绝原假设。</p>
<p>我们将样本相关系数作为检验统计量：</p>
<script type="math/tex; mode=display">
r = \frac{\sum_{i=1}^n(x_i-\overline x)(y_i-\overline y)}{\sqrt{\sum_{i=1}^n(x_i-\overline x)^2\sum_{i=1}^n(y_i-\overline y)^2}} = \frac{l_{xy}}{\sqrt{l_{xx}l_{yy}}}</script><p>下面我们用具体的图来理解样本相关系数的大小：</p>
<p><img src="/2021/10/11/%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/4.png" style="zoom: 50%;"></p>
<p>对于a、b两张图，直线是确定的，所有点都在直线上面。因此我们算出来的样本相关系数的绝对值算出来一定是1</p>
<p>对于c，我们发现这个点的分布基本是随机地，散乱的，此时其相关系数接近于0</p>
<p>对于d，我们发现点复合某个函数，并不是线性的，但是其相关系数也不等于0</p>
<p>对于e、f，状态是比较理想的。</p>
<h4 id="检验统计量-2"><a href="#检验统计量-2" class="headerlink" title="检验统计量"></a>检验统计量</h4><p>当$H_0$为真时，$|r|$应该比较小；当 $|r|$​  比较大的时候，应该拒绝原假设</p>
<p>因此，拒绝域为${|r|\geq c}$ ，其中，临界值c可以由$H_0$成立时的样本相关系数的分布确定，该分布与自由度$n-2$有关。</p>
<p>对给定的显著性水平$\alpha$ ,由$P(W) = P(|r|\geq c)=\alpha$ 可知，临界值c应该是$H<em>0:\rho = 0$成立下r的分布的$1-\alpha/2$ 分位数，故我们记为：$c = r</em>{1-\alpha/2}(n-2)$</p>
<p>那么，这个临界值的式子给出了，怎么获得呢? 因此我们要考虑三个检验之间的关系</p>
<h2 id="三种检验之间的关系"><a href="#三种检验之间的关系" class="headerlink" title="三种检验之间的关系"></a>三种检验之间的关系</h2><h3 id="t检验与F检验统计量的关系"><a href="#t检验与F检验统计量的关系" class="headerlink" title="t检验与F检验统计量的关系"></a>t检验与F检验统计量的关系</h3><script type="math/tex; mode=display">
t_0^2=(\frac{\hat\beta_1}{\hat\sigma/\sqrt{l_{xx}}})^2 = \frac{\hat\beta_1^2l_{xx}}{\sqrt{SS_E/(n-2)}} = \frac{SS_R}{SS_E/(n-2)} = F_0</script><p>其中，第三个等式成立是因为回归平方和$SS_R$与$\hat\beta_1$之间存在如下关系：</p>
<script type="math/tex; mode=display">
SS_R = \sum_{i=1}^n(\hat y_i-\overline y)^2 = \sum_{i=1}^n(\hat\beta_0+\hat\beta_1x_i-\overline y)^2\\
=\sum_{i=1}^n[\overline y+\hat\beta_1(x_i-\overline x)-\overline y]^2 \\
=\sum_{i=1}^n[\hat\beta_1(x_i-\overline x)]^2= \beta_1^2l_{xx}</script><p>因此，实际上F检验和t检验是等价的。</p>
<h3 id="F检验统计量与r的关系"><a href="#F检验统计量与r的关系" class="headerlink" title="F检验统计量与r的关系"></a>F检验统计量与r的关系</h3><script type="math/tex; mode=display">
r^2 = (\hat\beta_1\sqrt{\frac{l_{xx}}{l_{yy}}})^2 = \hat\beta_1^2\frac{l_{xx}}{l_{yy}} = \frac{SS_R}{SS_T} = \frac{SS_R}{SS_R+SS_E}</script><p>分子分母同除以 $SS_E/(n-2)$可以得到:</p>
<script type="math/tex; mode=display">
=\frac{SS_R/(SS_E/(n-2))}{SS_R/(SS_E/(n-2))+n-2}=\frac{F_0}{F_0+(n-2)}</script><p>这表明了 $|r|$ 是 $F<em>0$ 的严格单调增函数，因此可以从F分布的$1-\alpha$ 分位数 $F</em>{1-\alpha}(1,n-2)$ 得到相关系数检验 所需要确定的临界值 $r_{1-\alpha/2}(n-2)$, 即</p>
<script type="math/tex; mode=display">
r_{1-\alpha/2}(n-2) = \sqrt{\frac{F_{1-\alpha}(1,n-2)}{F_{1-\alpha}(1,n-2)+(n-2)}}</script><p>因此，$r^2$ 会常常作为回归分析中一项重要的指标。我们定义<strong>样本决定系数</strong>为<strong>回归平方和</strong>与<strong>总偏差平方和</strong>之比，即：</p>
<script type="math/tex; mode=display">
r^2 = \frac{SS_R}{SS_T}</script><p>样本决定系数$r^2$是一个回归直线与样本观测值拟合优度的相对指标，反应因变量的波动中能用自变量解释的比例。$r^2$ 的取值在0到1之前。 $r^2$​越接近1，拟合度越好</p>
<blockquote>
<p>通常来说我们更看中$R^2$​ 其定义是 $\frac{SS_R}{SS_T}$ ，在一元线性回归中，刚好其等于样本方差的平方即 $R^2 =r^2$​</p>
</blockquote>
<h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><ul>
<li>三种检验方法在一元线性回归模型下是等价的</li>
<li>但在多元线性回归场合，经推广F检验仍然可用，另两个检验就无法使用了</li>
<li>如果无法拒绝原假设，则可以认为回归方程不是显著的，导致这种情况可能有如下几种原因<ul>
<li>误差与正态假设严重偏离</li>
<li>Y与X无关</li>
<li>Y与X虽然相关，但不是线性关系</li>
<li>Y与X以外的因素有更密切的关系</li>
</ul>
</li>
</ul>
<h2 id="估计与预测"><a href="#估计与预测" class="headerlink" title="估计与预测"></a>估计与预测</h2><p>之前我们所说的都是建模，然后确定模型是否有效。现在我们终于要来做预测了。那么，我们对什么来做预测呢？——随机变量。</p>
<p>当$x=x_0$时，我们关心的是 $y_0 = \beta_0+\beta_1x_0+\epsilon_0$ </p>
<p>注意到，$y_0$是本身一个随机变量。 对于一个随机变量，在掌握了单个数值信息的情况下是很难刻画y的分布的，因此我们可以构造一个区间，使得$y_0$ 落在区间的概率为$1-\alpha$,即确定一个常数$\var$ 使得$P(|y_0-\hat y_0|\leq \var) = 1-\alpha$ ，称区间$[\hat y_0-\var,\hat y_0+\var]$ 为$y_0$的概率为$1-\alpha$ 的预测区间，这是一个<strong>预测问题</strong></p>
<h3 id="关于-E-y-0-​的估计"><a href="#关于-E-y-0-​的估计" class="headerlink" title="关于$E(y_0)$​的估计"></a>关于$E(y_0)$​的估计</h3><h4 id="点估计"><a href="#点估计" class="headerlink" title="点估计"></a>点估计</h4><p>我们只能通过期望。在$x=x_0$时，我们需要考虑$E(y_0) = \beta_0+\beta_1x_0$​ 一个直观的估计为：</p>
<script type="math/tex; mode=display">
\hat E(y_0) = \hat\beta_0+\hat\beta_1x_0</script><p>通常记为$\hat y_0$​ , 表示在 $x=x_0$ 时响应变量的估计值</p>
<h4 id="定理-4"><a href="#定理-4" class="headerlink" title="定理 4"></a>定理 4</h4><p>如果 $y_1,y_2\cdots,y_n$ 是相互独立的且$y_i$ 是正态分布随机变量，即 $y_i\sim N(\beta_0+\beta_1x_i,\sigma^2)$,那么，对给定的$x_0$</p>
<script type="math/tex; mode=display">
\hat y_0 = \hat\beta_0+\hat\beta_1x_0 \sim N(\beta_0+\beta_1x_0,(\frac{1}{n}+\frac{(x_0-\overline x)^2}{l_{xx}})\sigma^2)</script><h5 id="证明-2"><a href="#证明-2" class="headerlink" title="证明"></a>证明</h5><p>根据定理1可得：</p>
<script type="math/tex; mode=display">
\hat\beta_0\sim N(\beta_0,(\frac{1}{n}+\frac{\overline x^2}{l_{xx}})\sigma^2)\\
\hat\beta_1\sim N(\beta_1,\frac{\sigma^2}{l_{xx}})</script><p>因此在给定$x_0$的时候，$\hat y_0$ 也是 $y_1\cdots,y_n$的线性组合，因此$\hat y_0$ 也服从正态分布。其均值和方差为：</p>
<script type="math/tex; mode=display">
\begin{align}
E(\hat y_0)&= E(\hat\beta_0)+E(\hat\beta_1)x_0 = \beta_0+\beta_1x_0\\
Var(\hat y_0)& = Var(\hat\beta_0+\hat\beta_1x_0)\\
&=Var(\hat\beta_0)+Var(\hat\beta_1)x_0^2+2Cov(\hat\beta_0,\hat\beta_1)x_0\\
&=(\frac{1}{n}+\frac{\overline x^2}{l_{xx}})\sigma^2+\frac{x_0^2\sigma^2}{l_{xx}}-\frac{2\overline xx_0}{l_{xx}}\sigma^2\\
&=(\frac{1}{n}+\frac{(x_0-\overline x)^2}{l_{xx}})\sigma^2
\end{align}</script><h4 id="区间估计"><a href="#区间估计" class="headerlink" title="区间估计"></a>区间估计</h4><p>已知$\hat y_0$的分布为</p>
<script type="math/tex; mode=display">
\hat y_0 = \hat\beta_0+\hat\beta_1x_0 \sim N(\beta_0+\beta_1x_0,(\frac{1}{n}+\frac{(x_0-\overline x)^2}{l_{xx}})\sigma^2)</script><p>现在要构造枢轴量，其中，$\sigma^2$ 可以用其估计代替</p>
<p>于是，我们有：</p>
<script type="math/tex; mode=display">
\frac{(\hat y_0-E(y_0))/\sqrt{\frac{1}{n}+\frac{x_0-\overline x}{l_{xx}}\sigma^2}}{\sqrt{\frac{SS_E}{\sigma^2}/(n-2)}} =\frac{\hat y_0-E(y_0)}{\hat\sigma\sqrt{\frac{1}{n}+\frac{(x_0-\overline x)^2}{l_{xx}}}}\sim t(n-2)</script><p>因此，$E(y_0)$的置信水平为$1-\alpha$的置信区间为：</p>
<script type="math/tex; mode=display">
[\hat y_0-\var,\hat y_0+\var]</script><p>其中，</p>
<script type="math/tex; mode=display">
\var = t_{1-\alpha/2}(n-2)\hat\sigma\sqrt{\frac{1}{n}+\frac{(x_0-\overline x)^2}{l_{xx}}}</script><h3 id="关于-y-0-​​-的预测"><a href="#关于-y-0-​​-的预测" class="headerlink" title="关于$y_0 $​​ 的预测"></a>关于$y_0 $​​ 的预测</h3><h4 id="点预测"><a href="#点预测" class="headerlink" title="点预测"></a>点预测</h4><p>在预测$y_0$的时候，我们如果考虑点预测，那么通常我们还是取这个随机变量的均值，于是$y_0$的点预测也是$\hat y_0$</p>
<p>但是，由于$y_0$是一个连续随机变量，恰好取到一个点的概率为0，因此在实际应用中，对$y_0$进行区间预测更为合理。</p>
<h4 id="区间预测"><a href="#区间预测" class="headerlink" title="区间预测"></a>区间预测</h4><p>事实上 $y_0 = E(y_0)+\varepsilon_0$ ，因为通常假定$\varepsilon_0\sim N(0,\sigma^2)$ ，所以$y_0$的最有可能的取值仍然是$\hat y_0$ </p>
<p>因此，我们可以使用一个以$\hat y_0$为中心的区间：</p>
<script type="math/tex; mode=display">
[\hat y_0-\var,\hat y_0+\var]</script><p>作为$y_0$的取值范围，如何确定$\var$的值是需要进一步讨论的。</p>
<p>和$E(y_0)$的去测过程一样，我们要构造一个枢轴量</p>
<p>一方面，我们知道：</p>
<script type="math/tex; mode=display">
y_0\sim N(\beta_0+\beta_1x_0,\sigma^2)</script><p>其点预测我们之前已经推出：</p>
<script type="math/tex; mode=display">
\hat y_0 = \hat\beta_0+\hat\beta_1x_0 \sim N(\beta_0+\beta_1x_0,(\frac{1}{n}+\frac{(x_0-\overline x)^2}{l_{xx}})\sigma^2)</script><p>又因为 $y_0$ 与$\hat y_0$ 独立，所以：</p>
<script type="math/tex; mode=display">
y_0-\hat y_0 \sim N(0,(1+\frac{1}{n}+\frac{(x_0-\overline x)^2}{l_{xx}})\sigma^2)</script><p>另一方面，因为 ${(n-2)\hat\sigma^2}/{\sigma^2}\sim \mathcal{X^2}(n-2)$​​，而且$y_0,\hat y_0,\hat\sigma^2$相互独立，所以有：</p>
<script type="math/tex; mode=display">
\frac{y_0-\hat y_0}{\hat\sigma\sqrt{1+\frac{1}{n}+\frac{(x_0-\overline x)^2}{l_{xx}}}}\sim t(n-2)</script><p>因此，预测区间为：</p>
<script type="math/tex; mode=display">
[\hat y_0-\var,\hat y_0+\var]</script><p>其中，$\var$ 为：</p>
<script type="math/tex; mode=display">
\var = \var(x_0) = t_{1-\alpha/2}(n-2)\hat\sigma\sqrt{1+\frac{1}{n}+\frac{(x_0-\overline x)^2}{l_{xx}}}</script><p>我们发现，估计区间中的$\var$和预测区间中的$\var$ 是不一样的，预测区间中的$\var$会更加宽一点。因此我们发现，当预测的时候，波动更大，因为我们会引入新样本。</p>
<p><img src="/2021/10/11/%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/5.png"></p>
<p>观察上图，我们发现预测区间呈现一个喇叭口的形状，越接近$\overline x$ 越窄，越两变越宽。说明数据波动的越大，会导致$\sigma$的估计越大，最终导致$\var$越大。</p>
<p>此外我们还要知道，这个区间是和样本量有关系的，如果样本量越大，$n$越大，因此区间会越窄</p>
<p>此外，如果我们要预测的新样本和训练集的均值离得非常远，那么预测区间就越大，预测效果就越差。在一元线性回归中，表现为离均值$\overline x$越远预测效果越差。</p>

          
        
      
    </div>
    
    
    
    <div>
      
    </div>
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2021/10/07/%E5%A4%9A%E9%87%8D%E6%AF%94%E8%BE%83%E5%92%8C%E5%8F%8C%E5%9B%A0%E5%AD%90%E6%96%B9%E5%B7%AE%E5%88%86%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/%5Bobject%20Object%5D">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/10/07/%E5%A4%9A%E9%87%8D%E6%AF%94%E8%BE%83%E5%92%8C%E5%8F%8C%E5%9B%A0%E5%AD%90%E6%96%B9%E5%B7%AE%E5%88%86%E6%9E%90/" itemprop="url">多重比较和双因子方差分析</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-10-07T08:38:20+08:00">
                2021-10-07
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2022-01-05T13:13:02+08:00">
                2022-01-05
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="多重比较和双因子方差分析"><a href="#多重比较和双因子方差分析" class="headerlink" title="多重比较和双因子方差分析"></a>多重比较和双因子方差分析</h1><p>之前我们学习了单因子方差分析，单因子方差分析是用来比较分组之间均值是否相等的问题，但是当我们拒绝原假设之后，我们不知道哪些组是相等的，哪些组是不相等的。因此我们需要进一步考虑这个问题。</p>
<h2 id="多重比较"><a href="#多重比较" class="headerlink" title="多重比较"></a>多重比较</h2><h3 id="水平均值差的置信区间"><a href="#水平均值差的置信区间" class="headerlink" title="水平均值差的置信区间"></a>水平均值差的置信区间</h3><p>首先我们要想，到底是原假设成立的时候，因子A是显著的还是备择假设成立的时候因子A是显著的。答案是<strong>备择假设</strong>，因为当原假设成立的时候，不同组之间的均值是相等的，因此因子A是不显著的。</p>
<p>当因子A显著时，有充分的理由认为因子A的各个水平中<strong>至少存在一对的均值是不相等的</strong>。但这并不说明，所有的水平均值都是不相等的。那么，我们就想知道哪些水平的均值是不相等的。一个很自然的想法就是，给定一对水平$(i,i’)$  ，构造$\mu<em>i-\mu</em>{i’}$​ 的区间估计。如果该区间不包括0，就说明$\mu<em>i$和$\mu</em>{i’}$必不相等。</p>
<p>我们来构造区间估计的枢轴量：</p>
<ul>
<li>分布为：</li>
</ul>
<script type="math/tex; mode=display">
\overline{y_{i\cdot}}\sim N(\mu_i,\frac{\sigma^2}{m})~ \text{和}~~\overline{y_{i'}}\sim N(\mu_{i'},\frac{\sigma^2}{m})</script><p>因为两者独立，所以 $\overline y<em>{i\cdot}-\overline y</em>{i’}\sim N(\mu<em>i-\mu</em>{i’},2\sigma^2m^{-1})$​ . 对其进行标准化：</p>
<script type="math/tex; mode=display">
\frac{(\overline y_{i\cdot}-\overline y_{i'})-(\mu_i-\mu_{i'})}{\sqrt{2\sigma^2m^{-1}}}\sim N(0,1)</script><p>因为这个分布中$\sigma^2$是未知的，我们要用其估计 $\hat\sigma^2$来代替。估计哪里来呢？在两样本方差t检验中，定义的是和方差。那么在这个情况下，每一组都是对方差的一个估计，那么我们只要对每一组方差都进行一个加权平均就可以了。即$SS_E/n-a$  ，分母是由a个水平，每个水平是(m-1) 乘起来就是 $n-a$​​ 。 这样的估计更加稳定，因为有更多的组，波动更小。</p>
<p><img src="/2021/10/07/%E5%A4%9A%E9%87%8D%E6%AF%94%E8%BE%83%E5%92%8C%E5%8F%8C%E5%9B%A0%E5%AD%90%E6%96%B9%E5%B7%AE%E5%88%86%E6%9E%90/9.png"> </p>
<h3 id="多重比较问题"><a href="#多重比较问题" class="headerlink" title="多重比较问题"></a>多重比较问题</h3><p>问题又来了，如果是两样本t检验的区间估计，只存在一个区间，但是在这个问题中,一共有a个不同的水平，总共有 a(a-1)/2 对不同的水平组合，每一对水平都有一个置信水平为1的置信区间。然而，要求其同时成立的话，其联合置信水平就无法达到$1-\alpha$了</p>
<p>比如说，若$A_1,A_2\cdots,A_k$ 表示k个随机事件，且每个事件发生的概率均为$1-\alpha$ ,即 $P(A_i)=1-\alpha.i=1,2\cdots,k$ 则其共同发生的概率为：</p>
<script type="math/tex; mode=display">
\begin{align}
P(\cap_{i=1}^k A_i) &\leq P(A_1) = 1-\alpha\\
P(\cap_{i=1}^k A_i) &= 1-P(\cup_{i=1}^k\overline A_i)\\
&\geq 1-\sum_{i=1}^k P(\overline A_i) = 1-k(1-(1-\alpha))\\
&=1-k\alpha
\end{align}</script><p>这说明它们同时发生的概率实际上要在$1-k\alpha$ 和 $1-\alpha$ 之间，可能要比 $1-\alpha$ 小很多</p>
<p>那么，为了使得它们同时发生的概率不低于$1-\alpha$ ，一个很自然的方法就是把每个事件发生的概率都提高。也就是让$\alpha$ 取得更小，让置信水平从 $1-\alpha/2$ 提高到 $1-\alpha/(a(a-1)/2)$ </p>
<p>于是：</p>
<script type="math/tex; mode=display">
P(\cap_{i=1}^{a(a-1)/2}A_i) \geq 1-a(a-1)/2\cdot\frac{\alpha}{a(a-1)/2} = 1-\alpha</script><p>这叫做<em>Bonferroni</em>方法，虽然这种方法非常简单，但是会导致所得到的置信区间过于保守，精度很差</p>
<h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>在方差分析中，经F检验拒绝原假设，表明因子A是显著的，即a个水平的均值不全相等。进一步，我们需要确定哪些水平之间是存在差异的，哪些水平之间是没有差异的。</p>
<p>在$a(a&gt;2)$ 个水平均值中同时比较任意两个水平均值间有无明显差异的问题叫做<strong>多重比较</strong>，也就是说，在显著性水平为 $\alpha$ 同时检验 $a(a-1)/2$ 个假设</p>
<script type="math/tex; mode=display">
H_0^{ii'}:\mu_i = \mu_{i'} .\\1\leq i<i'\leq a</script><p>当$H<em>0^{ii’}$​ 成立时，$|\overline y</em>{i\cdot}-\overline y_{i’}|$​ 不应过大，过大就应该拒绝 $H_0^{ii’}$​</p>
<p>于是，在同时考察这些 $H_0^{ii’}$ 时，这些$H_0^{ii’}$中至少有一个不成立就构成了多重比较检验的拒绝域，即拒绝域的形式就是：</p>
<script type="math/tex; mode=display">
W=\bigcup_{1\leq i<i'\leq a}\{|\overline y_{i\cdot}-\overline y_{i'\cdot}|\geq c_{ii'}\}</script><p>其中$c_{ii’}$是临界值，由原假设 $H_0^{ii’}$成立时$P(W)=\alpha$ 来确定。 </p>
<h3 id="Tukey方法"><a href="#Tukey方法" class="headerlink" title="Tukey方法"></a>Tukey方法</h3><p>由多重比较我们可以得出，要求 $a(a-1)/2$​​​ 个临界值$c<em>{ii’}$​​ 这是很烦的一件事， 而Tukey方法则对其进行了简化，对所求的临界值提出一些合理的假设。因为由于<strong>各个水平下重复次数均相等</strong>，基于对称性一个很自然的要求是$c</em>{ii’}$​​是相等的，我们记为c。</p>
<script type="math/tex; mode=display">
P(W) = P(\bigcup_{i\leq i<i'\leq a}\{|\overline y_{i\cdot}-\overline y_{i'\cdot}|\geq c\})\\
=1-P(\bigcap_{1\leq i<i'\leq a}\{|\overline y_{i\cdot}-\overline y_{i'\cdot}|< c\})\\
=1-P(\max_{1\leq i<i'\leq a}\{|\overline y_{i\cdot}-\overline y_{i'\cdot}|< c\})\\
=P(\max_{1\leq i<i'\leq a}\{|\overline y_{i\cdot}-\overline y_{i'\cdot}|\geq c\})\\
=P(\max_{1\leq i<i'\leq a}|\frac{(\overline y_{i\cdot}-\mu)-(\overline y_{i'\cdot}-\mu)}{\hat\sigma/\sqrt m}|\geq \frac{c}{\hat\sigma/\sqrt m}\})\\
=P(\max_{i}\frac{(\overline y_{i\cdot}-\mu)}{\hat\sigma/\sqrt m}-\min_{i}\frac{\overline y_{i\cdot}-\mu}{\hat\sigma/\sqrt m}\geq \frac{c}{\hat\sigma/\sqrt m}\})\\</script><p>第一步就是 德摩根公式。</p>
<p>第二步：要求交集小于c，就是要让这里面最大的差值要小于c</p>
<p>第三步：正常转换</p>
<p>第四步：标准化，分子让其减去$\mu$和加上$\mu$,分母上除以$\hat\sigma/\sqrt m$ </p>
<p>第五步：我们要求第四步式子的最大值，一定是最大减最小值。</p>
<p>我们要确定c的值，就要将c反解出来。</p>
<p>我们令不等号左边的式子为一随机变量：这是一个很复杂的随机变量，既有矩又有次序统计量。</p>
<script type="math/tex; mode=display">
q(a,df)=\max_{i}\frac{(\overline y_{i\cdot}-\mu)}{\hat\sigma/\sqrt m}-\min_{i}\frac{\overline y_{i\cdot}-\mu}{\hat\sigma/\sqrt m}</script><p>因为 </p>
<script type="math/tex; mode=display">
\frac{\overline y_{i\cdot}-\mu}{\hat\sigma/\sqrt m}\sim t(n-a)</script><p>在原假设成立的情况下，$\mu_i$都为$\mu$ </p>
<p>$q(a,df)$ 可以看做a个独立同分布的自由度为df的t分布的随机变量的极差。一般称q为t化极差统计量。</p>
<p>这个分布并不是常见的分布，它与水平数目a和t分布的自由度 df=n-a有关，但是与$\mu,\sigma^2,m$​ 无关。</p>
<h4 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h4><p>那么，如何获得t化极差统计量的分布？可以通过蒙特卡罗方法获得：</p>
<p><img src="/2021/10/07/%E5%A4%9A%E9%87%8D%E6%AF%94%E8%BE%83%E5%92%8C%E5%8F%8C%E5%9B%A0%E5%AD%90%E6%96%B9%E5%B7%AE%E5%88%86%E6%9E%90/1.png" style="zoom:67%;"></p>
<h4 id="反解"><a href="#反解" class="headerlink" title="反解"></a>反解</h4><p>我们可以由：</p>
<script type="math/tex; mode=display">
P(W) = P(q(a,df)\geq \sqrt m c/\hat\sigma) = \alpha</script><p>来推出：</p>
<script type="math/tex; mode=display">
c=q_{1-\alpha}(a,df)\hat\sigma/\sqrt m</script><p>其中，$q_\alpha(a,df)$表示$q(a,df)$的$\alpha$分位数</p>
<h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4><p>在给定的显著性水平$\alpha$下，确定t化极差统计量的分位数$q<em>{1-\alpha}(a,df)$ 并计算 $c=q</em>{1-\alpha}(a,df)\hat\sigma/\sqrt m$</p>
<p>比较每一组样本均值的差与临界值c的大小，如果$|\overline y<em>{i\cdot}-\overline y</em>{i’\cdot}\geq c$</p>
<p>那么认为水平i与水平 $i’$ 之间有显著差异，反之则认为这两个水平之间无差异</p>
<h2 id="双因子方差分析"><a href="#双因子方差分析" class="headerlink" title="双因子方差分析"></a>双因子方差分析</h2><h3 id="主效应与交互效应"><a href="#主效应与交互效应" class="headerlink" title="主效应与交互效应"></a>主效应与交互效应</h3><p>事实上，在实际问题中，影响响应变量的因子数量不止一个，在这里我们考虑两个因子A和B。假定因子A有a个不同的水平，因子B有b个不同的水平，那么因子A和因子B的组合共有ab种。而且，在每一种因子组合下，对响应变量有m次重复观测。</p>
<p>当某一因子的水平改变时导致了响应变量发生变化，称该现象为<strong>因子效应</strong>。</p>
<p>某一因子对响应变量的直接影响，称为主效应</p>
<h4 id="主效应"><a href="#主效应" class="headerlink" title="主效应"></a>主效应</h4><p><img src="/2021/10/07/%E5%A4%9A%E9%87%8D%E6%AF%94%E8%BE%83%E5%92%8C%E5%8F%8C%E5%9B%A0%E5%AD%90%E6%96%B9%E5%B7%AE%E5%88%86%E6%9E%90/2.png" style="zoom:67%;"></p>
<p>在这种情况下：</p>
<p>因子A的主效应： $= \frac{40+50}{2}-\frac{20+30}{2}=20$</p>
<p>因子B的主效应： $=\frac{30+50}{2}-\frac{20+40}{2}=10$​</p>
<p>这表明了：</p>
<p>因子A从低水平增至高水平时，其平均响应变量增加了20个单位</p>
<p>因子B从低水平增至高水平时，其平均响应变量增加了10个单位</p>
<h4 id="交互效应"><a href="#交互效应" class="headerlink" title="交互效应"></a>交互效应</h4><p>除了主效应之外，因子对响应变量的影响还存在另外一种形式：</p>
<ul>
<li>在一些化工生产中，温度和材料都是影响产品质量的重要因素，不同材料在不同温度水平下对产品质量的影响是不同的</li>
<li>在线上购物行为中，消费者会存在从众心理，而线上商家会利用优惠券的让利形式进一步刺激消费者购买，因此在这些因素的共同作用下，电商主播直播带货非常火爆</li>
</ul>
<p>所以说，当对于两个因子来说，因子A和因子B给响应变量产生的影响有影响，这种影响不是因子A对响应变量直接的影响，而是一种间接作用，称为交互效应。比如说这种情况：</p>
<p><img src="/2021/10/07/%E5%A4%9A%E9%87%8D%E6%AF%94%E8%BE%83%E5%92%8C%E5%8F%8C%E5%9B%A0%E5%AD%90%E6%96%B9%E5%B7%AE%E5%88%86%E6%9E%90/3.png" style="zoom:67%;"></p>
<ul>
<li>在因子B处于低水平时，因子A的效应为：$60-20=40$</li>
<li>在因子B处于高水平时，因子A的效应为：$30-40=-10$</li>
</ul>
<p>这表明了，因子A的效应是依赖于因子B所选择的水平，由此可见因子A和因子B之间存在交互作用。交互作用的大小可以用因子B处于不同水平下因子A的效应的平均差来表示，即：</p>
<p>因子A和B的交互效应$=\frac{-10-40}{2}=-25$</p>
<h4 id="判断方法"><a href="#判断方法" class="headerlink" title="判断方法"></a>判断方法</h4><p>我们可以通过图像来判断两个因子之间是否存在交互效应：</p>
<p><img src="/2021/10/07/%E5%A4%9A%E9%87%8D%E6%AF%94%E8%BE%83%E5%92%8C%E5%8F%8C%E5%9B%A0%E5%AD%90%E6%96%B9%E5%B7%AE%E5%88%86%E6%9E%90/4.png" style="zoom:80%;"></p>
<p>在第一种情况下，因子A无论在什么水平下，因子B的增量是一样的，因此可以判断A和B是不交互的</p>
<p>在第二种情况下，因子A的水平高低对B的增量产生影响，反之亦然，因此可以判断A和B是互相交互的。</p>
<h3 id="双因子方差分析的模型及假设"><a href="#双因子方差分析的模型及假设" class="headerlink" title="双因子方差分析的模型及假设"></a>双因子方差分析的模型及假设</h3><p>那么我们有没有一个量化的方法来考虑这个问题呢？</p>
<p>首先，我们要给出一些假定：</p>
<ul>
<li><p>因子A有a个水平，$i=1,2\cdots,a$ </p>
</li>
<li><p>因子B有b个水平，$j=1,2\cdots,b$</p>
</li>
<li><p>$y_{ijk}$​ 表示因子A处于第i个水平和因子B处于第j个水平时响应变量的第k个观测值，$k=1,2\cdots,m$</p>
</li>
<li><p>在这个模型中，样本量为$n=abm$</p>
</li>
</ul>
<p>通常，数据按照下表的形式呈现：</p>
<p><img src="/2021/10/07/%E5%A4%9A%E9%87%8D%E6%AF%94%E8%BE%83%E5%92%8C%E5%8F%8C%E5%9B%A0%E5%AD%90%E6%96%B9%E5%B7%AE%E5%88%86%E6%9E%90/5.png" style="zoom:80%;"></p>
<p>和单因子方差分析的模型一样，双因子方差分析模型也有均值模型和效应模型</p>
<h4 id="均值模型"><a href="#均值模型" class="headerlink" title="均值模型"></a>均值模型</h4><p>在双因子方差分析的均值模型可以定义为：</p>
<script type="math/tex; mode=display">
y_{ijk} = \mu_{ij}+\epsilon_{ijk}\cases{i=1,2\cdots,a\\~\\j=1,2\cdots,b\\~\\k=1,2\cdots,m}</script><p>$\mu<em>{ij}$可以认为是不同因子水平组合下响应变量$y</em>{ijk}$的均值</p>
<h4 id="效应模型"><a href="#效应模型" class="headerlink" title="效应模型"></a>效应模型</h4><p>我们可以将$\mu_{ij}$分为四个部分，在双因子方差分析的效应模型可以定义为：</p>
<script type="math/tex; mode=display">
y_{ijk} = \mu+\alpha_i+\beta_j+(\alpha\beta)_{ij}+\epsilon_{ijk}\cases{i=1,2\cdots,a\\~\\j=1,2\cdots,b\\~\\k=1,2\cdots,m}</script><ul>
<li>$\mu$ 表示总体均值</li>
<li>$\alpha_i$表示因子A(行因子)的第i个水平效应(主效应)</li>
<li>$\beta_i$表示因子B(列因子)的第j个水平效应(主效应)</li>
<li>$(\alpha\beta)<em>{ij}$表示因子A和B之间的交互关系，注意不等价于$\alpha_i\cdot\beta_j$ , 我们需要将$(\alpha\beta)</em>{ij}$​看成是一个新的符号？为什么不成立呢？因为一旦成立，交互效应就会等于主效应的乘积，那么就成为一个约束、一个假设了</li>
</ul>
<p>此外，类似于单因子方差分析效应模型，为避免参数不可识别的问题，我们对这些效应参数需要一些约束</p>
<script type="math/tex; mode=display">
\begin{align}
&\sum_{i=1}^a \alpha_i = 0,\\ 
&\sum_{j=1}^b\beta_j = 0,\\ 
&\sum_{i=1}^a(\alpha\beta)_{ij} = \sum_{j=1}^b (\alpha\beta)_{ij} = 0
\end{align}</script><p>此外我们要对实验误差$\varepsilon_{ijk}$ 进行一个假设，将其看成是均值为0且方差为$\sigma^2$ 的正态分布随机变量，即：</p>
<script type="math/tex; mode=display">
\varepsilon_{ijk} \sim N(0,\sigma^2)</script><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>双因子方差分析模型的一般形式为：</p>
<script type="math/tex; mode=display">
\begin{align}
&y_{ijk} = \mu+\alpha_i+\beta_j+(\alpha\beta)_{ij}+\varepsilon_{ijk},\varepsilon_{ijk}\sim N(0,\sigma^2)\\~\\
&i=1,2\cdots,a;j=1,2\cdots,b;k=1,2\cdots,m\\~\\
s.t.&\sum_{i=1}^a \alpha_i = 0,\sum_{j=1}^b \beta_i = 0\\
&\sum_{i=1}^a(\alpha\beta)_{ij}=\sum_{j=1}^b{(\alpha\beta)_{ij}}=0\
\end{align}</script><h3 id="双因子方差分析的检验"><a href="#双因子方差分析的检验" class="headerlink" title="双因子方差分析的检验"></a>双因子方差分析的检验</h3><h4 id="检验问题"><a href="#检验问题" class="headerlink" title="检验问题"></a>检验问题</h4><p>我们要检验的问题是：在双因子方差分析模型中，我们不仅关心因子A和B的主效应是否显著，还关心因子A和B的交互作用是否显著。</p>
<p>我们要针对以下三组问题进行检验</p>
<ul>
<li>其一，判断因子A是否对响应变量有直接影响，即：</li>
</ul>
<script type="math/tex; mode=display">
H_0:\alpha_1=\alpha_2\cdots=\alpha_a = 0~~vs~~H_1:\text{因子A至少存在一个水平}\alpha_i\neq 0</script><ul>
<li>其二：判断因子B是否对响应变量有直接影响，即：</li>
</ul>
<script type="math/tex; mode=display">
H_0:\beta_1=\beta_2=\cdots=\beta_b = 0 ~~vs~~H_1:\text{因子B至少存在一个水平}\beta_j\neq 0</script><ul>
<li>其三，判断因子A和B是否对响应变量有交互作用，即：</li>
</ul>
<script type="math/tex; mode=display">
\begin{align}
&\text{H}_0 : \text{对于任意} i=1,2\cdots,a;j=1,2\cdots,b,(\alpha\beta)_{ij} = 0 ~~\text{均成立}\\~\\
&\text{H}_1: \text{因子A和B至少存在一组水平组合}(\alpha\beta)_{ij}\neq 0
\end{align}</script><h4 id="检验统计量"><a href="#检验统计量" class="headerlink" title="检验统计量"></a>检验统计量</h4><p>我们采用方差分解的想法来构建检验统计量。在介绍总偏差平方和分解公式之前，我们先来定义写必需符号</p>
<ul>
<li>$y_{i\cdot\cdot}$  表示在因子A第i个水平下响应变量的总和</li>
<li>$y_{\cdot j\cdot}$  表示在因子B第j个水平下响应变量的综合2</li>
<li>$y_{ij\cdot}$ 表示在因子A第i个水平下且因子B第j个水平下响应变量的总和</li>
<li>$y_{\cdots}$ 表示所有响应变量的总和</li>
<li>$\overline y<em>{i\cdot\cdot},\overline y</em>{\cdot j\cdot},\overline y<em>{ij\cdot},\overline y</em>{\cdots}$ 表示相应的均值</li>
</ul>
<p>我们可以用公式来表示这些符号，即：</p>
<script type="math/tex; mode=display">
\begin{align}
&y_{i\cdot\cdot} =\sum_{j=1}^b\sum_{k=1}^m y_{ijk}&\overline y_{i\cdot\cdot}=\frac{1}{bm}y_{i\cdot\cdot}\\
&y_{\cdot j\cdot} = \sum_{i=1}^a\sum_{k=1}^m y_{ijk}&\overline y_{\cdot j\cdot} =\frac{1}{am} y_{\cdot j\cdot}\\
&y_{ij\cdot} = \sum_{k=1}^m y_{ijk}&\overline y_{ij\cdot} = \frac{1}{m}y_{ij\cdot}\\
&y_{\cdots} = \sum_{i=1}^a\sum_{j=1}^b\sum_{k=1}^m y_{ijk} &\overline y_{\cdots} = \frac{1}{n}y_{\cdots}
\end{align}</script><p>模仿单因子方差分析，我们可以给出$SS_T$的式子：</p>
<script type="math/tex; mode=display">
\begin{align}
\sum_{i=1}^a\sum_{j=1}^b\sum_{k=1}^m(y_{ijk}-\overline y_{\cdots})^2 &= \sum_{i=1}^a\sum_{j=1}^b\sum_{k=1}^m((\overline y_{i\cdot\cdot}-\overline y_{\cdot\cdot\cdot})+(\overline y_{\cdot j\cdot}-\overline y_{ij\cdot})+(\overline y_{ij\cdot}-\overline y_{i\cdot\cdot}-\overline y_{\cdot j\cdot}+\overline y_{\cdot\cdot\cdot})+(\overline y_{ijk}-\overline y_{ij\cdot}))^2\\~\\
&=bm\sum_{i=1}^a (y_{i\cdot\cdot}-\overline y_{\cdot\cdot\cdot})^2+am\sum_{j=1}^b(\overline y_{\cdot j\cdot}-\overline y_{\cdot\cdot\cdot})^2+m\sum_{i=1}^a\sum_{j=1}^b(\overline y_{ij\cdot}-\overline y_{i\cdot\cdot}-\overline y_{\cdot j \cdot}+\overline y_{\cdot\cdot\cdot})^2+\sum_{i=1}^a\sum_{j-1}^b\sum_{k=1}^m(y_{ijk}-\overline y_{ij\cdot})^2
\end{align}</script><p>其中，中间项都是等于0的。用单因子的技巧就可以证明</p>
<ul>
<li>注意到，第一项是根据因子A对数据进行划分从而引起的偏差平方和，我们记为$SS_A$</li>
<li>第二项类似地可以记为$SS_B$</li>
<li>第三项可以写为$m\sum<em>{i=1}^a\sum</em>{j=1}^b((\overline y<em>{ij\cdot}-\overline y</em>{\cdot\cdot\cdot})-(\overline y<em>{i\cdot\cdot}-\overline y</em>{\cdot\cdot\cdot})-(\overline y<em>{\cdot j\cdot}-\overline y</em>{\cdot\cdot\cdot}))^2$ , 记为 $SS_{AB}$ </li>
<li>最后一项，为了排除因子A和B的效应之外响应变量的剩余偏差平方和，我们记为$SS_E$</li>
</ul>
<p>总的偏差平方和的分解公式可以简记为：</p>
<script type="math/tex; mode=display">
SS_T = SS_A+SS_B+SS_{AB}+SS_E</script><p>类似于单因子方差分析模型，在双因子方差分析模型中，我们可以类似地利用三个比值：</p>
<script type="math/tex; mode=display">
\frac{SS_A}{SS_E},\frac{SS_B}{SS_E},\frac{SS_{AB}}{SS_E}</script><p>我们需要推导出$SS<em>E$ 和在原假设成立时$SS_A,SS_B,SS</em>{AB}$的分布</p>
<p>因为$SS<em>A,SS_B,SS</em>{AB},SS<em>E$均为偏差平方和，所以一个合理的猜测是，这些偏差平方和的分布可能是一个卡方分布。根据Cochran定理，这些偏差平方和可以看做是多少个独立的随机变量的平方和。例如，$SS_A=bm\sum</em>{i=1}^a(\overline y<em>{i\cdot\cdot}-\overline y</em>{\cdot\cdot\cdot})$ 可以将数据按因子A的<strong>a个水平</strong>来进行分组，在第i组内响应变量的均值$\overline y<em>{i\cdot\cdot}$ 看作一个独立的样本，而$\overline y</em>{\cdot\cdot\cdot}$​​可以看做a组数据的样本均值。</p>
<p>$SS<em>A$​ 可以看做是这a个数据的离差平方和乘以一个常数。对于离差平方和来说，自由度可以看作样本个数减一。于是，$SS_A$​ 的自由度为a-1。类似地，我们可以得到$SS_B,SS</em>{AB},SS_E$ 的自由度，如下表</p>
<p><img src="/2021/10/07/%E5%A4%9A%E9%87%8D%E6%AF%94%E8%BE%83%E5%92%8C%E5%8F%8C%E5%9B%A0%E5%AD%90%E6%96%B9%E5%B7%AE%E5%88%86%E6%9E%90/6.png" alt="6"></p>
<p>类似于在单因子方差分析模型中的定义，偏差平方和除以相应的自由度之后所得到的被称为均方</p>
<p>其实，构造检验统计量实际上是均方的比值，而不是偏差平方和的比值，这是因为F分布随机变量的构造方式。根据之前的定理，我们发现 $MS_E$​​​ 一直在$\sigma^2$​​​ 附近波动。</p>
<p>只有当原假设成立时，$MS<em>A,MS_B,MS</em>{AB}$​​ 都会在$\sigma^2$​​ 附近波动的。于是，$\frac{MS<em>A}{MS_E},\frac{MS_B}{MS_E},\frac{MS</em>{AB}}{MS_E}$​​  都应该比较接近于1的，并不会特别大。如果差得比较远，就说明某一效应非常明显，我们就会拒绝原假设。</p>
<p>因此，我们可以通过这三个比值，是否大于临界值，从而判断原假设成立，还是备择假设成立</p>
<h4 id="均方的期望"><a href="#均方的期望" class="headerlink" title="均方的期望"></a>均方的期望</h4><p><img src="/2021/10/07/%E5%A4%9A%E9%87%8D%E6%AF%94%E8%BE%83%E5%92%8C%E5%8F%8C%E5%9B%A0%E5%AD%90%E6%96%B9%E5%B7%AE%E5%88%86%E6%9E%90/8.png" alt="6"></p>
<p>在双因子方差模型中，均方$MS<em>A,MS_B,MS</em>{AB},MS_E$ 的期望分别为：</p>
<script type="math/tex; mode=display">
\begin{align}
&E(MS_A)=E(\frac{SS_A}{a-1})=\sigma^2 +\frac{bm\sum_{i=1}^a \alpha_i^2}{a-1}\\
&E(MS_B)=E(\frac{SS_B}{b-1})=\sigma^2 +\frac{am\sum_{j=1}^b\beta_i^2}{b-1}\\
&E(MS_{AB})=E(\frac{SS_{AB}}{(a-1)(b-1)})=\sigma^2 +\frac{m\sum_{i=1}^a\sum_{j=1}^b(\alpha\beta)_{ij}^2}{(a-1)(b-1)}\\
&E(MS_E) = E(\frac{SS_E}{ab(m-1)})=\sigma^2
\end{align}</script><h4 id="如何确定临界值？"><a href="#如何确定临界值？" class="headerlink" title="如何确定临界值？"></a>如何确定临界值？</h4><p>当原假设成立的时候，类似单因子方差分析模型，我们可以推出</p>
<script type="math/tex; mode=display">
\begin{align}
&SS_A\sim \mathcal{X}^2(a-1)\\
&SS_B\sim \mathcal{X}^2(b-1)\\
&SS_{AB}\sim \mathcal{X}^2((a-1)(b-1))\\
&SS_E\sim \mathcal{X}^2(ab(m-1))
\end{align}</script><p>而且$SS<em>A,SS_B,SS</em>{AB},SS_E$ 都是相互独立的</p>
<p>于是，一个显而易见的结论是我们所需要的比值都是服从F分布的，即</p>
<script type="math/tex; mode=display">
\begin{align}
&\frac{MS_A}{MS_E}\sim F(a-1,ab(m-1))\\
&\frac{MS_B}{MS_E}\sim F(b-1,ab(m-1))\\
&\frac{MS_{AB}}{MS_E}\sim F((a-1)(b-1),ab(m-1))\\
\end{align}</script><p>由此，我们可以确定相应F分布的临界值</p>
<p><img src="/2021/10/07/%E5%A4%9A%E9%87%8D%E6%AF%94%E8%BE%83%E5%92%8C%E5%8F%8C%E5%9B%A0%E5%AD%90%E6%96%B9%E5%B7%AE%E5%88%86%E6%9E%90/7.png" alt="7"></p>

          
        
      
    </div>
    
    
    
    <div>
      
    </div>
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2021/09/24/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E7%AE%97%E6%B3%95-%E5%93%88%E5%B8%8C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/%5Bobject%20Object%5D">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/09/24/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E7%AE%97%E6%B3%95-%E5%93%88%E5%B8%8C/" itemprop="url">数据科学与工程算法-哈希</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-09-24T08:37:14+08:00">
                2021-09-24
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2024-01-10T13:30:02+08:00">
                2024-01-10
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="数据科学与工程算法-哈希算法"><a href="#数据科学与工程算法-哈希算法" class="headerlink" title="数据科学与工程算法-哈希算法"></a>数据科学与工程算法-哈希算法</h1><h2 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h2><p>首先我们提出两种情境：</p>
<ol>
<li>判断用户名是否被注册</li>
</ol>
<p>我们当然可以对用户名建立B+树索引，这样可以提高查询效率，复杂度为$O(\ln n)$ ，其中n为集合大小。</p>
<p>但是我们能不能找到$O(1)$复杂度的方法呢?</p>
<ol>
<li>文本冗余检测</li>
</ol>
<p>假设我们有搜索引擎定期爬取网页内容，但是爬取太多冗余文本会浪费存储空间。然而，将海量文本逐篇对比过于低效了，如何判断网页的内容是否冗余呢？</p>
<p>哈希函数我们都不陌生，用数学公式来表达就是：</p>
<script type="math/tex; mode=display">
h(\text{key}) = \text{value}\in\mathbb{Z^+}</script><p>用图片来显示就是：<br><img src="/2021/09/24/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E7%AE%97%E6%B3%95-%E5%93%88%E5%B8%8C/1.png" style="zoom:67%;"></p>
<h3 id="哈希函数的作用："><a href="#哈希函数的作用：" class="headerlink" title="哈希函数的作用："></a>哈希函数的作用：</h3><ul>
<li>压缩存储：哈希值所需的存储空间远小于输入关键词占用的空间<ul>
<li>网页URL哈希到某个未知，可以表示为一个整数</li>
<li>邮件地址哈希成一个整数</li>
</ul>
</li>
<li>无冲突：理想状态下，输入不同的关键词会得到不同的哈希值<ul>
<li>即使是两个差异很小的关键词也会得到两个完全不同的哈希值</li>
<li>相同关键词被相同哈希函数哈希，不可能得到两个不同的值</li>
</ul>
</li>
<li>不可逆：在不知道哈希函数的情形下，仅知道哈希值，不可能轻易地猜到此哈希值对应的关键词<ul>
<li>唯一找到关键词的方法是暴力算法</li>
<li>正是因为这一点，哈希函数称为最重要的密码学工具之一</li>
</ul>
</li>
</ul>
<h3 id="哈希表性能"><a href="#哈希表性能" class="headerlink" title="哈希表性能"></a>哈希表性能</h3><ul>
<li>哈希函数：将冲突最小化，使得key和value均匀地分布在整个表中</li>
<li>冲突解决策略：将key/value存储在不同的位置，或将多个key/value用链表串起来</li>
<li>哈希表大小：表过大会降低碰撞的可能，单会造成内存空间的浪费。过小的哈希会增加碰撞的可能性</li>
</ul>
<h2 id="布隆过滤器"><a href="#布隆过滤器" class="headerlink" title="布隆过滤器"></a>布隆过滤器</h2><p>布隆过滤器是为了应对碰撞而提出的一种高空间效率的概率数据结构，用于判断一个元素是否属于一个集合的成员。</p>
<p>布隆过滤器有广泛的应用：</p>
<ul>
<li>垃圾邮件地址过滤器：可以有效过滤垃圾邮件地址</li>
<li>拼写检查：能很快发现文字编辑软件中输入的错</li>
<li>重复检查：用于网络爬虫判断某个URL是否已经被爬取过了，或用户名是否被注册过</li>
</ul>
<p>那么布隆过滤器是怎么被提出的呢？我们已经知道哈希面临的问题是冲突，假设哈希函数是良好的，那么如果我们的位阵列长度为m个点，那么如果我们想将冲突率降低到例如 $1\%$ ，这个散列表就只能容纳 $m/100$​​​ 个元素。显然这很浪费空间效率。解决的办法也很简单，就是使用多重哈希，如果他们有一个判断出元素不在集合中，那肯定是不在的；如果它们都说在，虽然也有一定的可能性它们在说谎，不过直觉上判断这种事情的概率是比较低的。这就是布隆过滤器。</p>
<h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><p>它的优点是空间效率和查询时间都比一般的算法要好的多。隆过滤器存储空间和插入/查询时间都是常数。另外, Hash函数相互之间没有关系，方便由硬件并行实现。布隆过滤器不需要存储元素本身，在某些对保密要求非常严格的场合有优势。</p>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><p>但是布隆过滤器的缺点和优点一样明显。误算率是其中之一。随着存入的元素数量增加，误算率随之增加。常见的补救办法是建立一个小的白名单，存储那些可能被误判的元素。但是如果元素数量太少，则使用散列表足矣。</p>
<p>另外，一般情况下不能从布隆过滤器中删除元素。我们很容易想到把位列阵变成整数数组，每插入一个元素相应的计数器加1, 这样删除元素时将计数器减掉就可以了。然而要保证安全的删除元素并非如此简单。首先我们必须保证删除的元素的确在布隆过滤器里面. 这一点单凭这个过滤器是无法保证的。另外计数器回绕也会造成问题。</p>
<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>假定集合中有n个元素，位数组初始长度为m,每个位置都被置为0，那么给定一组k个哈希函数 $h_1,\cdots,h_k$ ，其中$h_i$ 的范围为${0,\cdots,m-1}$ ；这个例子中 $m=18,k=3$ </p>
<p>那么我们要插入三个元素${x,y,z}$ ,每个元素就会被哈希到位数组中的三个位置。如下图所示：</p>
<p><img src="/2021/09/24/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E7%AE%97%E6%B3%95-%E5%93%88%E5%B8%8C/2.png" style="zoom:67%;"></p>
<h3 id="元素插入"><a href="#元素插入" class="headerlink" title="元素插入"></a>元素插入</h3><p>现在我们约定3个哈希函数$h_1,h_2,h_3$，其中$h_i$的范围为 ${0,1,\cdots,9}$ ，初始状态长度为10 的位数组每个位置为置为0.</p>
<p><img src="/2021/09/24/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E7%AE%97%E6%B3%95-%E5%93%88%E5%B8%8C/3.png" style="zoom:67%;"></p>
<p>然后，元素$x_1$​被映射到了 ${1,4,9}$​​, 于是将1、4、9都改成1</p>
<p><img src="/2021/09/24/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E7%AE%97%E6%B3%95-%E5%93%88%E5%B8%8C/4.png" style="zoom:67%;"></p>
<p>接着元素$x_2$​被映射到了${4,5,8}$​, 于是4、5、8置为1，因为4已经是1了，所以不变</p>
<p><img src="/2021/09/24/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E7%AE%97%E6%B3%95-%E5%93%88%E5%B8%8C/5.png" style="zoom:67%;"></p>
<h3 id="元素查询"><a href="#元素查询" class="headerlink" title="元素查询"></a>元素查询</h3><p>现在位数组中存放着两个元素，我们要查询的时候，流程如下：</p>
<ul>
<li>查询 $y_1$, $H(y_1)= {1,4,9}\longrightarrow Yes$</li>
<li>查询 $y_2$, $H(y_2) = {0,4,8}\longrightarrow No$</li>
<li>查询$y_3$, $H(y_3)={1,5,8}$ 这并不是存入的元素，但是在1、5、8这三个位置上的值却都是1，因此出现了假阳性的情况</li>
</ul>
<p>所以说，布隆过滤器会出现误判的情况：</p>
<ul>
<li>虽然不会出现拒真的情形(假阴性不会出现 ), 是真的不会给你判成假的</li>
<li>但是<strong>可能会出现纳伪的情形</strong>(假阳性)</li>
</ul>
<h3 id="误判率分析"><a href="#误判率分析" class="headerlink" title="误判率分析"></a>误判率分析</h3><p>当插入一个元素到布隆过滤器，一个哈希函数未将某个特定位置置为1的概率为 $1-\frac{1}{m}$</p>
<p>当一个元素插入布隆过滤器后，k个哈希函数未将特定位置置为1的概率为 $(1-\frac{1}{m})^k$ </p>
<p>将n个元素插入布隆过滤器后，特定位置未被置为1的概率为 $(1-\frac{1}{m})^{kn}$ </p>
<p>因此，某个特定位置为1的概率为 $(1-(1-\frac{1}{m})^{kn})$</p>
<p>现在，在查询某个元素的时候，当k个哈希函数对应的位置均为1，则过滤器声称该元素属于该集合。但如果这个元素不属于集合，就会发生误判。其概率是：</p>
<script type="math/tex; mode=display">
f = ((1-(1-\frac{1}{m})^{kn}))^k\approx (1-e^{-\frac{kn}{m}})^k</script><p>可以近似是因为极限：</p>
<script type="math/tex; mode=display">
\lim\limits_{m\rightarrow \infty}(1-\frac{1}{m})^{kn} =\lim\limits_{m\rightarrow\infty}((1+\frac{1}{-m})^{-m})^{-\frac{kn}{m}} = e^{-1\cdot\frac{kn}{m}}</script><p>从这个概率可知:</p>
<ul>
<li>误判概率随着布隆过滤器<strong>位数组的长度m</strong>的增大而减小</li>
<li>随着更多元素的加入，误判概率随<strong>元素个数n</strong>的增加而增加</li>
</ul>
<p>那么如何降低误判概率呢？</p>
<ul>
<li>集合中元素个数相对固定，空间大小可能受限</li>
<li>可以通过适当选择哈希函数的个数即k值来<strong>最小化概率</strong>$(1-e^{-\frac{kn}{m}})^k$</li>
</ul>
<h2 id="局部位置敏感哈希-LSH"><a href="#局部位置敏感哈希-LSH" class="headerlink" title="局部位置敏感哈希(LSH)"></a>局部位置敏感哈希(LSH)</h2><p>现在有一个情境，就是对冗余文本的检测，将文章一一对比查找在性能上肯定说不过去。因此我们可以将文本分块, 然后将相似的块大概率会被哈希到同一个桶中，而不相似的文档大概率会被哈希到不同的桶中。到时候，我们就只在一个桶里进行比较就可以了。</p>
<p>利用这种技术，我们可以用来做抄袭检测、镜像网站的发现。</p>
<p>局部位置敏感哈希可以用来在海量文档中查找其中的冗余文本。步骤如下：</p>
<ul>
<li>Shingling ：将文档转换为集合</li>
<li>Min-Hashing：将大的几何转换为短的签名，同时保留相似性</li>
<li>局部敏感哈希：筛选或寻找相似文档候选对</li>
</ul>
<p><img src="/2021/09/24/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E7%AE%97%E6%B3%95-%E5%93%88%E5%B8%8C/7.png" style="zoom:67%;"></p>
<h4 id="Jaccard相似度"><a href="#Jaccard相似度" class="headerlink" title="Jaccard相似度"></a>Jaccard相似度</h4><p>给定集合A和B，Jaccard相似度定义为：</p>
<script type="math/tex; mode=display">
\text{Jaccord} (A,B) = \frac{|A\cap B|}{|A\cup B|}</script><p>给定集合A和B，Jaccord距离定义为：</p>
<script type="math/tex; mode=display">
d(A,B) = 1-\frac{|A\cap B|}{|A\cup B|}</script><p>Jaccard 相似度可以用来衡量文本之间的相似度</p>
<h3 id="Shingling文档的集合表示"><a href="#Shingling文档的集合表示" class="headerlink" title="Shingling文档的集合表示"></a>Shingling文档的集合表示</h3><p>首先，要对文本建模。</p>
<ul>
<li>定义Document为文档出现的单词集合</li>
<li>因为简单的分词会损失上下文信息，那么为了关注单词的顺序，我们需要定义Shingles</li>
</ul>
<p>文本中的 <em>K-Shingles</em> 是文本中连续k个token(字符、单词等)组成的序列。比如说，给定字符串 D = abcab,令k=2，则字符串的2-Shingles为</p>
<script type="math/tex; mode=display">
S(D)=\{ab,bc,ca\}</script><p>如果<em>k-Shingles</em>看做是multiset(可重复)，那么</p>
<script type="math/tex; mode=display">
S(D)=\{ab,bc,ca,ab\}</script><p>当我们得到了两个文档的$S(D)$​ 之后，我们就可以计算器Jaccord相似度.但是，当文档数量非常庞大时(假设为1000000)，那么待计算相似度的文档对有：</p>
<script type="math/tex; mode=display">
N(N-1)/2 \approx 5\times 10^{11}</script><p>计算机需要非常长时间才能进行如此庞大的计算。因此，我们需要在得到了k-Shingling之后，运用某个哈希函数将长度为k的字符串映射为桶编号。进一步将一系列集合表示成特征矩阵。其中，每行代表一个桶，每列代表一个文档。给定4篇文档：其转换图如下</p>
<p><img src="/2021/09/24/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E7%AE%97%E6%B3%95-%E5%93%88%E5%B8%8C/9.jpeg" style="zoom:67%;"></p>
<h3 id="Min-Hashing"><a href="#Min-Hashing" class="headerlink" title="Min-Hashing"></a>Min-Hashing</h3><p>最小哈希是一类特殊的哈希函数。其定义：</p>
<p>给定布尔向量$\bold{v}$​ 和随机排列$h$​  ,$h(v)$​ 为布尔向量$\bold{v}$​ 经过随机排列后得到的新向量，$mh(\bold{v})$ 为新得到的布尔向量$h(\bold v)$ 中第一个不为0的行号。函数$mh(\bold v)$被称为是最小哈希函数</p>
<p>比如说，我将上面这个表格打乱，得到：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>S1</th>
<th>S2</th>
<th>S3</th>
<th>S4</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>4</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>这里打乱之后，行号仍然按顺序标注，否则打乱就没有意义了。</p>
<p>按照竖着来看，$mh(\bold{v})$ 为新得到的布尔向量$h(\bold v)$ 中第一个不为0的行号。因此可以求出每个个文档的最小哈希值分别为： </p>
<script type="math/tex; mode=display">
mh(d_1)=1 ,mh(d_2)=4,mh(d_3)=1,mh(d_4)=0</script><h4 id="最小哈希与Jaccard相似度"><a href="#最小哈希与Jaccard相似度" class="headerlink" title="最小哈希与Jaccard相似度"></a>最小哈希与Jaccard相似度</h4><p>现在我们可以证明：在经过打乱后的两个集合计算得到的最小哈希值相等的概率，是等于这两个集合的Jaccard相似度的。</p>
<p>我们仅考虑集合S1和S2 这两列.那么这两列所在的行有以下三种类型：</p>
<ul>
<li>这一行的S1和S2都为1，记为X类</li>
<li>这一行只有一个值为1，一个值为0，记为Y类</li>
<li>这一行两个值都为0，记为Z类。</li>
</ul>
<p>假设属于X类的行有x个，属于Y类的行有y个，所以S1和S2交集的元素个数为x，并集的元素个数为x+y,所以$Jaccard(S_1,S_2) = \frac{x}{x+y} $</p>
<p>接下来计算最下哈希$h(S_1)=h(S_2)$​ 的概率，经过打乱之后，对特征值从上往下进行扫描，在碰到Y类行之前碰到X行的概率为$\frac{x}{x+y}$​; 又因为X类行中$h(S_1)=h(S_2)$ ,所以$h(S_1)=h(S_2)$的概率为$\frac{x}{x+y}$,即这两个集合Jaccard相似度。</p>
<p>因此，我们可以通过<strong>多次随机重排</strong>来求出多个不同的最小哈希值，则可以运用$mh(S_1)=mh(S_2)$ 发生的频率估计事件 $mh(S_1)=mh(S_2)$​ 发生的概率。</p>
<p>由此我们来引出最小哈希签名：</p>
<h4 id="最小哈希签名"><a href="#最小哈希签名" class="headerlink" title="最小哈希签名"></a>最小哈希签名</h4><p>上面是用一个行打乱来处理特征矩阵，然后就可以得到每个集合最小哈希值，这样多个集合就会有多个最小哈希值，这些值就可以组成一列</p>
<p>当我们用多个随机行打乱（假设为n个，分别为$h_1,h_2\cdots h_n$）来处理特征矩阵时，然后分别计算打乱后的这n个矩阵的最小哈希值；这样，对于集合S，就会有n个最小哈希值，这n个哈希值就可以组成一个列向量，为$[h_1(S), h_2(S)\cdots h_n(S)]$；</p>
<p>因此对于一个集合，经过上面的处理后，就能得到一个列向量；如果有m个集合，就会有m个列向量，每个列向量中有n个元素。把这m个列向量组成一个矩阵，这个矩阵就是特征矩阵的签名矩阵；</p>
<p>这个签名矩阵的列数与特征矩阵相同，但行数为n，也即哈希函数的个数。</p>
<p>通常来说，n都会比特征矩阵的行数要小很多，所以签名矩阵就会比特征矩阵小很多。</p>
<h4 id="最小哈希签名矩阵的计算"><a href="#最小哈希签名矩阵的计算" class="headerlink" title="最小哈希签名矩阵的计算"></a>最小哈希签名矩阵的计算</h4><p>现在我们来举一个具体的计算样例来理解最小哈希签名</p>
<p>首先我们用n来代表哈希函数的个数，通过哈希函数，我们可以来模拟行打乱的效果。</p>
<p>令$SIG(i,c)$表示签名矩阵中第$ i$ 个哈希函数在第$c$列上的元素。开始时，将所有的$SIG(i,c)$初始化为 Inf(无穷大)，然后对第r行进行如下处理：</p>
<ol>
<li>计算$h_1(r), h_2(r)\cdots h_n(r)$​​；r为行数</li>
<li>对于每一列c:<ul>
<li>如果c所在的第r行为0，则什么都不做；</li>
<li>如果c所在的第r行为1，则对于每个$i=1,2\cdots ,n$，将SIG(i,c)置为原来的$SIG(i,c)$和$h_i(r)$​之间的最小值。 </li>
</ul>
</li>
</ol>
<p>例如，考虑上面的特征矩阵，将abcde换成对应的行号，在后面加上两个哈希函数，其中$h_1(x)=(x+1) \mod 5\h_2(x) = (3\cdot x+1) \mod 5$​，注意这里x指的是行号：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>s1</th>
<th>s2</th>
<th>s3</th>
<th>s4</th>
<th>h1</th>
<th>h2</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>2</td>
<td>4</td>
</tr>
<tr>
<td>2</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>4</td>
<td>0</td>
</tr>
<tr>
<td>4</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
</tbody>
</table>
</div>
<p>接下来计算签名矩阵。一开始时，全部初始化为Inf:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>s1</th>
<th>s2</th>
<th>s3</th>
<th>s4</th>
</tr>
</thead>
<tbody>
<tr>
<td>h1</td>
<td>Inf</td>
<td>Inf</td>
<td>Inf</td>
<td>Inf</td>
</tr>
<tr>
<td>h2</td>
<td>Inf</td>
<td>Inf</td>
<td>Inf</td>
<td>Inf</td>
</tr>
</tbody>
</table>
</div>
<p>接着看特征矩阵中的第0行；这时S2和S3的值为0，所以无需改动；S1和S4的值为1，需改动。h1= 1，h2= 1。1比Inf小，所以需把S1和S4这两个位置对应的值替换掉，替换后效果如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>s1</th>
<th>s2</th>
<th>s3</th>
<th>s4</th>
</tr>
</thead>
<tbody>
<tr>
<td>h1</td>
<td>1</td>
<td>Inf</td>
<td>Inf</td>
<td>1</td>
</tr>
<tr>
<td>h2</td>
<td>1</td>
<td>Inf</td>
<td>Inf</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>接着看第1行；只有S3的值为1；此时h1= 2，h2= 4；对S3那一列进行替换，得到：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>s1</th>
<th>s2</th>
<th>s3</th>
<th>s4</th>
</tr>
</thead>
<tbody>
<tr>
<td>h1</td>
<td>1</td>
<td>Inf</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>h2</td>
<td>1</td>
<td>Inf</td>
<td>4</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>接着看第2行；S2和S4的值为1；h1=3，h2=2；因为签名矩阵S4那一列的两个值都为1，比3和2小，所以只需替换S2那一列：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>s1</th>
<th>s2</th>
<th>s3</th>
<th>s4</th>
</tr>
</thead>
<tbody>
<tr>
<td>h1</td>
<td>1</td>
<td>3</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>h2</td>
<td>1</td>
<td>2</td>
<td>4</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>接着看第3行；S1，S3和S4的值都为1，h1=4, h2= 0；替换后效果如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>s1</th>
<th>s2</th>
<th>s3</th>
<th>s4</th>
</tr>
</thead>
<tbody>
<tr>
<td>h1</td>
<td>1</td>
<td>3</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>h2</td>
<td>0</td>
<td>2</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p> 接着看第4行；S3值为1，h1=0, h2= 3，最终效果如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>s1</th>
<th>s2</th>
<th>s3</th>
<th>s4</th>
</tr>
</thead>
<tbody>
<tr>
<td>h1</td>
<td>1</td>
<td>3</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>h2</td>
<td>0</td>
<td>2</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>这样，所有的行都被遍历一次了，最终得到的签名矩阵如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>s1</th>
<th>s2</th>
<th>s3</th>
<th>s4</th>
</tr>
</thead>
<tbody>
<tr>
<td>h1</td>
<td>1</td>
<td>3</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>h2</td>
<td>0</td>
<td>2</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>基于这个签名矩阵，我们就可以<strong>估计原始集合</strong>之间的Jaccard相似度了。由于S1和S4对应的列向量完全一样，所以可以估计$SIM(S1,S4)=1$;  又比如说对于 S1和S3，两个元素中有一个一样，因此$SIM(S1,S3) = 0.5$</p>
<p>虽然用最小哈希签名矩阵可以近似<strong>估算两个集合的相似度</strong>，但是当哈希函数个数比较少的时候，估算误差可能会比较大，因此，需要选择恰当数量的哈希函数来降低估算误差。</p>
<h3 id="局部敏感哈希"><a href="#局部敏感哈希" class="headerlink" title="局部敏感哈希"></a>局部敏感哈希</h3><p>通过上面的方法处理过后，一篇文档可以用一个很小的签名矩阵来表示，节省下很多内存空间；但是，还有一个问题没有解决，那就是如果有很多篇文档，那么如果要找出相似度很高的文档，其中一种办法就是先计算出所有文档的签名矩阵，然后依次两两比较签名矩阵的相似度；这样做的缺点是当文档数量很多时，要比较的次数会非常大。那么<strong>我们可不可以只比较那些相似度可能会很高的文档，而直接忽略过那些相似度很低的文档</strong>。 </p>
<p>因此接下来我们就讨论这个问题的解决方法——<strong>局部敏感哈希</strong></p>
<p>首先，我们可以通过上面的方法得到一个签名矩阵，然后把这个矩阵划分成<strong>$b$个行条</strong>(band)，<strong>每个行条由$r$行组成</strong>。</p>
<p>对于每个行条，存在一个哈希函数能够将行条中的每r个整数组成的列向量（行条中的每一列）映射到某个桶中。可以对所有行条使用相同的哈希函数，但是对于每个行条我们都使用一个独立的桶数组，因此即便是不同行条中的相同列向量，也不会被哈希到同一个桶中。</p>
<p>这样，<strong>只要两个集合在某个行条中有落在相同桶的两列，这两个集合就被认为可能相似度比较高</strong>，作为后续计算的候选对；</p>
<p>而那些在所有行条中都不落在同一个桶中的两列，就会被认为相似度不会很高，而被直接忽略。下面直接看一个例子：</p>
<p><img src="/2021/09/24/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E7%AE%97%E6%B3%95-%E5%93%88%E5%B8%8C/10.png"></p>
<p>可以看出，<strong>行条1</strong>中第2列和第4列的内容都为[0,2,1]，所以这两列会落在行条1下的相同桶中，因此无论在剩下的3个行条中这两列是否有落在相同桶中，这两个集合都会成为候选对。在行条1中不相等的两列还有另外的3次机会成为候选对，因为他们只需在剩下的3个行条中有一次相等即可。</p>
<p>经过上面的处理后，我们就找出了相似度可能会很高的一些候选对，接下来我们只需对这些候选队进行比较就可以了，而直接忽略那些不是候选对的集合。这个方法适合用来计算相似度超过某个值的文档的相似度，而不适用于计算所有文档的相似度，因为那些相似度可能很低的文档已经被直接忽略了。</p>
<p>现在我们来定量分析一下：</p>
<p>假定将某个最小哈希签名矩阵划分为b组，每组有r行组成。此外，假定两个集合的Jaccard相似度为s。那么，两个集合被哈希到同一个桶中的概率与其Jaccard 相似度存在如下关系：</p>
<ul>
<li>在<strong>具体某个组</strong>中，这对集合所有的最<strong>小哈希值都相等的概率</strong>是$s^r$</li>
<li>在这个组中，这对集合至少有一个位置的最小哈希值不相等的概率是$(1-s^r)$，即它们没有被映射到同一个桶中。</li>
<li>在任何一个组中，这对集合都没有被映射到同一个桶中的概率是$(1-s^r)^b$</li>
<li>进而，在整个最小哈希签名矩阵中，至少在一个组中的两个集合被映射到同一个桶中的概率为$1-(1-s^r)^b$</li>
</ul>
<p>我们给定任意的r和b的值，以相似度s的值为横坐标，以概率$1-(1-s^r)^b$的概率为纵坐标，则在二维坐标系中展示了一条S曲线, 如下图所示：</p>
<p><img src="/2021/09/24/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E7%AE%97%E6%B3%95-%E5%93%88%E5%B8%8C/11.jpeg"></p>
<p>通常被映射到同一个桶中的概率的$\frac{1}{2}$处对应的s值被称为是<strong>相似度阈值</strong>。是一个关于b和r的函数。这一阈值的估计为$(1/b)^{1/r}$​ 。对于较大的b和r，s值大于相似度阈值的几何对很有可能被哈希到同一个桶中，而小于相似度阈值的不太可能被哈希到同一个桶中。</p>
<h2 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h2><h3 id="1"><a href="#1" class="headerlink" title="1"></a>1</h3><p>假设一个布隆过滤器的容量是 $8\times 10^9$ 位，集合中有 $1\times 10^9$ 个元素。如果使用3个哈希函数，试计算误判率。如果使用4个哈希函数，误判率如何？</p>
<ul>
<li>当 $k=3$ 的时候</li>
</ul>
<script type="math/tex; mode=display">
r =(1- \exp\{-\frac{kn}{m}\})^k = (1-\exp\{-\frac{3}{8}\})^3 \\</script><ul>
<li>当$k=4$时候</li>
</ul>
<script type="math/tex; mode=display">
r =(1- \exp\{-\frac{kn}{m}\})^k = (1-\exp\{-\frac{1}{2}\})^4 \\</script><h3 id="2"><a href="#2" class="headerlink" title="2"></a>2</h3><p>假定全集A有n个元素，随机从中抽取出两个子集 $A_1,A_2$ ，且每个自己都有m 个元素，求$A_1$和$A_2$ 两个集合的期望相似度。</p>
<p>首先，我们写出两个子集 Jacorrd 相似度的表达式. 为我们令两者的交集大小为a</p>
<script type="math/tex; mode=display">
Jacorrd(A_1,A_2) = \frac{a}{2m-a}</script><p>因此，我们可以计算期望 </p>
<script type="math/tex; mode=display">
E(JS(A_1,A_2)) = \sum_{a} P(a)JS(A_1,A_2)\\
P(a) = \frac{C_m^aC_{n-m}^{m-a}}{C_n^m}</script><p>但是，我们要分情况讨论，因为 $m\leq \frac{n}{2}$ 和 $m&gt;\frac{n}2$ 是不一样的</p>
<script type="math/tex; mode=display">
E(JS(A_1,A_2)) = \begin{cases}\sum_{a=0}^m P(a)\frac{a}{2m-a}&m\leq n/2 \\~\\ \sum_{a=2m-n}^m P(a)\frac{a}{2m-a}& m>{n}/{2} \end{cases}</script>
          
        
      
    </div>
    
    
    
    <div>
      
    </div>
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/8/">&lt;i class&#x3D;&quot;fa fa-angle-left&quot;&gt;&lt;&#x2F;i&gt;</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><span class="page-number current">9</span><a class="page-number" href="/page/10/">10</a><span class="space">&hellip;</span><a class="page-number" href="/page/44/">44</a><a class="extend next" rel="next" href="/page/10/">&lt;i class&#x3D;&quot;fa fa-angle-right&quot;&gt;&lt;&#x2F;i&gt;</a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/%5Bobject%20Object%5D"
                alt="Jason" />
            
              <p class="site-author-name" itemprop="name">Jason</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%20%7C%7C%20fa%20fa-archive">
              
                  <span class="site-state-item-count">435</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">61</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart" aria-hidden="true"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jason</span>
 <!--
  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>







-->
        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>




















  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v="></script>

  <script type="text/javascript" src="/js/src/motion.js?v="></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v="></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
