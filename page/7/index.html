<!DOCTYPE html>
<html lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jasonxqh.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Jason‘s Blog">
<meta property="og:url" content="https://jasonxqh.github.io/page/7/index.html">
<meta property="og:site_name" content="Jason‘s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Jason">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://jasonxqh.github.io/page/7/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Jason‘s Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Jason‘s Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2022/01/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" class="post-title-link" itemprop="url">机器学习-支持向量机</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-01-03 15:06:24" itemprop="dateCreated datePublished" datetime="2022-01-03T15:06:24+08:00">2022-01-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-01-04 19:49:50" itemprop="dateModified" datetime="2022-01-04T19:49:50+08:00">2022-01-04</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="机器学习-支持向量机"><a href="#机器学习-支持向量机" class="headerlink" title="机器学习-支持向量机"></a>机器学习-支持向量机</h1><p>学习支持向量机的时候，我们要一步一步来，从线性可分到线性不可分，一共有三种支持向量机。</p>
<ul>
<li><strong>线性可分支持向量机</strong>和<strong>线性支持向量机</strong>：输入空间和特征空间的元素一一对应，并将输入空间中的输入映射为特征空间中特特征向量</li>
<li>非线性支持向量机：利用一个从输入控件到特征空间的非线性映射将输入映射为特征向量</li>
</ul>
<p>支持向量机的学习是在<strong>特征空间</strong>进行的</p>
<h2 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h2><h3 id="线性可分支持向量机"><a href="#线性可分支持向量机" class="headerlink" title="线性可分支持向量机"></a>线性可分支持向量机</h3><p>支持向量机最简单的情况是<strong>线性可分支持向量机</strong>，或<strong>硬间隔支持向量机</strong> , 构建它的条件是训练数据线性可分。其学习策略是<strong>最大间隔法</strong>， 可以表示为凸二次规划问题，其原始最优化问题为</p>
<script type="math/tex; mode=display">
\min_{w,b}& \frac{1}{2}||w||^2\\
s.t. & y_i(w\cdot x_i+b)-1 \geq 0,i=1,2\cdots,N</script><p>求得最优化问题的解为 $w^<em>,b^</em>$ ，得到线性可分支持向量机，分离超平面是：</p>
<script type="math/tex; mode=display">
w^*\cdot x+b^* = 0</script><p>分类决策函数是</p>
<script type="math/tex; mode=display">
f(x) = sign(w^*\cdot x+b^*)</script><h3 id="线性支持向量机-软间隔支持向量机"><a href="#线性支持向量机-软间隔支持向量机" class="headerlink" title="线性支持向量机(软间隔支持向量机 )"></a>线性支持向量机(软间隔支持向量机 )</h3><h2 id="线性可分支持向量机-1"><a href="#线性可分支持向量机-1" class="headerlink" title="线性可分支持向量机"></a>线性可分支持向量机</h2><p>现在摆在我们面前的仍然是一个二分类问题。</p>
<ul>
<li>训练数据集：</li>
</ul>
<p>假设特征空间上的数据集</p>
<script type="math/tex; mode=display">
T = \{(x_1,y_1),\cdots,(x_N,y_N) \}</script><p>输入为特征向量 $x\in\mathcal X\subseteq \boldsymbol R^n$ ，$y_i\in\mathcal Y = {+1,-1},i=1,2\cdots,N$， $x_i$ 为第i个特征向量，也称为实例。$y_i$ 为 $x_i$ 的类标记，当$y_i=+1$ 时，称x为正例；当 $ y =-1 $时，称$x_i$为负例$(x_i,y_i)$称为样本点。 再假设训练数据集是线性可分的</p>
<ul>
<li><p>学习目标: 找到分离超平面 $w\cdot x+b=0$  。分离超平面将特征空间划分为两部分， 一部分是正类，一部分是负类。法向量指向的一侧为正类，另一侧为负类</p>
</li>
<li><p>学习策略：间隔最大化。</p>
</li>
</ul>
<p>如下图，感知机可能会得到多个不同(w,b)的超平面，但是svm最终只会选择间隔最大的超平面</p>
<p><img src="/2022/01/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/1.png" style="zoom:67%;"></p>
<blockquote>
<p>注意：</p>
<p>感知机利用误分类最小的策略，求得分离超平面。但是这个解是由无穷多个的，只要分对了就好</p>
<p>但是线性可分支持向量机利用<strong>间隔最大化</strong>来求最优分离超平面，这时候解就变成唯一的了</p>
</blockquote>
<ul>
<li>决策函数：<script type="math/tex; mode=display">
f(x) = sign(w^*\cdot x+b^*)</script></li>
</ul>
<h3 id="间隔"><a href="#间隔" class="headerlink" title="间隔"></a>间隔</h3><p>既然支持向量机要求间隔最大化，那么，间隔如何定义呢？</p>
<h4 id="函数间隔"><a href="#函数间隔" class="headerlink" title="函数间隔"></a>函数间隔</h4><p>首先，我们要定义函数间隔.</p>
<p>对于给定的训练数据集T和超平面$(w,b)$，定义超平面关于<strong>样本点</strong> $(x_i,y_i)$ 的函数间隔为：</p>
<script type="math/tex; mode=display">
\hat{\mathcal Y_{i} }= y_i(w\cdot x_i+b)</script><p>然后，我们定义超平面(w,b)关于<strong>训练数据集</strong>T的函数间隔为 : 超平面$(w,b)$ 关于T中所有样本点$(x_i,y_i)$  的函数间隔中的最小值。也就是</p>
<script type="math/tex; mode=display">
\hat { \mathcal {Y }} = \min_{i=1\cdots N} \hat{\mathcal Y_i}</script><p>这相当于”感性“认识间隔，因为当成比例得改变w和b的话，超平面并没有改变，但是函数间隔却变成了原来的两倍。因此我们还需要给出几何间隔的定义</p>
<h4 id="集合间隔"><a href="#集合间隔" class="headerlink" title="集合间隔"></a>集合间隔</h4><p>对于给定的训练数据集 T 和超平面$(w,b)$ ,定义超平面 $(w,b)$ 关于样本点$(x_i,y_i)$的几何间隔为：</p>
<script type="math/tex; mode=display">
\hat{\mathcal Y_{i} }= y_i(\frac{w}{||w||}\cdot x_i+\frac{b}{||w||})</script><p>然后，我们定义超平面(w,b)关于<strong>训练数据集</strong>T的几何间隔为 : 超平面$(w,b)$ 关于T中所有样本点$(x_i,y_i)$  的几何间隔中的最小值。也就是</p>
<script type="math/tex; mode=display">
\hat { \mathcal {Y }} = \min_{i=1\cdots N} \hat{\mathcal Y_i}</script><p>函数间隔和几何间隔有如下关系：</p>
<script type="math/tex; mode=display">
{\mathcal Y_i}=\frac{\hat{\mathcal Y_i}}{||w||}\\
\mathcal Y = \frac{\hat{\mathcal Y}}{||w||}\\</script><p>如果 $||w||=1$ ,那么函数间隔和几何间隔相等。</p>
<h3 id="间隔最大化"><a href="#间隔最大化" class="headerlink" title="间隔最大化"></a>间隔最大化</h3><p>学完了间隔，我们就要学习，怎么让间隔最大化(在线性可分中又叫硬间隔最大化) </p>
<p>一个感性又直观的解释是：对训练数据集找到几何间隔最大的超平面，意味着以<strong>充分大的确信度</strong>对训练数据进行分类。也就是说，我们不仅要将正负实例点分开，而且对最难分的实例点(距离超平面最近的点)也有足够大的确信度将它们分开。这样的超平面应该对未知的新实例有很好的分类预测能力。</p>
<p>因此我们可以这样来写一个<strong>约束最优化</strong>问题：</p>
<script type="math/tex; mode=display">
\max_{w,b} & \mathcal Y\\
s.t.& y_i(\frac{w}{||w||}x_i+\frac{b}{||w||})\geq \mathcal Y,i=1,2\cdots,N</script><p>用通俗的话来说，我们希望最大化超平面关于训练数据集的几何间隔 $\mathcal Y$ 。约束条件表示的是，超平面关于每个训练样本点的几何间隔<strong>至少是</strong>$\mathcal Y$ </p>
<p>上面这个式子比较麻烦，我们运用<strong>几何间隔和函数间隔</strong> 的关系，可以将上式改写为：</p>
<script type="math/tex; mode=display">
\max_{w,b} & \frac{\hat{\mathcal Y}}{||w||}\\
s.t.& y_i({w}x_i+{b})\geq \hat{\mathcal Y},i=1,2\cdots,N</script><p>然后， 我们还是不满意，还是觉得有点复杂。因此我们可以将$\hat {\mathcal Y}=1$ ,函数间隔的这一改变对优化问题的不等式没有影响。而且我们也不喜欢分数，因为最大化 $\frac{1}{||w||}$  和最小化 $\frac{1}{2}||w||^2$ 是等价的，因此可以得到最终版本的优化问题：</p>
<script type="math/tex; mode=display">
\max_{w,b} & \frac{1}{2}{||w||}^2 \\
s.t.& y_i({w}x_i+{b})-1\geq 0,i=1,2\cdots,N</script><h4 id="算法—最大间隔法"><a href="#算法—最大间隔法" class="headerlink" title="算法—最大间隔法"></a>算法—最大间隔法</h4><ul>
<li>输入： 线性可分训练数据集 $T = {(x_1,y_1),\cdots,(x_N,y_N) }$ ,其中，$x_i\in \mathcal X = \boldsymbol R^n,y_i\in\mathcal Y = {-1,+1},i=1,2\cdots,N$ </li>
<li>输出： 最大间隔分离超平面和决策函数</li>
</ul>
<ol>
<li>构造并<strong>求解</strong> 约束最优化问题：</li>
</ol>
<script type="math/tex; mode=display">
\max_{w,b} & \frac{1}{2}{||w||}^2 \\
s.t.& y_i({w}x_i+{b})-1\geq 0,i=1,2\cdots,N</script><p>求得最优解$w^<em>,b^</em> $</p>
<ol>
<li>得到分离超平面</li>
</ol>
<script type="math/tex; mode=display">
w^*\cdot x+b^* = 0</script><p>分类决策函数：</p>
<script type="math/tex; mode=display">
f(x) = sign(w^*\cdot x+b^* )</script><h4 id="支持向量和间隔边界"><a href="#支持向量和间隔边界" class="headerlink" title="支持向量和间隔边界"></a>支持向量和间隔边界</h4><p>在线性可分的情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例，我们称其为<strong>支持向量</strong>，支持向量是使得约束条件式等号成立的点，即：</p>
<script type="math/tex; mode=display">
y_i(w\cdot x_i+b)-1 = 0</script><p>对于 $y_i = +1$ 的正例点，支持向量在超平面 $w\cdot x+b=1$上</p>
<p>对于 $y_i = -1$ 的正例点，支持向量在超平面 $w\cdot x+b= -1$上</p>
<p>如下图，$H_1,H_2$ 上的点，就是支持向量</p>
<p><img src="/2022/01/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/3.png"></p>
<p>注意到 $H_1$和$H_2$ 平行，它们中间形成一条没有点的长带。长带的宽度等于 $\frac{2}{||w||}$,称$H_1$和$H_2$ 为<strong>间隔边界</strong></p>
<h3 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h3><p><img src="/2022/01/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/4.jpg"></p>
<p>第一步：列出优化问题。 这里有x是二维的，因此我们 $\boldsymbol w = (w_1,w_2)$</p>
<script type="math/tex; mode=display">
\max_{w,b} & \frac{1}{2}{||w_1+w_2||}^2 \\
s.t.& y_i({w_1}x_{1i}+w_2x_{2i}+{b})-1\geq 0,i=1,2\cdots,N</script><p>带入正例点和负例点，可得：</p>
<script type="math/tex; mode=display">
\begin{align}
&w_1+2w_2+b\geq1 &(1)\\
&2w_1+3w_2+b\geq1&(2)\\
&3w_1+3w_2+b\geq 1&(3)\\
&2w_1+w_2+b\leq -1\Leftrightarrow-2w_1-w_2-b\geq -1&(4)\\
&3w_1+2w_2+b\leq -1\Leftrightarrow-3w_1-2w_2-b\geq -1&(5)\\
\end{align}</script><p>第二步： 线性规划</p>
<p>我们要做的是尽量消去 b，然后再一个二维平面上做线性优化：</p>
<script type="math/tex; mode=display">
\begin{align}
(4)+(1)\Rightarrow&-w_1+w_2\geq 2\\
(5)+(1)\Rightarrow&-2w_1\geq 2\Rightarrow w_1\leq 1\\
(4)+(2) \Rightarrow&2w_2\geq 2\Rightarrow w_2\leq1\\
(5)+(2) \Rightarrow& -w_1+w_2\geq2\\
(4)+(3) \Rightarrow&w_1+2w_2\geq 2\\
(5)+(3) \Rightarrow&w_2\geq2\\
\end{align}</script><p>我们对上面式子做线性规划，可得：</p>
<p><img src="/2022/01/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/5.jpeg"></p>
<p>将 $(w_1,w_2)=(-1,2)$ 带入原方程，可得 $b=-2$</p>
<p>第三步：画出分离超平面，得到支持向量</p>
<p><img src="/2022/01/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/6.jpeg" style="zoom:50%;"></p>
<p>最终得到分离超平面为：</p>
<script type="math/tex; mode=display">
-x_1+2x_2-2 = 0</script><p>分类决策函数为 </p>
<script type="math/tex; mode=display">
f(x) = sign(-x_1+2x_2-2)</script><p>支持向量为：$(3,2),(1,2),(3,3)$</p>
<p>#</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2022/01/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/" class="post-title-link" itemprop="url">机器学习-朴素贝叶斯</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-01-02 23:21:43" itemprop="dateCreated datePublished" datetime="2022-01-02T23:21:43+08:00">2022-01-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-01-05 01:24:28" itemprop="dateModified" datetime="2022-01-05T01:24:28+08:00">2022-01-05</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="机器学习-朴素贝叶斯"><a href="#机器学习-朴素贝叶斯" class="headerlink" title="机器学习-朴素贝叶斯"></a>机器学习-朴素贝叶斯</h1><h2 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h2><ol>
<li>朴素贝叶斯法是典型的生成学习方法生成方法由训练数据<strong>学习联合概率分布</strong>$P(X,Y)$  然后求得后验概率分布$P(Y |X)$. 具体来说, 利用训练数据学习 $P(X|Y)$ 和 $P(Y)$ 的估计，得到联合概率分布:</li>
</ol>
<script type="math/tex; mode=display">
P(X,Y)= P(Y)P(X|Y)</script><p>概率估计方法可以是<strong>极大似然估计</strong> 或者是 <strong>贝叶斯估计</strong></p>
<ol>
<li>朴素贝叶斯法的基本假设是条件独立性</li>
</ol>
<script type="math/tex; mode=display">
P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},\cdots,X^{(n)}=x^{(n)}|Y=c_k)\\
=\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)</script><p>这是一个较强的假设.由于这一假设，模型包含的条件概率的数量大为减少，朴素贝叶斯法的学习与预测大为简化.因而朴素贝叶斯法高效，且易于实现.其缺点是分类的性能不一定很高.</p>
<ol>
<li>朴素贝叶斯法利用贝叶斯定理与学到的联合概率模型进行分类预测</li>
</ol>
<script type="math/tex; mode=display">
P(Y|X) = \frac{P(X,Y)}{P(X)} = \frac{P(Y)P(X|Y)}{\sum_YP(Y)P(X|Y)}</script><p>将输入x 分到后验概率最大的类y</p>
<script type="math/tex; mode=display">
y = \arg\max_{c_{k}} P(Y=c_k)\prod_{j=1}^nP(X_j=x^{(j)}|Y=c_k)</script><p>后验概率等价于0-1损失函数时的期望风险最小化.</p>
<h2 id="朴素贝叶斯学习与分类"><a href="#朴素贝叶斯学习与分类" class="headerlink" title="朴素贝叶斯学习与分类"></a>朴素贝叶斯学习与分类</h2><h3 id="基本方法"><a href="#基本方法" class="headerlink" title="基本方法"></a>基本方法</h3><h4 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h4><p>对于训练数据集：输入为特征向量 $x\in\mathcal X\subseteq \boldsymbol R^n$ ，输出位类标记 $y\in \mathcal Y ={c_1,c_2\cdots,c_k}$ </p>
<script type="math/tex; mode=display">
T = \{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \}</script><p>由 $P(X,Y)$ 独立同分布产生。</p>
<p>然后，朴素贝叶斯法 会通过训练数据集<strong>学习联合概率分布</strong> $P(X,Y)$ ，也就是学习<strong>先验概率分布</strong>以及<strong>条件概率分布</strong></p>
<ul>
<li><p>先验概率分布：</p>
<script type="math/tex; mode=display">
P(Y=c_k),k=1,2\cdots,K</script></li>
<li><p>条件概率分布</p>
<script type="math/tex; mode=display">
P(X=x|Y=c_k)  = P(X^{(1)}=x^{(1)},\cdots,X^{(n)}=x^{(n)}|Y=c_k),k=1,2\cdots,K</script></li>
</ul>
<p>于是，学习到联合概率分布 $P(X,Y)$</p>
<p>条件概率分布 $P(X=x|Y=c_k)$ 有着<strong>指数级数量</strong>的参数，其估计实际是不可行的。因此，朴素贝叶斯法对条件概率分布做了条件独立性假设。具体的，条件独立性假设是：</p>
<script type="math/tex; mode=display">
P(X^{(1)}=x^{(1)},\cdots,X^{(n)}=x^{(n)}|Y=c_k)\\
=\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)</script><p>条件独立假设等于是说：用于分类的特征在类确定的条件下都是<strong>条件独立的</strong>。这一假设使得朴素贝叶斯法变得简单，但有时会牺牲一定的分类准确率。</p>
<h4 id="分类过程"><a href="#分类过程" class="headerlink" title="分类过程"></a>分类过程</h4><p>朴素贝叶斯法分类时，对给定的输入x，通过学习到的模型计算后验概率分布 $P(Y=c_k|X=x)$ ，将后验概率最大的类作为$x$ 的输出。后验概率计算根据<strong>贝叶斯定理</strong>进行：</p>
<script type="math/tex; mode=display">
P(Y=c_k|X=x) = \frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_k P(X=x|Y=c_k)P(Y=c_k)}</script><p>将独立性假设带入上式，可得：</p>
<script type="math/tex; mode=display">
P(Y=c_k|X=x) = \frac{P(Y=c_k)\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_k P(X=x|Y=c_k)\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)} ,~~~k=1,2,\cdots,K</script><p>这是朴素贝叶斯法分类的基本公式。于是，朴素贝叶斯分类器科表示为：</p>
<script type="math/tex; mode=display">
y = f(x) = \arg\max_{c_k}\frac{P(Y=c_k)\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_k P(Y=c_k)\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)}</script><p>注意到，在上式中，分母对所有 $c_k$ 都是相同的，因此直接拿掉了</p>
<script type="math/tex; mode=display">
y = \arg\max_{c_{k}} P(Y=c_k)\prod_{j=1}^nP(X_j=x^{(j)}|Y=c_k)</script><h2 id="朴素贝叶斯的学习与分类算法"><a href="#朴素贝叶斯的学习与分类算法" class="headerlink" title="朴素贝叶斯的学习与分类算法"></a>朴素贝叶斯的学习与分类算法</h2><h3 id="朴素贝叶斯算法"><a href="#朴素贝叶斯算法" class="headerlink" title="朴素贝叶斯算法"></a>朴素贝叶斯算法</h3><p>输入：训练数据 $T = {(x<em>1,y_1),(x_2,y_2),\cdots,(x_N,y_N) }$ ，其中 $x_i = (x_i^{(1)},\cdots,x_i^{(n)})^T$.  $x_i^{(j)}$ 是第i个样本的第j 个特征，$x_i^{(j)}\in{a</em>{j1},\cdots,a<em>{jS_j} }$ , $a</em>{jl}$ 是第j个特征可能取的第 l 个值。</p>
<script type="math/tex; mode=display">
j=1,2\cdots,n\\
l=1,2\cdots,S_j\\
y_i\in \{c_1,c_2,\cdots,c_k\}</script><p>以及测试数据 ：实例x</p>
<p>输入：实例x的分类</p>
<ol>
<li>计算先验概率及条件概率</li>
</ol>
<script type="math/tex; mode=display">
P(Y=c_k) = \frac{\sum_{i=1}^NI(y_i=c_k)}{N},k=1,2\cdots,K\\~\\</script><p>第一步是计算输入 $c_k$ 的训练数据占所有训练数据的比值</p>
<script type="math/tex; mode=display">
P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^N I(x_i^{(j)}=a_{jl},y=c_k)}{\sum_{i=1}^N I(y_i=c_k)}\\~\\
j=1,2\cdots,n;~l=1,2\cdots,S_j;~y_i\in \{c_1,c_2,\cdots,c_k\}</script><p>第二步是计算条件概率</p>
<ol>
<li>对于给定的实例 $x=(x^{(1)},\cdots,x^{(n)})^T$ 计算：</li>
</ol>
<script type="math/tex; mode=display">
P(Y=c_k) \prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k) ,k=1,2\cdots,K</script><ol>
<li>确定实例 $x$ 的类</li>
</ol>
<script type="math/tex; mode=display">
y = \arg\max_{c_k}P(Y=c_k)\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)</script><h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><p>试由下表的训练数据学习一个朴素贝叶斯分类器并确定$x=(2,S)^T$的类标记 y. 表中$X^{(1)}, X^{(2)}$  为特征，取值的集合分别为，$A_1={1 , 2, 3} ,  A_2={S, M,L}$ ;  $Y$ 为类标记， $Y\in C = {1 , -1}$</p>
<p><img src="/2022/01/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/1.png" style="zoom:150%;"></p>
<ol>
<li>计算先验概率，首先统计正例和负例的个数。</li>
</ol>
<p>这里，正例有9个，负例有6个。因此:</p>
<script type="math/tex; mode=display">
P(Y=1) = 9/15\\
P(Y=-1) = 6/15</script><ol>
<li>计算条件概率，一共要计算 $2\times(3+3)=12$个概率</li>
</ol>
<script type="math/tex; mode=display">
\begin{align}
&P(X^{(1)}=1|Y=1)=\frac{2}{9},P(X^{(1)}=2|Y=1)=\frac{3}{9},P(X^{(1)}=3|Y=1)=\frac{4}{9}\\
&P(X^{(2)}=S|Y=1)=\frac{1}{9},P(X^{(2)}=M|Y=1)=\frac{4}{9},P(X^{(2)}=L|Y=1)=\frac{4}{9}\\
&P(X^{(1)}=1|Y=-1)=\frac{3}{6},P(X^{(1)}=2|Y=-1)=\frac{2}{6},P(X^{(1)}=3|Y=-1)=\frac{1}{6}\\
&P(X^{(2)}=S|Y=-1)=\frac{3}{6},P(X^{(2)}=M|Y=-1)=\frac{2}{6},P(X^{(2)}=L|Y=-1)=\frac{1}{6}\\
\end{align}</script><ol>
<li>计算后验概率，对给定的测试样本 $x=(2,S)^T$ 进行计算</li>
</ol>
<script type="math/tex; mode=display">
\begin{align}
&P(Y=1)P(X^{(1)}=2|Y=1)P(X^{(2)}=S|Y=1) =\frac{9}{15}\frac{3}{9}\frac{1}{9} = \frac{1}{45}\\
&P(Y=-1)P(X^{(1)}=2|Y=-1)P(X^{(2)}=S|Y=-1)=\frac{6}{15}\frac{2}{6}\frac{3}{6} = \frac{1}{15}
\end{align}</script><p>由于$P(Y=-1)P(X^{(1)}=2|Y=-11)P(X^{(2)}=S|Y=-1)$ 是两种情况中最大的，因此 最终的预测结果是 $y=-1$</p>
<h3 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h3><p>用极大似然估计可能会出现所要估计的概率值为0 的情况. 这时会影响到后验概率的计算结果(只要一个概率值为0就都为0)，使分类产生偏差解决这一问题的方法是采用贝叶斯估计</p>
<p>具体地</p>
<p>先验概率的贝叶斯估计是</p>
<script type="math/tex; mode=display">
P_\lambda (Y=c_k) = \frac{\sum_{i=1}^NI(y_i=c_k)+\lambda}{N+K\lambda}</script><p>条件概率的贝叶斯估计是</p>
<script type="math/tex; mode=display">
\begin{align}
&P_\lambda (X^{(j)}=a_{jl}|Y=c_{k}) = \frac{\sum_{i=1}^N I(x_i^{(j)}=a_{jl},y=c_k)+\lambda}{\sum_{i=1}^N I(y_i=c_k)+S_j\lambda }\\~\\
&j=1,2\cdots,n;~l=1,2\cdots,S_j;~y_i\in \{c_1,c_2,\cdots,c_k\}
\end{align}</script><p>式中 $\lambda \geq 0$, 等价于在随机变量各个取值的频数上赋予一个整数 $\lambda &gt;0$ . 当 $\lambda=0$ 时，就是极大似然估计。常常取 $\lambda = 1$ ,这称为拉普拉斯平滑 。 </p>
<p>显然，对任何 $l=1,2\cdots,S_j,k=1,2\cdots,K$ ,有条件概率大于0</p>
<script type="math/tex; mode=display">
\begin{align}
&P_\lambda (X^{j} = a_{jl}|Y=c_k) > 0\\~\\
&\sum_{l=1}^{S_j}P(X^{(j)}=a_{jl}|Y=c_k)=1\\
\end{align}</script><h4 id="例子-1"><a href="#例子-1" class="headerlink" title="例子"></a>例子</h4><p>试由下表的训练数据学习一个朴素贝叶斯分类器并确定$x=(2,S)^T$ 的类标记 $y$ 。 表中$X^{(1)}, X^{(2)}$  为特征，取值的集合分别为，$A_1={1 , 2, 3} ,  A_2={S, M,L}$  $Y$ 为类标记， $Y\in C = {1 , -1}$ </p>
<p>按照拉普拉斯平滑估计概率 ，即取  $\lambda = 1$</p>
<p><img src="/2022/01/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/1.png" style="zoom:150%;"></p>
<ol>
<li>先验概率的贝叶斯估计</li>
</ol>
<script type="math/tex; mode=display">
P(Y=1) = \frac{9+1}{15+2\times1} = \frac{10}{17}\\
P(Y=-1) = \frac{6+1}{15+2\times 1} =\frac{7}{17}\\</script><ol>
<li>计算条件概率</li>
</ol>
<script type="math/tex; mode=display">
\begin{align}
&P(X^{(1)}=1|Y=1)=\frac{2+1}{9+3*1}= \frac{3}{12},P(X^{(1)}=2|Y=1)=\frac{4}{12},P(X^{(1)}=3|Y=1)=\frac{5}{12}\\~\\
&P(X^{(2)}=S|Y=1)=\frac{2}{12},P(X^{(2)}=M|Y=1)=\frac{5}{12},P(X^{(2)}=L|Y=1)=\frac{5}{12}\\~\\
&P(X^{(1)}=1|Y=-1)=\frac{4}{9},P(X^{(1)}=2|Y=-1)=\frac{3}{9},P(X^{(1)}=3|Y=-1)=\frac{2}{9}\\~\\
&P(X^{(2)}=S|Y=-1)=\frac{4}{9},P(X^{(2)}=M|Y=-1)=\frac{3}{9},P(X^{(2)}=L|Y=-1)=\frac{2}{9}\\
\end{align}</script><p>由此我们可以看出，及时当前的条件样本数为0，条件概率也不会为0，而是 $p = \frac{\lambda}{\sum_{i=1}^N I(y_i=c_k)+S_j\lambda}$ </p>
<ol>
<li>因此，对于给定的 $x=(2,S)^T$ ，我们计算后验概率 </li>
</ol>
<script type="math/tex; mode=display">
\begin{align}
&P(Y=1)P(X^{(1)}=2|Y=1)P(X^{(2)}=S|Y=1) =\frac{10}{17}\frac{4}{12}\frac{2}{12} = \frac{5}{153}=0.0327\\
&P(Y=-1)P(X^{(1)}=2|Y=-1)P(X^{(2)}=S|Y=-1)=\frac{7}{17}\frac{3}{9}\frac{4}{9} = \frac{28}{459}=0.0610
\end{align}</script><p>由于$P(Y=-1)P(X^{(1)}=2|Y=-11)P(X^{(2)}=S|Y=-1)$ 是两种情况中最大的，因此 最终的预测结果是 $y=-1$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2022/01/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%84%9F%E7%9F%A5%E6%9C%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%84%9F%E7%9F%A5%E6%9C%BA/" class="post-title-link" itemprop="url">机器学习-感知机</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-01-02 20:34:43" itemprop="dateCreated datePublished" datetime="2022-01-02T20:34:43+08:00">2022-01-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-01-05 01:28:20" itemprop="dateModified" datetime="2022-01-05T01:28:20+08:00">2022-01-05</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="机器学习-感知机"><a href="#机器学习-感知机" class="headerlink" title="机器学习-感知机"></a>机器学习-感知机</h1><p>破防了，还有4天就要期末考了，现在才开始做机器学习的第一份笔记</p>
<h3 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h3><p><strong>其实感知机就是一个神经元</strong>。 它有如下几个特点</p>
<ul>
<li><p>输入为实例的特征向量，输出为实例的类别，取+1和-1</p>
</li>
<li><p>感知机对应于输入空间中将实例划分为正负两类的分离超平面，属于判别模型</p>
</li>
<li><p>导入基于误分类的损失函数</p>
</li>
<li><p>利用梯度下降法对损失函数进行极小化</p>
<ul>
<li>感知机学习算法具有简单而易于实现的优点，分为原始形式和对偶形式</li>
</ul>
</li>
</ul>
<p>接下来对感知机模型进行拆解：</p>
<p>第一个：输入部分</p>
<p>第二个：权重（就是w向量）；权重是在我们进行训练期间计算的值，初始呢我们是用一些初始值来可以进行随机的初始化，然后在学习过程当中对他们进行更新，最后我们模型学习完之后，这就是我们学习到的参数向量。我们用w来表示。</p>
<p>第三个：偏置项，这个偏置对应到上图在输入里边我们有一个常量1, 实际上这个常量1就相当于乘以1, 实际上就是, 后边的计算是和输入是无关的，相当于就是一个偏置，相当于i从1开始一直到n，还加上一个，这就是一个偏置。</p>
<p>由输入空间到输出空间的函数：</p>
<script type="math/tex; mode=display">
f(x) = sign(w\cdot x+b)</script><p>符号函数：</p>
<script type="math/tex; mode=display">
sign(x) = \begin{cases}+1&x\geq 0\\-1&x<0\end{cases}</script><h4 id="几何解释"><a href="#几何解释" class="headerlink" title="几何解释"></a>几何解释</h4><p>对于线性方程：</p>
<script type="math/tex; mode=display">
w\cdot x+b = 0</script><p>相当于在平面空间中的超平面S，w 是法向量，b是截距，用来分割开正负两种类别：</p>
<p><img src="/2022/01/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%84%9F%E7%9F%A5%E6%9C%BA/1.png"></p>
<h3 id="感知机学习策略"><a href="#感知机学习策略" class="headerlink" title="感知机学习策略"></a>感知机学习策略</h3><p>如何定义损失函数？</p>
<p>自然选择：误分类点的数目，但损失函数不是w,b连续可导，不宜优化。</p>
<p>另一选择：误分类点到超平面的总距离：</p>
<p>首先，距离函数：</p>
<script type="math/tex; mode=display">
d = \frac{|w\cdot x_0+b|}{||w||}</script><p>误分类点： $-y_i(w\cdot x_i+b)&gt;0$</p>
<p>误分类点距离：</p>
<script type="math/tex; mode=display">
-\frac{1}{||w||}\sum_{x_i\in M} y_i(w\cdot x_i+b)\\
w' = \frac{w}{||w||} ,b' = \frac{b}{||w||}</script><p>损失函数：</p>
<script type="math/tex; mode=display">
L(w,b) = -\sum_{x_{i}\in M} y_i(w\cdot x_i+b)</script><p>为了优化损失函数： </p>
<script type="math/tex; mode=display">
\min_{w,b}L(w,b) = -\sum_{x_i\in M}y_i(w\cdot x_i+b)</script><p>我们使用梯度下降法来更新损失函数。</p>
<h3 id="感知机学习算法"><a href="#感知机学习算法" class="headerlink" title="感知机学习算法"></a>感知机学习算法</h3><h4 id="原始形式"><a href="#原始形式" class="headerlink" title="原始形式"></a>原始形式</h4><p>输入：训练数据集 $T =\big{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \big}$, 期中 $x_i\in \mathcal X =R^n,y_i\in\mathcal y = {-1,+1}$ $i=1,2\cdots,N$ ， 学习率 $\eta(0&lt;\eta\leq1)$ </p>
<p>输出：w,b ： 感知机模型 $f(x) = sign(w\cdot x+b)$</p>
<ol>
<li><p>选取初值 $w_0,b_0$</p>
</li>
<li><p>在训练集中选取数据$(x_i,y_i)$</p>
</li>
<li><p>如果 $y_i(w\cdot x_i+b)\leq 0$, 说明分类错误:</p>
<script type="math/tex; mode=display">
\begin{align}
&w \leftarrow w+\eta ~y_ix_i\\
&b \leftarrow b+\eta y_i
\end{align}</script></li>
<li><p>转至2，直至训练集中没有误分类点</p>
</li>
</ol>
<p>比如说 正样本点是. $x_1=(3,3)^T,x_2=(4.3)^T$. 负样本点是 $x_3=(1,1)^T$. 试用感知机学习算法原始形式求感知机模型.</p>
<ol>
<li><p>取初值 $w_0 = 0,b_0= 0,\eta = 1$</p>
</li>
<li><p>对 $x_1 = (3,3)^T, y_1(w_0\cdot x_1+b_0)=0$ ，未能被正确分类，更新 $w,b$ </p>
</li>
</ol>
<script type="math/tex; mode=display">
\begin{align}
&w_1 = w_0+y_1x_1 = (3,3)^T\\
&b_1=b_0+y_1 = 1
\end{align}</script><p>​        得到新的线性模型： $w_1\cdot x+b_1 = 3x^{(1)}+3x^{(2)}+1$</p>
<ol>
<li>对于$x_1,x_2$显然模型大于0，被正确分类，对$x_3 = (1,1)^T$ 为负例 ，但计算得到$y_3(w_1\cdot x_3+b_1)&lt;0$ 被误分类, 更新$w,b$<script type="math/tex; mode=display">
\begin{align}
&w_2 = w_1+y_3x_3 = (2,2)^T\\
&b_2 = b_1+y_3 = 0
\end{align}</script></li>
</ol>
<p><img src="/2022/01/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%84%9F%E7%9F%A5%E6%9C%BA/2.png"></p>
<p>算法的收敛性：证明经过有限次迭代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型。</p>
<h4 id="对偶形式"><a href="#对偶形式" class="headerlink" title="对偶形式"></a>对偶形式</h4><p>感知机算法的对偶形式的基本想法是：</p>
<p>将w和b表示为实例$x_i$和标记 $y_i$ 的线性组会的形式，通过求解其系数而求得$w$和$b$</p>
<p>由于</p>
<script type="math/tex; mode=display">
\begin{align}
&w \leftarrow w+\eta ~y_ix_i\\
&b \leftarrow b+\eta y_i
\end{align}</script><p>最后学习到的$w,b$为：</p>
<script type="math/tex; mode=display">
\begin{align}
&w =\sum_{i=1}^N \alpha_i y_ix_i\\
&b =\sum_{i=1}^N \alpha_i y_i
\end{align}</script><p>输入：训练数据集 $T =\big{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \big}$, 期中 $x_i\in \mathcal X =R^n,y_i\in\mathcal y = {-1,+1}$ $i=1,2\cdots,N$ ， 学习率 $\eta(0&lt;\eta\leq1)$ </p>
<p>输出：感知机模型 $f(x) = sign\bigg(\sum_{j=1}^N\alpha_jy_jx_j\cdot x+b\bigg)$<br>其中 $\alpha = (\alpha_1,\cdots,\alpha_N)^T$ </p>
<ol>
<li><p>$\alpha\leftarrow 0,b\leftarrow 0,\eta\leftarrow 1$</p>
</li>
<li><p>在训练集中选取数据 $(x_i,y_i)$</p>
</li>
<li><p>如果  </p>
<script type="math/tex; mode=display">
\begin{align}
&y_i\bigg(\sum_{j=1}^N \alpha_jy_j(x_j\cdot x_i)+b \bigg)\leq 0\\
&\alpha_i\leftarrow \alpha_i+ \eta\\
&b\leftarrow b+\eta y_i
\end{align}</script></li>
<li><p>转至(2) 直到没有误分类数据</p>
</li>
</ol>
<p>比如说 正样本点是. $x_1=(3,3)^T,x_2=(4,3)^T$. $y_1=y_2=1$ ; 负样本点是 $x_3=(1,1)^T, y_3 = -1$ .试用感知机学习算法对偶形式求感知机模型.</p>
<ol>
<li><p>首先$\alpha_i$ 都是0，因为每个都没有误分，因此$b=\alpha_i\cdot y_i = 0$  所以这就是表2.2的第一列</p>
</li>
<li><p>接着，就是进入迭代过程，首先计算第一个点</p>
</li>
</ol>
<script type="math/tex; mode=display">
\begin{align}
&y_i(\sum\alpha_j y_j x_j\cdot x_i+b)\\
&=1*((0*1*(3,3)^T\cdot(3,3)+0*1*(4,3)^T\cdot(3,3)+0*-1*(1,1)^T\cdot(3,3))+0)\\
&=0
\end{align}</script><p>那么就出现误分类，则此时，更新$\boldsymbol \alpha,b$ ,而这就是表2.2的第二列</p>
<script type="math/tex; mode=display">
\alpha_1 = 0,\alpha_2=0,\alpha_3 = 0\\
b = 0+1\cdot1 = 1</script><ol>
<li>接着进行判断，第二个点，发现没有误分类，进行判断第三个点</li>
</ol>
<script type="math/tex; mode=display">
\begin{align}
&y_i(\sum\alpha_j y_j x_j\cdot x_i+b)\\
&=-1*((1*1*(3,3)^T\cdot(1,1)+0*1*(4,3)^T\cdot(1,1)+0*-1*(1,1)^T\cdot(1,1))+1)\\
&=-1*(((3,3)^T\cdot(1,1))+1)\\
&=-(6+1)\\
&=-7<0
\end{align}</script><p>因此该点为误分类点，此时，更新$\boldsymbol \alpha,b$ ,而这就是表2.2的第三列</p>
<script type="math/tex; mode=display">
\alpha_1 = 1,\alpha_2=0,\alpha_3 = 1\\
b = 1+1\cdot-1 = 0</script><p>以此类推</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>k</th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>$x_1$</td>
<td>$x_3$</td>
<td>$x_3$</td>
<td>$x_3$</td>
<td>$x_1$</td>
<td>$x_3$</td>
<td>$x_3$</td>
</tr>
<tr>
<td>$\alpha_1$</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>$\alpha_2$</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>$\alpha_3$</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>2</td>
<td>3</td>
<td>3</td>
<td>4</td>
<td>5</td>
</tr>
<tr>
<td>b</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>-1</td>
<td>-2</td>
<td>-1</td>
<td>-2</td>
<td>3</td>
</tr>
</tbody>
</table>
</div>
<h2 id="习题"><a href="#习题" class="headerlink" title="习题"></a>习题</h2><p>感知机因为是线性模型，所以不能表示复杂的函数，如异或(XOR) 。验证感知机为什么不能表示异或。</p>
<p>直观上，我们设正方形为True, 三角形为False</p>
<p><img src="/2022/01/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%84%9F%E7%9F%A5%E6%9C%BA/3.jpeg" style="zoom: 30%;"></p>
<p>那么，我们是没有办法找到一条直线将其分离的，如上图</p>
<p>在数学形式上：</p>
<script type="math/tex; mode=display">
\text{设}~~sign(x) = \begin{cases}1&x\geq 0\\-1&x<0 \end{cases}</script><p>我们令 $f(x) = sign(w\cdot x+b)$ $w$为权重$(w_1,w_2)$  ，b为偏置</p>
<p>设向量$\boldsymbol x = (x_1,x_2)$ ，用其表示异或问题</p>
<ul>
<li>若 $x_1=x_2=0,f(x)=-1$ 则有 $b&lt;0$</li>
<li>若 $x_1=0,x_2=1,f(x)=1$ 则有 $w_1\cdot 0+w_2\cdot1 +b &gt;0$ . 因此 $w_2&gt;-b&gt;0$</li>
<li>若 $x_1=01,x_2=0,f(x)=1$ 则有 $w_1\cdot 1+w_2\cdot0 +b &gt;0$ . 因此 $w_1&gt;-b&gt;0$</li>
<li>若 $x_1=x_2=1,f(x)=-1$ ，当前由上面三个式子，有 $w_2&gt;-b&gt;0,w_1&gt;-b&gt;0$ , 所以应该是 $w_1+w_2-(-b)&gt;0$ 。 但是这和 $f(x)=-1$   矛盾</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2022/01/01/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E7%AE%97%E6%B3%95-%E6%B5%81%E7%AE%97%E6%B3%95%E8%A1%A5%E5%85%85/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/01/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E7%AE%97%E6%B3%95-%E6%B5%81%E7%AE%97%E6%B3%95%E8%A1%A5%E5%85%85/" class="post-title-link" itemprop="url">数据科学与算法-流算法补充</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-01-01 10:59:07" itemprop="dateCreated datePublished" datetime="2022-01-01T10:59:07+08:00">2022-01-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-01-07 11:51:38" itemprop="dateModified" datetime="2022-01-07T11:51:38+08:00">2022-01-07</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="流算法补充"><a href="#流算法补充" class="headerlink" title="流算法补充"></a>流算法补充</h1><h2 id="Queries-over-a-sliding-window"><a href="#Queries-over-a-sliding-window" class="headerlink" title="Queries over a sliding window"></a>Queries over a sliding window</h2><p>在第五章流算法中，我们讲了一些数据挖掘算法：Basic Count Sketch，Count Sketch 和 Count-min Sketch。当时我们提了一嘴滑动窗口模型，但没有细讲。现在我们来对这个数据流模型做一个补充</p>
<p>比如说举一个Amazon的例子，数据流是亚马逊上的订单，数据流的值是0-1，其中，0代表数据流不包含某个商品，1代表数据流包含某个商品，我们希望统计在过去的n笔订单中，有多少订单包含商品 X(统计1的个数)</p>
<p><img src="/2022/01/01/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E7%AE%97%E6%B3%95-%E6%B5%81%E7%AE%97%E6%B3%95%E8%A1%A5%E5%85%85/1.png" style="zoom:67%;"></p>
<h3 id="朴素方法"><a href="#朴素方法" class="headerlink" title="朴素方法"></a>朴素方法</h3><p>要计算数据流中过去 k 个bits 有多少个1(k&lt;窗口大小N)，一个朴素的想法是：<strong>维持一个计数器</strong></p>
<p>每当有一个新的bit进来，就需要抛弃一个bit</p>
<ul>
<li>如果进来或抛弃的bit 是 0，那么什么都不用做</li>
<li>如果进来的bit 是1，那么计数器+1</li>
<li>如果进来的bit 是0，那么计数器-1</li>
</ul>
<p>这个算法的空间复杂度是$O(n)$,因为我们要时刻保持窗口中的数据；时间复杂度为$(O1)$ </p>
<p>那么，如果N 等于 1 billion 会怎么样？我们肯定不能将其全部保存下来，因此我们可以采用一个近似算法。</p>
<h4 id="尝试1"><a href="#尝试1" class="headerlink" title="尝试1"></a>尝试1</h4><p>我们尝试维护两个计数器：</p>
<p>S: 维护数据流中的1的个数</p>
<p>Z: 维护数据流中的0的个数</p>
<p>因此我们可以用这两个计数器的值，结合窗口大小去估算窗口内1的数值:$ = N\cdot \frac{S}{S+Z} $</p>
<p>但是这样又不对，因为采用这种方法，随便你窗口化到哪，对1的个数的估算都是不会变的。也就是说，加入之前数据流 1和0的比值为0.8：0.2，那么相当于假设之后来的数字也是0.8:0.2，但现实生活中事件发生的概率是不停变化的，因此是不可行的。</p>
<h3 id="DGIM-算法"><a href="#DGIM-算法" class="headerlink" title="DGIM 算法"></a>DGIM 算法</h3><p>因此我们提出了一种更高效的解决方法：DGIM算法</p>
<p>在DGIM算法中，我们对于一个长度为N的数据流，我们存储 $O(\log^2 N)$ 个 bits, $O(\log_2(N))$个桶 ，相当于一个亚线性的算法，而且我们要得目标的估计偏差不能超过 50%，也就是说，假设1的出现次数是100，那么我们的估计必须落在 50~150之间</p>
<p>首先假设对于二进制流，其中每个位可以用一个时间戳（timestamp）标志该位进入流的时间（对于大小为N的滑动窗口，该时间戳可以用O(logN)位表示）。</p>
<p>DGIM算法利用桶（bucket）对滑动窗口进行划分，每个桶保存以下信息：</p>
<ul>
<li>桶的最右边位即<strong>最后进入流的位的时间戳</strong>；</li>
<li><p>桶的大小，即桶中1的个数，该数目位2的幂。 </p>
<p>对于以上信息，保存时间戳需要logN的空间，保存桶的大小需要 logN 的空间。</p>
</li>
</ul>
<p>使用桶对滑动窗口进行划分时遵循以下5条规则：</p>
<ul>
<li>桶最右边的位必须是1；</li>
<li>1个位最多属于1个桶；</li>
<li>每种大小的桶有1个或者两个（从1到最大的桶的大小）；</li>
<li>每个桶的大小是2的幂；</li>
<li>桶的大小从右到左非降序排列；</li>
</ul>
<p>DGIM算法中数据结构的更新：</p>
<ul>
<li>每一个新的位进入滑动窗口后，最左边一个位从窗口中移出（同时从桶中移出）；如果最左边的桶的时间戳是当前时间戳减去N（也就是说桶里已经没有处于窗口中的位），则放弃这个桶；</li>
<li>对于新加入的位，如果其为0，则无操作；否则建立一个包含新加入位的大小为1的桶；</li>
<li>由于新增一个大小为1的桶而出现3个桶大小为1，则合并最左边的两个桶为一个大小为2的桶；合并之后可能出现3个大小为2的桶，则合并最左边两个大小为2的桶得到一个大小为4的桶……依次类推直到到达最左边的桶。</li>
</ul>
<p>我们用一个直观的例子来展现：</p>
<ul>
<li>这是我们的初始数据流：</li>
</ul>
<p><img src="/2022/01/01/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E7%AE%97%E6%B3%95-%E6%B5%81%E7%AE%97%E6%B3%95%E8%A1%A5%E5%85%85/2.png"></p>
<p>我们看到现在一共有长度为1的桶2个；长度为2的桶1个；长度为4的桶2个；长度为8的桶2个; 长度为16的桶1个，但是一部分在滑动窗口外，因此显示不出</p>
<ul>
<li>当到达了一个新的 1 ，如下图</li>
</ul>
<p><img src="/2022/01/01/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E7%AE%97%E6%B3%95-%E6%B5%81%E7%AE%97%E6%B3%95%E8%A1%A5%E5%85%85/3.png"></p>
<p>我们发现现在长度为1的桶有3个，超出了2个，因此要向上合并</p>
<p><img src="/2022/01/01/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E7%AE%97%E6%B3%95-%E6%B5%81%E7%AE%97%E6%B3%95%E8%A1%A5%E5%85%85/4.png"></p>
<ul>
<li>当到达了三个数据 101</li>
</ul>
<p><img src="/2022/01/01/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E7%AE%97%E6%B3%95-%E6%B5%81%E7%AE%97%E6%B3%95%E8%A1%A5%E5%85%85/5.png"></p>
<p>这时候，我们发现，合并了两个长度为1的桶，有3个长度为2的桶；合并了两个长度为2的桶，就有3个长度为4的桶；合并了两个长度为4的桶，有三个长度为8的桶。</p>
<p>最终，我们得到下图：</p>
<p><img src="/2022/01/01/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E7%AE%97%E6%B3%95-%E6%B5%81%E7%AE%97%E6%B3%95%E8%A1%A5%E5%85%85/7.png"></p>
<h4 id="估计"><a href="#估计" class="headerlink" title="估计"></a>估计</h4><p>最后，我们该怎么利用计数桶去怎么去估计1的个数？</p>
<ol>
<li>我们可以叠加滑动窗口中的桶的大小，除了最后一个桶，因为最后一个桶有一部分已经滑出窗口外了</li>
<li>因此，我们可以近似地认为最后这个桶(也是最大的桶)的大小变为原来的$\frac{1}{2}$ ,并加到结果中去</li>
</ol>
<p>用上面那个例子来估计，滑动窗口中，对1的个数的估计为：1+2+4+8+16+16/2 = 39个</p>
<p>因为一共有 $O(logn)$ 个桶，因此算法的时间复杂度也是 $O(\log n)$</p>
<h4 id="精度证明"><a href="#精度证明" class="headerlink" title="精度证明"></a>精度证明</h4><p>那么为什么DIGM算法，它的估算误差一定是小于50%的呢？</p>
<p>我们假设最后的桶的大小是 $2^r$ ，那么我们在加完前面的桶之后，最后要加上的size = $2^{r-1}$</p>
<p>如果这个桶没滑出去，刚好在窗口末尾停下了，那么我们就相当于少了了 $2^{r-1}$  </p>
<p>如果这个桶基本全滑出去了，在窗口内可能只留了1个数字，那么我们就相当于多了$2^{r-1}$  </p>
<p>但是，我们把之前的窗口全加起来： $1+2+4\cdots+2^{r-1} = 2^{r}-1$ ，他肯定是大于 等于$2^{r-1}$ 的，因此这个误差$2^{r-1}$ 肯定是小于50%的。</p>
<h4 id="Extension"><a href="#Extension" class="headerlink" title="Extension"></a>Extension</h4><p>现在，给出一个可以进一步削弱误差的方法。</p>
<p>除了第一个和最后一个桶之外，其余的桶，可以将数量维持在：r-1 个或者 r 个, (r &gt; 2)</p>
<p>这时候，这个算法的空间复杂度会变成 $O(r\log n)$ 但是误差率会从 50% 变成 $O(\frac{1}{r})$</p>
<p>此外，除了用来技术0.1，还可以用来维持一个平均值：</p>
<p><img src="/2022/01/01/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E7%AE%97%E6%B3%95-%E6%B5%81%E7%AE%97%E6%B3%95%E8%A1%A5%E5%85%85/8.png"></p>
<p>但是，这个算法只能进行一些简单的功能计算，要求桶内的值必须有可加性，因此推广性比较差</p>
<h2 id="Counting-Distinct-Elements"><a href="#Counting-Distinct-Elements" class="headerlink" title="Counting Distinct Elements"></a>Counting Distinct Elements</h2><p>给定一个数据流，我们来统计这个数据流中一共出现过多少不同的元素。</p>
<p>比如说，现在有一个集合，然后过来一个数据流，我们要统计多少这个集合的元素在这个数据流中出现了，多少没有出现。</p>
<p>还有其他应用场景：</p>
<ul>
<li>一个网页中出现了多少个不一样的词，词典是固定的，但词的分布是不一样的</li>
<li>一周之内，用户总共访问了多少个不同的网站</li>
</ul>
<p>一个很简单的方法就是：维护一个哈希表，然后过来一个数据我们映射一个数据，最后看哈希表中有哪些数据。但问题也是类似的，哈希表的大小事$O(n)$ 级别的，当数据流非常大的时候，这个哈希表是很难维护的。</p>
<p>因此，我们要考虑一种新的数据流算法</p>
<h3 id="Flajolet-Martin-Approach"><a href="#Flajolet-Martin-Approach" class="headerlink" title="Flajolet-Martin Approach"></a>Flajolet-Martin Approach</h3><p>FM 算法设计了一种空间复杂度为$O(\log_2(N))$  的算法</p>
<p>在这个算法中，对于长度为N的数据流，我们将其映射到 长度为$\log_2(N)$ 的哈希表中。</p>
<p>举例来说，给定序列{e1, e2, e3, e2}，独立元素数目N = 3。这里哈希数组的长度为$\log_2(4) = 2$ </p>
<p>假设给定哈希函数H(e)，有：</p>
<p>H(e1) = 00，TailZero(H(e1)) = 0</p>
<p>H(e2) = 01，TailZero(H(e2)) = 0</p>
<p>H(e3) = 10 ，TailZero(H(e3)) =1</p>
<p>第1步，将Max初始化为0；</p>
<p>第2步，对于序列中第1项e1，计算TailZero(H(e1)) = 0 </p>
<p>第3步，对于序列中第2项e2，计算TailZero(H(e2)) = 0  </p>
<p>第4步，对于序列中第3项e3，计算TailZero(H(e3)) = 1&gt; Max, 更新Max；</p>
<p>第5步，对于序列中第4项e2，计算TailZero(H(e2)) =0</p>
<p>第6步，估计独立元素数目为 $\hat N = 2^{\max} = 2^1 = 2$。</p>
<p>在这个简单例子中，实际值 $N = 3$，估计值$ \hat N =2$ </p>
<p>在实际应用中，为了减小误差，提高精度，我们通常采用一系列的哈希函数$H_1(e), H_2(e), H_3(e)$，计算一系列的Max值$\max_1,\max_2,\max_3$，从而估算一系列的估计值$2^{\max_1}, 2^{\max_2}, 2^{\max_3}$，最后进行综合得到最终的估计值。具体做法是：首先设计A*B个互不相同的哈希函数，分成A组，每组B个哈希函数；然后利用每组中的B个哈希函数计算出B个估计值;接着求出B个估计值的算术平均数为该组的估计值；最后选取各组的估计值的中位数作为最终的估计值。</p>
<p>举例来说，对于序列S，使用3*4 = 12个互不相同的哈希函数H(e)，分成3组，每组4个哈希函数，使用12个H(e)估算出12个估计值： </p>
<h2 id="Computing-Moments"><a href="#Computing-Moments" class="headerlink" title="Computing Moments"></a>Computing Moments</h2><p>首先给出Moments 的概念</p>
<p>假设数据流是由一些数字构成的，$m_i$ 代表元素i 在流中出现的次数，那么</p>
<p>$k^{th}$ Moment 就是：</p>
<script type="math/tex; mode=display">
\sum_{i\in A} (m_i)^k</script><p>刚才我们做的 FM 算法，就是$k^0$ moment</p>
<p>$k^0$ moment:  不同元素的数量</p>
<p>$k^1$ moment: 元素数量的计算，也就是数据流的长度N</p>
<p>$k^2$ moment = suprise number S : 衡量分布的不均匀程度</p>
<p>这里重点考虑 $k^2$ moment</p>
<p>比如说数据流A、B的长度为100，一共出现了11个不同的元素</p>
<p>数据流A各个元素的计数：10,9,9,9 ,9,9,9, 9,9,9,9 那么  $k^2 $ moment = 910</p>
<p>数据流B各个元素的计数：90,1,1,1,1,1,1,1,1,1,1 那么  $k^2 $ moment = 8110</p>
<h3 id="AMS-Method"><a href="#AMS-Method" class="headerlink" title="AMS Method"></a>AMS Method</h3><p>AMS 方法可以对所有moment的值进行一个无偏估计，这里就只针对 $k^2$ moment</p>
<p>我们对于一个变量 $X$</p>
<ul>
<li>用 $X.{el}$ </li>
<li>用 $X.val$ </li>
</ul>
<h2 id="Counting-Itemsets"><a href="#Counting-Itemsets" class="headerlink" title="Counting Itemsets"></a>Counting Itemsets</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2022/01/01/OLAP/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/01/OLAP/" class="post-title-link" itemprop="url">OLAP</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-01-01 10:48:36" itemprop="dateCreated datePublished" datetime="2022-01-01T10:48:36+08:00">2022-01-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-01-02 16:17:28" itemprop="dateModified" datetime="2022-01-02T16:17:28+08:00">2022-01-02</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="OLAP"><a href="#OLAP" class="headerlink" title="OLAP"></a>OLAP</h1><p>参考 ：<a href="https://www.zhihu.com/question/24110442/answer/851671343" target="_blank" rel="noopener">https://www.zhihu.com/question/24110442/answer/851671343</a></p>
<p>OLTP（on-line transaction processing）翻译为联机事务处理， OLAP（On-Line Analytical Processing）翻译为联机分析处理，从字面上来看OLTP是做事务处理，OLAP是做分析处理。从对数据库操作来看，OLTP主要是对数据的增删改，OLAP是对数据的查询。 </p>
<p>这里我们在多介绍一下OLAP</p>
<h2 id="数据仓库系统"><a href="#数据仓库系统" class="headerlink" title="数据仓库系统"></a>数据仓库系统</h2><p><strong>OLAP分析的分类：ROLAP与MOLAP</strong><br>OLAP分析 分为关系型联机分析处理（<strong>ROLAP</strong>）、多维联机分析处理（<strong>MOLAP</strong>）两种，他们的设计理念以及解决场景不一样，各有优劣。</p>
<h3 id="ROLAP"><a href="#ROLAP" class="headerlink" title="ROLAP"></a>ROLAP</h3><p>以ROLAP为代表的有<strong>传统关系型数据库</strong>、<strong>MPP分布式数据库</strong>以及<strong>基于Hadoop的Spark/Impala</strong>，它们是将数据块存储在关系型数据库当中的。</p>
<ul>
<li>优点<ul>
<li>是能<strong>同时连接明细数据和汇总数据</strong>，实时根据用户提出的需求对数据进行计算后返回给用户，所以用户使用相对比较灵活，可以随意选择维度组合来进行实时计算。</li>
<li>对高维数据、超大数据集有很好的扩展性</li>
<li>技术成熟</li>
</ul>
</li>
<li>正因为采用的实时计算技术，所以ROLAP的缺点也比较明显<ul>
<li>当计算的数据量达到一定级别或并发数达到一定级别的时候，<strong>一定会出现性能问题</strong>(就好比如果领导一次性给你安排非常多的工作，你一个人是无法马上将所有事情做完答复领导的)。</li>
<li>需要构建明确的索引</li>
</ul>
</li>
</ul>
<p>以传统关系型数据库为代表的如Teradata、Oracle等，由于传统架构可扩展性较差，所以<strong>对硬件的要求非常高</strong>，当计算的数据量达到千万，亿级别时，数据库的计算就会出现延时，使得用户不能及时得到响应，更别提高并发了。</p>
<p>MPP 分布式数据库则解决了一部分可扩展性问题，对硬件设备的要求也稍稍下降了（还是有一定的硬件要求)，在支持的数据体量（GB，TB级别）上有了很大的提升。当集群有几百、上千节点时，会出现性能瓶颈(增加再多节点，性能提升也不会很明显)，扩容成本同样不菲。</p>
<p>基于Hadoop的Spark/Impala，则对部署硬件的要求很低(常见服务器即可，只是其主要依靠内存计算来缩短响应时间，所以对内存要求较高)，在节点扩容上成本上相对较低，但当计算量达到一定级别或并发达到一定级别后，<strong>无法秒级响应</strong>，且<strong>容易出现内存溢出</strong>等问题。</p>
<h3 id="MOLAP"><a href="#MOLAP" class="headerlink" title="MOLAP"></a>MOLAP</h3><p>以MOLAP分析为代表的有Cognos，SSAS，Kylin等，设计理念<strong>是预先将客户的需求计算好以结果的形式存下来</strong>（比如一张表分为10个维度，5个度量，那客户提出的需求会有2的10次方种可能，然后将这么多种可能提前计算好存储下来)，<strong>当客户提出需求后，找到对应结果返回即可</strong>（好比你提前一天将领导明天会布置的任务先做好，明天领导布置对应任务后你直接告知他已做好）</p>
<ul>
<li>优点是当命中需求后返回非常快（所以MOLAP非常适合常见固定的分析场景），同等资源下支持的数据体量更大，支持的并发更多</li>
<li>缺点则是当表的维度越多，越复杂，其所需的磁盘存储空间则越大，构建cube也需要一定的时间。</li>
</ul>
<p>Cognos和SSAS是早期比较传统的产品，Cognos限制了Cube的大小(即限制了表的复杂度大小)，而SSAS的cube则受限于单机的容量，即需要专用的服务器来进行存储。</p>
<p>Apache Kylin则是目前技术较为先进的一款成熟产品，也是第一个由中国人贡献给Apache社区的顶级开源项目，它基于hadoop框架，Cube以分片的形式存储在不同节点上，Cube大小不受服务器配置限制，所以具备很好的可扩展性和对服务器要求很低，在扩容成本上就非常低廉。另外为了控制整体Cube的大小，Kylin给客户提供了建模的能力，即用户可以根据自身需要，对模型种的维度以及维度组合进行预先的构建，把一些不需要的维度和组合筛选掉，从而达到降低维度的目的，减少磁盘空间的占用。</p>
<p>Kylin的企业版产品，即Kyligence的产品，除了在性能、功能上做了很多优化之外，稳定性上也做了很大提升，还提供了智能建模功能，在满足用户需求的前提下，很大程度上减小了磁盘空间的浪费。</p>
<p>综上而言</p>
<ul>
<li><p>从可扩展性上看：Kylin=Impala/Spark&gt;MPP数据库&gt;传统数据库</p>
</li>
<li><p>从对硬件要求上看，传统数据库&gt;MPP数据库&gt;Impala/Spark&gt;=Kylin；</p>
</li>
<li>从响应效率上来看，不同的数据量、并发数，响应效率差别不一，但可以确定的是，要计算的数据量越大，并发的用户数越多，同等资源情况下预计算的响应效率会越发明显。</li>
</ul>
<h2 id="数据仓库模型"><a href="#数据仓库模型" class="headerlink" title="数据仓库模型"></a>数据仓库模型</h2><h3 id="Star-schema"><a href="#Star-schema" class="headerlink" title="Star schema"></a>Star schema</h3><p>在星型模型当中，<strong>一张事实表被若干张维度表所包围</strong>。每一个维度代表了一张表，有主键关联事实表当中的外键。</p>
<ul>
<li>所有的事实都必须保持同一个粒度</li>
<li>不同的维度之间没有任何关联</li>
</ul>
<p><img src="/2022/01/01/OLAP/1.png"></p>
<h3 id="雪花模型"><a href="#雪花模型" class="headerlink" title="雪花模型"></a>雪花模型</h3><p> 雪花模型是在基于星型模型之上拓展来的，<strong>每一个维度可以再扩散出更多的维度</strong>，根据维度的层级拆分成颗粒度不同的多张表。</p>
<ul>
<li>优点是减少维度表的数据量，在进行join查询时有效提升查询速度</li>
<li>缺点是需要额外维护维度表的数量</li>
</ul>
<p><img src="/2022/01/01/OLAP/2.png"></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>在规范化方面：</p>
<ul>
<li><strong>雪花模型</strong>比较符合数据库范式的理念设计方式比较正规，数据冗余少</li>
<li>非规范化的数据模型可能会违反完整性和一致性</li>
</ul>
<p>在查询复杂度方面：</p>
<ul>
<li><strong>雪花模型</strong>在查询的时候可能需要join多张表从而导致查询效率下降，此外规范化操作在后期维护比较复杂。</li>
<li><strong>星型模型</strong>能够提升查询效率，因为生成的事实表已经经过预处理，主要的数据都在事实表里面，所以只要扫描实时表就能够进行大量的查询，而不必进行大量的join。而且维表数据一般比较少，可直接放入内存进行join以提升效率，</li>
</ul>
<p>在可读性方面：</p>
<ul>
<li><strong>星型模型</strong>的事实表可读性比较好，不用关联多个表就能获取大部分核心信息，设计维护相对比较简答。</li>
</ul>
<p>数据仓库大多数时候是比较适合<strong>使用星型模型构建底层数据Hive表</strong>，通过大量的冗余来提升查询效率，星型模型对OLAP的分析引擎支持比较友好，这一点在Kylin中比较能体现。</p>
<p>而雪花模型在关系型数据库中如MySQL，Oracle中非常常见，尤其像电商的数据库表。在数据仓库中雪花模型的应用场景比较少，但也不是没有，所以在具体设计的时候，可以考虑是不是能结合两者的优点参与设计，以此达到设计的最优化目的。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>星型模型</th>
<th>雪花模型</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据总量</td>
<td>多</td>
<td>少</td>
</tr>
<tr>
<td>可读性</td>
<td>容易</td>
<td>差</td>
</tr>
<tr>
<td>表个数</td>
<td>少</td>
<td>多</td>
</tr>
<tr>
<td>查询速度</td>
<td>快</td>
<td>慢</td>
</tr>
<tr>
<td>冗余度</td>
<td>高</td>
<td>低</td>
</tr>
<tr>
<td>对实时表的情况</td>
<td>增加宽度</td>
<td>字段比较少，冗余低</td>
</tr>
<tr>
<td>扩展性</td>
<td>差</td>
<td>好</td>
</tr>
</tbody>
</table>
</div>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2022/01/01/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%89%A9%E5%B1%95%E6%80%A7%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/01/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%89%A9%E5%B1%95%E6%80%A7%E9%97%AE%E9%A2%98/" class="post-title-link" itemprop="url">数据库扩展性问题</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-01-01 10:48:27" itemprop="dateCreated datePublished" datetime="2022-01-01T10:48:27+08:00">2022-01-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-06-24 10:00:46" itemprop="dateModified" datetime="2022-06-24T10:00:46+08:00">2022-06-24</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="数据库扩展性问题"><a href="#数据库扩展性问题" class="headerlink" title="数据库扩展性问题"></a>数据库扩展性问题</h1><h2 id="NoSQL"><a href="#NoSQL" class="headerlink" title="NoSQL"></a>NoSQL</h2><p>NoSQL数据库有四大分类：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>分类</th>
<th>Examples</th>
<th>典型应用场景</th>
<th>数据模型</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>键值 key-value</td>
<td>Riak,Redis,Voldmort</td>
<td>内容缓存，用于处理大量数据的高访问负载，也用于一些日志系统等</td>
<td>Key指向Value的键值对，通常用哈希表实现</td>
<td>查找速度快</td>
<td>数据无结构化，通常只被当做字符串或者二进制数据</td>
</tr>
<tr>
<td>列存储数据库 wide-column</td>
<td>Cassandra,HBase</td>
<td>分布式的文件系统</td>
<td>以列簇式存储，将同一列数据存在一起</td>
<td>查找速度快，可扩展性强，更容易进行分布式扩展</td>
<td>功能相对局限</td>
</tr>
<tr>
<td>文档型数据 document</td>
<td>MongoDB, CouchDB</td>
<td>web应用</td>
<td>Key-Value对应的键值对，Value为结构化数据</td>
<td>数据结构要求不严格，表结构可变，不需要像关系型数据库一样，需要预先定义表结构</td>
<td>查询性能不高，而且缺乏统一查询语法</td>
</tr>
<tr>
<td>图形数据库 Graph</td>
<td>Neo4J,HyperGraphDB</td>
<td>社交网络，推荐系统。专注于构建关系图谱</td>
<td>图结构</td>
<td>利用图结构相关算法</td>
<td>很多时候需要对整个图做计算才能得出需要的信息。不太好做分布式扩展</td>
</tr>
</tbody>
</table>
</div>
<h3 id="NoSQL和SQL不同的开发过程"><a href="#NoSQL和SQL不同的开发过程" class="headerlink" title="NoSQL和SQL不同的开发过程"></a>NoSQL和SQL不同的开发过程</h3><ul>
<li>传统的SQL数据库设计流程</li>
</ul>
<p>概念模型(Conceptual Model) ——&gt; 模式(Schema) ——&gt; 物理设计优化(Physical Design) ——&gt; 分库分表(Sharding)</p>
<ul>
<li>NoSQL的数据库设计流程</li>
</ul>
<p>应用功能 (App Operation) ——&gt; 模式(Schema) ——&gt; 横向扩展(Scaling)</p>
<h3 id="NOSQL的优势"><a href="#NOSQL的优势" class="headerlink" title="NOSQL的优势"></a>NOSQL的优势</h3><ul>
<li><p><strong>易扩展</strong><br>NoSQL数据库种类繁多，但是一个共同的特点都是去掉关系数据库的关系型特性。数据之间无关系，这样就非常容易扩展。也无形之间，在架构的层面上带来了可扩展的能力。</p>
</li>
<li><p><strong>大数据量，高性能</strong><br>NoSQL数据库都具有非常高的读写性能，尤其在大数据量下，同样表现优秀。这得益于它的无关系性，数据库的结构简单。一般MySQL使用Query Cache，每次表的更新Cache就失效，是一种大粒度的Cache，在针对web2.0的交互频繁的应用，Cache性能不高。而NoSQL的Cache是记录级的，是一种细粒度的Cache，所以NoSQL在这个层面上来说就要性能高很多了。</p>
</li>
<li><p><strong>灵活的数据模型</strong><br>NoSQL无需事先为要存储的数据建立字段，随时可以存储自定义的数据格式。而在关系数据库里，增删字段是一件非常麻烦的事情。如果是非常大数据量的表，增加字段简直就是一个噩梦。</p>
</li>
<li><p><strong>高可用</strong><br>NoSQL在不太影响性能的情况，就可以方便的实现高可用的架构。比如Cassandra，HBase模型，通过复制模型也能实现高可用。</p>
</li>
</ul>
<h3 id="SQL-的劣势"><a href="#SQL-的劣势" class="headerlink" title="SQL 的劣势"></a>SQL 的劣势</h3><ul>
<li><p><strong>大数据场景下I/O较高</strong></p>
<p>因为数据是按行存储，即使只针对其中某一列进行运算，关系型数据库也会将整行数据从存储设备中读入内存，导致I/O较高</p>
</li>
<li><p><strong>存储的是行记录，无法存储数据结构</strong> </p>
</li>
<li><p><strong>表结构schema扩展不方便</strong><br> 如要需要修改表结构，需要执行执行DDL(data definition language)，语句修改，修改期间会导致锁表，部分服务不可用</p>
</li>
<li><p><strong>全文搜索功能较弱</strong><br> 关系型数据库下只能够进行子字符串的匹配查询，当表的数据逐渐变大的时候，like查询的匹配会非常慢，即使在有索引的情况下。况且关系型数据库也不应该对文本字段进行索引</p>
</li>
<li><p><strong>存储和处理复杂关系型数据功能较弱</strong><br> 许多应用程序需要了解和导航高度连接数据之间的关系，才能启用社交应用程序、推荐引擎、欺诈检测、知识图谱、生命科学和 IT/网络等用例。然而传统的关系数据库并不善于处理数据点之间的关系。它们的表格数据模型和严格的模式使它们很难添加新的或不同种类的关联信息。</p>
</li>
</ul>
<h2 id="CAP"><a href="#CAP" class="headerlink" title="CAP"></a>CAP</h2><p>CAP理论非常重要，在分布式数据库的设计中起了很关键的理论支持(包括区块链)。我们先对CAP进行定义：</p>
<ul>
<li>C（一致性 Consistency）：所有节点访问同一份最新的数据副本。(副本既可以是备份数据，也可以是冗余数据，比如索引)</li>
<li>A（可用性 Availability）：每次请求都能获取到非错的响应——但是不保证获取的数据是否为最新的数据</li>
<li>P（分区容错 Partitioning Tolerance）：通信故障的时候，系统的任意节点都可以正常工作。(以实际效果而言，分区就相当于对通信的时限要求，系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择)</li>
</ul>
<p>任何一个分布式数据库只能在C、A和P三者中兼顾两个</p>
<h3 id="一致性"><a href="#一致性" class="headerlink" title="一致性"></a>一致性</h3><p><strong>这里指的是强一致性，而最终一致性后续讨论</strong><br>在写操作完成后开始的任何读操作都必须返回该值，或者后续写操作的结果<br>也就是说，在一致性系统中，一旦客户端将值写入任何一台服务器并获得响应，那么之后client从其他任何服务器读取的都是刚写入的数据</p>
<p><strong>用如下系统进行解释</strong></p>
<p><img src="/2022/01/01/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%89%A9%E5%B1%95%E6%80%A7%E9%97%AE%E9%A2%98/1.png"></p>
<ol>
<li>客户端向G1写入数据v1，并等待响应</li>
<li>此时，G1服务器的数据为v1，而G2服务器的数据为v0，两者不一致</li>
<li>接着，在返回响应给客户端之前，G2服务器会自动同步G1服务器的数据，使得G2服务器的数据也是v1</li>
<li>一致性保证了不管向哪台服务器（比如这边向G1）写入数据，其他的服务器（G2）能实时同步数据</li>
<li>G2已经同步了G1的数据，会告诉G1，我已经同步了</li>
<li>G1接收了所有同步服务器的已同步的报告，才将“写入成功”信息响应给client</li>
<li>client再发起请求，读取G2的数据</li>
<li>此时得到的响应是v1，即使client从未写入数据到G2</li>
</ol>
<h3 id="可用性"><a href="#可用性" class="headerlink" title="可用性"></a>可用性</h3><p>系统中非故障节点收到的每个请求都必须有响应<br>在可用系统中，如果我们的客户端向服务器发送请求，并且服务器未崩溃，则服务器必须最终响应客户端，不允许服务器忽略客户的请求</p>
<h3 id="分区容错性"><a href="#分区容错性" class="headerlink" title="分区容错性"></a>分区容错性</h3><p><strong>允许网络丢失从一个节点发送到另一个节点的任意多条消息，即不同步 </strong><br>也就是说，G1和G2发送给对方的任何消息都是可以放弃的，也就是说G1和G2可能因为各种意外情况，导致无法成功进行同步，分布式系统要能容忍这种情况。</p>
<p><img src="/2022/01/01/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%89%A9%E5%B1%95%E6%80%A7%E9%97%AE%E9%A2%98/2.png"></p>
<h3 id="CAP三者不可能同时满足"><a href="#CAP三者不可能同时满足" class="headerlink" title="CAP三者不可能同时满足"></a>CAP三者不可能同时满足</h3><p>假设确实存在三者能同时满足的系统</p>
<ul>
<li>那么我们要做的第一件事就是分区我们的系统，由于满足分区容错性，也就是说可能因为通信不佳等情况，G1和G2之间是没有同步</li>
</ul>
<p><img src="/2022/01/01/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%89%A9%E5%B1%95%E6%80%A7%E9%97%AE%E9%A2%98/3.png"></p>
<ul>
<li>接下来，我们的客户端将v1写入G1，但G1和G2之间是不同步的，所以如下G1是v1数据，G2是v0数据。</li>
</ul>
<p><img src="/2022/01/01/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%89%A9%E5%B1%95%E6%80%A7%E9%97%AE%E9%A2%98/4.png"></p>
<ul>
<li>由于要满足可用性，即一定要返回数据，所以G1必须在数据没有同步给G2的前提下返回数据给client，如下</li>
</ul>
<p><img src="/2022/01/01/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%89%A9%E5%B1%95%E6%80%A7%E9%97%AE%E9%A2%98/5.png"></p>
<ul>
<li>接下去，client请求的是G2服务器，由于G2服务器的数据是v0，所以client得到的数据是v0</li>
</ul>
<p><img src="/2022/01/01/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%89%A9%E5%B1%95%E6%80%A7%E9%97%AE%E9%A2%98/6.png"></p>
<p>很明显，G1返回的是v1数据，G2返回的是v0数据，两者不一致。<br>其余情况也有类似推导，也就是说CAP三者不能同时出现。</p>
<h3 id="CAP三者如何权衡"><a href="#CAP三者如何权衡" class="headerlink" title="CAP三者如何权衡"></a>CAP三者如何权衡</h3><h4 id="三选二利弊如何"><a href="#三选二利弊如何" class="headerlink" title="三选二利弊如何"></a>三选二利弊如何</h4><ul>
<li>CA (Consistency + Availability)：关注一致性和可用性，它需要非常严格的全体一致的协议，比如“两阶段提交”（2PC）。CA 系统不能容忍网络错误或节点错误，一旦出现这样的问题，整个系统就会拒绝写请求，因为它并不知道对面的那个结点是否挂掉了，还是只是网络问题。唯一安全的做法就是把自己变成只读的。<ul>
<li>注意：redis 和 MongoDB 均满足CP原则。</li>
</ul>
</li>
<li>CP (consistency + partition tolerance)：关注一致性和分区容忍性。它关注的是系统里大多数人的一致性协议，比如：Paxos 算法 (Quorum 类的算法)。这样的系统只需要保证大多数结点数据一致，而少数的结点会在没有同步到最新版本的数据时变成不可用的状态。这样能够提供一部分的可用性。</li>
<li>AP (availability + partition tolerance)：这样的系统关心可用性和分区容忍性。因此，这样的系统不能达成一致性，需要给出数据冲突，给出数据冲突就需要维护数据版本。Dynamo 就是这样的系统。</li>
</ul>
<p><img src="/2022/01/01/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%89%A9%E5%B1%95%E6%80%A7%E9%97%AE%E9%A2%98/7.png"></p>
<h4 id="如何进行三选二"><a href="#如何进行三选二" class="headerlink" title="如何进行三选二"></a>如何进行三选二</h4><p><strong>权衡三者的关键点取决于业务</strong><br>放弃了一致性，满足分区容错，那么节点之间就有可能失去联系，为了高可用，每个节点只能用本地数据提供服务，而这样会容易导致全局数据不一致性。对于互联网应用来说（如新浪，网易），机器数量庞大，节点分散，网络故障再正常不过了，那么此时就是保障AP，放弃C的场景，而从实际中理解，像门户网站这种偶尔没有一致性是能接受的，但不能访问问题就非常大了。</p>
<p>对于银行来说，就是<strong>必须保证强一致性</strong>，也就是说C必须存在，那么就只用CA和CP两种情况。</p>
<ul>
<li>当保障强一致性和可用性（CA），那么一旦出现通信故障，系统将完全不可用。</li>
<li>另一方面，如果保障了强一致性和分区容错（CP），那么就具备了部分可用性。</li>
</ul>
<p>实际究竟应该选择什么，是需要通过业务场景进行权衡的（并不是所有情况都是CP好于CA，只能查看信息但不能更新信息有时候还不如直接拒绝服务）</p>
<h2 id="NewSQL"><a href="#NewSQL" class="headerlink" title="NewSQL"></a>NewSQL</h2><p>newSQL 提供了与 noSQL 相同的可扩展性，而且仍基于关系模型，还保留了极其成熟的 SQL 作为查询语言，保证了ACID事务特性。</p>
<p>简单来讲，newSQL 就是在传统关系型数据库上集成了 noSQL 强大的可扩展性。</p>
<p>传统的SQL架构设计基因中是没有分布式的，而 newSQL 生于云时代，天生就是分布式架构。</p>
<p>newSQL 的主要特性：</p>
<ul>
<li>SQL 支持，支持复杂查询和大数据分析。</li>
<li>支持 ACID 事务，支持隔离级别。</li>
<li>弹性伸缩，扩容缩容对于业务层完全透明。</li>
<li>高可用，自动容灾。</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>SQL</th>
<th>NoSQL</th>
<th>NewSQL</th>
</tr>
</thead>
<tbody>
<tr>
<td>关系模型</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>SQL语句</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>ACID</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>水平扩展</td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>大数据</td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>无结构化</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
</tr>
</tbody>
</table>
</div>
<h3 id="CAP再探"><a href="#CAP再探" class="headerlink" title="CAP再探"></a>CAP再探</h3><p>当在分布式系统上发生通信故障的时候(Partitioning)，由于P是必须要有的，因此只有CP和AP两种选择。</p>
<p>对于A，也有两种方式： Node Avalability 和 Service Availability</p>
<ul>
<li>Node Availability (nA)<ul>
<li>通信故障发生时，任一节点都可用。 </li>
</ul>
</li>
<li>Service Availability (sA)<ul>
<li>通信故障发生时，部分节点不可用。但是，总有一部分节点可用。系统可以将用户自动切换到可用的节点。使得服务不中断。</li>
</ul>
</li>
</ul>
<p>因此，对于 CnAP来说，只能在CP和nAP中二选一，但是对于CsAP，在某种条件下，可以兼顾(使用Raft.Paxos)</p>
<h3 id="Spanner数据库"><a href="#Spanner数据库" class="headerlink" title="Spanner数据库"></a>Spanner数据库</h3><p>Spanner 就是利用 CsAP的NewSQL数据库。Spanner最重要的设计点就是做全球数据库，要求可扩展性、多数据版本、多replica数据一致性和事务。是第一个款能实现全球数据分布而且还能实现分布式事务的数据库系统，从他的架构图来看底层也是基于Colossus(GFS二代)来存储文件，只不过上层的tablet还通过Paxos协议在做数据的replica。这点和HBase当前1.1版本引进的region高可用有点类似，但看起来又不全是。 </p>
<h4 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h4><p>Spanner 是由一系列的 zone 组成, zone 是 Spanner 中的部署的单元, 一般会在某 datacenter 部署一个 zone, 但是也可以有多个 zone。数据副本就是存放在这一系列的 zone 中, zone 之间的物理距离越大, 数据的安全性越高。</p>
<p><img src="/2022/01/01/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%89%A9%E5%B1%95%E6%80%A7%E9%97%AE%E9%A2%98/8.jpeg" style="zoom:150%;"></p>
<blockquote>
<p>universemaster：是一个控制台, 监控universe里所有zone状态信息, 用于debug；</p>
<p>placement driver：帮助维持特定副本数量，自动搬迁数据，实现负载均衡；</p>
<p>zonemaster：管理 spanserver 上的数据；</p>
<p>location proxy：作用不详, 可能是为 client 提供数据的位置信息, client 要先访问它来定位需要访问的 spanserver；</p>
<p>spanserver：对 client 提供服务, 包括读写数据。</p>
</blockquote>
<p> <strong>Spanserver 架构</strong></p>
<p><img src="/2022/01/01/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%89%A9%E5%B1%95%E6%80%A7%E9%97%AE%E9%A2%98/9.jpeg" style="zoom:150%;"></p>
<p>在Spanner中，tablet的概念和Bigtable类似，都是B-tree的存储结构，和HBase的region对等关系。只不过在Spanner中创造了目录的概念，把不同表中相同rowkey前缀的数据放入到同一个directory中。这也是之所有能在几十TB数据量情况下做到多表关联的必要条件(而并非现在传说的遵循标准关系型数据库的schema模式可以随意来设计) </p>
<p>同时，Spanner引入了Group概念，这个和HBase当前的Region Group类似，不同的Dir归属于不同的Group，数据以Group为单位进行逻辑上的存储。Group内数据被同一个Paxos管理起来，而不同的Group间Paxos不同。当然目录可以在Group之间来回移动，以达到负载均衡和容灾的目的。 </p>
<p>为了实现分布式事务，除了Paxos协议和两阶段提交。还有一个非常重要的前提，就是不同客户端提交的<strong>事务时间戳需要非常精确</strong>，因为这个决定了究竟哪个事务在前，哪个事务在后。这个时间戳的概念比较容易被误读成数据时间戳。在这里Spanner定义了读写事务、只读事务、快照读。读写事务对时间精确性要求较高，也许在10ms内会同时有客户端在读写同一条数据，谁先谁后对最终结果影响很大。只读事务和快照读目前还没分太清楚，大概是指定时间戳版本的读，读当前最新还是读一个历史版本 </p>
<h4 id="Timestamps-Based-Protocals"><a href="#Timestamps-Based-Protocals" class="headerlink" title="Timestamps-Based Protocals"></a>Timestamps-Based Protocals</h4><p><a href="https://blog.csdn.net/weixin_45583158/article/details/100143234" target="_blank" rel="noopener">https://blog.csdn.net/weixin_45583158/article/details/100143234</a></p>
<p>每个事务在进入系统时都会被授予一个时间戳。如果一个旧的事务 $T_i$有时间戳 $TS(T_i)$，一个新的交易$T_j$被分配时间戳$TS(T_j)$，使$TS(T_i)&lt;TS(T_j)$。</p>
<p>该协议管理并发的执行，使时间戳决定可序列化的顺序。</p>
<p>为了保证这种行为，协议为每个数据Q保持两个时间戳值。</p>
<ul>
<li>W-timestamp(Q)是任何成功执行<strong>write(Q)</strong> 的事务的最大时间戳。</li>
<li>R-timestamp(Q)是任何成功执行<strong>read(Q)</strong> 的事务的最大时间戳。</li>
</ul>
<h4 id="Multiversion-Schemes"><a href="#Multiversion-Schemes" class="headerlink" title="Multiversion Schemes"></a>Multiversion Schemes</h4><p>Multiversion Schemes 保留数据项的旧版本，以增加并发性。</p>
<ul>
<li>Multiversion Timestamp Ordering</li>
<li>Multiversion Two-Phase Locking</li>
</ul>
<p>每一次成功的写入都会导致所写数据项的新版本的产生。</p>
<p>我们使用时间戳来标记版本。</p>
<p>当一个read(Q)操作被发出时，根据事务的时间戳选择Q的适当版本，并返回所选版本的值。 </p>
<p>执行读取操作时不必等待，因为适当的版本会立即返回。</p>
<h4 id="Multiversion-Timestamp-Ordering"><a href="#Multiversion-Timestamp-Ordering" class="headerlink" title="Multiversion Timestamp Ordering"></a>Multiversion Timestamp Ordering</h4><p>每个数据项Q都有一连串的版本<Q1, q2,...., qm>。每个版本$Q_k$包含三个数据字段。</Q1,></p>
<ul>
<li>Content — 版本$Q_k$的值。</li>
<li>W-timestamp($Q_k$) — 创建(写) 版本$Q_k$的事务的时间戳</li>
<li>R-timestamp($Q_k$) — 成功读取版本$Q_k$的事务的最大时间戳</li>
</ul>
<p>当一个事务$T_i$创建了Q的新版本$Q_k$，$Q_k$的 W-timestamp和R-timestamp 被初始化为$TS(T_i)$。<br>每当一个事务$T_j$读取$Q_k$，并且$TS(T_j)&gt;R-timestamp(Q_k)$时，$Q_k$的R-timestamp被更新。</p>
<p>假设事务$T_i$发出一个<strong>read(Q)</strong>或<strong>write(Q)</strong>操作。 让$Q_k$表示$Q$的版本，其写入时间戳是小于或等于$TS(T_i)$的最大写入时间戳。</p>
<ol>
<li>如果事务$T_i$发出 read(Q)，那么返回的值就是版本$Q_k$的内容。</li>
<li>如果事务$T_i$发出一个 write(Q)<ol>
<li>如果$TS(T_i)&lt; R-timestamp(Q_k)$，则事务$T_i$被回滚。</li>
<li>如果$TS(T_i) = W-timestamp(Q_k)$，$Q_k$的内容被覆盖。</li>
<li>否则将创建一个新的Q版本。</li>
</ol>
</li>
</ol>
<p>请注意</p>
<ul>
<li>读取总是成功的</li>
<li>如果其他事务$T_j$（在由时间戳值定义的序列化顺序中）应该读取$T_i$的写，但已经读取了比$T_i$更早的事务创建的版本，那么$T_i$的写就会被拒绝。</li>
<li>协议保证可序列化</li>
</ul>
<h4 id="Reads-in-spanner"><a href="#Reads-in-spanner" class="headerlink" title="Reads in spanner"></a>Reads in spanner</h4><h5 id="Snapshot-read"><a href="#Snapshot-read" class="headerlink" title="Snapshot read"></a>Snapshot read</h5><p>Snapshot read 指的是读取过去时间的某个快照, 无需加锁。</p>
<p>client可以指定一个 timestamp t, 或者时间范围，Spanner 会寻找一个数据充分更新好的 replica 来提供读服务。</p>
<p>所谓数据充分更新好, 是指 t &lt;= tsafe，其中 tsafe 定义如下:</p>
<script type="math/tex; mode=display">
t_{safe} = \min(t_{safe}^{paxos},t_{safe}^{TM})</script><p>前者指的已经提交的事务 timestamp, 后者指的是正在 2PC 过程中未决的事务 timestamp - 1。</p>
<p>如果 t &gt; tsafe，Spanner 需要等待 replica 一段时间, 待 tsafe 推进后再进行读操作。</p>
<p>基于 External Consistency 特性，Spanner 可以感知操作的先后顺序, 给定一个时间戳 t, Spanner 能够明确哪些是历史数据, 并提供一致的快照。 </p>
<h4 id="Read-only-transactions"><a href="#Read-only-transactions" class="headerlink" title="Read-only transactions"></a>Read-only transactions</h4><ul>
<li><p>Assign timestamp sread and do snapshot read at sread</p>
</li>
<li><p>$s_{read}  = TT.now().latest() $ 保证外部一致性（线性化）。</p>
</li>
<li><p>应该分配最古老的时间戳，保持外部一致性，以避免阻塞。</p>
<ul>
<li>For read at single paxos group: <ul>
<li>Let LastTS() = timestamp of the last committed write at the Paxos group. </li>
<li>如果没有准备好的事务，让 $s_{read} = LastTS()$很容易满足外部一致性：事务将看到最后一次写入的结果。</li>
</ul>
</li>
<li>在一般情况下，选择TT.now().latest()更简单。</li>
</ul>
</li>
</ul>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>Spanner 的 TureTime API 设计非常巧妙, 保证了绝对时间一定是落在 TT.now() 返回的区间之中。基于这个保证, 使得分布在全球的 spanserver 分配的 timestamp 都是基于同一参考系, 具有可比性。进而让 Spanner 能够感知分布式系统中的事件的先后顺序, 保证了 External Consistency。</p>
<p>但是 TureTime API 要求对误差的测量具有非常高的要求, 如果实际误差 &gt; 预计的误差, 那么绝对时间很可能就飘到了区间之外, 后续的操作都无法保证宣称的 External Consistency。另外, Spanener 的大部分响应时间与误差的大小是正相关的。</p>
<p>自 Spanner 在 OSDI12 发表论文后, Spanner 一直在努力减少误差, 并提高测量误差的技术[3], 但是并没有透露更多细节。</p>
<p>在一个大规模的分布式系统中，集中分配时间戳是不可行的。解决方案: TrueTime设备</p>
<ul>
<li>GPS时钟</li>
<li>原子钟</li>
</ul>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>分布式多版本数据库<ul>
<li>General-Purpose Transactions（ACID）</li>
<li>SQL查询语言</li>
<li>模式化的表(Schematized Tables)</li>
<li>半关系型数据模型（F1 is more relational/SQL）</li>
</ul>
</li>
<li>重点：管理跨数据中心的复制</li>
<li>特点：提供外部一致的读写。<ul>
<li>提供外部一致的读和写。</li>
<li>全局一致的跨数据库读取 </li>
</ul>
</li>
</ul>
<blockquote>
<p>F1最初定位是一个SQL查询引擎，本来是架构在Mysql的分布式集群上。由于Mysql本身的reshared以及分布式事务上的冲突几乎是无解，最终放弃Mysql分布式存储，而转而使用Spanner。单独的F1并不能称为一款数据库。这点在F1论文题目有所误导，更准确的来讲F1是一款SQL执行引擎，包含了二级索引等一些常见的传统数据库功能。和Spanner结合起来才完成了Nosql+Sql=newSQL的壮举。</p>
</blockquote>
<p>Spanner/F1似乎是第一个能称得上可真正扩展的分布式SQL系统。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2022/01/01/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/01/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1/" class="post-title-link" itemprop="url">分布式数据库事务</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-01-01 10:48:16" itemprop="dateCreated datePublished" datetime="2022-01-01T10:48:16+08:00">2022-01-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-06-23 21:40:02" itemprop="dateModified" datetime="2022-06-23T21:40:02+08:00">2022-06-23</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="分布式数据库事务"><a href="#分布式数据库事务" class="headerlink" title="分布式数据库事务"></a>分布式数据库事务</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>对于部署在云上的服务，仅仅由一台服务器来承载流量是不够的，因此数据库会被扩展到多台主机上。主要有两种扩展模式：</p>
<ul>
<li>分库/分表的扩展模式。这意思就是，在每一个节点(服务器)上，运行一套独立的数据库，然后在上面铺一层中间件，在收到了用户的请求之后，由中间件分发给节点来处理。运用这种模式，数据需要划分得特别干净</li>
</ul>
<p><img src="/2022/01/01/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1/2.png" style="zoom:67%;"></p>
<ul>
<li>很多时候我们无法做到划分出这么干净的数据，因此还有一种拓展方式是：并行/分布式数据库。 就是我们不把数据划分的任务交给中间件，而是直接构建一个分布式数据库系统，比如说 OceanBase。如下：</li>
</ul>
<p><img src="/2022/01/01/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1/1.png" style="zoom:67%;"></p>
<p>对于分布式数据库，电脑间的交互肯定是需要的，但是我们尽量要减少这种交互，因为网络交互存在带宽限制。</p>
<p>我们要了解三种 数据库构架设计： Shared Everthting、Shared Nothing、和Shared Disk：</p>
<ol>
<li>Shared Everthting:一般是针对单个主机，完全透明共享CPU/MEMORY/IO，并行处理能力是最差的，典型的代表SQLServer</li>
<li>Shared Disk：各个处理单元使用自己的私有 CPU和Memory，共享磁盘系统。典型的代表Oracle Rac， 它是数据共享，可通过增加节点来提高并行处理的能力，扩展能力较好。其类似于SMP（对称多处理）模式，但是当存储器接口达到饱和的时候，增加节点并不能获得更高的性能 。</li>
<li>Shared Nothing：各个处理单元都有自己私有的CPU/内存/硬盘等，不存在共享资源，类似于MPP（大规模并行处理）模式，各处理单元之间通过协议通信，并行处理和扩展能力更好。典型代表DB2 DPF和Hadoop ，<strong>各节点相互独立，各自处理自己的数据</strong>，处理后的结果可能向上层汇总或在节点间流转。</li>
</ol>
<p>我们常说的 Sharding 其实就是Share Nothing架构，它是把某个表从物理存储上被水平分割，并分配给多台服务器（或多个实例），每台服务器可以独立工作，具备共同的schema，比如MySQL Proxy和Google的各种架构，只需增加服务器数就可以增加处理能力和容量。</p>
<p>但是，一旦网络的带宽和IO带宽达到相同数量级的时候，就相当于电脑访问自己硬盘中的数据和访问其他节点中的数据所花费的开销类似，这时候share nothing的优势就不明显了。</p>
<h1 id="分布式事务的ACID"><a href="#分布式事务的ACID" class="headerlink" title="分布式事务的ACID"></a>分布式事务的ACID</h1><p>那么，如果使用Share nothing的话，我们该如何对数据进行切分? 可以使用哈希</p>
<p>在分布式数据库中，我们又该如何维持数据的正确性呢？</p>
<p>说到数据库事务就不得不说，数据库事务中的四大特性，ACID:</p>
<ul>
<li>A:原子性(Atomicity)</li>
</ul>
<p>一个事务(transaction)中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。</p>
<p>就像你买东西要么交钱收货一起都执行，要么要是发不出货，就退钱。</p>
<ul>
<li>C:一致性(Consistency)</li>
</ul>
<p>事务的一致性指的是在一个事务执行之前和执行之后数据库都必须处于一致性状态。如果事务成功地完成，那么系统中所有变化将正确地应用，系统处于有效状态。如果在事务中出现错误，那么系统中的所有变化将自动地回滚，系统返回到原始状态。</p>
<ul>
<li>I:隔离性(Isolation)</li>
</ul>
<p>指的是在并发环境中，当不同的事务同时操纵相同的数据时，每个事务都有各自的完整数据空间。由并发事务所做的修改必须与任何其他并发事务所做的修改隔离。事务查看数据更新时，数据所处的状态要么是另一事务修改它之前的状态，要么是另一事务修改它之后的状态，事务不会查看到中间状态的数据。</p>
<p>打个比方，你买东西这个事情，是不影响其他人的。</p>
<ul>
<li>D:持久性(Durability)</li>
</ul>
<p>指的是只要事务成功结束，它对数据库所做的更新就必须永久保存下来。即使发生系统崩溃，重新启动数据库系统后，数据库还能恢复到事务成功结束时的状态。</p>
<p>打个比方，你买东西的时候需要记录在账本上，即使老板忘记了那也有据可查。 </p>
<h2 id="隔离性"><a href="#隔离性" class="headerlink" title="隔离性"></a>隔离性</h2><p>首先我们来讲分布式数据库事务的隔离性。</p>
<p>在两个节点分别处理两个子事务的情况下，是否只要保证了单个节点的原子性，就可以保证整体的原子性呢？</p>
<p>显然不是，我们以两个节点为例。</p>
<p>现在有两个事务，交给两个节点去处理，可以有以下两种处理方式</p>
<p><img src="/2022/01/01/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1/3.jpg"></p>
<p>第一种方式，是序列化的，在每个节点上，都是先运行T1，再运行T2，而且T1、T2都符合原子性的标准。假设X=Y=0，那么最后它们都等于200</p>
<p>第二种方式，是非序列化的，在Node X上先执行T1.在Node Y上先执行T2。那那么这时候，由于二者是同步进行的，没有序列化，因此会互相造成干扰。如果X=Y=0, 那么，最后X = 200,Y=100</p>
<p>因此，当在分布式数据库中考虑序列化，我们要注意更多，不能依靠单个节点上的事务机制来保证所有事务的原子性。</p>
<p>在分布式数据库执行事务，我们需要考虑两类schedule：</p>
<ul>
<li>Local schedule</li>
<li>Global schedule</li>
</ul>
<p>因此，对于 Global Transaction，如果我们要将其序列化执行，也需要满足两个条件：</p>
<ul>
<li>每一个 Local Schedule 都必须是 可序列化的 (基本条件)</li>
<li>对于 Global Transaction 的所有 sub-transactions ，都以相同的顺序出现在所有站点的等效串行时间表中，不能出现在X 节点上顺序是 T1-&gt;T2，在Y节点上顺序是 T2-&gt;T1的情况</li>
</ul>
<h3 id="Lock"><a href="#Lock" class="headerlink" title="Lock"></a>Lock</h3><h4 id="两阶段锁"><a href="#两阶段锁" class="headerlink" title="两阶段锁"></a>两阶段锁</h4><p>在介绍分布式两阶段锁之前，我们先来学习两阶段锁，它用于单机事务中的一致性和隔离性</p>
<p>在一个事务操作中，分为<code>加锁阶段</code>和<code>解锁阶段</code>，且所有的加锁操作在解锁操作之前，具体如下图所示：</p>
<p><img src="/2022/01/01/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1/8.png"></p>
<ul>
<li>加锁时机</li>
</ul>
<p>当对记录进行更新操作或者<code>select for update(X锁)、lock in share mode(S锁)</code>时，会对记录进行加锁，锁的种类很多，不在此赘述。</p>
<ul>
<li>何时解锁</li>
</ul>
<p>在一个事务中，只有在<code>commit</code>或者<code>rollback</code>时，才是解锁阶段。</p>
<ul>
<li>二阶段加锁最佳实践</li>
</ul>
<p>下面举个具体的例子，来讲述二段锁对应用性能的影响，我们举个库存扣减的例子：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">方案一：</span><br><span class="line"><span class="keyword">start</span> <span class="keyword">transaction</span>;</span><br><span class="line">// 锁定用户账户表</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t_accout <span class="keyword">where</span> acount_id=<span class="number">234</span> <span class="keyword">for</span> <span class="keyword">update</span></span><br><span class="line">//生成订单</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> t_trans;</span><br><span class="line">// 减库存</span><br><span class="line"><span class="keyword">update</span> t_inventory <span class="keyword">set</span> <span class="keyword">num</span>=<span class="keyword">num</span><span class="number">-3</span> <span class="keyword">where</span> <span class="keyword">id</span>=$&#123;<span class="keyword">id</span>&#125; <span class="keyword">and</span> <span class="keyword">num</span>&gt;=<span class="number">3</span>;</span><br><span class="line"><span class="keyword">commit</span>;</span><br><span class="line"></span><br><span class="line">方案二：</span><br><span class="line"><span class="keyword">start</span> <span class="keyword">transaction</span>;</span><br><span class="line">// 减库存</span><br><span class="line"><span class="keyword">update</span> t_inventory <span class="keyword">set</span> <span class="keyword">num</span>=<span class="keyword">num</span><span class="number">-3</span> <span class="keyword">where</span> <span class="keyword">id</span>=$&#123;<span class="keyword">id</span>&#125; <span class="keyword">and</span> <span class="keyword">num</span>&gt;=<span class="number">3</span>;</span><br><span class="line">// 锁定用户账户表</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t_accout <span class="keyword">where</span> acount_id=<span class="number">234</span> <span class="keyword">for</span> <span class="keyword">update</span></span><br><span class="line">//生成订单</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> t_trans;</span><br><span class="line"><span class="keyword">commit</span>;</span><br></pre></td></tr></table></figure>
<p>我们的应用通过<code>JDBC</code>操作数据库时，底层本质上还是走<code>TCP</code>进行通信，<code>MySQL协议</code>是一种<code>停-等式协议</code>(和<code>http</code>协议类似，每发送完一个分组就停止发送，等待对方的确认,在收到确认后再发送下一个分组)，既然通过网络进行通信，就必然会有延迟，两种方案的网络通信时序图如下：</p>
<p><img src="/2022/01/01/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1/6.jpeg"></p>
<p>由于商品库存往往是最致命的热点，是整个服务的热点。如果采用第一种方案的话，<code>TPS</code>理论上可以提升<code>3rt/rt=3</code>倍。而这是在一个事务中只有3条SQL的情况，理论上多一条SQL就多一个rt时间。</p>
<p>另外，当<strong>更新操作到达数据库的那个点</strong>，才算加锁成功。<code>commit</code>到达数据库的时候才算解锁成功。所以，更新操作的前半个<code>rt</code>和<code>commit</code>操作的后半个<code>rt</code>都不计算在整个锁库存的时间内。</p>
<ul>
<li>性能优化</li>
</ul>
<p>从上面的例子可以看出，在一个事务操作中，将对最热点记录的操作放到事务的最后面，这样可以显著地提高服务的<code>吞吐量</code>。</p>
<ul>
<li>select for update 和 update where的最优选择</li>
</ul>
<p>我们可以将一些简单的判断逻辑写到update操作的谓词里面，这样可以减少加锁的时间，如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">方案一：</span><br><span class="line"><span class="keyword">start</span> <span class="keyword">transaction</span></span><br><span class="line"><span class="keyword">num</span> = <span class="keyword">select</span> <span class="keyword">count</span> <span class="keyword">from</span> t_inventory <span class="keyword">where</span> <span class="keyword">id</span>=<span class="number">234</span> <span class="keyword">for</span> <span class="keyword">update</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">count</span> &gt;= <span class="number">3</span>:</span><br><span class="line">    <span class="keyword">update</span> t_inventory <span class="keyword">set</span> <span class="keyword">num</span>=<span class="keyword">num</span><span class="number">-3</span> <span class="keyword">where</span> <span class="keyword">id</span>=<span class="number">234</span></span><br><span class="line">    <span class="keyword">commit</span> </span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">rollback</span></span><br></pre></td></tr></table></figure>
<p>方案二：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">start</span> <span class="keyword">transaction</span>:</span><br><span class="line">    <span class="built_in">int</span> affectedRows = <span class="keyword">update</span> t_inventory <span class="keyword">set</span> <span class="keyword">num</span>=<span class="keyword">num</span><span class="number">-3</span> <span class="keyword">where</span> <span class="keyword">id</span>=<span class="number">234</span> <span class="keyword">and</span> <span class="keyword">num</span>&gt;=<span class="number">3</span></span><br><span class="line">    <span class="keyword">if</span> affectedRows &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">commit</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">rollback</span></span><br></pre></td></tr></table></figure>
<p>延时图如下：</p>
<p><img src="/2022/01/01/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1/7.jpeg"></p>
<p>从上图可以看出，加了update谓词以后，一个事务少了1rt的锁记录时间（update谓词和select for update对记录加的都是X锁，所以效果是一样的）。</p>
<h4 id="Centralized-2PL"><a href="#Centralized-2PL" class="headerlink" title="Centralized 2PL"></a>Centralized 2PL</h4><p>Centralized 2PL 是一种将锁的管理责任只委托给一个节点的方法, 也叫做 Primary 2PL 算法. </p>
<p>首先我们要了解一下 Centralized 2PL中的一些名词：</p>
<ul>
<li>Coordinating TM (Transaction Manager) </li>
<li>Participating sites : 那些要进行数据库操作的地方</li>
<li>LM (Lock Manager)</li>
</ul>
<h5 id="流程示意图"><a href="#流程示意图" class="headerlink" title="流程示意图"></a>流程示意图</h5><p><img src="/2022/01/01/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1/9.png"></p>
<p><img src="/2022/01/01/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1/10.png"></p>
<ul>
<li>首先，Coordinating TM 代理站点向 Centralized LM发送请求加个锁</li>
<li>LM 同意TM的请求，帮其加锁</li>
<li>TM告诉目标站点：帮你请求到锁了，你可以进行数据库操作了</li>
<li>站点结束数据操作之后，告诉TM</li>
<li>TM向LM汇报：操作完成了，现在你可以释放锁了</li>
</ul>
<p>Centrailized 2PL 主要有以下特点：</p>
<ul>
<li>有一个维护所有锁信息的<strong>单一节点</strong></li>
<li>一个锁管理器(LM)适用于整个DDBMS。</li>
<li>参与全局事务的Local transaction managers(TM) 从 LM中请求和释放锁。或者Transaction Coordinator(可以理解为总代理) 可以代表Participating Sites提出所有的锁请求。</li>
<li>优点：容易发现死锁</li>
<li>劣势：可扩展性差</li>
</ul>
<h4 id="Distributed-2PL"><a href="#Distributed-2PL" class="headerlink" title="Distributed 2PL"></a>Distributed 2PL</h4><p>Distributed 2PL 和 Centralized 2PL 不同，它在每个站点都有一个LM，同时，只有一个TM。它们的通讯逻辑也发生了变化</p>
<p><img src="/2022/01/01/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1/11.png"></p>
<p><img src="/2022/01/01/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1/12.png"></p>
<p>由于现在每个站点都有一个 LM，因此站点中的LM不再和数据操作隔离，可以直接对本站点的数据库进行操作。</p>
<p>它们之间的交互逻辑如下：</p>
<ul>
<li>TM 告诉 某一个站点的LM: 哥，你们数据库中有个操作，请你加个锁呗</li>
<li>LM 回答：得嘞，没问题，这就给他加，就不麻烦你了哈</li>
<li>当前站点的数据库操作完成后，比较害羞，对TM说：哥哥，我好了，你和LM说让他把锁松开</li>
<li>TM然后再对 LM说，差不多得了，可以释放锁了</li>
</ul>
<p>分布式二阶段锁的特点是：</p>
<ul>
<li>每个站点都有一个LM。因此每一个站点都有一个 时序表(scheduler)</li>
<li>每个LM处理该站点的数据的锁请求。</li>
<li>并发控制是<strong>通过 Participating sites 的 LMs 的合作</strong> 来完成的</li>
<li>优点：更好的扩展性</li>
<li>缺点：难以探测到死锁的发生 </li>
</ul>
<h2 id="原子性和持久性"><a href="#原子性和持久性" class="headerlink" title="原子性和持久性"></a>原子性和持久性</h2><p>分布式数据库事务的原子性和持久性是通过多阶段提交来实现的。</p>
<p><a href="https://blog.csdn.net/skyie53101517/article/details/80741868" target="_blank" rel="noopener">https://blog.csdn.net/skyie53101517/article/details/80741868</a></p>
<h3 id="2PC"><a href="#2PC" class="headerlink" title="2PC"></a>2PC</h3><p>二阶段提交(Two-phaseCommit)是指，在计算机网络以及数据库领域内，为了使基于分布式系统架构下的所有节点在进行事务提交时保持一致性而设计的一种算法(Algorithm)。通常，二阶段提交也被称为是一种协议(Protocol)。在分布式系统中，每个节点虽然可以知晓自己的操作时成功或者失败，却无法知道其他节点的操作的成功或失败。当一个事务跨越多个节点时，为了保持事务的ACID特性，需要引入一个作为<strong>协调者(coordinator)</strong>的组件来统一掌控所有节点(称作<strong>参与者(participant)</strong>)的操作结果并最终指示这些节点是否要把操作结果进行真正的提交(比如将更新后的数据写入磁盘等等)。因此，<strong>二阶段提交的算法思路可以概括为：参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。</strong></p>
<h4 id="准备阶段-voting-phase"><a href="#准备阶段-voting-phase" class="headerlink" title="准备阶段 voting phase"></a>准备阶段 voting phase</h4><p>事务协调者(事务管理器)给每个参与者(资源管理器)发送Prepare消息，每个参与者要么直接返回失败(如权限验证失败)，要么在本地执行事务，写本地的redo和undo日志，但不提交，到达一种“万事俱备，只欠东风”的状态。</p>
<p>可以进一步将准备阶段分为以下三个步骤：</p>
<blockquote>
<p>1）协调者节点向所有参与者节点询问是否可以执行提交操作(vote)，并开始等待各参与者节点的响应。</p>
<p>2）参与者节点执行询问发起为止的所有事务操作，并将Undo信息和Redo信息写入日志。（注意：若成功这里其实每个参与者已经执行了事务操作）</p>
<p>3）各参与者节点响应协调者节点发起的询问。如果参与者节点的事务操作实际执行成功，则它返回一个”同意”消息；如果参与者节点的事务操作实际执行失败，则它返回一个”中止”消息。</p>
</blockquote>
<h4 id="提交阶段-decision-phase"><a href="#提交阶段-decision-phase" class="headerlink" title="提交阶段 decision phase"></a>提交阶段 decision phase</h4><p>如果协调者收到了参与者的失败消息或者超时，<strong>直接给每个参与者发送回滚(Rollback)消息</strong>；否则，<strong>发送提交(Commit)消息</strong>；参与者根据协调者的指令执行提交或者回滚操作，释放所有事务处理过程中使用的锁资源。(注意:必须在最后阶段释放锁资源)</p>
<p>接下来分两种情况分别讨论提交阶段的过程。</p>
<ul>
<li>当协调者节点从<strong>所有参与者节点获得的相应消息都为”同意”</strong>时:</li>
</ul>
<blockquote>
<p>1）协调者节点向所有参与者节点发出”正式提交(commit)”的请求。</p>
<p>2）参与者节点正式完成操作，并释放在整个事务期间内占用的资源。</p>
<p>3）参与者节点向协调者节点发送”完成”消息。</p>
<p>4）协调者节点受到所有参与者节点反馈的”完成”消息后，完成事务。</p>
</blockquote>
<p><img src="/2022/01/01/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1/13.png"></p>
<p>如果任一参与者节点在第一阶段返回的响应消息为”中止”，或者 协调者节点在第一阶段的询问超时之前无法获取所有参与者节点的响应消息时：</p>
<blockquote>
<p>1）协调者节点向所有参与者节点发出”回滚操作(rollback)”的请求。</p>
<p>2）参与者节点利用之前写入的Undo信息执行回滚，并释放在整个事务期间内占用的资源。</p>
<p>3）参与者节点向协调者节点发送”回滚完成”消息。</p>
<p>4）协调者节点受到所有参与者节点反馈的”回滚完成”消息后，取消事务。</p>
</blockquote>
<p><img src="/2022/01/01/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1/14.png"></p>
<p>不管最后结果如何，第二阶段都会结束当前事务。</p>
<p>二阶段提交看起来确实能够提供原子性的操作，但是不幸的事，二阶段提交还是有几个<strong>缺点</strong>的：</p>
<blockquote>
<p>1、<strong>同步阻塞问题</strong>。执行过程中，<strong>所有参与节点都是事务阻塞型的</strong>。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。</p>
<p>2、<strong>单点故障</strong>。由于协调者的重要性，<strong>一旦协调者发生故障, 参与者会一直阻塞下去</strong>。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题）</p>
<p>3、<strong>数据不一致</strong>。在提交阶段中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，这回导致只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据部一致性的现象。</p>
<p>4、二阶段无法解决的问题：协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。</p>
</blockquote>
<p>由于二阶段提交存在着诸如同步阻塞、单点问题、脑裂等缺陷，所以，研究者们在二阶段提交的基础上做了改进，提出了三阶段提交。</p>
<h3 id="3PC"><a href="#3PC" class="headerlink" title="3PC"></a>3PC</h3><blockquote>
<p>三阶段提交（Three-phase commit），也叫三阶段提交协议（Three-phase commit protocol），是二阶段提交（2PC）的改进版本。</p>
</blockquote>
<p>与两阶段提交不同的是，三阶段提交有两个改动点。</p>
<ol>
<li><p>引入超时机制。同时在协调者和参与者中都引入超时机制。</p>
</li>
<li><p>在第一阶段和第二阶段中插入一个准备阶段。<strong>保证了在最后提交阶段之前各参与节点的状态是一致的</strong></p>
</li>
</ol>
<p>也就是说，除了引入超时机制之外，3PC把2PC的准备阶段再次一分为二，这样三阶段提交有 <code>CanCommit</code> 、 <code>PreCommit</code> 、 <code>DoCommit</code>三个阶段。</p>
<h4 id="CanCommit阶段"><a href="#CanCommit阶段" class="headerlink" title="CanCommit阶段"></a>CanCommit阶段</h4><p>3PC的CanCommit阶段其实和2PC的准备阶段很像。协调者向参与者发送commit请求，参与者如果可以提交就返回Yes响应，否则返回No响应。</p>
<blockquote>
<p><strong>1.事务询问</strong> 协调者向参与者发送CanCommit请求。询问是否可以执行事务提交操作。然后开始等待参与者的响应。</p>
<p><strong>2.响应反馈</strong> 参与者接到CanCommit请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回Yes响应，并进入预备状态。否则反馈No</p>
</blockquote>
<p>优点：不像2pc 第一个阶段就开始锁表，3pc的阶段一是为了先排除个别参与者不具备提交事务能力的前提下，而避免锁表。</p>
<h4 id="PreCommit阶段"><a href="#PreCommit阶段" class="headerlink" title="PreCommit阶段"></a>PreCommit阶段</h4><p>协调者根据参与者的反应情况来决定是否可以执行事务的PreCommit操作。根据响应情况，有以下两种可能。</p>
<p><strong>假如协调者从所有的参与者获得的反馈都是Yes响应，那么就会执行事务的预执行。</strong></p>
<blockquote>
<p><strong>1.发送预提交请求</strong> 协调者向参与者发送PreCommit请求，并进入Prepared阶段。</p>
<p><strong>2.事务预提交</strong> 参与者接收到PreCommit请求后，会执行事务操作，并将undo和redo信息记录到事务日志中。</p>
<p><strong>3.响应反馈</strong> 如果参与者成功的执行了事务操作，则返回ACK响应，同时开始等待最终指令。</p>
</blockquote>
<p><strong>假如有任何一个参与者向协调者发送了No响应;  或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。</strong></p>
<blockquote>
<p><strong>1.发送中断请求</strong> 协调者向所有参与者发送abort请求。</p>
<p><strong>2.中断事务</strong> 参与者收到来自协调者的abort请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。</p>
</blockquote>
<h4 id="doCommit阶段"><a href="#doCommit阶段" class="headerlink" title="doCommit阶段"></a>doCommit阶段</h4><p>该阶段进行真正的事务提交，也可以分为以下两种情况。</p>
<p><strong>执行提交</strong></p>
<blockquote>
<p><strong>1.发送提交请求</strong> 协调接收到参与者发送的ACK响应，那么他将从预提交状态进入到提交状态。并向所有参与者发送doCommit请求。</p>
<p><strong>2.事务提交</strong> 参与者接收到doCommit请求之后，执行正式的事务提交。并在完成事务提交之后释放所有事务资源。</p>
<p><strong>3.响应反馈</strong> 事务提交完之后，向协调者发送Ack响应。</p>
<p><strong>4.完成事务</strong> 协调者接收到所有参与者的ack响应之后，完成事务。</p>
</blockquote>
<p><strong>中断事务</strong> 协调者没有接收到参与者发送的ACK响应（可能是接受者发送的不是ACK响应，也可能响应超时），那么就会执行中断事务。</p>
<blockquote>
<p><strong>1.发送中断请求</strong> 协调者向所有参与者发送abort请求</p>
<p><strong>2.事务回滚</strong> 参与者接收到abort请求之后，利用其在阶段二记录的undo信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。</p>
<p><strong>3.反馈结果</strong> 参与者完成事务回滚之后，向协调者发送ACK消息</p>
<p><strong>4.中断事务</strong> 协调者接收到参与者反馈的ACK消息之后，执行事务的中断。</p>
</blockquote>
<p><img src="/2022/01/01/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1/15.png"></p>
<h3 id="2PC与3PC的区别"><a href="#2PC与3PC的区别" class="headerlink" title="2PC与3PC的区别"></a>2PC与3PC的区别</h3><p>相对于2PC，3PC<strong>主要解决的单点故障问题</strong>，并减少阻塞。在2PC中，只有协调者有超时机制；但是在3PC中，协调者和参与者都有超时机制，因此即使因为网络原因协调者与参与者断开通信， 参与者在超时后也会自动提交commit，这样防止了一直锁表的风险，而不会一直持有事务资源并处于阻塞状态。</p>
<p>但是这种机制也会导致数据一致性问题，因为，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况。</p>
<hr>
<p>了解了2PC和3PC之后，我们可以发现，无论是二阶段提交还是三阶段提交都无法彻底解决分布式的一致性问题。Google Chubby的作者Mike Burrows说过， <code>there is only one consensus protocol, and that’s Paxos” – all other approaches are just broken versions of Paxos.</code> 意即<strong>世上只有一种一致性算法，那就是Paxos</strong>，所有其他一致性算法都是Paxos算法的不完整版</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="/2022/01/01/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1/16.png"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/" class="post-title-link" itemprop="url">计算机视觉-检测和分割</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-12-23 13:12:16" itemprop="dateCreated datePublished" datetime="2021-12-23T13:12:16+08:00">2021-12-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-06-25 23:31:00" itemprop="dateModified" datetime="2022-06-25T23:31:00+08:00">2022-06-25</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="计算机视觉-检测和分割"><a href="#计算机视觉-检测和分割" class="headerlink" title="计算机视觉-检测和分割"></a>计算机视觉-检测和分割</h1><p>在第一章我们了解过常见的CV任务，主要有：图像分类、语义分割、分类+定位、目标检测、实例分割</p>
<p><img src="/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/1.jpg" style="zoom:80%;"></p>
<h2 id="图像分类"><a href="#图像分类" class="headerlink" title="图像分类"></a>图像分类</h2><p>CV最常见的就是图像分类了，比如说利用AlexNet对图像进行分类，最后输出对图像的预测分数</p>
<p><img src="/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/2.jpg" style="zoom:80%;"></p>
<p>在图像分类的基础上，延伸出来几个其他作用，下面一一介绍</p>
<h2 id="语义分割"><a href="#语义分割" class="headerlink" title="语义分割"></a>语义分割</h2><p>语义分割就是对于输入的图片，将每一个像素划分成一个类别。如下：</p>
<p><img src="/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/3.jpg" style="zoom:80%;"></p>
<h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h4><p>那么语义分割怎么做呢？</p>
<p>对于图像分类，是对图像整体进行类别预测，而对于语义分割，是对每一个像素进行分类。同时，不区分相同类别的个体。</p>
<h4 id="Naive-Idea-滑窗"><a href="#Naive-Idea-滑窗" class="headerlink" title="Naive Idea: 滑窗"></a>Naive Idea: 滑窗</h4><p><img src="/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/7.png" style="zoom:80%;"></p>
<p>我们可以拿一个窗口再图像上滑动，每次滑动一个像素点，然后用CNN去判断这个滑窗属于什么类别，然后将滑窗的中心点划为这个类。</p>
<p>虽然利用滑窗可以达到比较好的效果，但是这个方法的计算代价及其昂贵</p>
<h4 id="对所有像素点同时进行预测"><a href="#对所有像素点同时进行预测" class="headerlink" title="对所有像素点同时进行预测"></a>对所有像素点同时进行预测</h4><p>我们可以对整个图像进行卷积</p>
<p><img src="/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/8.png" style="zoom:80%;"></p>
<p>将损失函数定义为 对所有像素点进行分类的交叉熵之和，然后最小化损失函数得到预测结果。这个方法相比于滑窗效率有所提升，但是对内存的要求很大，卷积计算依然昂贵。</p>
<p>因此我们要在中间过程中对feature map 进行下采样，得到隐层向量，然后再进行上采样复原图片，进行损失函数的计算。</p>
<p><img src="/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/9.png" style="zoom:80%;"></p>
<h4 id="上采样"><a href="#上采样" class="headerlink" title="上采样"></a>上采样</h4><p>下采样是池化和卷积，上采样就是下采样的逆转方法——反池化和反卷积。</p>
<p>比如说，反池化有几种选择：我们可以将一个元素的邻居都赋相同的值，也可以将只赋值给区域中的固定位置，其他位置设为0</p>
<p><img src="/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/10.png" style="zoom:80%;"></p>
<p>当然，还有一种就是借鉴池化反向传播的方法，将值赋给池化时的对应位置</p>
<p><img src="/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/11.png" style="zoom:80%;"></p>
<p>不管是max unpooling, nearest neigbor 还是Bed of Nails，都和池化一样是没有参数的。 还有一种上采样方法：deconvolution,则需要用参数，而且是可学习的。</p>
<p>如下图， 我们将输入值乘以一个 $3\times 3$的”反卷积核” ，就相当于”扩张”了。如下图所示：</p>
<p><img src="/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/12.png" style="zoom:80%;"></p>
<p>然后，对每个输入值乘以卷积核，并对重叠的位置进行累积(当然平均也可以)，得到上采样后的结果，可以得到如下图所示：</p>
<p><img src="/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/13.png" style="zoom:80%;"></p>
<h2 id="分类-定位"><a href="#分类-定位" class="headerlink" title="分类+定位"></a>分类+定位</h2><p>分类加定位(单目标检测)，就是识别出一张图片最主要的对象，然后框起来。如下</p>
<p><img src="/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/4.jpg" style="zoom:80%;"></p>
<h3 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h3><p><img src="/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/14.png" style="zoom:80%;"></p>
<p>和图像分类的区别就在于，经过卷积之后，需要做两个全连接层，一个全连接层做的工作是分类，另一个则是输出一个长度为4向量，代表了这个框的四个坐标。这样就完成了分类的预测和定位的预测。</p>
<p>分类的预测用softmax 损失函数，定位的预测用L2损失函数(回归)，将这两个损失函数相加，共同优化，得到最终的结果。</p>
<p>因此，理论上只要有固定数量的点位，我们都可以对其进行预测。这项技术可以用于姿态估计</p>
<p><img src="/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/15.png" style="zoom:80%;"></p>
<h2 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h2><p>将分类+定位中的单个物体拓展为多个物体，就变成了目标检测任务</p>
<p><img src="/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/5.png" style="zoom: 50%;"></p>
<h3 id="方法-2"><a href="#方法-2" class="headerlink" title="方法"></a>方法</h3><p>在进行分类+定位任务的时候，我们可以将问题简化为预测5个值(类别以及框的定位)；但是对于目标检测任务，由于一张图片中存在多个目标，我事先是不知道有多少物体的，因此需要预测很多数值以及很多类别。</p>
<h4 id="Naive-idea"><a href="#Naive-idea" class="headerlink" title="Naive idea"></a>Naive idea</h4><p>一个朴素的额方法就是，用不同的大小、位置、长宽比的矩形区域对图片进行卷积计算，用来预测类别以及框框位置。但很显然这种方法的代价极其昂贵。</p>
<p><img src="/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/16.png" style="zoom: 100%;"></p>
<h4 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h4><p>因此我们设想，可不可以预先寻找一些可能存在物体的区域？ 这就是 Region Proposals(RP)</p>
<p>我们可以用 Selective Search 这种传统的图形学算法，来找到几个很可能存在物体的区域。通常速度为 2000区域/秒</p>
<p><img src="/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/17.png" style="zoom: 100%;"></p>
<p>在一张图片中经过RP之后，会在图片上标记出一些区域，我们称其为(Regions of Interest,ROI) 由于区域大小存在差异，因此会先转换成大小统一的feature map</p>
<p>然后经过CNN 对每一个区域进行分类以及利用回归来校正 ROI 的坐标</p>
<p><img src="/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/18.png" style="zoom: 100%;"></p>
<p>虽然这个算法效果非常好，肯定包含了我们需要检测的目标，但是，这种方法也很慢，需要对至少好几千个ROI分别做卷积。</p>
<h4 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h4><p>为了优化上面这个算法，我们可以先卷积，后RP，这就是所谓的 Fast R-CNN . 示意图如下：</p>
<p><img src="/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/19.png" style="zoom: 100%;"></p>
<h4 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h4><p>比 Fast R-CNN更快的，是Faster R-CNN。 这种方法优化了RP方法，使用RPN来进行物体检测。其示意图如下：</p>
<p><img src="/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/20.png" style="zoom: 100%;"></p>
<p>该方法使用了4个损失函数：</p>
<ul>
<li>RPN 是否包含物体</li>
<li>RPN 边框坐标</li>
<li>物体的最终类别</li>
<li>边框的额最终目标</li>
</ul>
<p>RPN 不同于传统的图像算法，<strong>是基于数据驱动、会不断优化自己的</strong>。训练完成以后，当RPN得到了feature map，它会去<strong>预测</strong>到底哪些区域是存在目标物体的，而不是使用selective search的方法去检索。</p>
<p>因此，到最后PR会被优化成一个线性计算，效率会非常高。</p>
<p>那么，RPN 是怎么被训练的呢？</p>
<p>首先，得到了一个 20x15 的feature map，我们就将其划分为20x15个block. </p>
<p>接着，我们把每一个block 为中心，构建一个 anchor box. 如下图所示</p>
<p><img src="/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/23.png" style="zoom: 100%;"></p>
<p>然后，我们去预测每个anchor box 里面是否包含物体。也就是输出 $1\times20\times15$ 个预测数据</p>
<p>在预测是否包含物体的时候，还需要预测每一个anchor box的真是坐标，因此又要输出$4\times20\times15$个数据</p>
<p>通常，每一个像素点，有k 个大小不同的anchor box 需要我们去预测，因此，还要在原来的基础上乘以 $K$ </p>
<p>最后，我们会选取<strong>含物体概率最高</strong>的100个区域作为 Region  Proposal</p>
<p><img src="/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/22.png" style="zoom: 100%;"></p>
<p>整个训练一共分为两个阶段</p>
<ul>
<li>第一阶段<ul>
<li>CNN 提取 feature</li>
<li>RPN</li>
</ul>
</li>
<li>第二阶段<ul>
<li>ROI Pooling</li>
<li>分类</li>
<li>回归(校正 ROI)</li>
</ul>
</li>
</ul>
<p><img src="/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/21.png" style="zoom: 100%;"></p>
<h4 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h4><p>YOLO 是 You Only Look Once 的缩写。</p>
<p>YOLO 是直接在图片上划分出base boxes(类似于anchor boxes),对每个base box进行回归，并预测这个框有多大概率真的包含目标物体(背景)。</p>
<p>YOLO在实际项目中的效果是很好的，比faster R-CNN更快。但是由于没有提取feature map，所以精度会差一点</p>
<p><img src="/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/24.png" style="zoom: 100%;"></p>
<p><a href="https://arxiv.org/pdf/1512.02325.pdf" target="_blank" rel="noopener">SSD: Single-Shot MultiBox Detector</a></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li><p>Feature 提取网络</p>
<ul>
<li>VGG 16</li>
<li>ResNet-101</li>
<li>Inception V2</li>
<li>Inception V3</li>
<li>Mobile Net</li>
</ul>
</li>
<li><p>主体结构</p>
<ul>
<li>两阶段： Faster R-CNN</li>
<li>一阶段: YOLO/SSD</li>
<li>混合：R-FCN</li>
</ul>
</li>
<li><p>经验总结</p>
<ul>
<li><p>Faster R-CNN 相对较慢但精度较高</p>
</li>
<li><p>YOLO/SSD 速度更快，但精度较低</p>
</li>
<li><p>Feature 提取可以使用更深更大的模型</p>
</li>
</ul>
</li>
</ul>
<h2 id="实例分割"><a href="#实例分割" class="headerlink" title="实例分割"></a>实例分割</h2><p>在实例分割任务中，需要将检测到的目标用不同的颜色标注出来。但在语义分割里面，可能这些目标都是一个颜色的。</p>
<p><img src="/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/26.png"></p>
<h3 id="方法-3"><a href="#方法-3" class="headerlink" title="方法"></a>方法</h3><p>实例分割就是将前面的一些方法整合起来。其中最经典的一个模型就是Mask R-CNN</p>
<h4 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h4><p>Mask R-CNN前面的结构和Faster R-CNN 非常像</p>
<p>先利用目标检测的方法，检测图片中那些区域可能存在物体</p>
<p>再利用语义分割的方法，将这个区域里面的像素点进行分类</p>
<p><img src="/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/25.png"></p>
<p><a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf" target="_blank" rel="noopener">https://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf</a></p>
<h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><p><img src="/2021/12/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2/27.png"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2021/12/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/" class="post-title-link" itemprop="url">计算机视觉-图像分类</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-12-22 22:26:57" itemprop="dateCreated datePublished" datetime="2021-12-22T22:26:57+08:00">2021-12-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-03-07 22:56:50" itemprop="dateModified" datetime="2022-03-07T22:56:50+08:00">2022-03-07</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="计算机视觉-图像分类"><a href="#计算机视觉-图像分类" class="headerlink" title="计算机视觉-图像分类"></a>计算机视觉-图像分类</h1><p>在上一节课我们主要讲了计算机视觉的最主要的几个任务：图像分类、定位+分割、目标检测、语义分割、实例分割。现在我们来着重看计算机视觉中最基础的任务——图像分类。有了图像分类，我们才可以做定位+分割、目标检测等更高阶的任务。</p>
<h2 id="图像分类任务"><a href="#图像分类任务" class="headerlink" title="图像分类任务"></a>图像分类任务</h2><p>图像分类任务，顾名思义，就是对于一张输入的图片，通过某种方法让计算机算出这张图片属于什么类型。比如说一张狗图，对于人脑而言，可以很简单的辨别出它是狗；但是对计算机来说，它只能给出一组概率，比如说狗的概率0.8, 猫的概率0.1,狼的概率0.1，最终判断它是狗</p>
<p><img src="/2021/12/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/1.png" alt="1" style="zoom:67%;"></p>
<h3 id="对计算机的挑战"><a href="#对计算机的挑战" class="headerlink" title="对计算机的挑战"></a>对计算机的挑战</h3><h4 id="语义鸿沟"><a href="#语义鸿沟" class="headerlink" title="语义鸿沟"></a>语义鸿沟</h4><p>其实，这任务对计算机来说是比较困难的，因为涉及到<strong>语义鸿沟</strong>，对于计算机来说，图片对他来说只是有一个一个像素组成的，每个像素都是一组 $[0,255]$的数字。如下所示：</p>
<p><img src="/2021/12/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/2.png" style="zoom:67%;"></p>
<h4 id="尺度、视角差异"><a href="#尺度、视角差异" class="headerlink" title="尺度、视角差异"></a>尺度、视角差异</h4><p>当对一个物体的拍摄距离或者拍摄角度产生变化的时候，对人脑而言可能很快就能意识到只是尺度发生了变化，但是物体还是一样的。但是对于计算机来说，他所看到的像素点可能发生了天翻地覆的变化，因此这也是一大难题</p>
<h4 id="类内差异"><a href="#类内差异" class="headerlink" title="类内差异"></a>类内差异</h4><p>因为狗有很多品种，不同品种的狗可能样貌、体型、颜色会有很大的变化，大脑可以马上明白过来这虽然是不同的品种，但都属于狗这一类。但对于计算机来说，就可能将它们划归为不同的类去了。</p>
<h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><p><img src="/2021/12/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/3.png" alt="1" style="zoom:67%;"></p>
<h2 id="传统方法"><a href="#传统方法" class="headerlink" title="传统方法"></a>传统方法</h2><p>传统方法是利用边缘检测方法，将图片转换成边和线。然后，我们需要在这些边和线中找出可以被捕获到的特征，比如眼睛、鼻子、毛发等。根据这些特征去判断图片中的物体是否属于狗。            </p>
<p><img src="/2021/12/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/4.png" style="zoom:67%;"></p>
<p>但是这种算法，存在很大的缺陷：</p>
<ol>
<li>鲁棒性很差，对于不同的光线、角度、背景，可能会失效</li>
<li>可扩展性很差(延展性较差)，需要为每一种物体设计专门的算法。</li>
<li>需要我们 Hard code</li>
</ol>
<h2 id="数据驱动的方法"><a href="#数据驱动的方法" class="headerlink" title="数据驱动的方法"></a>数据驱动的方法</h2><p>传统方法的局限性迫使我们找到高效的，基于数据驱动的方法。构建一个模型的的基本步骤如下：</p>
<p><img src="/2021/12/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/5.png" style="zoom:67%;"></p>
<h3 id="NN-Nearest-Neighbor"><a href="#NN-Nearest-Neighbor" class="headerlink" title="NN(Nearest Neighbor)"></a>NN(Nearest Neighbor)</h3><p>我们 现在来介绍第一个算法：NN。 它是一个<strong>香草分类器</strong>，也就是最简单的分类器。</p>
<p>在训练阶段，它会存储下所有的图像和标签。在测试阶段，对于每一个样本，都会遍历所有的类，计算样本和类之间的距离，然后使用距离最近的图像的标签来预测新图像的标签。</p>
<p>我们常常用的是 L1距离(曼哈顿距离)</p>
<p><img src="/2021/12/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/6.png" style="zoom:67%;"></p>
<p>效果展示，我们根据像素之间的距离将其分为了不同的颜色，代表了不同的类。我们发现，几个红点很明显被紫色包围了，但是分类器还是将其分成了红色的点。这些离群值会导致整个分类器并不是很稳定。</p>
<p><img src="/2021/12/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/7.png" style="zoom:67%;"></p>
<p>我们发现，</p>
<h3 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h3><p>KNN 和NN 不同，NN是只计算最近的邻居是谁，而 KNN 是计算K个距离最近的邻居，并从中找到占据多数的类别作为预测结果</p>
<p><img src="/2021/12/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/8.png" style="zoom:67%;"></p>
<p>我们发现，使用KNN的方法中间会有空白部分。这可能是因为，这一块区域里。到相邻几类的距离事项等的，因此分类器没办法判别到底是哪一类。</p>
<h3 id="距离选择"><a href="#距离选择" class="headerlink" title="距离选择"></a>距离选择</h3><p>除了L1距离，我们还可以用L2距离(欧氏距离)：</p>
<p><img src="/2021/12/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/9.png"></p>
<p><img src="/2021/12/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/10.png"></p>
<h4 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h4><p>那么，对于K-NN中的超参数K，我们又该怎么去选择呢？</p>
<ul>
<li>猜想1：我们可以是模型在整个数据集上表现最好</li>
</ul>
<p>这是不行的，这会导致超参数在现有数据集上完美分类，造成过拟合。举一个极端的例子，我们如果去k=1，那么对于所有点来说，都属于同一类，那么k=1在这个情况下是完美的超参，但这显然是很荒唐的</p>
<ul>
<li>猜想2：使模型在测试集上表现最好</li>
</ul>
<p>这也是不行的，因为这就相当于看到了答案反推模型，我调参的目的就是服务于测试集，这会污染我们的模型，是一种<strong>作弊行为</strong>。可想而知，用这种方法做出来的模型在测试集上会有很好的表现，但是，在其他数据集上的表现会很拉跨。</p>
<ul>
<li>猜想3：如下图所示</li>
</ul>
<p><img src="/2021/12/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/12.png"></p>
<p>我们将数据划分为训练集、验证集和测试集，三者隔离开来。我们在验证集上找到最好的表现，然后将模型应用于测试集，查看最终表现。这样test中的数据就不会对算法本身造成影响，在接触到测试集前，模型对其一无所知，这样才能”毫无干扰“得做出预测。</p>
<ul>
<li>猜想4：交叉验证(cross validation), 这是在第三种方法基础上的衍生，我们可以将数据集拆分成更多块，并获得更好的泛化性能。如下图所示，我们将每一块成为一个Fold。对于每一个Fold，我们都会有一次机会把它作为验证器，其他Folds作为训练集，对于每一次训练，都会获得一组最好的超参数组合。最后我们取其所长，获得最后的超参搭配</li>
</ul>
<p><img src="/2021/12/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/11.png"></p>
<p>但是对于大型数据和深度学习较少使用这种方法。因为训练起来会比较麻烦。</p>
<h4 id="超参选择可视化"><a href="#超参选择可视化" class="headerlink" title="超参选择可视化"></a>超参选择可视化</h4><p><img src="/2021/12/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/13.png"></p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>(K)NN 方法的优点很简洁明了：简单</p>
<p>同时，KNN也存在很多缺点：</p>
<ul>
<li><p>训练阶段时简单的标签记忆</p>
<ul>
<li>预测效率低下(20%左右)，训练时需要$O(N)$的复杂度</li>
<li>lazy learner,在训练阶段并不会做任何泛化；而图片分类需要具备一定的泛化能力的分类器</li>
</ul>
</li>
<li><p>像素距离和图像信息之间存在语义鸿沟</p>
<ul>
<li>图像像素相近(颜色相近)并不等于 图像包含的信息接近</li>
</ul>
</li>
<li><p>对训练集数据分布要求会很高、</p>
<ul>
<li>首先我们要知道，只要点够多，KNN在训练的时候可以近似于任何曲线(可以推广到平面)。但是点不够平均的话，KNN模拟出来的函数会误差很大。</li>
</ul>
<p><img src="/2021/12/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/14.png"></p>
<p><img src="/2021/12/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/15.png"></p>
<ul>
<li>为了提高模型预测的准确率，我们的训练集需要在整个像素空间中均匀分布，导致维度诅咒(curse of dimensionality)。也就是说只要增加一个维度，就需要对数据集扩大指数倍才能满足训练的需要。</li>
</ul>
</li>
</ul>
<h3 id="Linear-classifier"><a href="#Linear-classifier" class="headerlink" title="Linear classifier"></a>Linear classifier</h3><p><img src="/2021/12/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/16.png"></p>
<p>当然，线性分类器也有很难应用的数据分布：</p>
<p><img src="/2021/12/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/17.png"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonxqh.github.io/2021/12/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E4%BC%98%E5%8C%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jason">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason‘s Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E4%BC%98%E5%8C%96/" class="post-title-link" itemprop="url">计算机视觉-损失函数和优化</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-12-20 21:05:51" itemprop="dateCreated datePublished" datetime="2021-12-20T21:05:51+08:00">2021-12-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-12-27 12:24:44" itemprop="dateModified" datetime="2021-12-27T12:24:44+08:00">2021-12-27</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="损失函数和优化"><a href="#损失函数和优化" class="headerlink" title="损失函数和优化"></a>损失函数和优化</h1><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>对于线性分类器，常常需要定义一个衡量输出分数好坏的函数——<strong>损失函数</strong>。然后，我们可以通过这个损失函数，去反推参数，最终让损失函数达到最小值，这就是<strong>优化</strong>的过程。</p>
<p>我们来举一个例子：</p>
<p>我们令训练集为 ${(x<em>i,y_i )}</em>{i=1}^N$ , $x_i$ 为第i个图片，$y_i\in Z$ 为它的类别标签；$f(\boldsymbol W,\boldsymbol x_i)$ 为第i个图片的输出分数, $\boldsymbol W$ 是权重矩阵，计算公式如下所示：</p>
<p><img src="/2021/12/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E4%BC%98%E5%8C%96/5.png"></p>
<p>由此，我们可以计算损失函数</p>
<ul>
<li>第 i 条数据的损失函数 $L_i = l(f(\boldsymbol W,\boldsymbol x_i),y_i)$</li>
<li>训练样本总体损失 $L = \frac{1}{N}\sum_{i=1}^N L_i$ </li>
</ul>
<p>接下来我们来介绍两种线性分类器常用的损失函数 </p>
<h3 id="SVM-Loss"><a href="#SVM-Loss" class="headerlink" title="SVM Loss"></a>SVM Loss</h3><p>SVM loss 也叫做 multiclass SVM loss, 首先我们给出定义：</p>
<ol>
<li><p>训练集为 ${(x<em>i,y_i )}</em>{i=1}^N$ , $x_i$ 为第i个图片，$y_i\in Z$ 为它的类别标签</p>
</li>
<li><p>我们令 $\boldsymbol s = f(\boldsymbol W,\boldsymbol x_i)$, 即图片i的所有类别分数</p>
</li>
<li><p>然后，我们令 SVM loss为： $L<em>i = \sum</em>{j\neq y<em>i}\max(0,s_j-s</em>{y_i}+1)$ </p>
<ul>
<li>$s_j$ 代表第 $i$ 张图片在第 $j$ 个分类上的分数</li>
<li>$s_{y_i}$ 代表这张图片原来类别的分数 </li>
</ul>
</li>
</ol>
<p>为什么要加上1这个常数呢？</p>
<p>答：如果不加上1，想得到0损失的话，那么只要正确类别的分数比其他所有错误类别的分数大就可以了，具体大多少不用管。因此可能出现两个类的分数很接近，正确的类别并不突出。如果加上1以后，至少要让正确类别的分数比其他类别的分数大1。也就是<strong>进一步拉大正确类别与错误类别的差距</strong>，让模型更加高效。</p>
<p>如果全部正确得对图片进行分类，此时对所有的 $s<em>j$ 肯定是远小于 $s</em>{y<em>i}$ 的。因此此时$L_i = \sum</em>{j\neq y_i}0= 0$ </p>
<p>画出图像后我们发现，SVM loss的函数形状就像一个铰链一样，因此也被称为 Hinge Loss</p>
<p><img src="/2021/12/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E4%BC%98%E5%8C%96/2.png" style="zoom:67%;"></p>
<h4 id="实例计算"><a href="#实例计算" class="headerlink" title="实例计算"></a>实例计算</h4><p>现在，我们可以通过一个具体的例子来计算svm loss</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>狗图片1</th>
<th>猫图片1</th>
<th>狼图片1</th>
<th>狗图片1</th>
</tr>
</thead>
<tbody>
<tr>
<td>狗</td>
<td><strong>7.9</strong></td>
<td>3.3</td>
<td>-1.1</td>
<td><strong>2.3</strong></td>
</tr>
<tr>
<td>猫</td>
<td>5.8</td>
<td><strong>-0.8</strong></td>
<td>3.4</td>
<td>1.5</td>
</tr>
<tr>
<td>狼</td>
<td>-1.9</td>
<td>6.5</td>
<td><strong>2.5</strong></td>
<td>4.4</td>
</tr>
</tbody>
</table>
</div>
<p>图片1-4的loss:</p>
<script type="math/tex; mode=display">
\begin{align}
&L_1 = \max(0,5.8-7.9+1)+\max(0,-1.9-7.9+1)=0\\
&L_2 = \max(0,3.3-(-0.8)+1)+\max(0,6.5-(-0.8)+1)=13.4\\
&L_3 = \max(0,-1.1-2.5+1)+\max(0,3.4-2.5+1)=1.9\\
&L_4 = \max(0,1.5-2.3+1)+\max(0,4.4-2.3+1)=3.3\\
\end{align}</script><p>总的loss:</p>
<script type="math/tex; mode=display">
L = \frac{1}{N}\sum_{i=1}^N L_i = \frac{1}{4}(0+13.4+1.9+3.3) = 4.65</script><h5 id="QA"><a href="#QA" class="headerlink" title="QA"></a>QA</h5><p>Q : 如果输出分数发生微小改变(比如$\pm0.001$)，对SVM loss的计算会造成什么影响</p>
<p>A : $s<em>j-s</em>{y_i}&lt;-1$ 时，没有影响。反之，会略为改变损失函数的值</p>
<hr>
<p>Q: SVM 损失函数的最大值和最小值分别为多少</p>
<p>A: 最小值为0(全分对)，最大值为正无穷(理论上)，可用于代码 正确性检查</p>
<hr>
<p>Q: 加入初始化 $\boldsymbol W$ 接近0，导致所有输出分数都$\approx 0$,那么$L_i$ 约等于多少？</p>
<p>A: 由公式可得：$L<em>i = \sum</em>{j\neq y_i}\max(0-0+1)$ 。 因此此时 $L_i$ 的分数为 $C-1$， $C$ 为类别数，这里为 $3-1=2$ 。此问题也可以用于代码检测</p>
<hr>
<p>Q: 加入去掉$j\neq y_i$ 的限制，损失函数该如何变化？</p>
<p>A: 损失函数会增加1</p>
<hr>
<p>Q: 加入在$L<em>i$中使用$\max(s_j-s</em>{y<em>i}+2)$ 代替 $\max(s_j-s</em>{y_i}+1)$ ，有什么影响？</p>
<p>A: 不会改变分类的结果，SVM loss 只关注输出分数之间的差异，这里的常数只起到scale参数的作用，会导致权重矩阵的变化。</p>
<hr>
<p>Q: 加入 $\boldsymbol W$ 使得 $L=0$ (完美)，轻微 $\boldsymbol W$ 是否唯一？</p>
<p>A: 不唯一，因为$\boldsymbol W\times c $ 也可以使得$L=0$, $c$ 为任意正整数。因为权重矩阵的整体放大是不会影响到结果的。我们可以通过添加正则项来选取更好的 $\boldsymbol W $ ,增强模型的泛化能力</p>
<h3 id="Softmax-Loss"><a href="#Softmax-Loss" class="headerlink" title="Softmax Loss"></a>Softmax Loss</h3><p>第二个常用的损失函数是 softmax loss(cross-entropy loss). 用这个损失函数构造的分类器也被称为是Softmax分类器(多类别逻辑回归)</p>
<p>我们同样给出定义：</p>
<ol>
<li><p>训练集为 ${(x<em>i,y_i )}</em>{i=1}^N$ , $x_i$ 为第i个图片，$y_i\in Z$ 为它的类别标签</p>
</li>
<li><p>我们令 $\boldsymbol s = f(\boldsymbol W,\boldsymbol x_i)$, 即图片i的所有类别分数</p>
</li>
<li><p>softmax loss ：</p>
<script type="math/tex; mode=display">
L_i = -\log (\frac{e^{s_{y_i}}}{\sum_j e^{s_j}})</script><ul>
<li>同样的，$s_{y_i}$ 代表这张图片原来类别的分数 </li>
<li>$s_j$ 代表第 $i$ 张图片在第 $j$ 个分类上的分数</li>
</ul>
</li>
</ol>
<p>如果是正确分类的话，$s<em>{y_i}$是特别大的，因此 $(\frac{e^{s</em>{y_i}}}{\sum_j e^{s_j}})$ 接近于1，$L_i$ 就接近于0</p>
<h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><h5 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h5><p>首先我们要了解交叉熵的概念</p>
<p>我们知道熵(entropy) 是用来衡量概率<strong>分布Q</strong>的不确定性的:</p>
<script type="math/tex; mode=display">
H(Q) = -\sum_i q_i \log q_i</script><p>那么交叉熵(Cross-entropy) 则是用来衡量概率<strong>分布P</strong>服从概率<strong>分布Q</strong>的不确定性,就是把对数后面的q改为p：</p>
<script type="math/tex; mode=display">
H(Q,P) = -\sum_{i}q_i\log p_i</script><p>比如说这里有个例子：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>晴天</th>
<th>多云</th>
<th>下雨</th>
</tr>
</thead>
<tbody>
<tr>
<td>真实天气</td>
<td>50%</td>
<td>30%</td>
<td>20%</td>
</tr>
<tr>
<td>天气预报</td>
<td>40%</td>
<td>30%</td>
<td>30%</td>
</tr>
</tbody>
</table>
</div>
<p>可以计算真实天气的不确定性：</p>
<script type="math/tex; mode=display">
H(\text{真实}) = -(0.5\log(0.5)+0.3\log(0.3)+0.2\log(0.2))=0.45</script><p>当只有一种天气的时候，可知： $H(真实) = -(1\log 1) = 0$ </p>
<p>利用交叉熵，可以计算天气预报服从真是天气的不确定性：</p>
<script type="math/tex; mode=display">
H(真实,预报) = -(0.5\log(0.4)+0.3\log(0.3)+0.2\log(0.3)) = 0.46</script><p>正常情况下，我们是利用 KL 散度(相对熵)来衡量两者分布的差异性：</p>
<script type="math/tex; mode=display">
D_{KL} (Q||P) = H(Q,P)-H(Q) = -\sum q_i\log p_i-(-\sum_i q_i\log q_i) =\sum_i q_i\log\frac{q_i}{p_i}</script><p>但在计算softmax loss时，我们只考虑交叉熵的计算即可。因为在图像分类场景下，真实的分类是确定的, $H(\text{真实})=0$ , 所以说交叉熵和KL散度在图像分类情况下是等价的</p>
<h5 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h5><p>首先，我们可以将分数通过 softmax 函数转换为概率，即：</p>
<script type="math/tex; mode=display">
\begin{align}
&\text{分数}: \boldsymbol s = f(\boldsymbol W,\boldsymbol x_i)\\
&\text{概率}: P(y=k|x_i) = \frac{e^{s_k}}{\sum_j e^{s_j}}
\end{align}</script><p><img src="/2021/12/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E4%BC%98%E5%8C%96/3.png"></p>
<p>接着，我们利用交叉熵，计算第i个图片的损失：</p>
<script type="math/tex; mode=display">
L_i = -\log P(y=y_i|x_i) = -\log (\frac{e^{s_{y_1}}}{\sum_j e^{s_j}})</script><ul>
<li>其中，$y_i$ 代表正确的标签</li>
</ul>
<h4 id="实例计算-1"><a href="#实例计算-1" class="headerlink" title="实例计算"></a>实例计算</h4><p>我们还是那 SVM loss的那个例子来计算，得到了分数以后我们先计算预测的每个类别的概率，得到蓝色框框。</p>
<p>然后通过计算cross entropy 来得到 $L_1$ </p>
<p><img src="/2021/12/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E4%BC%98%E5%8C%96/7.png"></p>
<h5 id="QA-1"><a href="#QA-1" class="headerlink" title="QA"></a>QA</h5><p>Q: 如果有输出分数发生微小改变(比如 $\pm 0.1$)，损失函数是否发生改变？</p>
<p>A: 使得，正确类别和错误类别输出分数差距越大，损失函数就越小</p>
<hr>
<p>Q: 损失函数$L_i$ 的最大值和最小值分别是多少？</p>
<p>A: 最小值为0(理论上)，最大值为正无穷(理论上)</p>
<hr>
<p>Q: 加入初始化 $\boldsymbol W$ 接近0，导致所有输出分数都$\approx 0$ ，那么$L$ 约等于多少？</p>
<p>A: $L<em>i = -\log (\frac{e^{s</em>{y_1}}}{\sum_j e^{s_j}})$ , 当输出分数都约等于0的时候，$L_i = -\log(1/C)=\log C$, 其中$C$为类别数，这里为 $\log 3$</p>
<hr>
<h4 id="两类分类器的对比"><a href="#两类分类器的对比" class="headerlink" title="两类分类器的对比"></a>两类分类器的对比</h4><p><img src="/2021/12/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E4%BC%98%E5%8C%96/4.png"></p>
<p>为什么左边的是SVM分类器，右边的是Softmax分类器？</p>
<p>因为SVM有一个boundary，而且并不关注所有类别到这个分类器的距离。他只要求其他类别比正确类别的分数小于1(boundary)即可。因此我们看到，左边的分类器目标是最大化 margin即可，并不需要计算每张图片到中心的距离。</p>
<p>而Softmax分类器，每个图片都参与了关于loss的计算。我们看到右边的分类器，需要一一计算每张图片到超平面的距离。</p>
<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>正则项的意义：1. 缩小参数空间 2. 调整参数偏好的分布 3. 提高模型泛化能力。我们希望的分类器是下面这张图中的绿色线条。因为蓝色曲线已经过拟合了 ，在其他数据集上的表现会比较糟糕。</p>
<p><img src="/2021/12/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E4%BC%98%E5%8C%96/6.png"></p>
<p>怎么正则化呢？我们可以修改 loss 函数</p>
<script type="math/tex; mode=display">
L(\boldsymbol W) = \frac{1}{N}\sum_{i=1}^N l(f(\boldsymbol W,x_i),y_i) +\lambda R(\boldsymbol W)</script><p>前一部分是数据损失，后一部分是正则项，用来防止模型过拟合训练集。其中$\lambda$是超参数，代表正则化的强度；正则项 $R(\boldsymbol W)$ 有 L1正则，L2正则以及两种混合起来的正则。</p>
<h4 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h4><p>L1正则就是将每个权重的绝对值累加。</p>
<script type="math/tex; mode=display">
L1: \sum_k\sum_l|w_{k,l}|</script><h4 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h4><p>L2正则是将每个权重求平方之后累加：</p>
<script type="math/tex; mode=display">
L2: \sum_k\sum_l|w_{k,l}^2|</script><p>此外还有Dropout, Batch normalization, Stochastic depth 和 Fractional pooling 等方法</p>
<h4 id="L1-vs-L2"><a href="#L1-vs-L2" class="headerlink" title="L1 vs L2"></a>L1 vs L2</h4><p>我们给个例子：</p>
<script type="math/tex; mode=display">
\begin{align}
&x= [1,1,1]\\
&\boldsymbol W_1 = [1,0,0]\\
&\boldsymbol W_2 = [0.3,-0.1,0.8]\\
&\boldsymbol W_1^T x = \boldsymbol W_2^T x = 1\\
\end{align}</script><p>使用$L1$ 正则化，对于$\boldsymbol {W_1,W_2}$， 可得：</p>
<script type="math/tex; mode=display">
\boldsymbol W_1 : \sum_{k}\sum_l|\boldsymbol W_1| = 1\\
\boldsymbol W_2 : \sum_{k}\sum_l|\boldsymbol W_2| = 1.2</script><p>使用$L_2$ 正则化，可得：</p>
<script type="math/tex; mode=display">
\boldsymbol W_1 : \sum_{k}\sum_l|\boldsymbol W^2_1| = 1\\
\boldsymbol W_2 : \sum_{k}\sum_l|\boldsymbol W^2_2| = 0.74</script><p>我们看到，$L_1$ 偏向于是参数集中在少数输入像素上</p>
<p>$L_2$ 偏向于是参数分布在所有像素上</p>
<h3 id="损失函数计算流程"><a href="#损失函数计算流程" class="headerlink" title="损失函数计算流程"></a>损失函数计算流程</h3><ol>
<li>初始化$\boldsymbol W$</li>
<li>计算 $s= f(\boldsymbol W,x_i)$</li>
<li>选择损失函数，计算 $L<em>总 = \frac{1}{N}\sum</em>{i=1}^N L_i+\lambda R(\boldsymbol W)$ 得到 Total Loss</li>
<li>通过优化算法，修改原来的参数矩阵 $\boldsymbol W$ </li>
<li>重复2-4, 直到收敛</li>
</ol>
<h2 id="图像特征抽取"><a href="#图像特征抽取" class="headerlink" title="图像特征抽取"></a>图像特征抽取</h2><p>线性分类器是可以应用于图像的，如果直接用像素点来计算，效果其实是很差的。因此往往需要对原始像素做特征抽取，利用抽取的特征来训练模型，以提高预测性能。如下所示：</p>
<p><img src="/2021/12/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E4%BC%98%E5%8C%96/8.png"></p>
<p>在深度神经网络(DNN)中的前几层，其实就是一个提取图像特征的过程。那么在线性分类器中，我们有什么方法呢？</p>
<h3 id="Hue-Histogram"><a href="#Hue-Histogram" class="headerlink" title="Hue Histogram"></a>Hue Histogram</h3><ol>
<li>建立色相哈希表</li>
<li>哈希每个像素值，并计算每个key中像素的个数</li>
<li>将哈希结果作为模型输入</li>
</ol>
<h3 id="HoG"><a href="#HoG" class="headerlink" title="HoG"></a>HoG</h3><p>HoG 的全称是：Histogram of Oriented Gradients(方向梯度直方图)</p>
<p><img src="/2021/12/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E4%BC%98%E5%8C%96/9.png"></p>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><p>我们的目标是最小化损失函数 $L(\boldsymbol W)$ ，因为这时候预测的最接近于真实值。</p>
<p>假设只有一个参数$w$：</p>
<ul>
<li>导数 $\frac{dL(w)}{dw} = \lim\limits_{h\rightarrow 0}\frac{L(w+h)-L(w)}{h}$ 代表$L$ 在$w$ 的切线斜率，即 $L(w)$ 在该店的变化速率及方向</li>
<li>因此沿导数反方向微调w即可减小$L(w)$</li>
</ul>
<p>如下图所示：</p>
<p><img src="/2021/12/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E4%BC%98%E5%8C%96/10.png"></p>
<p>在多维情况下，即$\boldsymbol W$ 为向量</p>
<ul>
<li><p>偏导数$[\frac{\partial L(\boldsymbol W)}{\partial w<em>1},\frac{L(\boldsymbol W)}{\partial w_2},\cdots,\frac{L(\boldsymbol W)}{\partial w_n}]$ 代表L 在 $\boldsymbol W$ 处沿每个维度的变化速率和方向，称为梯度(gradient).记为 $\nabla</em>{\boldsymbol W}L$ 或者 $grad(L(\boldsymbol W))$</p>
</li>
<li><p>$\nabla_{\boldsymbol W}L$ 和方向向量$\boldsymbol v$ 的点积记为该方向的斜率(方向导数) ,公式如下：</p>
<script type="math/tex; mode=display">
\nabla_{\boldsymbol W}L\cdot v = |\nabla_{\boldsymbol W}L ||v|\cos\theta</script><p>那么，当$\cos\theta =1$ 的时候，达到最大值，即$\boldsymbol v$ 和 $\nabla<em>{\boldsymbol W}L$ 同方向，所以负梯度 $-\nabla</em>{\boldsymbol W}L$ 代表 $L$ 下降最快的方向(最陡峭)。</p>
</li>
<li><p>沿 $-\nabla_{\boldsymbol W}L$ 方向微调$\boldsymbol W$ 即可快速减小 $L(\boldsymbol W)$ </p>
<script type="math/tex; mode=display">
\boldsymbol W_{new} = \boldsymbol W -\lambda\nabla_{\boldsymbol W}L</script><p>超参数$\lambda $ 为 step size 或 learning rate， 代表梯度下降的快慢，如果太小会导致梯度下降过慢，算法效率低下；太大会导致找不到梯度最优点。在<a href="https://jasonxqh.github.io/2021/12/02/计算机视觉-神经网络的训练/">计算机视觉-神经网络的训练</a>中，我们还会仔细探讨这个问题。</p>
</li>
</ul>
<h3 id="数值梯度"><a href="#数值梯度" class="headerlink" title="数值梯度"></a>数值梯度</h3><p>数值梯度就是根据导数的定义去求得梯度。比如说关于权重向量$\boldsymbol W$，手动计算梯度：</p>
<p><img src="/2021/12/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E4%BC%98%E5%8C%96/11.png"></p>
<p>但这样计算开销太大，需要遍历所有参数，并计算损失函数和梯度。显然，当参数过多时采用这种方法是不可行的</p>
<h3 id="解析梯度"><a href="#解析梯度" class="headerlink" title="解析梯度"></a>解析梯度</h3><p>我们可以对损失函数求偏导，编写梯度公式，以此来直接计算梯度.即：</p>
<script type="math/tex; mode=display">
\nabla_{\boldsymbol W}L = [\frac{\partial L(\boldsymbol W)}{\partial w_1},\frac{L(\boldsymbol W)}{\partial w_2},\cdots,\frac{L(\boldsymbol W)}{\partial w_n}]</script><h4 id="Hinge-Loss"><a href="#Hinge-Loss" class="headerlink" title="Hinge Loss"></a>Hinge Loss</h4><p><a href="http://ningyuwhut.github.io/cn/2018/01/gradient-of-svm-loss/" target="_blank" rel="noopener">http://ningyuwhut.github.io/cn/2018/01/gradient-of-svm-loss/</a></p>
<p>实现非向量化的svm还是比较简单的，其实只要两层训练即可，</p>
<p>第一层循环遍历每个样本,第二层循环遍历每个类;</p>
<p>在第二层循环中需要完成两件事: 1.计算当前类的loss， 2.同时计算对<strong>当前类</strong>和<strong>样本正确类</strong>的梯度。</p>
<p>当$i\neq y_j$ 的时候，我们需要对当前类的梯度进行修改</p>
<script type="math/tex; mode=display">
\frac{d L_i}{d w_j} = \mathbb 1(w_j^Tx_i-w^T_{y_i}x_i+\Delta>0)\begin{bmatrix}x_{i1}\\x_{i2}\\\vdots\\ x_{iD} \end{bmatrix} \\
=\mathbb 1(w_j^Tx_i-w^T_{y_i}x_i+\Delta>0) x_i^T</script><p>类似的，对于正确类的样本，我们这么更新：</p>
<script type="math/tex; mode=display">
\frac{d L_i}{d w_{y_j}} = -\sum_{j\neq y_i}\mathbb 1(w_j^Tx_i-w^T_{y_i}x_i+\Delta>0)\begin{bmatrix}x_{i1}\\x_{i2}\\\vdots\\ x_{iD} \end{bmatrix} \\
=-\sum_{j\neq y_i}\mathbb 1(w_j^Tx_i-w^T_{y_i}x_i+\Delta>0) x_i^T</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svm_loss_naive</span><span class="params">(W, X, y, reg)</span>:</span></span><br><span class="line">		dW = np.zeros(W.shape)  <span class="comment"># initialize the gradient as zero</span></span><br><span class="line">                            <span class="comment"># compute the loss and the gradient</span></span><br><span class="line">    num_classes = W.shape[<span class="number">1</span>]</span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    final_result = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_train):<span class="comment"># 循环每个样本</span></span><br><span class="line">        scores = X[i].dot(W)</span><br><span class="line">        correct_class_score = scores[y[i]]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(num_classes): <span class="comment"># 循环每个类别</span></span><br><span class="line">            <span class="keyword">if</span> j == y[i]: <span class="comment"># 如果该样本属于当前类，continue</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            margin = scores[j] - correct_class_score + <span class="number">1</span>  <span class="comment"># 计算svm loss</span></span><br><span class="line">            <span class="keyword">if</span> margin &gt; <span class="number">0</span>:</span><br><span class="line">                loss += margin <span class="comment"># loss是累加</span></span><br><span class="line">                dW[:, j] += X[i].T  <span class="comment"># 更新当前类的梯度</span></span><br><span class="line">                dW[:, y[i]] += -X[i].T  <span class="comment"># 更新正确类的梯度</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Right now the loss is a sum over all training examples, but we want it</span></span><br><span class="line">    <span class="comment"># to be an average instead so we divide by num_train.</span></span><br><span class="line">    loss /= num_train <span class="comment">#需要除以样本数量</span></span><br><span class="line">    dW /= num_train</span><br><span class="line">    <span class="comment"># Add regularization to the loss.</span></span><br><span class="line">    loss += reg * np.sum(W * W) <span class="comment"># 需要加上正则项</span></span><br><span class="line">    dW += reg * W</span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure>
<h4 id="Cross-entropy-Loss"><a href="#Cross-entropy-Loss" class="headerlink" title="Cross entropy Loss"></a>Cross entropy Loss</h4><p><a href="https://www.cnblogs.com/makefile/p/softmax.html" target="_blank" rel="noopener">https://www.cnblogs.com/makefile/p/softmax.html</a></p>
<p>和SVM loss一样，我们同时要有两个循环，第一层循环遍历每个样本,第二层循环遍历每个类;我们要分两种情况进行讨论，当当前样本属于当前类的时候，当当前样本不属于当前类的时候。</p>
<p>首先，我们知道每个样本的softmax loss是这么计算的：</p>
<script type="math/tex; mode=display">
L_i = -\log(\frac{e^{f_{y_i}}}{\sum_j e^{f_j}})</script><p>其中，f 即计算来的分数：</p>
<script type="math/tex; mode=display">
f_{y_i} = w_{y_i}^T x\\
f_j = w_j^Tx</script><p>所以，如果我们要计算损失函数关于参数矩阵$w$ 的导数，需要用到高链式法则：</p>
<script type="math/tex; mode=display">
\frac{dL}{dw} = \frac{dL}{df}\cdot\frac{df}{dw}</script><p>其中，$\frac{df}{dw} = x$, 我们主要来看 $\frac{dL}{df}$</p>
<h5 id="当-j-neq-y-i-时"><a href="#当-j-neq-y-i-时" class="headerlink" title="当 $j\neq y_i$ 时"></a>当 $j\neq y_i$ 时</h5><p>我们就不需要计算正确分类时的损失，只需要针对<strong>分母中的错误项</strong>求偏导,因为分母中也包含正确项系数，因此在求导的时候需将其提出，以此计算错误分类时的损失</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{dL_i}{df} &= \Bigg(-\log(\frac{e^{f_{y_i}}}{\sum_je^{f_j}}) \Bigg)' = -\frac{\sum_je^{f_j}}{e^{f_{y_j}}} \cdot e^{f_{y_j}}(\frac{1}{\sum_j e^{f_j}})'\\
&=-\frac{\sum_je^{f_j}}{e^{f_{y_j}}} \cdot e^{f_{y_j}}\cdot(-e^{f_{y_j}})\cdot\frac{1}{(\sum_j e^{f_j})^2}\\
&=\frac{e^{f_{y_i}}}{\sum_je^{f_j}}\\
\end{align}</script><h5 id="当-j-y-i-时"><a href="#当-j-y-i-时" class="headerlink" title="当 $j == y_i$时"></a>当 $j == y_i$时</h5><p>此时，我们需要对分子求偏导，来计算正确类的损失</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{dL_i}{df} &= \Bigg(-\log(\frac{e^{f_{y_i}}}{\sum_je^{f_j}}) \Bigg)' = -\frac{\sum_je^{f_j}}{e^{f_{y_j}}} \cdot (e^{f_{y_j}}\frac{1}{\sum_j e^{f_j}})'\\
&=-\frac{\sum_je^{f_j}}{e^{f_{y_j}}} \cdot [{e^{f_{y_i}}(\sum_j e^{f_j})^{-1}+e^{f_{y_i}}\cdot\frac{(-1\cdot e^{f_{y_i}})}{(\sum_je^{f_j})^2}}]\\
&=\frac{e^{f_{y_j}}}{\sum_j e^{f_j}}-1 \\

\end{align}</script><p>现在，我们已经计算了两种情况下的梯度，我们可以用示性函数将其统一起来：</p>
<script type="math/tex; mode=display">
\frac{dL_i}{df} = \frac{e^{f_{y_j}}}{\sum_j e^{f_j}}- \mathbb 1(j==y_i)</script><p>因此，</p>
<script type="math/tex; mode=display">
\frac{dL}{dW_i} = (\frac{e^{f_{y_j}}}{\sum_j e^{f_j}}- \mathbb 1(j==y_i))x</script><p>Softmax_loss_naive方法代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一层循环，遍历样本</span></span><br><span class="line"><span class="keyword">for</span> ii <span class="keyword">in</span> range(num_train):</span><br><span class="line">  current_scores = scores[ii, :]</span><br><span class="line"> </span><br><span class="line">  <span class="comment"># Fix for numerical stability by subtracting max from score vector.</span></span><br><span class="line">  <span class="comment"># important! make them range between infinity to zero</span></span><br><span class="line">  shift_scores = current_scores - np.max(current_scores)</span><br><span class="line"> </span><br><span class="line">  <span class="comment"># Calculate loss for this example.</span></span><br><span class="line">  loss_ii = -shift_scores[y[ii]] + np.log(np.sum(np.exp(shift_scores)))</span><br><span class="line">  loss += loss_ii</span><br><span class="line"><span class="comment"># 第二层循环，遍历类 		</span></span><br><span class="line">  <span class="keyword">for</span> jj <span class="keyword">in</span> range(num_classes):</span><br><span class="line">    softmax_score = np.exp(shift_scores[jj]) / np.sum(np.exp(shift_scores))</span><br><span class="line">		<span class="comment"># 分两类进行权重矩阵的更新</span></span><br><span class="line">    <span class="keyword">if</span> jj == y[ii]:</span><br><span class="line">      dW[:, jj] += (<span class="number">-1</span> + softmax_score) * X[ii]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      dW[:, jj] += softmax_score * X[ii]</span><br><span class="line">      </span><br><span class="line">   <span class="comment"># Average over the batch and add our regularization term.</span></span><br><span class="line">loss /= num_train</span><br><span class="line">loss += reg * np.sum(W*W)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Average over the batch and add derivative of regularization term.</span></span><br><span class="line">dW /= num_train</span><br><span class="line">dW += <span class="number">2</span>*reg*W</span><br></pre></td></tr></table></figure>
<h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>其实梯度下降也有很多种方法，除了正常的梯度下降(GD) 之外，还有批量梯度下降法(Batch Gradiant Descent, BGD) 和 随机梯度下降(Stochastic Gradiant Descent, SGD)</p>
<p>这里主要拿 GD和SGD做一个比较</p>
<h4 id="GD"><a href="#GD" class="headerlink" title="GD"></a>GD</h4><ul>
<li>优势： 每次迭代 loss 下降很快</li>
<li>劣势: 一次迭代需要遍历所有数据，并容易陷入局部最小值，导致梯度无法更新</li>
</ul>
<h4 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h4><p>随机梯度下降是说，每次选取一个sample集(minibatch，一般大小为32/64/128/256). 然后，利用sample集上的损失来计算近似梯度，进行梯度下降。</p>
<ul>
<li>优势: 迭代更新速度快，并且往往因为 mini-batch含有噪声而避开 local minima</li>
<li>劣势: 每次迭代的 loss 下降很漫</li>
</ul>
<p>由于数据量较大，训练深度神经网络(DNN) 的时候基本使用SGD，以及其他性能更佳的优化方法。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/6/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><span class="space">&hellip;</span><a class="page-number" href="/page/44/">44</a><a class="extend next" rel="next" href="/page/8/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Jason</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">439</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">68</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jason</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

    

  

</body>
</html>
